Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:42 (message):
  OpenMP not found


-- Using llamafile
-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD - Success
-- ARM feature DOTPROD enabled
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8 - Success
-- ARM feature MATMUL_INT8 enabled
-- Using runtime weight conversion of Q4_0 to Q4_0_x_x to enable optimized GEMM/GEMV kernels
-- Including CPU backend
CMake Warning at ggml/src/ggml-amx/CMakeLists.txt:104 (message):
  AMX requires x86 and gcc version > 11.0.  Turning off GGML_AMX.


-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (1.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m1.619s
user	0m0.707s
sys	0m0.966s
++ nproc
+ make -j10
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  6%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  6%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-aarch64.c.o
[  6%] Built target sha256
[  6%] Built target build_info
[  6%] Built target xxhash
[  6%] Built target sha1
[  7%] Linking CXX shared library libggml-base.dylib
[  7%] Built target ggml-base
[  8%] Generate assembly for embedded Metal library
Embedding Metal library
[  8%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu-quants.c.o
[  8%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  8%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu.c.o
[  9%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu-aarch64.c.o
[ 10%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu.cpp.o
[ 11%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Linking CXX shared library libggml-blas.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-cpu
[ 13%] Built target ggml-blas
[ 13%] Linking C shared library libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 15%] Linking CXX shared library libggml.dylib
[ 15%] Built target ggml
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 19%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 19%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Linking CXX shared library libllama.dylib
[ 21%] Built target llama-gguf
[ 21%] Built target llama
[ 21%] Built target llama-gguf-hash
[ 21%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 21%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 21%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 22%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 22%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 22%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 24%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 24%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 24%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 25%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 26%] Linking C executable ../bin/test-c
[ 28%] Linking CXX executable ../../bin/llama-simple
[ 28%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 30%] Linking CXX executable ../../bin/llama-simple-chat
[ 30%] Linking CXX executable ../../bin/llama-quantize-stats
[ 31%] Linking CXX executable ../../bin/llama-run
[ 31%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 31%] Built target llava
[ 31%] Linking CXX static library libcommon.a
[ 31%] Built target llama-simple-chat
[ 31%] Built target test-c
[ 31%] Built target llama-simple
[ 31%] Built target llama-run
[ 31%] Linking CXX static library libllava_static.a
[ 32%] Linking CXX shared library libllava_shared.dylib
[ 32%] Built target llama-quantize-stats
[ 32%] Built target common
[ 32%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 32%] Built target llava_static
[ 33%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 36%] Built target llava_shared
[ 37%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 38%] Linking CXX executable ../bin/test-tokenizer-0
[ 38%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 40%] Linking CXX executable ../bin/test-arg-parser
[ 41%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 42%] Linking CXX executable ../bin/test-sampling
[ 43%] Linking CXX executable ../bin/test-log
[ 44%] Linking CXX executable ../bin/test-grammar-parser
[ 45%] Linking CXX executable ../bin/test-chat-template
[ 46%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 46%] Built target test-tokenizer-0
[ 47%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 47%] Built target test-tokenizer-1-spm
[ 47%] Built target test-tokenizer-1-bpe
[ 47%] Built target test-arg-parser
[ 47%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 47%] Built target test-sampling
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Built target test-log
[ 47%] Built target test-chat-template
[ 47%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 47%] Built target test-grammar-parser
[ 48%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 49%] Linking CXX executable ../bin/test-llama-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 52%] Linking CXX executable ../bin/test-backend-ops
[ 54%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 54%] Linking CXX executable ../bin/test-barrier
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 54%] Built target test-grammar-integration
[ 55%] Linking CXX executable ../bin/test-model-load-cancel
[ 55%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-autorelease
[ 58%] Linking CXX executable ../bin/test-quantize-fns
[ 58%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 58%] Built target test-llama-grammar
[ 59%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 59%] Linking CXX executable ../bin/test-quantize-perf
[ 60%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 60%] Built target test-backend-ops
[ 60%] Built target test-barrier
[ 61%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 61%] Built target test-model-load-cancel
[ 62%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 62%] Linking CXX executable ../../bin/llama-batched-bench
[ 62%] Linking CXX executable ../bin/test-rope
[ 62%] Built target test-quantize-fns
[ 62%] Built target test-autorelease
[ 62%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 62%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 62%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 62%] Linking CXX executable ../../bin/llama-batched
[ 62%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 62%] Built target test-quantize-perf
[ 63%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 64%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 65%] Linking CXX executable ../../bin/llama-embedding
[ 66%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 68%] Built target test-rope
[ 68%] Built target llama-batched-bench
[ 68%] Built target test-json-schema-to-grammar
[ 68%] Built target llama-batched
[ 68%] Linking CXX executable ../../bin/llama-gguf-split
[ 68%] Linking CXX executable ../../bin/llama-gritlm
[ 68%] Linking CXX executable ../../bin/llama-imatrix
[ 69%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 70%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 70%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 70%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 70%] Built target llama-eval-callback
[ 70%] Built target llama-gbnf-validator
[ 70%] Built target llama-embedding
[ 70%] Linking CXX executable ../../bin/llama-infill
[ 70%] Linking CXX executable ../../bin/llama-bench
[ 71%] Linking CXX executable ../../bin/llama-lookahead
[ 71%] Built target llama-gguf-split
[ 71%] Built target llama-gritlm
[ 72%] Linking CXX executable ../../bin/llama-lookup
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 72%] Built target llama-imatrix
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 73%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 74%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 75%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 77%] Built target llama-infill
[ 78%] Linking CXX executable ../../bin/llama-lookup-stats
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 78%] Built target llama-bench
[ 78%] Built target llama-lookahead
[ 78%] Linking CXX executable ../../bin/llama-parallel
[ 78%] Built target llama-lookup
[ 78%] Linking CXX executable ../../bin/llama-passkey
[ 78%] Generating loading.html.hpp
[ 79%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 81%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 81%] Built target llama-lookup-merge
[ 81%] Built target llama-lookup-create
[ 82%] Generating completion.js.hpp
[ 82%] Built target llama-lookup-stats
[ 82%] Built target llama-cli
[ 83%] Generating deps_daisyui.min.css.hpp
[ 83%] Built target llama-parallel
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Linking CXX executable ../../bin/llama-perplexity
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Generating deps_markdown-it.js.hpp
[ 84%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Generating deps_tailwindcss.js.hpp
[ 85%] Built target llama-passkey
[ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 85%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 87%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Built target llama-quantize
[ 88%] Built target llama-retrieval
[ 88%] Built target llama-perplexity
[ 89%] Generating deps_vue.esm-browser.js.hpp
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Built target llama-speculative
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Built target llama-speculative-simple
[ 91%] Built target llama-save-load-state
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Generating index.html.hpp
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Built target llama-tokenize
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Linking CXX executable ../../bin/llama-export-lora
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 99%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 99%] Built target llama-convert-llama2c-to-ggml
[ 99%] Built target llama-cvector-generator
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-vdot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.461s
user	0m5.759s
sys	0m8.372s

main: quantize time =  6060.22 ms
main:    total time =  6060.22 ms

main: quantize time =  2087.86 ms
main:    total time =  2087.86 ms

main: quantize time =  1690.13 ms
main:    total time =  1690.13 ms

main: quantize time =  2132.36 ms
main:    total time =  2132.36 ms

main: quantize time =  3476.58 ms
main:    total time =  3476.58 ms

main: quantize time =  5486.32 ms
main:    total time =  5486.32 ms

main: quantize time =  5827.83 ms
main:    total time =  5827.83 ms

main: quantize time =  6999.11 ms
main:    total time =  6999.11 ms

main: quantize time =  6022.97 ms
main:    total time =  6022.97 ms

main: quantize time =  4589.82 ms
main:    total time =  4589.82 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.123 I build: 4223 (a3a3048e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.246 I main: llama backend init
0.00.000.263 I main: load the model and apply lora adapter, if any
0.00.030.315 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.041.592 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.041.614 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.041.618 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.041.619 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.041.620 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.041.620 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.041.621 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.041.627 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.041.627 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.041.628 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.041.629 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.041.630 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.041.630 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.041.631 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.041.635 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.041.635 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.041.636 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.050.817 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.053.329 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.060.608 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.060.610 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.060.610 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.060.611 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.060.611 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.060.612 I llama_model_loader: - type  f32:  194 tensors
0.00.060.613 I llama_model_loader: - type  f16:   98 tensors
0.00.090.917 I llm_load_vocab: special tokens cache size = 25
0.00.097.713 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.097.715 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.097.716 I llm_load_print_meta: arch             = gptneox
0.00.097.716 I llm_load_print_meta: vocab type       = BPE
0.00.097.716 I llm_load_print_meta: n_vocab          = 50304
0.00.097.716 I llm_load_print_meta: n_merges         = 50009
0.00.097.716 I llm_load_print_meta: vocab_only       = 0
0.00.097.716 I llm_load_print_meta: n_ctx_train      = 2048
0.00.097.717 I llm_load_print_meta: n_embd           = 2048
0.00.097.717 I llm_load_print_meta: n_layer          = 24
0.00.097.719 I llm_load_print_meta: n_head           = 16
0.00.097.720 I llm_load_print_meta: n_head_kv        = 16
0.00.097.720 I llm_load_print_meta: n_rot            = 32
0.00.097.720 I llm_load_print_meta: n_swa            = 0
0.00.097.721 I llm_load_print_meta: n_embd_head_k    = 128
0.00.097.721 I llm_load_print_meta: n_embd_head_v    = 128
0.00.097.722 I llm_load_print_meta: n_gqa            = 1
0.00.097.723 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.097.724 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.097.724 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.097.724 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.097.725 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.097.725 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.097.725 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.097.726 I llm_load_print_meta: n_ff             = 8192
0.00.097.726 I llm_load_print_meta: n_expert         = 0
0.00.097.726 I llm_load_print_meta: n_expert_used    = 0
0.00.097.726 I llm_load_print_meta: causal attn      = 1
0.00.097.726 I llm_load_print_meta: pooling type     = 0
0.00.097.726 I llm_load_print_meta: rope type        = 2
0.00.097.726 I llm_load_print_meta: rope scaling     = linear
0.00.097.727 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.097.727 I llm_load_print_meta: freq_scale_train = 1
0.00.097.727 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.097.728 I llm_load_print_meta: rope_finetuned   = unknown
0.00.097.728 I llm_load_print_meta: ssm_d_conv       = 0
0.00.097.728 I llm_load_print_meta: ssm_d_inner      = 0
0.00.097.728 I llm_load_print_meta: ssm_d_state      = 0
0.00.097.728 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.097.728 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.097.741 I llm_load_print_meta: model type       = 1.4B
0.00.097.741 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.097.742 I llm_load_print_meta: model params     = 1.41 B
0.00.097.744 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.097.744 I llm_load_print_meta: general.name     = 1.4B
0.00.097.744 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.097.744 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.097.744 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.097.745 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.097.745 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.097.745 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.097.746 I llm_load_print_meta: max token length = 1024
0.00.100.276 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.100.276 I llm_load_tensors: offloading output layer to GPU
0.00.100.276 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.100.293 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.100.294 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.101.228 I llama_new_context_with_model: n_seq_max     = 1
0.00.101.229 I llama_new_context_with_model: n_ctx         = 2048
0.00.101.229 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.101.229 I llama_new_context_with_model: n_batch       = 2048
0.00.101.229 I llama_new_context_with_model: n_ubatch      = 512
0.00.101.229 I llama_new_context_with_model: flash_attn    = 0
0.00.101.230 I llama_new_context_with_model: freq_base     = 10000.0
0.00.101.230 I llama_new_context_with_model: freq_scale    = 1
0.00.101.230 I ggml_metal_init: allocating
0.00.101.239 I ggml_metal_init: found device: Apple M4
0.00.101.243 I ggml_metal_init: picking default device: Apple M4
0.00.101.863 I ggml_metal_init: using embedded metal library
0.00.123.101 I ggml_metal_init: GPU name:   Apple M4
0.00.123.103 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.123.104 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.123.104 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.123.104 I ggml_metal_init: simdgroup reduction   = true
0.00.123.105 I ggml_metal_init: simdgroup matrix mul. = true
0.00.123.105 I ggml_metal_init: has bfloat            = true
0.00.123.105 I ggml_metal_init: use bfloat            = true
0.00.123.105 I ggml_metal_init: hasUnifiedMemory      = true
0.00.123.106 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.174.478 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.174.483 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.174.503 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.175.429 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.175.431 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.175.431 I llama_new_context_with_model: graph nodes  = 967
0.00.175.431 I llama_new_context_with_model: graph splits = 2
0.00.175.470 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.254.048 I main: llama threadpool init, n_threads = 4
0.00.254.083 I 
0.00.254.112 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.254.113 I 
0.00.254.182 I sampler seed: 1234
0.00.254.187 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.254.210 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.254.212 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.254.212 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling depressed, and I've had a hard time getting out of bed. I think the only thing I

0.02.099.008 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51189.62 tokens per second)
0.02.099.009 I llama_perf_context_print:        load time =     223.72 ms
0.02.099.010 I llama_perf_context_print: prompt eval time =      37.88 ms /     7 tokens (    5.41 ms per token,   184.82 tokens per second)
0.02.099.011 I llama_perf_context_print:        eval time =    1803.80 ms /    63 runs   (   28.63 ms per token,    34.93 tokens per second)
0.02.099.011 I llama_perf_context_print:       total time =    1844.96 ms /    70 tokens
0.02.099.178 I ggml_metal_free: deallocating

real	0m2.420s
user	0m0.144s
sys	0m0.100s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4223 (a3a3048e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.009.625 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.534 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.028.539 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.542 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.544 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.545 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.545 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.545 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.547 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.547 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.547 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.548 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.548 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.548 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.549 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.551 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.551 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.551 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.446 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.563 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.442 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.444 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.444 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.445 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.445 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.445 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.037.446 I llama_model_loader: - type  f32:  194 tensors
0.00.037.446 I llama_model_loader: - type q8_0:   98 tensors
0.00.061.938 I llm_load_vocab: special tokens cache size = 25
0.00.069.127 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.069.130 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.069.130 I llm_load_print_meta: arch             = gptneox
0.00.069.130 I llm_load_print_meta: vocab type       = BPE
0.00.069.131 I llm_load_print_meta: n_vocab          = 50304
0.00.069.131 I llm_load_print_meta: n_merges         = 50009
0.00.069.131 I llm_load_print_meta: vocab_only       = 0
0.00.069.131 I llm_load_print_meta: n_ctx_train      = 2048
0.00.069.131 I llm_load_print_meta: n_embd           = 2048
0.00.069.131 I llm_load_print_meta: n_layer          = 24
0.00.069.135 I llm_load_print_meta: n_head           = 16
0.00.069.136 I llm_load_print_meta: n_head_kv        = 16
0.00.069.136 I llm_load_print_meta: n_rot            = 32
0.00.069.136 I llm_load_print_meta: n_swa            = 0
0.00.069.136 I llm_load_print_meta: n_embd_head_k    = 128
0.00.069.137 I llm_load_print_meta: n_embd_head_v    = 128
0.00.069.137 I llm_load_print_meta: n_gqa            = 1
0.00.069.140 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.069.141 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.069.142 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.069.142 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.069.142 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.069.142 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.069.142 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.069.144 I llm_load_print_meta: n_ff             = 8192
0.00.069.144 I llm_load_print_meta: n_expert         = 0
0.00.069.145 I llm_load_print_meta: n_expert_used    = 0
0.00.069.145 I llm_load_print_meta: causal attn      = 1
0.00.069.145 I llm_load_print_meta: pooling type     = 0
0.00.069.147 I llm_load_print_meta: rope type        = 2
0.00.069.147 I llm_load_print_meta: rope scaling     = linear
0.00.069.147 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.069.148 I llm_load_print_meta: freq_scale_train = 1
0.00.069.148 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.069.148 I llm_load_print_meta: rope_finetuned   = unknown
0.00.069.148 I llm_load_print_meta: ssm_d_conv       = 0
0.00.069.148 I llm_load_print_meta: ssm_d_inner      = 0
0.00.069.148 I llm_load_print_meta: ssm_d_state      = 0
0.00.069.149 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.069.149 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.069.165 I llm_load_print_meta: model type       = 1.4B
0.00.069.165 I llm_load_print_meta: model ftype      = Q8_0
0.00.069.166 I llm_load_print_meta: model params     = 1.41 B
0.00.069.166 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.069.167 I llm_load_print_meta: general.name     = 1.4B
0.00.069.167 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.069.167 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.069.168 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.069.169 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.069.169 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.069.169 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.069.169 I llm_load_print_meta: max token length = 1024
0.00.071.728 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.071.728 I llm_load_tensors: offloading output layer to GPU
0.00.071.728 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.071.739 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.071.740 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.072.879 I llama_new_context_with_model: n_seq_max     = 1
0.00.072.880 I llama_new_context_with_model: n_ctx         = 2048
0.00.072.880 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.072.880 I llama_new_context_with_model: n_batch       = 2048
0.00.072.881 I llama_new_context_with_model: n_ubatch      = 512
0.00.072.881 I llama_new_context_with_model: flash_attn    = 0
0.00.072.881 I llama_new_context_with_model: freq_base     = 10000.0
0.00.072.882 I llama_new_context_with_model: freq_scale    = 1
0.00.072.882 I ggml_metal_init: allocating
0.00.072.889 I ggml_metal_init: found device: Apple M4
0.00.072.891 I ggml_metal_init: picking default device: Apple M4
0.00.073.622 I ggml_metal_init: using embedded metal library
0.00.076.134 I ggml_metal_init: GPU name:   Apple M4
0.00.076.136 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.076.136 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.076.136 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.076.137 I ggml_metal_init: simdgroup reduction   = true
0.00.076.137 I ggml_metal_init: simdgroup matrix mul. = true
0.00.076.137 I ggml_metal_init: has bfloat            = true
0.00.076.137 I ggml_metal_init: use bfloat            = true
0.00.076.138 I ggml_metal_init: hasUnifiedMemory      = true
0.00.076.138 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.111.488 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.111.502 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.111.537 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.112.672 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.112.673 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.112.673 I llama_new_context_with_model: graph nodes  = 967
0.00.112.674 I llama_new_context_with_model: graph splits = 2
0.00.112.689 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.402.941 I main: llama threadpool init, n_threads = 4
0.01.402.974 I 
0.01.403.003 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.01.403.007 I 
0.01.403.230 I sampler seed: 1234
0.01.403.234 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.403.279 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.403.281 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.403.281 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.490.915 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 57028.11 tokens per second)
0.02.490.915 I llama_perf_context_print:        load time =    1393.31 ms
0.02.490.916 I llama_perf_context_print: prompt eval time =      36.78 ms /     7 tokens (    5.25 ms per token,   190.31 tokens per second)
0.02.490.917 I llama_perf_context_print:        eval time =    1047.81 ms /    63 runs   (   16.63 ms per token,    60.13 tokens per second)
0.02.490.921 I llama_perf_context_print:       total time =    1087.98 ms /    70 tokens
0.02.491.105 I ggml_metal_free: deallocating

real	0m2.509s
user	0m0.116s
sys	0m0.225s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4223 (a3a3048e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.010.639 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.126 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.131 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.134 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.135 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.135 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.136 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.136 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.137 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.137 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.137 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.138 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.138 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.139 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.139 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.141 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.141 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.141 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.011 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.138 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.993 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.994 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.995 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.995 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.996 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.996 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.997 I llama_model_loader: - type  f32:  194 tensors
0.00.025.997 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.997 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.637 I llm_load_vocab: special tokens cache size = 25
0.00.052.756 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.759 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.759 I llm_load_print_meta: arch             = gptneox
0.00.052.760 I llm_load_print_meta: vocab type       = BPE
0.00.052.760 I llm_load_print_meta: n_vocab          = 50304
0.00.052.760 I llm_load_print_meta: n_merges         = 50009
0.00.052.760 I llm_load_print_meta: vocab_only       = 0
0.00.052.760 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.761 I llm_load_print_meta: n_embd           = 2048
0.00.052.761 I llm_load_print_meta: n_layer          = 24
0.00.052.765 I llm_load_print_meta: n_head           = 16
0.00.052.769 I llm_load_print_meta: n_head_kv        = 16
0.00.052.769 I llm_load_print_meta: n_rot            = 32
0.00.052.770 I llm_load_print_meta: n_swa            = 0
0.00.052.771 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.771 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.772 I llm_load_print_meta: n_gqa            = 1
0.00.052.772 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.773 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.774 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.774 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.776 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.776 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.776 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.777 I llm_load_print_meta: n_ff             = 8192
0.00.052.777 I llm_load_print_meta: n_expert         = 0
0.00.052.777 I llm_load_print_meta: n_expert_used    = 0
0.00.052.777 I llm_load_print_meta: causal attn      = 1
0.00.052.777 I llm_load_print_meta: pooling type     = 0
0.00.052.777 I llm_load_print_meta: rope type        = 2
0.00.052.778 I llm_load_print_meta: rope scaling     = linear
0.00.052.778 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.778 I llm_load_print_meta: freq_scale_train = 1
0.00.052.778 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.779 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.779 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.779 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.779 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.779 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.779 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.793 I llm_load_print_meta: model type       = 1.4B
0.00.052.793 I llm_load_print_meta: model ftype      = Q4_0
0.00.052.793 I llm_load_print_meta: model params     = 1.41 B
0.00.052.794 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.052.795 I llm_load_print_meta: general.name     = 1.4B
0.00.052.796 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.796 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.796 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.796 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.796 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.797 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.797 I llm_load_print_meta: max token length = 1024
0.00.055.079 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.079 I llm_load_tensors: offloading output layer to GPU
0.00.055.080 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.091 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.055.092 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.056.054 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.054 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.055 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.055 I llama_new_context_with_model: n_batch       = 2048
0.00.056.055 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.055 I llama_new_context_with_model: flash_attn    = 0
0.00.056.056 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.056 I llama_new_context_with_model: freq_scale    = 1
0.00.056.056 I ggml_metal_init: allocating
0.00.056.066 I ggml_metal_init: found device: Apple M4
0.00.056.069 I ggml_metal_init: picking default device: Apple M4
0.00.056.742 I ggml_metal_init: using embedded metal library
0.00.058.867 I ggml_metal_init: GPU name:   Apple M4
0.00.058.869 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.869 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.869 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.870 I ggml_metal_init: simdgroup reduction   = true
0.00.058.870 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.870 I ggml_metal_init: has bfloat            = true
0.00.058.870 I ggml_metal_init: use bfloat            = true
0.00.058.871 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.871 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.090.062 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.090.071 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.090.093 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.091.185 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.091.186 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.091.187 I llama_new_context_with_model: graph nodes  = 967
0.00.091.187 I llama_new_context_with_model: graph splits = 2
0.00.091.215 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.685.519 I main: llama threadpool init, n_threads = 4
0.00.685.557 I 
0.00.685.584 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.685.585 I 
0.00.685.822 I sampler seed: 1234
0.00.685.826 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.685.837 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.685.838 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.685.838 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.361.495 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58677.69 tokens per second)
0.01.361.495 I llama_perf_context_print:        load time =     674.88 ms
0.01.361.496 I llama_perf_context_print: prompt eval time =      37.86 ms /     7 tokens (    5.41 ms per token,   184.87 tokens per second)
0.01.361.496 I llama_perf_context_print:        eval time =     634.85 ms /    63 runs   (   10.08 ms per token,    99.24 tokens per second)
0.01.361.497 I llama_perf_context_print:       total time =     675.98 ms /    70 tokens
0.01.361.698 I ggml_metal_free: deallocating

real	0m1.380s
user	0m0.109s
sys	0m0.152s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4223 (a3a3048e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.011.403 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.290 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.018.293 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.299 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.300 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.300 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.301 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.301 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.302 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.304 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.304 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.305 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.305 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.305 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.306 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.307 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.308 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.308 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.174 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.233 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.004 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.005 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.005 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.005 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.006 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.006 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.027.007 I llama_model_loader: - type  f32:  194 tensors
0.00.027.007 I llama_model_loader: - type q4_1:   97 tensors
0.00.027.007 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.124 I llm_load_vocab: special tokens cache size = 25
0.00.053.154 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.157 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.157 I llm_load_print_meta: arch             = gptneox
0.00.053.157 I llm_load_print_meta: vocab type       = BPE
0.00.053.158 I llm_load_print_meta: n_vocab          = 50304
0.00.053.158 I llm_load_print_meta: n_merges         = 50009
0.00.053.158 I llm_load_print_meta: vocab_only       = 0
0.00.053.158 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.158 I llm_load_print_meta: n_embd           = 2048
0.00.053.158 I llm_load_print_meta: n_layer          = 24
0.00.053.161 I llm_load_print_meta: n_head           = 16
0.00.053.162 I llm_load_print_meta: n_head_kv        = 16
0.00.053.162 I llm_load_print_meta: n_rot            = 32
0.00.053.162 I llm_load_print_meta: n_swa            = 0
0.00.053.162 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.163 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.163 I llm_load_print_meta: n_gqa            = 1
0.00.053.164 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.165 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.165 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.167 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.167 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.168 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.168 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.168 I llm_load_print_meta: n_ff             = 8192
0.00.053.169 I llm_load_print_meta: n_expert         = 0
0.00.053.169 I llm_load_print_meta: n_expert_used    = 0
0.00.053.169 I llm_load_print_meta: causal attn      = 1
0.00.053.169 I llm_load_print_meta: pooling type     = 0
0.00.053.169 I llm_load_print_meta: rope type        = 2
0.00.053.170 I llm_load_print_meta: rope scaling     = linear
0.00.053.170 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.171 I llm_load_print_meta: freq_scale_train = 1
0.00.053.171 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.172 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.172 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.172 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.172 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.172 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.172 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.184 I llm_load_print_meta: model type       = 1.4B
0.00.053.185 I llm_load_print_meta: model ftype      = Q4_1
0.00.053.185 I llm_load_print_meta: model params     = 1.41 B
0.00.053.187 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.053.187 I llm_load_print_meta: general.name     = 1.4B
0.00.053.187 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.187 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.187 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.187 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.188 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.188 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.188 I llm_load_print_meta: max token length = 1024
0.00.055.172 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.173 I llm_load_tensors: offloading output layer to GPU
0.00.055.173 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.183 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.055.184 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.056.171 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.172 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.172 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.172 I llama_new_context_with_model: n_batch       = 2048
0.00.056.172 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.173 I llama_new_context_with_model: flash_attn    = 0
0.00.056.173 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.173 I llama_new_context_with_model: freq_scale    = 1
0.00.056.174 I ggml_metal_init: allocating
0.00.056.180 I ggml_metal_init: found device: Apple M4
0.00.056.182 I ggml_metal_init: picking default device: Apple M4
0.00.056.785 I ggml_metal_init: using embedded metal library
0.00.058.716 I ggml_metal_init: GPU name:   Apple M4
0.00.058.717 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.718 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.718 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.718 I ggml_metal_init: simdgroup reduction   = true
0.00.058.718 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.718 I ggml_metal_init: has bfloat            = true
0.00.058.719 I ggml_metal_init: use bfloat            = true
0.00.058.719 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.720 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.301 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.306 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.322 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.367 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.368 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.368 I llama_new_context_with_model: graph nodes  = 967
0.00.087.368 I llama_new_context_with_model: graph splits = 2
0.00.087.391 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.650.362 I main: llama threadpool init, n_threads = 4
0.00.650.397 I 
0.00.650.424 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.650.424 I 
0.00.650.566 I sampler seed: 1234
0.00.650.571 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.650.581 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.650.581 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.650.581 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.374.858 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 58970.10 tokens per second)
0.01.374.859 I llama_perf_context_print:        load time =     638.96 ms
0.01.374.860 I llama_perf_context_print: prompt eval time =      32.74 ms /     7 tokens (    4.68 ms per token,   213.83 tokens per second)
0.01.374.861 I llama_perf_context_print:        eval time =     688.56 ms /    63 runs   (   10.93 ms per token,    91.50 tokens per second)
0.01.374.861 I llama_perf_context_print:       total time =     724.50 ms /    70 tokens
0.01.375.040 I ggml_metal_free: deallocating

real	0m1.400s
user	0m0.107s
sys	0m0.141s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4223 (a3a3048e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.935 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.021 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.025 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.026 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.031 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.031 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.032 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.032 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.033 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.033 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.035 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.035 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.035 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.035 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.036 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.037 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.037 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.038 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.193 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.301 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.400 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.401 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.402 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.402 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.402 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.403 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.403 I llama_model_loader: - type  f32:  194 tensors
0.00.025.404 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.404 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.982 I llm_load_vocab: special tokens cache size = 25
0.00.051.990 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.992 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.993 I llm_load_print_meta: arch             = gptneox
0.00.051.993 I llm_load_print_meta: vocab type       = BPE
0.00.051.993 I llm_load_print_meta: n_vocab          = 50304
0.00.051.994 I llm_load_print_meta: n_merges         = 50009
0.00.051.994 I llm_load_print_meta: vocab_only       = 0
0.00.051.994 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.994 I llm_load_print_meta: n_embd           = 2048
0.00.051.994 I llm_load_print_meta: n_layer          = 24
0.00.051.997 I llm_load_print_meta: n_head           = 16
0.00.051.998 I llm_load_print_meta: n_head_kv        = 16
0.00.051.998 I llm_load_print_meta: n_rot            = 32
0.00.051.998 I llm_load_print_meta: n_swa            = 0
0.00.051.998 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.998 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.999 I llm_load_print_meta: n_gqa            = 1
0.00.052.000 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.000 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.001 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.001 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.001 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.002 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.002 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.002 I llm_load_print_meta: n_ff             = 8192
0.00.052.003 I llm_load_print_meta: n_expert         = 0
0.00.052.003 I llm_load_print_meta: n_expert_used    = 0
0.00.052.003 I llm_load_print_meta: causal attn      = 1
0.00.052.003 I llm_load_print_meta: pooling type     = 0
0.00.052.003 I llm_load_print_meta: rope type        = 2
0.00.052.004 I llm_load_print_meta: rope scaling     = linear
0.00.052.004 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.004 I llm_load_print_meta: freq_scale_train = 1
0.00.052.005 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.005 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.005 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.007 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.008 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.008 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.008 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.015 I llm_load_print_meta: model type       = 1.4B
0.00.052.015 I llm_load_print_meta: model ftype      = Q5_0
0.00.052.015 I llm_load_print_meta: model params     = 1.41 B
0.00.052.016 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.052.016 I llm_load_print_meta: general.name     = 1.4B
0.00.052.016 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.016 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.016 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.017 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.017 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.017 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.017 I llm_load_print_meta: max token length = 1024
0.00.053.778 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.778 I llm_load_tensors: offloading output layer to GPU
0.00.053.779 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.784 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.784 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.706 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.707 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.707 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.707 I llama_new_context_with_model: n_batch       = 2048
0.00.054.707 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.708 I llama_new_context_with_model: flash_attn    = 0
0.00.054.708 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.708 I llama_new_context_with_model: freq_scale    = 1
0.00.054.709 I ggml_metal_init: allocating
0.00.054.712 I ggml_metal_init: found device: Apple M4
0.00.054.714 I ggml_metal_init: picking default device: Apple M4
0.00.055.249 I ggml_metal_init: using embedded metal library
0.00.057.175 I ggml_metal_init: GPU name:   Apple M4
0.00.057.177 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.177 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.177 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.178 I ggml_metal_init: simdgroup reduction   = true
0.00.057.178 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.178 I ggml_metal_init: has bfloat            = true
0.00.057.178 I ggml_metal_init: use bfloat            = true
0.00.057.179 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.179 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.115 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.127 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.147 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.151 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.152 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.152 I llama_new_context_with_model: graph nodes  = 967
0.00.086.153 I llama_new_context_with_model: graph splits = 2
0.00.086.165 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.786.225 I main: llama threadpool init, n_threads = 4
0.00.786.264 I 
0.00.786.310 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.786.312 I 
0.00.786.535 I sampler seed: 1234
0.00.786.540 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.786.582 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.786.584 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.786.584 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.572.075 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59314.95 tokens per second)
0.01.572.075 I llama_perf_context_print:        load time =     777.28 ms
0.01.572.076 I llama_perf_context_print: prompt eval time =      36.58 ms /     7 tokens (    5.23 ms per token,   191.37 tokens per second)
0.01.572.077 I llama_perf_context_print:        eval time =     745.94 ms /    63 runs   (   11.84 ms per token,    84.46 tokens per second)
0.01.572.077 I llama_perf_context_print:       total time =     785.86 ms /    70 tokens
0.01.572.263 I ggml_metal_free: deallocating

real	0m1.588s
user	0m0.109s
sys	0m0.172s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4223 (a3a3048e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.009.694 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.523 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.527 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.529 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.534 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.535 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.535 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.535 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.536 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.537 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.537 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.537 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.538 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.538 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.538 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.540 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.540 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.540 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.493 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.595 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.508 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.509 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.509 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.509 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.510 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.510 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.511 I llama_model_loader: - type  f32:  194 tensors
0.00.025.511 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.512 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.432 I llm_load_vocab: special tokens cache size = 25
0.00.052.615 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.617 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.618 I llm_load_print_meta: arch             = gptneox
0.00.052.618 I llm_load_print_meta: vocab type       = BPE
0.00.052.618 I llm_load_print_meta: n_vocab          = 50304
0.00.052.619 I llm_load_print_meta: n_merges         = 50009
0.00.052.619 I llm_load_print_meta: vocab_only       = 0
0.00.052.619 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.619 I llm_load_print_meta: n_embd           = 2048
0.00.052.619 I llm_load_print_meta: n_layer          = 24
0.00.052.622 I llm_load_print_meta: n_head           = 16
0.00.052.623 I llm_load_print_meta: n_head_kv        = 16
0.00.052.623 I llm_load_print_meta: n_rot            = 32
0.00.052.623 I llm_load_print_meta: n_swa            = 0
0.00.052.624 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.624 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.625 I llm_load_print_meta: n_gqa            = 1
0.00.052.625 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.626 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.627 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.627 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.627 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.627 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.627 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.628 I llm_load_print_meta: n_ff             = 8192
0.00.052.628 I llm_load_print_meta: n_expert         = 0
0.00.052.628 I llm_load_print_meta: n_expert_used    = 0
0.00.052.628 I llm_load_print_meta: causal attn      = 1
0.00.052.629 I llm_load_print_meta: pooling type     = 0
0.00.052.631 I llm_load_print_meta: rope type        = 2
0.00.052.631 I llm_load_print_meta: rope scaling     = linear
0.00.052.631 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.632 I llm_load_print_meta: freq_scale_train = 1
0.00.052.632 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.632 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.632 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.633 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.633 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.633 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.633 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.645 I llm_load_print_meta: model type       = 1.4B
0.00.052.645 I llm_load_print_meta: model ftype      = Q5_1
0.00.052.645 I llm_load_print_meta: model params     = 1.41 B
0.00.052.646 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.052.646 I llm_load_print_meta: general.name     = 1.4B
0.00.052.646 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.646 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.647 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.647 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.647 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.647 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.647 I llm_load_print_meta: max token length = 1024
0.00.054.703 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.703 I llm_load_tensors: offloading output layer to GPU
0.00.054.703 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.713 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.054.714 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.055.657 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.658 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.658 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.659 I llama_new_context_with_model: n_batch       = 2048
0.00.055.659 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.659 I llama_new_context_with_model: flash_attn    = 0
0.00.055.659 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.660 I llama_new_context_with_model: freq_scale    = 1
0.00.055.660 I ggml_metal_init: allocating
0.00.055.663 I ggml_metal_init: found device: Apple M4
0.00.055.665 I ggml_metal_init: picking default device: Apple M4
0.00.056.209 I ggml_metal_init: using embedded metal library
0.00.058.134 I ggml_metal_init: GPU name:   Apple M4
0.00.058.135 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.136 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.136 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.136 I ggml_metal_init: simdgroup reduction   = true
0.00.058.136 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.137 I ggml_metal_init: has bfloat            = true
0.00.058.137 I ggml_metal_init: use bfloat            = true
0.00.058.137 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.138 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.169 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.175 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.192 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.327 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.329 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.329 I llama_new_context_with_model: graph nodes  = 967
0.00.087.329 I llama_new_context_with_model: graph splits = 2
0.00.087.352 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.793.189 I main: llama threadpool init, n_threads = 4
0.00.793.226 I 
0.00.793.257 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.793.258 I 
0.00.793.490 I sampler seed: 1234
0.00.793.496 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.793.506 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.793.507 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.793.507 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, literature and art.

If one's life is not happy, one can have no other way but

0.01.633.962 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57536.47 tokens per second)
0.01.633.963 I llama_perf_context_print:        load time =     783.49 ms
0.01.633.963 I llama_perf_context_print: prompt eval time =      40.47 ms /     7 tokens (    5.78 ms per token,   172.96 tokens per second)
0.01.633.964 I llama_perf_context_print:        eval time =     796.95 ms /    63 runs   (   12.65 ms per token,    79.05 tokens per second)
0.01.633.964 I llama_perf_context_print:       total time =     840.78 ms /    70 tokens
0.01.634.156 I ggml_metal_free: deallocating

real	0m1.654s
user	0m0.111s
sys	0m0.167s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4223 (a3a3048e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.010.780 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.614 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.619 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.621 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.621 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.622 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.622 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.622 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.623 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.623 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.624 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.624 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.626 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.627 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.627 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.628 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.629 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.629 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.434 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.521 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.304 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.306 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.306 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.306 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.306 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.307 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.307 I llama_model_loader: - type  f32:  194 tensors
0.00.025.308 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.308 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.308 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.475 I llm_load_vocab: special tokens cache size = 25
0.00.051.554 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.557 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.558 I llm_load_print_meta: arch             = gptneox
0.00.051.558 I llm_load_print_meta: vocab type       = BPE
0.00.051.558 I llm_load_print_meta: n_vocab          = 50304
0.00.051.558 I llm_load_print_meta: n_merges         = 50009
0.00.051.558 I llm_load_print_meta: vocab_only       = 0
0.00.051.559 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.559 I llm_load_print_meta: n_embd           = 2048
0.00.051.559 I llm_load_print_meta: n_layer          = 24
0.00.051.562 I llm_load_print_meta: n_head           = 16
0.00.051.563 I llm_load_print_meta: n_head_kv        = 16
0.00.051.563 I llm_load_print_meta: n_rot            = 32
0.00.051.563 I llm_load_print_meta: n_swa            = 0
0.00.051.564 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.564 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.564 I llm_load_print_meta: n_gqa            = 1
0.00.051.565 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.566 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.566 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.567 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.567 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.567 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.567 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.568 I llm_load_print_meta: n_ff             = 8192
0.00.051.568 I llm_load_print_meta: n_expert         = 0
0.00.051.568 I llm_load_print_meta: n_expert_used    = 0
0.00.051.568 I llm_load_print_meta: causal attn      = 1
0.00.051.568 I llm_load_print_meta: pooling type     = 0
0.00.051.571 I llm_load_print_meta: rope type        = 2
0.00.051.571 I llm_load_print_meta: rope scaling     = linear
0.00.051.571 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.572 I llm_load_print_meta: freq_scale_train = 1
0.00.051.572 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.572 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.572 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.574 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.574 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.574 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.574 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.586 I llm_load_print_meta: model type       = 1.4B
0.00.051.587 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.588 I llm_load_print_meta: model params     = 1.41 B
0.00.051.588 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.588 I llm_load_print_meta: general.name     = 1.4B
0.00.051.589 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.589 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.589 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.589 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.589 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.589 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.590 I llm_load_print_meta: max token length = 1024
0.00.053.426 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.426 I llm_load_tensors: offloading output layer to GPU
0.00.053.427 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.437 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.438 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.372 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.373 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.373 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.373 I llama_new_context_with_model: n_batch       = 2048
0.00.054.373 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.374 I llama_new_context_with_model: flash_attn    = 0
0.00.054.374 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.374 I llama_new_context_with_model: freq_scale    = 1
0.00.054.375 I ggml_metal_init: allocating
0.00.054.381 I ggml_metal_init: found device: Apple M4
0.00.054.383 I ggml_metal_init: picking default device: Apple M4
0.00.054.919 I ggml_metal_init: using embedded metal library
0.00.056.844 I ggml_metal_init: GPU name:   Apple M4
0.00.056.845 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.846 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.846 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.846 I ggml_metal_init: simdgroup reduction   = true
0.00.056.847 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.847 I ggml_metal_init: has bfloat            = true
0.00.056.847 I ggml_metal_init: use bfloat            = true
0.00.056.847 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.848 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.931 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.936 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.953 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.018 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.019 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.020 I llama_new_context_with_model: graph nodes  = 967
0.00.086.020 I llama_new_context_with_model: graph splits = 2
0.00.086.043 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.521.937 I main: llama threadpool init, n_threads = 4
0.00.521.977 I 
0.00.522.004 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.522.006 I 
0.00.522.232 I sampler seed: 1234
0.00.522.238 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.522.277 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.522.281 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.522.281 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.204.497 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61312.61 tokens per second)
0.01.204.497 I llama_perf_context_print:        load time =     511.15 ms
0.01.204.498 I llama_perf_context_print: prompt eval time =      35.82 ms /     7 tokens (    5.12 ms per token,   195.43 tokens per second)
0.01.204.499 I llama_perf_context_print:        eval time =     643.39 ms /    63 runs   (   10.21 ms per token,    97.92 tokens per second)
0.01.204.500 I llama_perf_context_print:       total time =     682.56 ms /    70 tokens
0.01.204.672 I ggml_metal_free: deallocating

real	0m1.225s
user	0m0.109s
sys	0m0.122s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4223 (a3a3048e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.008.840 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.169 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.173 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.175 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.175 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.176 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.176 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.176 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.177 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.177 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.179 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.179 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.179 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.180 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.180 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.183 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.183 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.183 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.003 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.087 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.900 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.901 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.902 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.902 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.902 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.902 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.903 I llama_model_loader: - type  f32:  194 tensors
0.00.023.903 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.903 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.903 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.904 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.230 I llm_load_vocab: special tokens cache size = 25
0.00.050.393 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.396 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.396 I llm_load_print_meta: arch             = gptneox
0.00.050.396 I llm_load_print_meta: vocab type       = BPE
0.00.050.397 I llm_load_print_meta: n_vocab          = 50304
0.00.050.397 I llm_load_print_meta: n_merges         = 50009
0.00.050.397 I llm_load_print_meta: vocab_only       = 0
0.00.050.397 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.397 I llm_load_print_meta: n_embd           = 2048
0.00.050.397 I llm_load_print_meta: n_layer          = 24
0.00.050.400 I llm_load_print_meta: n_head           = 16
0.00.050.401 I llm_load_print_meta: n_head_kv        = 16
0.00.050.401 I llm_load_print_meta: n_rot            = 32
0.00.050.401 I llm_load_print_meta: n_swa            = 0
0.00.050.401 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.402 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.402 I llm_load_print_meta: n_gqa            = 1
0.00.050.403 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.404 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.404 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.405 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.405 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.405 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.405 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.406 I llm_load_print_meta: n_ff             = 8192
0.00.050.408 I llm_load_print_meta: n_expert         = 0
0.00.050.410 I llm_load_print_meta: n_expert_used    = 0
0.00.050.410 I llm_load_print_meta: causal attn      = 1
0.00.050.410 I llm_load_print_meta: pooling type     = 0
0.00.050.410 I llm_load_print_meta: rope type        = 2
0.00.050.410 I llm_load_print_meta: rope scaling     = linear
0.00.050.411 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.411 I llm_load_print_meta: freq_scale_train = 1
0.00.050.411 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.412 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.412 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.412 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.412 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.412 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.412 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.424 I llm_load_print_meta: model type       = 1.4B
0.00.050.424 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.425 I llm_load_print_meta: model params     = 1.41 B
0.00.050.425 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.425 I llm_load_print_meta: general.name     = 1.4B
0.00.050.427 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.427 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.427 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.427 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.428 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.428 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.428 I llm_load_print_meta: max token length = 1024
0.00.052.298 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.298 I llm_load_tensors: offloading output layer to GPU
0.00.052.298 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.308 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.309 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.214 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.215 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.215 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.215 I llama_new_context_with_model: n_batch       = 2048
0.00.053.215 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.215 I llama_new_context_with_model: flash_attn    = 0
0.00.053.216 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.216 I llama_new_context_with_model: freq_scale    = 1
0.00.053.217 I ggml_metal_init: allocating
0.00.053.220 I ggml_metal_init: found device: Apple M4
0.00.053.222 I ggml_metal_init: picking default device: Apple M4
0.00.053.771 I ggml_metal_init: using embedded metal library
0.00.055.661 I ggml_metal_init: GPU name:   Apple M4
0.00.055.664 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.664 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.665 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.665 I ggml_metal_init: simdgroup reduction   = true
0.00.055.665 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.665 I ggml_metal_init: has bfloat            = true
0.00.055.665 I ggml_metal_init: use bfloat            = true
0.00.055.666 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.666 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.290 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.297 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.316 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.289 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.290 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.290 I llama_new_context_with_model: graph nodes  = 967
0.00.084.291 I llama_new_context_with_model: graph splits = 2
0.00.084.312 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.547.621 I main: llama threadpool init, n_threads = 4
0.00.547.660 I 
0.00.547.698 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.547.699 I 
0.00.547.930 I sampler seed: 1234
0.00.547.935 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.547.945 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.547.947 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.547.947 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do that is most useful to my fellow men?'"

-Albert Einstein

"The way

0.01.293.545 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57770.55 tokens per second)
0.01.293.546 I llama_perf_context_print:        load time =     538.78 ms
0.01.293.547 I llama_perf_context_print: prompt eval time =      39.54 ms /     7 tokens (    5.65 ms per token,   177.06 tokens per second)
0.01.293.548 I llama_perf_context_print:        eval time =     702.98 ms /    63 runs   (   11.16 ms per token,    89.62 tokens per second)
0.01.293.548 I llama_perf_context_print:       total time =     745.93 ms /    70 tokens
0.01.293.717 I ggml_metal_free: deallocating

real	0m1.309s
user	0m0.108s
sys	0m0.129s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4223 (a3a3048e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.010.603 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.842 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.848 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.849 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.850 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.850 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.850 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.851 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.851 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.852 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.852 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.852 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.853 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.853 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.853 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.855 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.855 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.855 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.626 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.688 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.404 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.405 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.405 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.405 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.405 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.406 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.406 I llama_model_loader: - type  f32:  194 tensors
0.00.025.406 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.406 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.407 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.549 I llm_load_vocab: special tokens cache size = 25
0.00.051.534 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.537 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.537 I llm_load_print_meta: arch             = gptneox
0.00.051.537 I llm_load_print_meta: vocab type       = BPE
0.00.051.538 I llm_load_print_meta: n_vocab          = 50304
0.00.051.538 I llm_load_print_meta: n_merges         = 50009
0.00.051.538 I llm_load_print_meta: vocab_only       = 0
0.00.051.538 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.538 I llm_load_print_meta: n_embd           = 2048
0.00.051.539 I llm_load_print_meta: n_layer          = 24
0.00.051.541 I llm_load_print_meta: n_head           = 16
0.00.051.542 I llm_load_print_meta: n_head_kv        = 16
0.00.051.542 I llm_load_print_meta: n_rot            = 32
0.00.051.542 I llm_load_print_meta: n_swa            = 0
0.00.051.542 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.543 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.543 I llm_load_print_meta: n_gqa            = 1
0.00.051.544 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.545 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.545 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.545 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.545 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.546 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.546 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.546 I llm_load_print_meta: n_ff             = 8192
0.00.051.547 I llm_load_print_meta: n_expert         = 0
0.00.051.548 I llm_load_print_meta: n_expert_used    = 0
0.00.051.550 I llm_load_print_meta: causal attn      = 1
0.00.051.550 I llm_load_print_meta: pooling type     = 0
0.00.051.550 I llm_load_print_meta: rope type        = 2
0.00.051.551 I llm_load_print_meta: rope scaling     = linear
0.00.051.551 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.552 I llm_load_print_meta: freq_scale_train = 1
0.00.051.552 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.552 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.552 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.552 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.552 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.553 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.553 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.566 I llm_load_print_meta: model type       = 1.4B
0.00.051.566 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.567 I llm_load_print_meta: model params     = 1.41 B
0.00.051.567 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.567 I llm_load_print_meta: general.name     = 1.4B
0.00.051.568 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.569 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.570 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.570 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.570 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.570 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.570 I llm_load_print_meta: max token length = 1024
0.00.053.559 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.560 I llm_load_tensors: offloading output layer to GPU
0.00.053.560 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.570 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.571 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.503 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.504 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.504 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.504 I llama_new_context_with_model: n_batch       = 2048
0.00.054.504 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.504 I llama_new_context_with_model: flash_attn    = 0
0.00.054.505 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.505 I llama_new_context_with_model: freq_scale    = 1
0.00.054.506 I ggml_metal_init: allocating
0.00.054.514 I ggml_metal_init: found device: Apple M4
0.00.054.516 I ggml_metal_init: picking default device: Apple M4
0.00.055.064 I ggml_metal_init: using embedded metal library
0.00.057.031 I ggml_metal_init: GPU name:   Apple M4
0.00.057.032 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.033 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.033 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.035 I ggml_metal_init: simdgroup reduction   = true
0.00.057.035 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.035 I ggml_metal_init: has bfloat            = true
0.00.057.035 I ggml_metal_init: use bfloat            = true
0.00.057.036 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.036 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.832 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.840 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.858 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.896 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.897 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.897 I llama_new_context_with_model: graph nodes  = 967
0.00.085.898 I llama_new_context_with_model: graph splits = 2
0.00.085.921 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.620.269 I main: llama threadpool init, n_threads = 4
0.00.620.306 I 
0.00.620.333 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.620.334 I 
0.00.620.565 I sampler seed: 1234
0.00.620.569 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.620.627 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.620.632 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.620.632 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the greatest power to create the future we want to see. I believe that we are in a wonderful time when people from all over the world are coming together to create a better world. I believe that we are going

0.01.376.294 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55905.51 tokens per second)
0.01.376.294 I llama_perf_context_print:        load time =     609.66 ms
0.01.376.295 I llama_perf_context_print: prompt eval time =      36.63 ms /     7 tokens (    5.23 ms per token,   191.07 tokens per second)
0.01.376.296 I llama_perf_context_print:        eval time =     715.95 ms /    63 runs   (   11.36 ms per token,    87.99 tokens per second)
0.01.376.301 I llama_perf_context_print:       total time =     756.03 ms /    70 tokens
0.01.376.477 I ggml_metal_free: deallocating

real	0m1.395s
user	0m0.107s
sys	0m0.143s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4223 (a3a3048e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.008.702 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.280 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.285 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.287 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.287 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.287 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.288 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.288 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.290 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.290 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.291 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.291 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.291 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.292 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.292 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.295 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.295 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.295 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.157 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.236 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.086 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.087 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.087 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.088 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.088 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.088 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.089 I llama_model_loader: - type  f32:  194 tensors
0.00.024.089 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.089 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.175 I llm_load_vocab: special tokens cache size = 25
0.00.050.109 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.112 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.112 I llm_load_print_meta: arch             = gptneox
0.00.050.112 I llm_load_print_meta: vocab type       = BPE
0.00.050.112 I llm_load_print_meta: n_vocab          = 50304
0.00.050.113 I llm_load_print_meta: n_merges         = 50009
0.00.050.113 I llm_load_print_meta: vocab_only       = 0
0.00.050.113 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.113 I llm_load_print_meta: n_embd           = 2048
0.00.050.113 I llm_load_print_meta: n_layer          = 24
0.00.050.116 I llm_load_print_meta: n_head           = 16
0.00.050.117 I llm_load_print_meta: n_head_kv        = 16
0.00.050.117 I llm_load_print_meta: n_rot            = 32
0.00.050.117 I llm_load_print_meta: n_swa            = 0
0.00.050.118 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.118 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.118 I llm_load_print_meta: n_gqa            = 1
0.00.050.119 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.120 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.121 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.121 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.121 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.121 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.121 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.122 I llm_load_print_meta: n_ff             = 8192
0.00.050.123 I llm_load_print_meta: n_expert         = 0
0.00.050.123 I llm_load_print_meta: n_expert_used    = 0
0.00.050.124 I llm_load_print_meta: causal attn      = 1
0.00.050.126 I llm_load_print_meta: pooling type     = 0
0.00.050.126 I llm_load_print_meta: rope type        = 2
0.00.050.126 I llm_load_print_meta: rope scaling     = linear
0.00.050.126 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.128 I llm_load_print_meta: freq_scale_train = 1
0.00.050.128 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.128 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.128 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.128 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.129 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.129 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.129 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.140 I llm_load_print_meta: model type       = 1.4B
0.00.050.141 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.141 I llm_load_print_meta: model params     = 1.41 B
0.00.050.142 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.145 I llm_load_print_meta: general.name     = 1.4B
0.00.050.145 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.145 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.145 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.145 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.146 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.146 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.146 I llm_load_print_meta: max token length = 1024
0.00.051.747 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.747 I llm_load_tensors: offloading output layer to GPU
0.00.051.747 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.758 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.759 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.611 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.612 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.612 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.613 I llama_new_context_with_model: n_batch       = 2048
0.00.052.613 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.613 I llama_new_context_with_model: flash_attn    = 0
0.00.052.614 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.614 I llama_new_context_with_model: freq_scale    = 1
0.00.052.614 I ggml_metal_init: allocating
0.00.052.618 I ggml_metal_init: found device: Apple M4
0.00.052.620 I ggml_metal_init: picking default device: Apple M4
0.00.053.155 I ggml_metal_init: using embedded metal library
0.00.055.062 I ggml_metal_init: GPU name:   Apple M4
0.00.055.063 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.065 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.065 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.065 I ggml_metal_init: simdgroup reduction   = true
0.00.055.065 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.065 I ggml_metal_init: has bfloat            = true
0.00.055.066 I ggml_metal_init: use bfloat            = true
0.00.055.066 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.067 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.409 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.418 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.439 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.427 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.083.428 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.083.428 I llama_new_context_with_model: graph nodes  = 967
0.00.083.429 I llama_new_context_with_model: graph splits = 2
0.00.083.451 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.706.920 I main: llama threadpool init, n_threads = 4
0.00.706.954 I 
0.00.706.984 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.706.986 I 
0.00.707.224 I sampler seed: 1234
0.00.707.228 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.707.285 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.707.287 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.707.287 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.553.332 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57258.06 tokens per second)
0.01.553.332 I llama_perf_context_print:        load time =     698.21 ms
0.01.553.333 I llama_perf_context_print: prompt eval time =      42.53 ms /     7 tokens (    6.08 ms per token,   164.58 tokens per second)
0.01.553.334 I llama_perf_context_print:        eval time =     800.41 ms /    63 runs   (   12.70 ms per token,    78.71 tokens per second)
0.01.553.334 I llama_perf_context_print:       total time =     846.41 ms /    70 tokens
0.01.553.515 I ggml_metal_free: deallocating

real	0m1.570s
user	0m0.108s
sys	0m0.162s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4223 (a3a3048e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.585 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.969 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.975 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.976 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.977 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.977 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.977 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.978 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.979 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.979 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.979 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.980 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.981 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.981 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.982 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.984 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.985 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.985 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.796 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.895 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.129 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.130 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.130 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.131 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.131 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.131 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.132 I llama_model_loader: - type  f32:  194 tensors
0.00.025.132 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.173 I llm_load_vocab: special tokens cache size = 25
0.00.051.315 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.318 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.318 I llm_load_print_meta: arch             = gptneox
0.00.051.319 I llm_load_print_meta: vocab type       = BPE
0.00.051.319 I llm_load_print_meta: n_vocab          = 50304
0.00.051.319 I llm_load_print_meta: n_merges         = 50009
0.00.051.320 I llm_load_print_meta: vocab_only       = 0
0.00.051.320 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.320 I llm_load_print_meta: n_embd           = 2048
0.00.051.320 I llm_load_print_meta: n_layer          = 24
0.00.051.323 I llm_load_print_meta: n_head           = 16
0.00.051.324 I llm_load_print_meta: n_head_kv        = 16
0.00.051.325 I llm_load_print_meta: n_rot            = 32
0.00.051.325 I llm_load_print_meta: n_swa            = 0
0.00.051.325 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.325 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.326 I llm_load_print_meta: n_gqa            = 1
0.00.051.327 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.327 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.328 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.328 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.328 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.329 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.329 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.330 I llm_load_print_meta: n_ff             = 8192
0.00.051.330 I llm_load_print_meta: n_expert         = 0
0.00.051.330 I llm_load_print_meta: n_expert_used    = 0
0.00.051.330 I llm_load_print_meta: causal attn      = 1
0.00.051.332 I llm_load_print_meta: pooling type     = 0
0.00.051.333 I llm_load_print_meta: rope type        = 2
0.00.051.333 I llm_load_print_meta: rope scaling     = linear
0.00.051.334 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.334 I llm_load_print_meta: freq_scale_train = 1
0.00.051.334 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.334 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.335 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.335 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.335 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.335 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.335 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.342 I llm_load_print_meta: model type       = 1.4B
0.00.051.342 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.343 I llm_load_print_meta: model params     = 1.41 B
0.00.051.343 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.343 I llm_load_print_meta: general.name     = 1.4B
0.00.051.343 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.344 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.344 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.344 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.344 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.344 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.344 I llm_load_print_meta: max token length = 1024
0.00.053.284 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.284 I llm_load_tensors: offloading output layer to GPU
0.00.053.285 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.294 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.296 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.184 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.184 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.185 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.185 I llama_new_context_with_model: n_batch       = 2048
0.00.054.185 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.185 I llama_new_context_with_model: flash_attn    = 0
0.00.054.186 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.186 I llama_new_context_with_model: freq_scale    = 1
0.00.054.186 I ggml_metal_init: allocating
0.00.054.189 I ggml_metal_init: found device: Apple M4
0.00.054.191 I ggml_metal_init: picking default device: Apple M4
0.00.054.742 I ggml_metal_init: using embedded metal library
0.00.056.674 I ggml_metal_init: GPU name:   Apple M4
0.00.056.675 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.676 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.676 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.676 I ggml_metal_init: simdgroup reduction   = true
0.00.056.678 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.678 I ggml_metal_init: has bfloat            = true
0.00.056.678 I ggml_metal_init: use bfloat            = true
0.00.056.678 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.680 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.971 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.976 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.995 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.956 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.958 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.958 I llama_new_context_with_model: graph nodes  = 967
0.00.084.959 I llama_new_context_with_model: graph splits = 2
0.00.084.981 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.770.260 I main: llama threadpool init, n_threads = 4
0.00.770.294 I 
0.00.770.319 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.770.319 I 
0.00.770.560 I sampler seed: 1234
0.00.770.564 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.770.583 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.770.584 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.770.584 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.640.819 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54322.88 tokens per second)
0.01.640.819 I llama_perf_context_print:        load time =     760.67 ms
0.01.640.820 I llama_perf_context_print: prompt eval time =      38.43 ms /     7 tokens (    5.49 ms per token,   182.14 tokens per second)
0.01.640.825 I llama_perf_context_print:        eval time =     828.69 ms /    63 runs   (   13.15 ms per token,    76.02 tokens per second)
0.01.640.825 I llama_perf_context_print:       total time =     870.56 ms /    70 tokens
0.01.641.000 I ggml_metal_free: deallocating

real	0m1.660s
user	0m0.109s
sys	0m0.175s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.781 I build: 4223 (a3a3048e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.019.900 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.032.462 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.032.467 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.469 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.470 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.470 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.470 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.471 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.472 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.472 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.472 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.473 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.473 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.474 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.474 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.478 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.479 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.479 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.040.384 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.042.497 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.543 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.049.545 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.546 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.049.546 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.049.547 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.049.547 I llama_model_loader: - type  f32:  194 tensors
0.00.049.548 I llama_model_loader: - type  f16:   98 tensors
0.00.078.465 I llm_load_vocab: special tokens cache size = 25
0.00.085.084 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.085.087 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.085.087 I llm_load_print_meta: arch             = gptneox
0.00.085.088 I llm_load_print_meta: vocab type       = BPE
0.00.085.088 I llm_load_print_meta: n_vocab          = 50304
0.00.085.088 I llm_load_print_meta: n_merges         = 50009
0.00.085.088 I llm_load_print_meta: vocab_only       = 0
0.00.085.088 I llm_load_print_meta: n_ctx_train      = 2048
0.00.085.089 I llm_load_print_meta: n_embd           = 2048
0.00.085.089 I llm_load_print_meta: n_layer          = 24
0.00.085.091 I llm_load_print_meta: n_head           = 16
0.00.085.092 I llm_load_print_meta: n_head_kv        = 16
0.00.085.092 I llm_load_print_meta: n_rot            = 32
0.00.085.092 I llm_load_print_meta: n_swa            = 0
0.00.085.092 I llm_load_print_meta: n_embd_head_k    = 128
0.00.085.092 I llm_load_print_meta: n_embd_head_v    = 128
0.00.085.093 I llm_load_print_meta: n_gqa            = 1
0.00.085.093 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.085.094 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.085.094 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.085.097 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.085.097 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.085.097 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.085.097 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.085.098 I llm_load_print_meta: n_ff             = 8192
0.00.085.098 I llm_load_print_meta: n_expert         = 0
0.00.085.098 I llm_load_print_meta: n_expert_used    = 0
0.00.085.098 I llm_load_print_meta: causal attn      = 1
0.00.085.098 I llm_load_print_meta: pooling type     = 0
0.00.085.098 I llm_load_print_meta: rope type        = 2
0.00.085.098 I llm_load_print_meta: rope scaling     = linear
0.00.085.100 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.085.101 I llm_load_print_meta: freq_scale_train = 1
0.00.085.101 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.085.101 I llm_load_print_meta: rope_finetuned   = unknown
0.00.085.101 I llm_load_print_meta: ssm_d_conv       = 0
0.00.085.101 I llm_load_print_meta: ssm_d_inner      = 0
0.00.085.101 I llm_load_print_meta: ssm_d_state      = 0
0.00.085.102 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.085.102 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.085.114 I llm_load_print_meta: model type       = 1.4B
0.00.085.114 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.085.115 I llm_load_print_meta: model params     = 1.41 B
0.00.085.115 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.085.115 I llm_load_print_meta: general.name     = 1.4B
0.00.085.116 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.085.116 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.085.116 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.085.116 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.085.117 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.085.117 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.085.117 I llm_load_print_meta: max token length = 1024
0.00.087.596 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.087.597 I llm_load_tensors: offloading output layer to GPU
0.00.087.597 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.087.607 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.087.608 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.088.521 I llama_new_context_with_model: n_seq_max     = 1
0.00.088.522 I llama_new_context_with_model: n_ctx         = 128
0.00.088.522 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.088.523 I llama_new_context_with_model: n_batch       = 128
0.00.088.523 I llama_new_context_with_model: n_ubatch      = 128
0.00.088.523 I llama_new_context_with_model: flash_attn    = 0
0.00.088.523 I llama_new_context_with_model: freq_base     = 10000.0
0.00.088.524 I llama_new_context_with_model: freq_scale    = 1
0.00.088.524 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.088.524 I ggml_metal_init: allocating
0.00.088.527 I ggml_metal_init: found device: Apple M4
0.00.088.530 I ggml_metal_init: picking default device: Apple M4
0.00.089.093 I ggml_metal_init: using embedded metal library
0.00.091.165 I ggml_metal_init: GPU name:   Apple M4
0.00.091.166 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.091.167 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.091.167 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.091.167 I ggml_metal_init: simdgroup reduction   = true
0.00.091.167 I ggml_metal_init: simdgroup matrix mul. = true
0.00.091.167 I ggml_metal_init: has bfloat            = true
0.00.091.168 I ggml_metal_init: use bfloat            = true
0.00.091.168 I ggml_metal_init: hasUnifiedMemory      = true
0.00.091.169 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.100.487 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.100.489 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.100.502 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.101.357 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.101.358 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.101.359 I llama_new_context_with_model: graph nodes  = 967
0.00.101.359 I llama_new_context_with_model: graph splits = 2
0.00.101.366 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.870.762 I 
0.00.870.799 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.870.805 I perplexity: tokenizing the input ..
0.00.882.317 I perplexity: tokenization took 11.51 ms
0.00.882.342 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.003.736 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.006.153 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.006.184 I llama_perf_context_print:        load time =     850.85 ms
0.01.006.186 I llama_perf_context_print: prompt eval time =     121.02 ms /   128 tokens (    0.95 ms per token,  1057.71 tokens per second)
0.01.006.187 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.006.188 I llama_perf_context_print:       total time =     135.42 ms /   129 tokens
0.01.006.988 I ggml_metal_free: deallocating

real	0m1.202s
user	0m0.123s
sys	0m0.200s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.141 I build: 4223 (a3a3048e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.929 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.331 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.020.336 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.337 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.338 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.338 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.339 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.339 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.340 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.340 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.341 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.341 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.343 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.344 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.344 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.346 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.346 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.348 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.063 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.657 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.531 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.533 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.533 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.534 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.534 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.535 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.536 I llama_model_loader: - type  f32:  194 tensors
0.00.033.536 I llama_model_loader: - type q8_0:   98 tensors
0.00.060.024 I llm_load_vocab: special tokens cache size = 25
0.00.066.308 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.066.311 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.066.311 I llm_load_print_meta: arch             = gptneox
0.00.066.311 I llm_load_print_meta: vocab type       = BPE
0.00.066.311 I llm_load_print_meta: n_vocab          = 50304
0.00.066.312 I llm_load_print_meta: n_merges         = 50009
0.00.066.312 I llm_load_print_meta: vocab_only       = 0
0.00.066.312 I llm_load_print_meta: n_ctx_train      = 2048
0.00.066.312 I llm_load_print_meta: n_embd           = 2048
0.00.066.312 I llm_load_print_meta: n_layer          = 24
0.00.066.316 I llm_load_print_meta: n_head           = 16
0.00.066.316 I llm_load_print_meta: n_head_kv        = 16
0.00.066.316 I llm_load_print_meta: n_rot            = 32
0.00.066.317 I llm_load_print_meta: n_swa            = 0
0.00.066.317 I llm_load_print_meta: n_embd_head_k    = 128
0.00.066.317 I llm_load_print_meta: n_embd_head_v    = 128
0.00.066.318 I llm_load_print_meta: n_gqa            = 1
0.00.066.321 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.066.321 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.066.323 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.066.323 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.066.323 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.066.323 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.066.329 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.066.337 I llm_load_print_meta: n_ff             = 8192
0.00.066.337 I llm_load_print_meta: n_expert         = 0
0.00.066.337 I llm_load_print_meta: n_expert_used    = 0
0.00.066.337 I llm_load_print_meta: causal attn      = 1
0.00.066.337 I llm_load_print_meta: pooling type     = 0
0.00.066.337 I llm_load_print_meta: rope type        = 2
0.00.066.338 I llm_load_print_meta: rope scaling     = linear
0.00.066.338 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.066.338 I llm_load_print_meta: freq_scale_train = 1
0.00.066.339 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.066.339 I llm_load_print_meta: rope_finetuned   = unknown
0.00.066.339 I llm_load_print_meta: ssm_d_conv       = 0
0.00.066.339 I llm_load_print_meta: ssm_d_inner      = 0
0.00.066.339 I llm_load_print_meta: ssm_d_state      = 0
0.00.066.339 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.066.339 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.066.351 I llm_load_print_meta: model type       = 1.4B
0.00.066.352 I llm_load_print_meta: model ftype      = Q8_0
0.00.066.352 I llm_load_print_meta: model params     = 1.41 B
0.00.066.352 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.066.353 I llm_load_print_meta: general.name     = 1.4B
0.00.066.353 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.066.353 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.066.354 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.066.354 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.066.354 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.066.355 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.066.355 I llm_load_print_meta: max token length = 1024
0.00.068.576 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.068.577 I llm_load_tensors: offloading output layer to GPU
0.00.068.577 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.068.587 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.068.588 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.069.502 I llama_new_context_with_model: n_seq_max     = 1
0.00.069.503 I llama_new_context_with_model: n_ctx         = 128
0.00.069.503 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.069.503 I llama_new_context_with_model: n_batch       = 128
0.00.069.503 I llama_new_context_with_model: n_ubatch      = 128
0.00.069.503 I llama_new_context_with_model: flash_attn    = 0
0.00.069.504 I llama_new_context_with_model: freq_base     = 10000.0
0.00.069.504 I llama_new_context_with_model: freq_scale    = 1
0.00.069.504 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.069.505 I ggml_metal_init: allocating
0.00.069.511 I ggml_metal_init: found device: Apple M4
0.00.069.514 I ggml_metal_init: picking default device: Apple M4
0.00.070.072 I ggml_metal_init: using embedded metal library
0.00.072.165 I ggml_metal_init: GPU name:   Apple M4
0.00.072.167 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.072.167 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.072.168 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.072.168 I ggml_metal_init: simdgroup reduction   = true
0.00.072.168 I ggml_metal_init: simdgroup matrix mul. = true
0.00.072.168 I ggml_metal_init: has bfloat            = true
0.00.072.168 I ggml_metal_init: use bfloat            = true
0.00.072.169 I ggml_metal_init: hasUnifiedMemory      = true
0.00.072.171 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.080.619 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.080.623 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.080.636 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.081.518 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.081.520 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.081.520 I llama_new_context_with_model: graph nodes  = 967
0.00.081.520 I llama_new_context_with_model: graph splits = 2
0.00.081.533 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.926.719 I 
0.00.926.744 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.926.748 I perplexity: tokenizing the input ..
0.00.935.013 I perplexity: tokenization took 8.264 ms
0.00.935.024 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.057.241 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.058.672 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.058.695 I llama_perf_context_print:        load time =     914.79 ms
0.01.058.696 I llama_perf_context_print: prompt eval time =     121.99 ms /   128 tokens (    0.95 ms per token,  1049.30 tokens per second)
0.01.058.697 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.058.697 I llama_perf_context_print:       total time =     131.98 ms /   129 tokens
0.01.059.283 I ggml_metal_free: deallocating

real	0m1.078s
user	0m0.095s
sys	0m0.157s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4223 (a3a3048e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.068 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.942 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.946 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.953 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.954 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.954 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.955 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.955 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.956 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.956 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.956 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.957 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.957 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.957 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.958 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.959 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.960 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.960 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.738 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.829 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.643 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.645 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.645 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.645 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.646 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.646 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.646 I llama_model_loader: - type  f32:  194 tensors
0.00.024.647 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.647 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.617 I llm_load_vocab: special tokens cache size = 25
0.00.051.618 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.621 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.621 I llm_load_print_meta: arch             = gptneox
0.00.051.621 I llm_load_print_meta: vocab type       = BPE
0.00.051.621 I llm_load_print_meta: n_vocab          = 50304
0.00.051.622 I llm_load_print_meta: n_merges         = 50009
0.00.051.622 I llm_load_print_meta: vocab_only       = 0
0.00.051.622 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.622 I llm_load_print_meta: n_embd           = 2048
0.00.051.622 I llm_load_print_meta: n_layer          = 24
0.00.051.626 I llm_load_print_meta: n_head           = 16
0.00.051.626 I llm_load_print_meta: n_head_kv        = 16
0.00.051.627 I llm_load_print_meta: n_rot            = 32
0.00.051.627 I llm_load_print_meta: n_swa            = 0
0.00.051.627 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.627 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.628 I llm_load_print_meta: n_gqa            = 1
0.00.051.629 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.630 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.630 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.631 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.631 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.632 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.639 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.648 I llm_load_print_meta: n_ff             = 8192
0.00.051.648 I llm_load_print_meta: n_expert         = 0
0.00.051.648 I llm_load_print_meta: n_expert_used    = 0
0.00.051.648 I llm_load_print_meta: causal attn      = 1
0.00.051.648 I llm_load_print_meta: pooling type     = 0
0.00.051.648 I llm_load_print_meta: rope type        = 2
0.00.051.649 I llm_load_print_meta: rope scaling     = linear
0.00.051.650 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.650 I llm_load_print_meta: freq_scale_train = 1
0.00.051.650 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.651 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.651 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.651 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.651 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.651 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.651 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.663 I llm_load_print_meta: model type       = 1.4B
0.00.051.664 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.664 I llm_load_print_meta: model params     = 1.41 B
0.00.051.665 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.665 I llm_load_print_meta: general.name     = 1.4B
0.00.051.665 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.665 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.665 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.665 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.666 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.666 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.666 I llm_load_print_meta: max token length = 1024
0.00.053.564 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.564 I llm_load_tensors: offloading output layer to GPU
0.00.053.564 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.574 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.576 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.054.482 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.483 I llama_new_context_with_model: n_ctx         = 128
0.00.054.483 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.483 I llama_new_context_with_model: n_batch       = 128
0.00.054.483 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.483 I llama_new_context_with_model: flash_attn    = 0
0.00.054.484 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.484 I llama_new_context_with_model: freq_scale    = 1
0.00.054.484 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.485 I ggml_metal_init: allocating
0.00.054.488 I ggml_metal_init: found device: Apple M4
0.00.054.490 I ggml_metal_init: picking default device: Apple M4
0.00.055.022 I ggml_metal_init: using embedded metal library
0.00.056.961 I ggml_metal_init: GPU name:   Apple M4
0.00.056.962 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.962 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.963 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.963 I ggml_metal_init: simdgroup reduction   = true
0.00.056.963 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.963 I ggml_metal_init: has bfloat            = true
0.00.056.963 I ggml_metal_init: use bfloat            = true
0.00.056.964 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.965 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.267 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.270 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.285 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.229 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.230 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.230 I llama_new_context_with_model: graph nodes  = 967
0.00.067.230 I llama_new_context_with_model: graph splits = 2
0.00.067.242 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.626.895 I 
0.00.626.926 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.626.930 I perplexity: tokenizing the input ..
0.00.635.158 I perplexity: tokenization took 8.226 ms
0.00.635.169 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.757.328 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.758.692 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.758.704 I llama_perf_context_print:        load time =     616.82 ms
0.00.758.705 I llama_perf_context_print: prompt eval time =     121.93 ms /   128 tokens (    0.95 ms per token,  1049.77 tokens per second)
0.00.758.708 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.758.708 I llama_perf_context_print:       total time =     131.81 ms /   129 tokens
0.00.759.214 I ggml_metal_free: deallocating

real	0m0.777s
user	0m0.079s
sys	0m0.109s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4223 (a3a3048e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.628 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.683 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.686 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.688 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.689 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.689 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.690 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.690 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.691 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.691 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.692 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.692 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.692 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.693 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.693 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.694 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.695 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.695 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.459 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.519 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.279 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.280 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.281 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.281 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.281 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.282 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.282 I llama_model_loader: - type  f32:  194 tensors
0.00.023.283 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.283 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.161 I llm_load_vocab: special tokens cache size = 25
0.00.049.143 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.145 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.146 I llm_load_print_meta: arch             = gptneox
0.00.049.146 I llm_load_print_meta: vocab type       = BPE
0.00.049.146 I llm_load_print_meta: n_vocab          = 50304
0.00.049.146 I llm_load_print_meta: n_merges         = 50009
0.00.049.147 I llm_load_print_meta: vocab_only       = 0
0.00.049.147 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.147 I llm_load_print_meta: n_embd           = 2048
0.00.049.147 I llm_load_print_meta: n_layer          = 24
0.00.049.150 I llm_load_print_meta: n_head           = 16
0.00.049.151 I llm_load_print_meta: n_head_kv        = 16
0.00.049.151 I llm_load_print_meta: n_rot            = 32
0.00.049.151 I llm_load_print_meta: n_swa            = 0
0.00.049.151 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.151 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.152 I llm_load_print_meta: n_gqa            = 1
0.00.049.153 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.154 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.154 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.155 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.155 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.155 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.155 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.156 I llm_load_print_meta: n_ff             = 8192
0.00.049.156 I llm_load_print_meta: n_expert         = 0
0.00.049.156 I llm_load_print_meta: n_expert_used    = 0
0.00.049.156 I llm_load_print_meta: causal attn      = 1
0.00.049.157 I llm_load_print_meta: pooling type     = 0
0.00.049.157 I llm_load_print_meta: rope type        = 2
0.00.049.157 I llm_load_print_meta: rope scaling     = linear
0.00.049.159 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.159 I llm_load_print_meta: freq_scale_train = 1
0.00.049.159 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.159 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.160 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.160 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.160 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.160 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.160 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.171 I llm_load_print_meta: model type       = 1.4B
0.00.049.172 I llm_load_print_meta: model ftype      = Q4_1
0.00.049.172 I llm_load_print_meta: model params     = 1.41 B
0.00.049.172 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.049.173 I llm_load_print_meta: general.name     = 1.4B
0.00.049.173 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.173 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.173 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.173 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.174 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.174 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.174 I llm_load_print_meta: max token length = 1024
0.00.050.687 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.688 I llm_load_tensors: offloading output layer to GPU
0.00.050.688 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.697 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.050.698 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.051.522 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.523 I llama_new_context_with_model: n_ctx         = 128
0.00.051.523 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.523 I llama_new_context_with_model: n_batch       = 128
0.00.051.523 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.523 I llama_new_context_with_model: flash_attn    = 0
0.00.051.524 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.524 I llama_new_context_with_model: freq_scale    = 1
0.00.051.525 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.525 I ggml_metal_init: allocating
0.00.051.531 I ggml_metal_init: found device: Apple M4
0.00.051.534 I ggml_metal_init: picking default device: Apple M4
0.00.052.059 I ggml_metal_init: using embedded metal library
0.00.054.003 I ggml_metal_init: GPU name:   Apple M4
0.00.054.004 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.005 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.005 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.005 I ggml_metal_init: simdgroup reduction   = true
0.00.054.006 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.006 I ggml_metal_init: has bfloat            = true
0.00.054.006 I ggml_metal_init: use bfloat            = true
0.00.054.006 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.007 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.910 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.062.912 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.062.925 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.063.766 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.063.767 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.063.767 I llama_new_context_with_model: graph nodes  = 967
0.00.063.767 I llama_new_context_with_model: graph splits = 2
0.00.063.780 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.607.971 I 
0.00.608.011 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.608.017 I perplexity: tokenizing the input ..
0.00.615.842 I perplexity: tokenization took 7.821 ms
0.00.615.853 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.738.227 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.739.671 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.739.684 I llama_perf_context_print:        load time =     599.34 ms
0.00.739.685 I llama_perf_context_print: prompt eval time =     122.15 ms /   128 tokens (    0.95 ms per token,  1047.92 tokens per second)
0.00.739.686 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.739.686 I llama_perf_context_print:       total time =     131.72 ms /   129 tokens
0.00.740.110 I ggml_metal_free: deallocating

real	0m0.753s
user	0m0.077s
sys	0m0.110s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4223 (a3a3048e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.537 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.298 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.302 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.304 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.305 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.305 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.305 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.305 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.306 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.307 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.307 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.307 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.308 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.308 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.309 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.310 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.311 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.311 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.133 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.181 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.062 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.063 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.064 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.064 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.064 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.064 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.065 I llama_model_loader: - type  f32:  194 tensors
0.00.024.065 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.065 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.070 I llm_load_vocab: special tokens cache size = 25
0.00.050.045 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.047 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.048 I llm_load_print_meta: arch             = gptneox
0.00.050.048 I llm_load_print_meta: vocab type       = BPE
0.00.050.048 I llm_load_print_meta: n_vocab          = 50304
0.00.050.048 I llm_load_print_meta: n_merges         = 50009
0.00.050.049 I llm_load_print_meta: vocab_only       = 0
0.00.050.049 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.049 I llm_load_print_meta: n_embd           = 2048
0.00.050.049 I llm_load_print_meta: n_layer          = 24
0.00.050.052 I llm_load_print_meta: n_head           = 16
0.00.050.053 I llm_load_print_meta: n_head_kv        = 16
0.00.050.053 I llm_load_print_meta: n_rot            = 32
0.00.050.053 I llm_load_print_meta: n_swa            = 0
0.00.050.053 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.053 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.054 I llm_load_print_meta: n_gqa            = 1
0.00.050.055 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.055 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.056 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.056 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.056 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.057 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.057 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.057 I llm_load_print_meta: n_ff             = 8192
0.00.050.058 I llm_load_print_meta: n_expert         = 0
0.00.050.058 I llm_load_print_meta: n_expert_used    = 0
0.00.050.058 I llm_load_print_meta: causal attn      = 1
0.00.050.058 I llm_load_print_meta: pooling type     = 0
0.00.050.058 I llm_load_print_meta: rope type        = 2
0.00.050.059 I llm_load_print_meta: rope scaling     = linear
0.00.050.059 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.060 I llm_load_print_meta: freq_scale_train = 1
0.00.050.061 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.063 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.063 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.063 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.063 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.063 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.063 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.070 I llm_load_print_meta: model type       = 1.4B
0.00.050.071 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.071 I llm_load_print_meta: model params     = 1.41 B
0.00.050.073 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.073 I llm_load_print_meta: general.name     = 1.4B
0.00.050.074 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.074 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.074 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.074 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.075 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.075 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.075 I llm_load_print_meta: max token length = 1024
0.00.051.818 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.818 I llm_load_tensors: offloading output layer to GPU
0.00.051.819 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.823 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.051.824 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.052.701 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.702 I llama_new_context_with_model: n_ctx         = 128
0.00.052.702 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.702 I llama_new_context_with_model: n_batch       = 128
0.00.052.702 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.703 I llama_new_context_with_model: flash_attn    = 0
0.00.052.703 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.703 I llama_new_context_with_model: freq_scale    = 1
0.00.052.704 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.704 I ggml_metal_init: allocating
0.00.052.710 I ggml_metal_init: found device: Apple M4
0.00.052.712 I ggml_metal_init: picking default device: Apple M4
0.00.053.244 I ggml_metal_init: using embedded metal library
0.00.055.225 I ggml_metal_init: GPU name:   Apple M4
0.00.055.226 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.227 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.227 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.227 I ggml_metal_init: simdgroup reduction   = true
0.00.055.228 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.228 I ggml_metal_init: has bfloat            = true
0.00.055.228 I ggml_metal_init: use bfloat            = true
0.00.055.228 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.229 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.243 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.248 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.264 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.087 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.089 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.089 I llama_new_context_with_model: graph nodes  = 967
0.00.065.089 I llama_new_context_with_model: graph splits = 2
0.00.065.096 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.734.878 I 
0.00.734.915 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.734.920 I perplexity: tokenizing the input ..
0.00.743.163 I perplexity: tokenization took 8.242 ms
0.00.743.178 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.878.602 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.879.948 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.879.968 I llama_perf_context_print:        load time =     725.34 ms
0.00.879.969 I llama_perf_context_print: prompt eval time =     135.20 ms /   128 tokens (    1.06 ms per token,   946.77 tokens per second)
0.00.879.970 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.879.970 I llama_perf_context_print:       total time =     145.09 ms /   129 tokens
0.00.880.402 I ggml_metal_free: deallocating

real	0m0.897s
user	0m0.078s
sys	0m0.128s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4223 (a3a3048e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.231 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.182 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.186 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.193 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.193 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.194 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.194 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.194 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.195 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.197 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.198 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.198 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.198 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.199 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.199 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.201 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.201 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.201 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.089 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.131 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.042 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.043 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.043 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.044 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.044 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.044 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.045 I llama_model_loader: - type  f32:  194 tensors
0.00.024.045 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.045 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.038 I llm_load_vocab: special tokens cache size = 25
0.00.051.034 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.037 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.037 I llm_load_print_meta: arch             = gptneox
0.00.051.037 I llm_load_print_meta: vocab type       = BPE
0.00.051.038 I llm_load_print_meta: n_vocab          = 50304
0.00.051.038 I llm_load_print_meta: n_merges         = 50009
0.00.051.038 I llm_load_print_meta: vocab_only       = 0
0.00.051.038 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.038 I llm_load_print_meta: n_embd           = 2048
0.00.051.038 I llm_load_print_meta: n_layer          = 24
0.00.051.041 I llm_load_print_meta: n_head           = 16
0.00.051.042 I llm_load_print_meta: n_head_kv        = 16
0.00.051.042 I llm_load_print_meta: n_rot            = 32
0.00.051.043 I llm_load_print_meta: n_swa            = 0
0.00.051.043 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.045 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.045 I llm_load_print_meta: n_gqa            = 1
0.00.051.046 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.047 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.047 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.048 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.048 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.048 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.048 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.049 I llm_load_print_meta: n_ff             = 8192
0.00.051.049 I llm_load_print_meta: n_expert         = 0
0.00.051.049 I llm_load_print_meta: n_expert_used    = 0
0.00.051.049 I llm_load_print_meta: causal attn      = 1
0.00.051.049 I llm_load_print_meta: pooling type     = 0
0.00.051.050 I llm_load_print_meta: rope type        = 2
0.00.051.050 I llm_load_print_meta: rope scaling     = linear
0.00.051.051 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.055 I llm_load_print_meta: freq_scale_train = 1
0.00.051.055 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.056 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.056 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.056 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.057 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.057 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.058 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.069 I llm_load_print_meta: model type       = 1.4B
0.00.051.070 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.070 I llm_load_print_meta: model params     = 1.41 B
0.00.051.071 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.071 I llm_load_print_meta: general.name     = 1.4B
0.00.051.071 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.071 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.071 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.072 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.072 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.072 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.072 I llm_load_print_meta: max token length = 1024
0.00.053.096 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.096 I llm_load_tensors: offloading output layer to GPU
0.00.053.096 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.107 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.108 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.019 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.020 I llama_new_context_with_model: n_ctx         = 128
0.00.054.020 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.020 I llama_new_context_with_model: n_batch       = 128
0.00.054.020 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.020 I llama_new_context_with_model: flash_attn    = 0
0.00.054.021 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.021 I llama_new_context_with_model: freq_scale    = 1
0.00.054.021 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.022 I ggml_metal_init: allocating
0.00.054.027 I ggml_metal_init: found device: Apple M4
0.00.054.030 I ggml_metal_init: picking default device: Apple M4
0.00.054.612 I ggml_metal_init: using embedded metal library
0.00.056.544 I ggml_metal_init: GPU name:   Apple M4
0.00.056.545 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.545 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.546 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.546 I ggml_metal_init: simdgroup reduction   = true
0.00.056.546 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.546 I ggml_metal_init: has bfloat            = true
0.00.056.546 I ggml_metal_init: use bfloat            = true
0.00.056.547 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.547 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.722 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.726 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.741 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.563 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.564 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.564 I llama_new_context_with_model: graph nodes  = 967
0.00.066.564 I llama_new_context_with_model: graph splits = 2
0.00.066.577 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.727.342 I 
0.00.727.441 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.727.450 I perplexity: tokenizing the input ..
0.00.735.313 I perplexity: tokenization took 7.861 ms
0.00.735.326 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.870.282 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.871.665 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.871.679 I llama_perf_context_print:        load time =     718.10 ms
0.00.871.680 I llama_perf_context_print: prompt eval time =     134.71 ms /   128 tokens (    1.05 ms per token,   950.16 tokens per second)
0.00.871.681 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.871.682 I llama_perf_context_print:       total time =     144.35 ms /   129 tokens
0.00.872.066 I ggml_metal_free: deallocating

real	0m0.885s
user	0m0.078s
sys	0m0.116s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4223 (a3a3048e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.873 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.379 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.383 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.385 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.385 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.386 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.386 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.386 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.387 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.388 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.388 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.388 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.388 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.389 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.389 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.391 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.391 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.391 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.213 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.271 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.043 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.045 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.045 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.045 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.045 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.046 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.046 I llama_model_loader: - type  f32:  194 tensors
0.00.024.047 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.047 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.047 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.766 I llm_load_vocab: special tokens cache size = 25
0.00.050.802 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.804 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.805 I llm_load_print_meta: arch             = gptneox
0.00.050.805 I llm_load_print_meta: vocab type       = BPE
0.00.050.805 I llm_load_print_meta: n_vocab          = 50304
0.00.050.805 I llm_load_print_meta: n_merges         = 50009
0.00.050.806 I llm_load_print_meta: vocab_only       = 0
0.00.050.806 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.806 I llm_load_print_meta: n_embd           = 2048
0.00.050.806 I llm_load_print_meta: n_layer          = 24
0.00.050.809 I llm_load_print_meta: n_head           = 16
0.00.050.810 I llm_load_print_meta: n_head_kv        = 16
0.00.050.810 I llm_load_print_meta: n_rot            = 32
0.00.050.810 I llm_load_print_meta: n_swa            = 0
0.00.050.812 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.812 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.814 I llm_load_print_meta: n_gqa            = 1
0.00.050.814 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.815 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.816 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.816 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.816 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.816 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.816 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.823 I llm_load_print_meta: n_ff             = 8192
0.00.050.824 I llm_load_print_meta: n_expert         = 0
0.00.050.824 I llm_load_print_meta: n_expert_used    = 0
0.00.050.824 I llm_load_print_meta: causal attn      = 1
0.00.050.824 I llm_load_print_meta: pooling type     = 0
0.00.050.825 I llm_load_print_meta: rope type        = 2
0.00.050.825 I llm_load_print_meta: rope scaling     = linear
0.00.050.827 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.827 I llm_load_print_meta: freq_scale_train = 1
0.00.050.827 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.827 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.827 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.827 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.828 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.829 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.829 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.841 I llm_load_print_meta: model type       = 1.4B
0.00.050.842 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.842 I llm_load_print_meta: model params     = 1.41 B
0.00.050.842 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.843 I llm_load_print_meta: general.name     = 1.4B
0.00.050.843 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.843 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.843 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.843 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.843 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.844 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.845 I llm_load_print_meta: max token length = 1024
0.00.052.673 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.673 I llm_load_tensors: offloading output layer to GPU
0.00.052.674 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.684 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.685 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.589 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.590 I llama_new_context_with_model: n_ctx         = 128
0.00.053.590 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.590 I llama_new_context_with_model: n_batch       = 128
0.00.053.590 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.591 I llama_new_context_with_model: flash_attn    = 0
0.00.053.591 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.591 I llama_new_context_with_model: freq_scale    = 1
0.00.053.592 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.592 I ggml_metal_init: allocating
0.00.053.595 I ggml_metal_init: found device: Apple M4
0.00.053.597 I ggml_metal_init: picking default device: Apple M4
0.00.054.140 I ggml_metal_init: using embedded metal library
0.00.056.078 I ggml_metal_init: GPU name:   Apple M4
0.00.056.080 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.080 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.081 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.081 I ggml_metal_init: simdgroup reduction   = true
0.00.056.081 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.081 I ggml_metal_init: has bfloat            = true
0.00.056.081 I ggml_metal_init: use bfloat            = true
0.00.056.082 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.083 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.990 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.993 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.009 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.884 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.885 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.885 I llama_new_context_with_model: graph nodes  = 967
0.00.065.886 I llama_new_context_with_model: graph splits = 2
0.00.065.897 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.470.593 I 
0.00.470.643 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.470.650 I perplexity: tokenizing the input ..
0.00.478.536 I perplexity: tokenization took 7.885 ms
0.00.478.547 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.611.081 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.612.407 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.612.424 I llama_perf_context_print:        load time =     460.71 ms
0.00.612.425 I llama_perf_context_print: prompt eval time =     132.29 ms /   128 tokens (    1.03 ms per token,   967.55 tokens per second)
0.00.612.426 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.612.427 I llama_perf_context_print:       total time =     141.83 ms /   129 tokens
0.00.612.869 I ggml_metal_free: deallocating

real	0m0.631s
user	0m0.078s
sys	0m0.086s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4223 (a3a3048e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.355 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.159 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.164 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.166 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.166 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.167 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.167 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.167 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.168 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.168 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.169 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.170 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.171 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.171 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.171 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.174 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.174 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.175 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.012 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.075 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.876 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.877 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.877 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.878 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.878 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.878 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.879 I llama_model_loader: - type  f32:  194 tensors
0.00.024.879 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.880 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.880 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.880 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.566 I llm_load_vocab: special tokens cache size = 25
0.00.051.388 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.391 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.391 I llm_load_print_meta: arch             = gptneox
0.00.051.392 I llm_load_print_meta: vocab type       = BPE
0.00.051.392 I llm_load_print_meta: n_vocab          = 50304
0.00.051.392 I llm_load_print_meta: n_merges         = 50009
0.00.051.392 I llm_load_print_meta: vocab_only       = 0
0.00.051.392 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.392 I llm_load_print_meta: n_embd           = 2048
0.00.051.393 I llm_load_print_meta: n_layer          = 24
0.00.051.396 I llm_load_print_meta: n_head           = 16
0.00.051.397 I llm_load_print_meta: n_head_kv        = 16
0.00.051.397 I llm_load_print_meta: n_rot            = 32
0.00.051.397 I llm_load_print_meta: n_swa            = 0
0.00.051.397 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.397 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.398 I llm_load_print_meta: n_gqa            = 1
0.00.051.399 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.400 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.400 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.401 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.401 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.401 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.401 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.402 I llm_load_print_meta: n_ff             = 8192
0.00.051.402 I llm_load_print_meta: n_expert         = 0
0.00.051.402 I llm_load_print_meta: n_expert_used    = 0
0.00.051.402 I llm_load_print_meta: causal attn      = 1
0.00.051.403 I llm_load_print_meta: pooling type     = 0
0.00.051.403 I llm_load_print_meta: rope type        = 2
0.00.051.403 I llm_load_print_meta: rope scaling     = linear
0.00.051.403 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.404 I llm_load_print_meta: freq_scale_train = 1
0.00.051.404 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.404 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.404 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.404 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.405 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.405 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.405 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.417 I llm_load_print_meta: model type       = 1.4B
0.00.051.417 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.418 I llm_load_print_meta: model params     = 1.41 B
0.00.051.418 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.418 I llm_load_print_meta: general.name     = 1.4B
0.00.051.419 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.419 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.419 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.419 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.419 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.420 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.420 I llm_load_print_meta: max token length = 1024
0.00.053.267 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.267 I llm_load_tensors: offloading output layer to GPU
0.00.053.267 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.277 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.278 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.161 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.161 I llama_new_context_with_model: n_ctx         = 128
0.00.054.162 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.162 I llama_new_context_with_model: n_batch       = 128
0.00.054.162 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.162 I llama_new_context_with_model: flash_attn    = 0
0.00.054.162 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.163 I llama_new_context_with_model: freq_scale    = 1
0.00.054.163 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.163 I ggml_metal_init: allocating
0.00.054.167 I ggml_metal_init: found device: Apple M4
0.00.054.169 I ggml_metal_init: picking default device: Apple M4
0.00.054.740 I ggml_metal_init: using embedded metal library
0.00.056.661 I ggml_metal_init: GPU name:   Apple M4
0.00.056.662 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.662 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.663 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.663 I ggml_metal_init: simdgroup reduction   = true
0.00.056.663 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.663 I ggml_metal_init: has bfloat            = true
0.00.056.663 I ggml_metal_init: use bfloat            = true
0.00.056.664 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.664 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.607 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.611 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.624 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.485 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.486 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.486 I llama_new_context_with_model: graph nodes  = 967
0.00.066.486 I llama_new_context_with_model: graph splits = 2
0.00.066.499 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.520.462 I 
0.00.520.500 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.520.507 I perplexity: tokenizing the input ..
0.00.528.777 I perplexity: tokenization took 8.267 ms
0.00.528.788 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.660.247 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.661.582 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.661.597 I llama_perf_context_print:        load time =     510.10 ms
0.00.661.598 I llama_perf_context_print: prompt eval time =     131.23 ms /   128 tokens (    1.03 ms per token,   975.37 tokens per second)
0.00.661.599 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.661.600 I llama_perf_context_print:       total time =     141.14 ms /   129 tokens
0.00.661.918 I ggml_metal_free: deallocating

real	0m0.677s
user	0m0.079s
sys	0m0.095s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4223 (a3a3048e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.650 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.423 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.427 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.432 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.432 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.432 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.434 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.434 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.435 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.436 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.436 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.436 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.437 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.437 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.438 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.439 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.439 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.440 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.229 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.288 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.059 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.060 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.060 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.061 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.061 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.061 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.062 I llama_model_loader: - type  f32:  194 tensors
0.00.024.062 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.063 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.063 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.625 I llm_load_vocab: special tokens cache size = 25
0.00.050.535 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.538 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.539 I llm_load_print_meta: arch             = gptneox
0.00.050.539 I llm_load_print_meta: vocab type       = BPE
0.00.050.539 I llm_load_print_meta: n_vocab          = 50304
0.00.050.539 I llm_load_print_meta: n_merges         = 50009
0.00.050.540 I llm_load_print_meta: vocab_only       = 0
0.00.050.540 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.540 I llm_load_print_meta: n_embd           = 2048
0.00.050.540 I llm_load_print_meta: n_layer          = 24
0.00.050.543 I llm_load_print_meta: n_head           = 16
0.00.050.544 I llm_load_print_meta: n_head_kv        = 16
0.00.050.544 I llm_load_print_meta: n_rot            = 32
0.00.050.544 I llm_load_print_meta: n_swa            = 0
0.00.050.544 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.544 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.547 I llm_load_print_meta: n_gqa            = 1
0.00.050.548 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.548 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.549 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.549 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.550 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.551 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.551 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.552 I llm_load_print_meta: n_ff             = 8192
0.00.050.552 I llm_load_print_meta: n_expert         = 0
0.00.050.552 I llm_load_print_meta: n_expert_used    = 0
0.00.050.552 I llm_load_print_meta: causal attn      = 1
0.00.050.553 I llm_load_print_meta: pooling type     = 0
0.00.050.553 I llm_load_print_meta: rope type        = 2
0.00.050.553 I llm_load_print_meta: rope scaling     = linear
0.00.050.553 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.554 I llm_load_print_meta: freq_scale_train = 1
0.00.050.554 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.554 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.554 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.554 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.554 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.555 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.555 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.566 I llm_load_print_meta: model type       = 1.4B
0.00.050.566 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.567 I llm_load_print_meta: model params     = 1.41 B
0.00.050.567 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.567 I llm_load_print_meta: general.name     = 1.4B
0.00.050.567 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.568 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.569 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.569 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.570 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.570 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.570 I llm_load_print_meta: max token length = 1024
0.00.052.056 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.057 I llm_load_tensors: offloading output layer to GPU
0.00.052.057 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.066 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.067 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.896 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.897 I llama_new_context_with_model: n_ctx         = 128
0.00.052.897 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.897 I llama_new_context_with_model: n_batch       = 128
0.00.052.897 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.897 I llama_new_context_with_model: flash_attn    = 0
0.00.052.898 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.898 I llama_new_context_with_model: freq_scale    = 1
0.00.052.898 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.899 I ggml_metal_init: allocating
0.00.052.903 I ggml_metal_init: found device: Apple M4
0.00.052.905 I ggml_metal_init: picking default device: Apple M4
0.00.053.439 I ggml_metal_init: using embedded metal library
0.00.055.330 I ggml_metal_init: GPU name:   Apple M4
0.00.055.331 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.331 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.332 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.332 I ggml_metal_init: simdgroup reduction   = true
0.00.055.332 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.332 I ggml_metal_init: has bfloat            = true
0.00.055.332 I ggml_metal_init: use bfloat            = true
0.00.055.333 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.333 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.618 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.620 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.633 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.509 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.510 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.511 I llama_new_context_with_model: graph nodes  = 967
0.00.065.511 I llama_new_context_with_model: graph splits = 2
0.00.065.524 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.575.873 I 
0.00.575.900 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.575.903 I perplexity: tokenizing the input ..
0.00.583.876 I perplexity: tokenization took 7.971 ms
0.00.583.887 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.718.395 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.719.745 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.719.766 I llama_perf_context_print:        load time =     566.22 ms
0.00.719.767 I llama_perf_context_print: prompt eval time =     134.27 ms /   128 tokens (    1.05 ms per token,   953.29 tokens per second)
0.00.719.768 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.719.769 I llama_perf_context_print:       total time =     143.89 ms /   129 tokens
0.00.720.191 I ggml_metal_free: deallocating

real	0m0.736s
user	0m0.078s
sys	0m0.109s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4223 (a3a3048e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.568 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.372 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.376 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.378 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.379 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.379 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.379 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.380 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.381 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.381 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.381 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.382 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.382 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.382 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.383 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.384 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.385 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.385 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.285 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.361 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.195 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.197 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.197 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.197 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.198 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.198 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.198 I llama_model_loader: - type  f32:  194 tensors
0.00.023.199 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.199 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.011 I llm_load_vocab: special tokens cache size = 25
0.00.050.162 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.165 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.165 I llm_load_print_meta: arch             = gptneox
0.00.050.166 I llm_load_print_meta: vocab type       = BPE
0.00.050.166 I llm_load_print_meta: n_vocab          = 50304
0.00.050.166 I llm_load_print_meta: n_merges         = 50009
0.00.050.166 I llm_load_print_meta: vocab_only       = 0
0.00.050.166 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.166 I llm_load_print_meta: n_embd           = 2048
0.00.050.167 I llm_load_print_meta: n_layer          = 24
0.00.050.169 I llm_load_print_meta: n_head           = 16
0.00.050.170 I llm_load_print_meta: n_head_kv        = 16
0.00.050.171 I llm_load_print_meta: n_rot            = 32
0.00.050.171 I llm_load_print_meta: n_swa            = 0
0.00.050.171 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.171 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.172 I llm_load_print_meta: n_gqa            = 1
0.00.050.174 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.175 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.175 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.175 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.176 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.176 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.176 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.177 I llm_load_print_meta: n_ff             = 8192
0.00.050.177 I llm_load_print_meta: n_expert         = 0
0.00.050.177 I llm_load_print_meta: n_expert_used    = 0
0.00.050.177 I llm_load_print_meta: causal attn      = 1
0.00.050.177 I llm_load_print_meta: pooling type     = 0
0.00.050.178 I llm_load_print_meta: rope type        = 2
0.00.050.178 I llm_load_print_meta: rope scaling     = linear
0.00.050.178 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.178 I llm_load_print_meta: freq_scale_train = 1
0.00.050.179 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.179 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.179 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.179 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.179 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.180 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.180 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.192 I llm_load_print_meta: model type       = 1.4B
0.00.050.194 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.194 I llm_load_print_meta: model params     = 1.41 B
0.00.050.195 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.195 I llm_load_print_meta: general.name     = 1.4B
0.00.050.195 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.195 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.195 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.195 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.196 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.196 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.196 I llm_load_print_meta: max token length = 1024
0.00.052.223 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.224 I llm_load_tensors: offloading output layer to GPU
0.00.052.224 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.234 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.235 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.180 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.181 I llama_new_context_with_model: n_ctx         = 128
0.00.053.181 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.181 I llama_new_context_with_model: n_batch       = 128
0.00.053.182 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.182 I llama_new_context_with_model: flash_attn    = 0
0.00.053.182 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.182 I llama_new_context_with_model: freq_scale    = 1
0.00.053.183 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.183 I ggml_metal_init: allocating
0.00.053.189 I ggml_metal_init: found device: Apple M4
0.00.053.191 I ggml_metal_init: picking default device: Apple M4
0.00.053.716 I ggml_metal_init: using embedded metal library
0.00.055.669 I ggml_metal_init: GPU name:   Apple M4
0.00.055.670 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.671 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.671 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.671 I ggml_metal_init: simdgroup reduction   = true
0.00.055.671 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.671 I ggml_metal_init: has bfloat            = true
0.00.055.672 I ggml_metal_init: use bfloat            = true
0.00.055.672 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.672 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.754 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.757 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.780 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.692 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.693 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.693 I llama_new_context_with_model: graph nodes  = 967
0.00.065.693 I llama_new_context_with_model: graph splits = 2
0.00.065.706 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.650.656 I 
0.00.650.697 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.650.702 I perplexity: tokenizing the input ..
0.00.659.061 I perplexity: tokenization took 8.357 ms
0.00.659.076 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.799.880 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.801.290 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.801.307 I llama_perf_context_print:        load time =     642.08 ms
0.00.801.308 I llama_perf_context_print: prompt eval time =     140.57 ms /   128 tokens (    1.10 ms per token,   910.57 tokens per second)
0.00.801.309 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.801.309 I llama_perf_context_print:       total time =     150.65 ms /   129 tokens
0.00.801.637 I ggml_metal_free: deallocating

real	0m0.814s
user	0m0.079s
sys	0m0.125s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4223 (a3a3048e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.872 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.510 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.514 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.516 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.521 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.522 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.522 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.522 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.523 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.525 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.525 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.525 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.526 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.526 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.526 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.528 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.528 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.528 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.427 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.503 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.318 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.319 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.319 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.320 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.320 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.320 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.321 I llama_model_loader: - type  f32:  194 tensors
0.00.024.321 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.231 I llm_load_vocab: special tokens cache size = 25
0.00.050.003 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.006 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.006 I llm_load_print_meta: arch             = gptneox
0.00.050.007 I llm_load_print_meta: vocab type       = BPE
0.00.050.007 I llm_load_print_meta: n_vocab          = 50304
0.00.050.007 I llm_load_print_meta: n_merges         = 50009
0.00.050.007 I llm_load_print_meta: vocab_only       = 0
0.00.050.007 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.008 I llm_load_print_meta: n_embd           = 2048
0.00.050.008 I llm_load_print_meta: n_layer          = 24
0.00.050.010 I llm_load_print_meta: n_head           = 16
0.00.050.011 I llm_load_print_meta: n_head_kv        = 16
0.00.050.011 I llm_load_print_meta: n_rot            = 32
0.00.050.012 I llm_load_print_meta: n_swa            = 0
0.00.050.012 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.012 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.013 I llm_load_print_meta: n_gqa            = 1
0.00.050.013 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.014 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.015 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.015 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.015 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.015 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.015 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.016 I llm_load_print_meta: n_ff             = 8192
0.00.050.018 I llm_load_print_meta: n_expert         = 0
0.00.050.020 I llm_load_print_meta: n_expert_used    = 0
0.00.050.020 I llm_load_print_meta: causal attn      = 1
0.00.050.020 I llm_load_print_meta: pooling type     = 0
0.00.050.020 I llm_load_print_meta: rope type        = 2
0.00.050.020 I llm_load_print_meta: rope scaling     = linear
0.00.050.021 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.021 I llm_load_print_meta: freq_scale_train = 1
0.00.050.021 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.021 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.022 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.022 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.022 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.022 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.022 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.034 I llm_load_print_meta: model type       = 1.4B
0.00.050.035 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.035 I llm_load_print_meta: model params     = 1.41 B
0.00.050.035 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.035 I llm_load_print_meta: general.name     = 1.4B
0.00.050.036 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.036 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.036 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.037 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.038 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.038 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.038 I llm_load_print_meta: max token length = 1024
0.00.052.047 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.047 I llm_load_tensors: offloading output layer to GPU
0.00.052.047 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.057 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.058 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.959 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.960 I llama_new_context_with_model: n_ctx         = 128
0.00.052.960 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.960 I llama_new_context_with_model: n_batch       = 128
0.00.052.960 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.961 I llama_new_context_with_model: flash_attn    = 0
0.00.052.961 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.961 I llama_new_context_with_model: freq_scale    = 1
0.00.052.962 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.962 I ggml_metal_init: allocating
0.00.052.965 I ggml_metal_init: found device: Apple M4
0.00.052.967 I ggml_metal_init: picking default device: Apple M4
0.00.053.492 I ggml_metal_init: using embedded metal library
0.00.055.407 I ggml_metal_init: GPU name:   Apple M4
0.00.055.410 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.410 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.410 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.410 I ggml_metal_init: simdgroup reduction   = true
0.00.055.411 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.411 I ggml_metal_init: has bfloat            = true
0.00.055.411 I ggml_metal_init: use bfloat            = true
0.00.055.411 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.412 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.463 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.465 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.479 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.407 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.408 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.408 I llama_new_context_with_model: graph nodes  = 967
0.00.065.409 I llama_new_context_with_model: graph splits = 2
0.00.065.421 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.258.678 I 
0.00.258.703 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.258.706 I perplexity: tokenizing the input ..
0.00.266.594 I perplexity: tokenization took 7.887 ms
0.00.266.609 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.405.828 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.407.156 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.407.167 I llama_perf_context_print:        load time =     248.80 ms
0.00.407.168 I llama_perf_context_print: prompt eval time =     138.99 ms /   128 tokens (    1.09 ms per token,   920.91 tokens per second)
0.00.407.168 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.407.169 I llama_perf_context_print:       total time =     148.49 ms /   129 tokens
0.00.407.478 I ggml_metal_free: deallocating

real	0m0.424s
user	0m0.077s
sys	0m0.057s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.275 I build: 4223 (a3a3048e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.020 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.811 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.033.817 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.820 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.820 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.821 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.821 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.822 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.823 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.826 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.827 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.828 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.828 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.829 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.829 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.831 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.832 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.832 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.376 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.233 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.558 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.049.560 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.561 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.049.561 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.049.561 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.049.562 I llama_model_loader: - type  f32:  194 tensors
0.00.049.562 I llama_model_loader: - type  f16:   98 tensors
0.00.076.953 I llm_load_vocab: special tokens cache size = 25
0.00.083.419 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.083.422 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.083.422 I llm_load_print_meta: arch             = gptneox
0.00.083.422 I llm_load_print_meta: vocab type       = BPE
0.00.083.423 I llm_load_print_meta: n_vocab          = 50304
0.00.083.423 I llm_load_print_meta: n_merges         = 50009
0.00.083.423 I llm_load_print_meta: vocab_only       = 0
0.00.083.423 I llm_load_print_meta: n_ctx_train      = 2048
0.00.083.423 I llm_load_print_meta: n_embd           = 2048
0.00.083.423 I llm_load_print_meta: n_layer          = 24
0.00.083.426 I llm_load_print_meta: n_head           = 16
0.00.083.427 I llm_load_print_meta: n_head_kv        = 16
0.00.083.427 I llm_load_print_meta: n_rot            = 32
0.00.083.429 I llm_load_print_meta: n_swa            = 0
0.00.083.429 I llm_load_print_meta: n_embd_head_k    = 128
0.00.083.429 I llm_load_print_meta: n_embd_head_v    = 128
0.00.083.430 I llm_load_print_meta: n_gqa            = 1
0.00.083.431 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.083.436 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.083.436 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.083.437 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.083.437 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.083.437 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.083.437 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.083.438 I llm_load_print_meta: n_ff             = 8192
0.00.083.438 I llm_load_print_meta: n_expert         = 0
0.00.083.438 I llm_load_print_meta: n_expert_used    = 0
0.00.083.438 I llm_load_print_meta: causal attn      = 1
0.00.083.439 I llm_load_print_meta: pooling type     = 0
0.00.083.439 I llm_load_print_meta: rope type        = 2
0.00.083.440 I llm_load_print_meta: rope scaling     = linear
0.00.083.441 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.083.442 I llm_load_print_meta: freq_scale_train = 1
0.00.083.442 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.083.442 I llm_load_print_meta: rope_finetuned   = unknown
0.00.083.442 I llm_load_print_meta: ssm_d_conv       = 0
0.00.083.442 I llm_load_print_meta: ssm_d_inner      = 0
0.00.083.442 I llm_load_print_meta: ssm_d_state      = 0
0.00.083.443 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.083.443 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.083.456 I llm_load_print_meta: model type       = 1.4B
0.00.083.456 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.083.456 I llm_load_print_meta: model params     = 1.41 B
0.00.083.458 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.083.458 I llm_load_print_meta: general.name     = 1.4B
0.00.083.458 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.083.458 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.083.459 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.083.459 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.083.459 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.083.459 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.083.460 I llm_load_print_meta: max token length = 1024
0.00.085.928 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.085.929 I llm_load_tensors: offloading output layer to GPU
0.00.085.929 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.085.939 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.085.941 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.086.978 I llama_new_context_with_model: n_seq_max     = 1
0.00.086.979 I llama_new_context_with_model: n_ctx         = 128
0.00.086.979 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.086.979 I llama_new_context_with_model: n_batch       = 128
0.00.086.979 I llama_new_context_with_model: n_ubatch      = 128
0.00.086.979 I llama_new_context_with_model: flash_attn    = 0
0.00.086.980 I llama_new_context_with_model: freq_base     = 10000.0
0.00.086.980 I llama_new_context_with_model: freq_scale    = 1
0.00.086.980 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.086.981 I ggml_metal_init: allocating
0.00.086.984 I ggml_metal_init: found device: Apple M4
0.00.086.986 I ggml_metal_init: picking default device: Apple M4
0.00.087.544 I ggml_metal_init: using embedded metal library
0.00.089.605 I ggml_metal_init: GPU name:   Apple M4
0.00.089.606 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.089.607 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.089.607 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.089.607 I ggml_metal_init: simdgroup reduction   = true
0.00.089.608 I ggml_metal_init: simdgroup matrix mul. = true
0.00.089.608 I ggml_metal_init: has bfloat            = true
0.00.089.608 I ggml_metal_init: use bfloat            = true
0.00.089.608 I ggml_metal_init: hasUnifiedMemory      = true
0.00.089.609 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.098.783 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.098.785 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.098.800 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.099.732 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.099.733 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.099.734 I llama_new_context_with_model: graph nodes  = 967
0.00.099.734 I llama_new_context_with_model: graph splits = 2
0.00.099.746 I 
0.00.099.786 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.099.788 I compute_imatrix: tokenizing the input ..
0.00.106.831 I compute_imatrix: tokenization took 7.043 ms
0.00.106.833 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.567.577 I compute_imatrix: 1.46 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.570.660 I llama_perf_context_print:        load time =    1545.56 ms
0.01.570.661 I llama_perf_context_print: prompt eval time =    1460.12 ms /   128 tokens (   11.41 ms per token,    87.66 tokens per second)
0.01.570.662 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.570.662 I llama_perf_context_print:       total time =    1548.63 ms /   129 tokens
0.01.571.649 I ggml_metal_free: deallocating

real	0m1.760s
user	0m0.164s
sys	0m0.264s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4223 (a3a3048e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14c90b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14c90bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14c90c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14c90c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14c90cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14c90d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14c90da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14c90dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14c90e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14c90eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14c90efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14c90f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14c90ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14c910770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14c910f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14c9116a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14c911dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14c9124e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14c912c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14c9133d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14c913af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14c914210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14c914930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14c9151d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14c9158f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14c915bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14c9161c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14c916e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14c917370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14c917630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14c917ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14c917d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14c918620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14c918b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14c918e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14c9192c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14c919760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14c919c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14c91a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14c91a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14c91a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14c91ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14c91b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14c91b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14c91ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14c91c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14c91c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14c91cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14c91d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14c91dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14c91e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14c91e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14c91ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14c91f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14c91fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14c9200b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14c920550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14c920810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14c920e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14c921610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14c9218d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14c921d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14c922210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14c9226b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14c922b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14c922ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14c923490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14c923930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14c923dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14c924270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14c924710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14c924bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14c925050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14c9254f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14c925990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14c925e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14c9262d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14c926770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14c926c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14c9270b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14c927550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14c9279f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14c927e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14c928330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14c9287d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14c928c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14c929110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14c9295b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14c929a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14c929ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14c92a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14c92a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14c92acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14c92b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14c92b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14c92bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14c92bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14c91ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14c92c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14c92ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14c92cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14c92d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14c92d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14c92dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14c92e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14c92e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14c92eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14c92ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14c92f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14c92f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14c92fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14c9301c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14c930660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14c930b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14c930fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14c931440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14c9318e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14c931d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14c932220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14c9326c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14c932b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14c933000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14c9334a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14c933940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14c933de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14c934280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14c934720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14c934bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14c935060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14c935500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14c9359a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14c935e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14c9362e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14c936780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14c936c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14c9370c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14c937560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14c937a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14c937ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14c938340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14c9387e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14c938c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14c939120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14c9395c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14c939a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14c939f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14c93a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14c93a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14c93ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14c93b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14c93b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14c93bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14c93bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14c93c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14c93ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14c93cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14c93d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14c93d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14c93dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14c93e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14c93e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14c93efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14c93f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14c93fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14c940240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14c9406e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14c940b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14c941330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14c941880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14c941dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14c942320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14c942870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14c942dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14c943310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14c943860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14c943db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14c944300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14c944850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14c944da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14c9452f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14c945840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14c945d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14c9462e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14c946830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14c946d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14c9472d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14c947820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14c947d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14c9482c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14c948810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14c948d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14c9492b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14c949800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14c949d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14c94a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14c94a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14c94ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14c94b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14c94b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14c94bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14c94c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14c94c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14c94cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14c94d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14c94d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14c94dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14c94e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14c94e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14c94ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14c94f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14c94f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14c94fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14c950240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14c950790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14c950ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14c951230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14c951780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14c951cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14c952220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14c952770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14c952cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14c953210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14c953760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14c953cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14c954150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14c9545f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14c954a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14c954f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14c9553d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14c955870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14c955d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14c9561b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14c956650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14c956af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14c956f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14c957430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14c9578d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14c957e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14c958540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14c958c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14c959380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14c959aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14c959d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14c95a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14c95a980 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.134.658 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14ca052a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14ca05710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14ca05b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14ca05ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14ca06460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14ca068d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14ca06d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14ca071b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14ca07620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14ca07a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14ca07f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14ca085f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14ca09110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14ca098c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14ca0a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14ca0a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14ca0af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14ca0b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14ca0bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14ca0c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14ca0cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14ca0d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14ca0d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14ca0e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14ca0e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14ca0eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14ca0eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14ca0f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14ca0f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14ca0faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14ca0fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14ca10500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14ca10970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14ca10c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14ca110a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14ca11510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14ca11a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14ca11f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14ca12470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14ca12970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14ca12e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14ca13370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14ca13870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14ca13d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14ca14270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14ca146e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14ca14b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14ca14fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14ca15430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14ca158a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14ca15d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14ca16180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14ca165f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14ca16a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14ca16ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14ca176a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14ca17b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14ca17e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14ca18410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14ca18c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14ca190a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14ca19540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14ca199e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14ca19e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14ca1a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14ca1a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14ca1ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14ca1b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14ca1b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14ca1ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14ca1bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14ca1c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14ca1c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14ca1ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14ca1d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14ca1d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14ca1daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14ca1df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14ca1e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14ca1e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14ca1ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14ca1f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14ca1f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14ca1fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14ca1ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14ca20440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14ca208e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14ca20d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14ca21220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14ca216c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14ca21b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14ca22000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14ca224a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14ca22940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14ca22de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14ca23280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14ca23720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14ca23bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14ca24060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14ca24500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14ca249a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14ca24e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14ca252e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14ca25780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14ca25c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14ca260c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14ca26560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14ca26a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14ca26ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14ca27340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14ca277e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14ca27c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14ca28120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14ca285c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14ca28a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14ca28f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14ca293a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14ca29840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14ca29ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14ca2a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14ca2a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14ca2aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14ca2af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14ca2b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14ca2b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14ca2bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14ca2c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14ca2c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14ca2cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14ca2cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14ca2d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14ca2d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14ca2dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14ca2e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14ca2e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14ca2eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14ca2f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14ca2f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14ca2f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14ca2fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14ca302a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14ca30740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14ca30be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14ca31080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14ca31520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14ca319c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14ca31e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14ca32300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14ca327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14ca32c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14ca330e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14ca33580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14ca33a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14ca33f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14ca344c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14ca34a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14ca34f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14ca35220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14ca35830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14ca35e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14ca36450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14ca36a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14ca37070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14ca37860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14ca37d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14ca381a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14ca38640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14ca38df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14ca39340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14ca39890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14ca39de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14ca3a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14ca3a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14ca3add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14ca3b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14ca3b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14ca3bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14ca3c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14ca3c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14ca3cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14ca3d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14ca3d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14ca3dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14ca3e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14ca3e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14ca3ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14ca3f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14ca3f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14ca3fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14ca402d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14ca40820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14ca40d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14ca412c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14ca41810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14ca41d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14ca422b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14ca42800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14ca42d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14ca432a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14ca437f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14ca43d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14ca44290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14ca447e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14ca44d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14ca45280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14ca457d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14ca45d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14ca46270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14ca467c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14ca46d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14ca47260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14ca477b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14ca47d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14ca48250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14ca487a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14ca48cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14ca49240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14ca49790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14ca49ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14ca4a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14ca4a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14ca4acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14ca4b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14ca4b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14ca4bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14ca4c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14ca4c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14ca4c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14ca4ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14ca4d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14ca4d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14ca4dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14ca4e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14ca4e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14ca4ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14ca4eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14ca4f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14ca4f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14ca50000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14ca50720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14ca50e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14ca51560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14ca51820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14ca51e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14ca52440 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14ca05200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14ca05670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14ca05ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14ca05f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14ca063c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14ca06830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14ca06ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14ca07110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14ca07580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14ca079f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14ca07e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14ca08440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14ca08d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14ca094b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14ca09c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14ca0a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14ca0aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14ca0b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14ca0b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14ca0c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14ca0c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14ca0cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14ca0d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14ca0dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14ca0e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14ca0e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14ca0ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14ca0f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14ca0f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14ca0fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14ca0ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14ca10390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14ca10800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14ca10ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14ca10f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14ca113a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14ca11810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14ca11c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14ca120f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14ca12560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14ca129d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14ca12e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14ca132b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14ca13720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14ca13b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14ca14000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14ca14470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14ca148e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14ca14d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14ca151c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14ca15630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14ca15aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14ca15f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14ca16380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14ca167f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14ca16c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14ca170d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14ca17540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14ca179b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14ca17e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14ca18290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14ca18700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14ca18b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14ca18fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14ca19450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14ca198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14ca19d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14ca1a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14ca1a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14ca1aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14ca1aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14ca1b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14ca1b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14ca1bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14ca1c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14ca1c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14ca1c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14ca1ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14ca1d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14ca1d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14ca1db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14ca1dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14ca1e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14ca1e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14ca1ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14ca1f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14ca1f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14ca1fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14ca1fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14ca20340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14ca207b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14ca20c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14ca21090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14ca21500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14ca21970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14ca21de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14ca22250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14ca226c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14ca22b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14ca22fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14ca23410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14ca23880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14ca23cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14ca24160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14ca245d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14ca24a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14ca24eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14ca25320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14ca25790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14ca25c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14ca26070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14ca264e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14ca26950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14ca26dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14ca27230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14ca276a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14ca27b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14ca27f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14ca283f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14ca28860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14ca28cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14ca29140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14ca295b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14ca29a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14ca29e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14ca2a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14ca2a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14ca2abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14ca2b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14ca2b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14ca2b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14ca2bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14ca2c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14ca2c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14ca2caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14ca2cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14ca2d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14ca2d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14ca2dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14ca2e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14ca2e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14ca2ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14ca2ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14ca2f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14ca2f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14ca2fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14ca30030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14ca304a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14ca30910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14ca30d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14ca311f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14ca31660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14ca31ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14ca31f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14ca323b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14ca32820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14ca32c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14ca33100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14ca33570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14ca339e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14ca33e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14ca342c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14ca34730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14ca34ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14ca35010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14ca35480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14ca358f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14ca36070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14ca364e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14ca36950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14ca36dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14ca37230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14ca376a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14ca37b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14ca37f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14ca383f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14ca38860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14ca38cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14ca39140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14ca395b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14ca39a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14ca39e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14ca3a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14ca3a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14ca3abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14ca3b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14ca3b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14ca3b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14ca3bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14ca3c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14ca3c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14ca3caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14ca3cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14ca3d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14c90bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14c94a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14c94adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14c94b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14c94b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14c94bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14c94bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14c94c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14c94c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14c94cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14c94d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14c94d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14c94da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14c94dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14c94e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14c94e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14c94ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14c94f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14c94f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14c94f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14c94fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14c950240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14c9506b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14c950b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14c950f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14c951400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14c951870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14c951ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14c952150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14c9525c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14c952a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14c952ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14c953310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14c953780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14c953bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14c954060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14c9544d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14c954940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14c954db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14c955220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14c955690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14c955b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14c955f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14c9563e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14c956c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14c957330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14c957a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14c958110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14c958580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14c9589f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14c958e60 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.847s
user	0m0.291s
sys	0m0.326s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4223 (a3a3048e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x137e0b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x137e0ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x137e0c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x137e0c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x137e0cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x137e0d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x137e0d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x137e0dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x137e0e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x137e0e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x137e0ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x137e0f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x137e0fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x137e10420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x137e10c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x137e11350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x137e11a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x137e12190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x137e128b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x137e13080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x137e137a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x137e13ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x137e145e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x137e14e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x137e155a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x137e15860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x137e15e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x137e16ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x137e17020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x137e172e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x137e17780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x137e17a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x137e182d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x137e18810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x137e18ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x137e18f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x137e19410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x137e198b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x137e19d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x137e1a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x137e1a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x137e1ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x137e1afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x137e1b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x137e1b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x137e1bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x137e1c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x137e1cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x137e1d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x137e1d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x137e1dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x137e1e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x137e1eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x137e1f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x137e1f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x137e1fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x137e20200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x137e204c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x137e20ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x137e212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x137e21580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x137e21a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x137e21ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x137e22360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x137e22800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x137e22ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x137e23140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x137e235e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x137e23a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x137e23f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x137e243c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x137e24860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x137e24d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x137e251a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x137e25640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x137e25ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x137e25f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x137e26420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x137e268c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x137e26d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x137e27200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x137e276a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x137e27b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x137e27fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x137e28480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x137e28920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x137e28dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x137e29260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x137e29700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x137e29ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x137e2a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x137e2a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x137e2a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x137e2ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x137e2b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x137e2b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x137e2bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x137e1c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x137e2c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x137e2c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x137e2cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x137e2d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x137e2d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x137e2d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x137e2de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x137e2e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x137e2e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x137e2ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x137e2f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x137e2f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x137e2f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x137e2fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x137e30310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x137e307b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x137e30c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x137e310f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x137e31590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x137e31a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x137e31ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x137e32370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x137e32810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x137e32cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x137e33150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x137e335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x137e33a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x137e33f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x137e343d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x137e34870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x137e34d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x137e351b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x137e35650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x137e35af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x137e35f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x137e36430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x137e368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x137e36d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x137e37210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x137e376b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x137e37b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x137e37ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x137e38490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x137e38930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x137e38dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x137e39270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x137e39710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x137e39bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x137e3a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x137e3a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x137e3a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x137e3ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x137e3b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x137e3b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x137e3bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x137e3c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x137e3c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x137e3cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x137e3d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x137e3d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x137e3da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x137e3e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x137e3e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x137e3ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x137e3f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x137e3fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x137e3fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x137e40390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x137e40830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x137e40fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x137e41530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x137e41a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x137e41fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x137e42520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x137e42a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x137e42fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x137e43510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x137e43a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x137e43fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x137e44500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x137e44a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x137e44fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x137e454f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x137e45a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x137e45f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x137e464e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x137e46a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x137e46f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x137e474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x137e47a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x137e47f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x137e484c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x137e48a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x137e48f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x137e494b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x137e49a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x137e49f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x137e4a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x137e4a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x137e4af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x137e4b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x137e4b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x137e4bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x137e4c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x137e4c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x137e4cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x137e4d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x137e4d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x137e4df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x137e4e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x137e4e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x137e4ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x137e4f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x137e4f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x137e4fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x137e50440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x137e50990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x137e50ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x137e51430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x137e51980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x137e51ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x137e52420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x137e52970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x137e52ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x137e53410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x137e53960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x137e53e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x137e542a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x137e54740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x137e54be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x137e55080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x137e55520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x137e559c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x137e55e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x137e56300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x137e567a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x137e56c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x137e570e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x137e57580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x137e57ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x137e581f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x137e58910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x137e59030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x137e59750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x137e59a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x137e5a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x137e5a630 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.095.307 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x137f094e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x137f09950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x137f09dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x137f0a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x137f0a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x137f0ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x137f0af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x137f0b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x137f0b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x137f0bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x137f0c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x137f0c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x137f0d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x137f0dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x137f0e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x137f0ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x137f0f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x137f0f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x137f10030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x137f10800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x137f10f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x137f11640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x137f11d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x137f12480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x137f12ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x137f12e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x137f13120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x137f13590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x137f13a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x137f13e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x137f14370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x137f14880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x137f14cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x137f14fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x137f15420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x137f15890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x137f15df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x137f162f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x137f167f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x137f16cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x137f171f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x137f176f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x137f17bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x137f180f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x137f185f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x137f18a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x137f18ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x137f19340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x137f197b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x137f19c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x137f1a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x137f1a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x137f1a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x137f1ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x137f1b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x137f1ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x137f1bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x137f1c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x137f1c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x137f1cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x137f1d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x137f1d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x137f1dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x137f1e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x137f1e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x137f1eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x137f1efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x137f1f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x137f1f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x137f1fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x137f20260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x137f20700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x137f20ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x137f21040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x137f214e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x137f21980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x137f21e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x137f222c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x137f22760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x137f22c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x137f230a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x137f23540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x137f239e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x137f23e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x137f24320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x137f247c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x137f24c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x137f25100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x137f255a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x137f25a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x137f25ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x137f26380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x137f26820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x137f26cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x137f27160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x137f27600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x137f27aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x137f27f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x137f283e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x137f28880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x137f28d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x137f291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x137f29660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x137f29b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x137f29fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x137f2a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x137f2a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x137f2ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x137f2b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x137f2b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x137f2bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x137f2c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x137f2c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x137f2c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x137f2cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x137f2d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x137f2d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x137f2dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x137f2e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x137f2e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x137f2e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x137f2ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x137f2f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x137f2f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x137f2fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x137f300c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x137f30560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x137f30a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x137f30ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x137f31340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x137f317e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x137f31c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x137f32120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x137f325c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x137f32a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x137f32f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x137f333a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x137f33840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x137f33ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x137f34180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x137f34620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x137f34ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x137f34f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x137f35400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x137f358a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x137f35d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x137f361e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x137f36680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x137f36b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x137f36fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x137f37460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x137f37900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x137f37da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x137f382f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x137f38840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x137f38d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x137f392e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x137f395a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x137f39bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x137f3a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x137f3a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x137f3ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x137f3b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x137f3bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x137f3c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x137f3c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x137f3c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x137f3d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x137f3d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x137f3dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x137f3e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x137f3e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x137f3ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x137f3f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x137f3f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x137f3fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x137f40140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x137f40690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x137f40be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x137f41130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x137f41680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x137f41bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x137f42120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x137f42670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x137f42bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x137f43110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x137f43660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x137f43bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x137f44100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x137f44650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x137f44ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x137f450f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x137f45640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x137f45b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x137f460e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x137f46630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x137f46b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x137f470d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x137f47620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x137f47b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x137f480c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x137f48610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x137f48b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x137f490b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x137f49600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x137f49b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x137f4a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x137f4a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x137f4ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x137f4b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x137f4b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x137f4bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x137f4c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x137f4c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x137f4cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x137f4d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x137f4d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x137f4db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x137f4e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x137f4e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x137f4eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x137f4f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x137f4f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x137f4faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x137f4ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x137f50430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x137f508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x137f50d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x137f51210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x137f516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x137f51b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x137f51ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x137f52490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x137f52930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x137f52dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x137f53270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x137f53710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x137f53c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x137f54380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x137f54aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x137f551c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x137f558e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x137f55ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x137f561b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x137f567c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x139805aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x139805f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x139806380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1398067f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x139806c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1398070d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x139807540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1398079b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x139807e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x139808290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x139808700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x139808df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x139809910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13980a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13980a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13980aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13980b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13980be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13980c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13980cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13980d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13980dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13980e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13980e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13980f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13980f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13980f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13980fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13980fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1398102f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x139810760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x139810c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x139811100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1398113c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x139811830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x139811ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x139812110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x139812580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1398129f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x139812e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1398132d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x139813740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x139813bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x139814020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x139814490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x139814900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x139814d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1398151e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x139815650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x139815ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x139815f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1398163a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x139816810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x139816c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1398170f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x139817560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x139817ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x139817fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x139818440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1398188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x139818d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x139819190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x139819600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x139819a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x139819ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13981a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13981a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13981ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13981b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13981b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13981b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13981bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13981c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13981c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13981cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13981cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13981d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13981d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13981dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13981e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13981e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13981ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13981eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13981f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13981f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13981fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x139820080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1398204f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x139820960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x139820dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x139821240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1398216b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x139821b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x139821f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x139822400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x139822870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x139822ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x139823150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1398235c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x139823a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x139823ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x139824310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x139824780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x139824bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x139825060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1398254d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x139825940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x139825db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x139826220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x139826690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x139826b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x139826f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1398273e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x139827850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x139827cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x139828130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1398285a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x139828a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x139828e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1398292f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x139829760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x139829bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13982a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13982a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13982a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13982ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13982b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13982b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13982bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13982bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13982c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13982c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13982cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13982d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13982d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13982d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13982de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13982e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13982e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13982ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13982f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13982f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13982f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13982fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1398301e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x139830650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x139830ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x139830f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1398313a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x139831810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x139831c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1398320f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x139832560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1398329d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x139832e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1398332b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x139833720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x139833b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x139834000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x139834470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1398348e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x139834d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1398351c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x139835630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x139835aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x139835f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x139836380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x139836f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1398371d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x139837490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x139837900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x139837d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1398381e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x139838650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x139838ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x139838f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1398393a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x139839810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x139839c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13983a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13983a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13983a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13983ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13983b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13983b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13983bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13983c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13983c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13983c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13983cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13983d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13983d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13983daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13983df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13983e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13983e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13983ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13983f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13983f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13983f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13983fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x139840290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x139840700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x139840b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x139840fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x139841450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1398418c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x139841d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1398421a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x139842610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x139842a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x139842ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x139843360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1398437d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x139843c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1398440b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x139844520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x139844990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x139844e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x139845270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1398456e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x139845b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x139845fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x139846430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1398468a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x139846d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x139847180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1398475f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x139847a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x139847ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x139848340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1398487b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x139848c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x139849090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x139849500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x139849970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x139849de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13984a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13984ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13984b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13984bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13984c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13984c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13984c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13984cce0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this


second run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this


single seq run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this

real	0m0.932s
user	0m0.241s
sys	0m0.147s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
