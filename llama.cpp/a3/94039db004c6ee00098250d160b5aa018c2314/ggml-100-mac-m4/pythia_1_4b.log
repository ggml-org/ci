Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.3s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.597s
user	0m0.849s
sys	0m1.292s
++ nproc
+ make -j10
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Built target sha256
[  4%] Built target build_info
[  5%] Built target sha1
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target xxhash
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 11%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Built target ggml-blas
[ 12%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama
[ 25%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 25%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 25%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 26%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 29%] Linking C executable ../bin/test-c
[ 32%] Linking CXX executable ../../bin/llama-simple-chat
[ 32%] Linking CXX executable ../../bin/llama-simple
[ 32%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 35%] Linking CXX static library libcommon.a
[ 35%] Built target llava
[ 35%] Built target llama-simple-chat
[ 35%] Built target llama-quantize-stats
[ 35%] Built target llama-simple
[ 35%] Built target test-c
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Linking CXX static library libllava_static.a
[ 36%] Built target common
[ 36%] Built target llava_static
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 38%] Built target llava_shared
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 44%] Linking CXX executable ../bin/test-tokenizer-0
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-chat
[ 45%] Linking CXX executable ../bin/test-grammar-integration
[ 45%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 47%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-sampling
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Built target test-llama-grammar
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-json-schema-to-grammar
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-sampling
[ 48%] Built target test-grammar-integration
[ 48%] Built target test-chat
[ 48%] Built target test-log
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-arg-parser
[ 60%] Linking CXX executable ../bin/test-chat-template
[ 60%] Linking CXX executable ../bin/test-model-load-cancel
[ 62%] Linking CXX executable ../bin/test-barrier
[ 62%] Linking CXX executable ../bin/test-backend-ops
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Linking CXX executable ../bin/test-gguf
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Linking CXX executable ../bin/test-autorelease
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-arg-parser
[ 63%] Built target test-model-load-cancel
[ 63%] Built target test-backend-ops
[ 63%] Built target test-chat-template
[ 63%] Built target test-barrier
[ 63%] Built target test-gguf
[ 63%] Built target test-quantize-perf
[ 63%] Built target test-autorelease
[ 63%] Built target test-rope
[ 63%] Built target test-quantize-fns
[ 63%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 66%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 66%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 66%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-embedding
[ 67%] Linking CXX executable ../../bin/llama-batched
[ 67%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Linking CXX executable ../../bin/llama-batched-bench
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 70%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 72%] Linking CXX executable ../../bin/llama-gritlm
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-batched-bench
[ 73%] Built target llama-batched
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-imatrix
[ 73%] Built target llama-embedding
[ 73%] Built target llama-gritlm
[ 73%] Built target llama-infill
[ 73%] Built target llama-bench
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-lookup-merge
[ 80%] Linking CXX executable ../../bin/llama-lookahead
[ 80%] Linking CXX executable ../../bin/llama-lookup-stats
[ 80%] Linking CXX executable ../../bin/llama-cli
[ 80%] Linking CXX executable ../../bin/llama-lookup-create
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Linking CXX executable ../../bin/llama-lookup
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 81%] Built target llama-passkey
[ 81%] Built target llama-lookup-merge
[ 81%] Built target llama-lookup-create
[ 81%] Built target llama-lookup
[ 81%] Built target llama-cli
[ 81%] Built target llama-lookup-stats
[ 81%] Built target llama-perplexity
[ 81%] Built target llama-quantize
[ 81%] Built target llama-parallel
[ 81%] Built target llama-lookahead
[ 81%] Generating loading.html.hpp
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 81%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 82%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 82%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 82%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 82%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 82%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 83%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 84%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 85%] Generating index.html.gz.hpp
[ 87%] Linking CXX executable ../../bin/llama-retrieval
[ 87%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-save-load-state
[ 88%] Linking CXX executable ../../bin/llama-tts
[ 89%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 90%] Linking CXX executable ../../bin/llama-speculative
[ 91%] Linking CXX executable ../../bin/llama-tokenize
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Linking CXX executable ../../bin/llama-run
[ 91%] Built target llama-tts
[ 91%] Built target llama-speculative
[ 91%] Built target llama-save-load-state
[ 91%] Built target llama-retrieval
[ 91%] Built target llama-tokenize
[ 91%] Built target llama-speculative-simple
[ 91%] Built target llama-convert-llama2c-to-ggml
[ 91%] Built target llama-gen-docs
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 94%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 94%] Built target llama-run
[ 94%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 95%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 95%] Linking CXX executable ../../bin/llama-cvector-generator
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.373s
user	0m6.435s
sys	0m10.680s

main: quantize time =  3862.08 ms
main:    total time =  3862.08 ms

main: quantize time =  4623.74 ms
main:    total time =  4623.74 ms

main: quantize time =  3589.47 ms
main:    total time =  3589.47 ms

main: quantize time =  3355.55 ms
main:    total time =  3355.55 ms

main: quantize time =  2196.48 ms
main:    total time =  2196.48 ms

main: quantize time =  5305.83 ms
main:    total time =  5305.83 ms

main: quantize time =  5945.98 ms
main:    total time =  5945.98 ms

main: quantize time =  7065.64 ms
main:    total time =  7065.64 ms

main: quantize time =  5966.05 ms
main:    total time =  5966.05 ms

main: quantize time =  4356.30 ms
main:    total time =  4356.30 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.222 I build: 4702 (a394039d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.388 I main: llama backend init
0.00.000.395 I main: load the model and apply lora adapter, if any
0.00.054.467 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.067.124 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.067.143 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.067.147 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.067.148 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.067.149 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.067.149 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.067.150 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.067.152 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.067.153 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.067.153 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.067.154 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.067.155 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.067.155 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.067.162 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.067.168 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.067.168 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.067.169 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.074.196 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.076.382 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.085.891 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.085.901 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.085.901 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.085.902 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.085.903 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.085.904 I llama_model_loader: - type  f32:  194 tensors
0.00.085.904 I llama_model_loader: - type  f16:   98 tensors
0.00.085.914 I print_info: file format = GGUF V3 (latest)
0.00.085.915 I print_info: file type   = all F32 (guessed)
0.00.085.917 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.102.217 I load: special tokens cache size = 25
0.00.112.049 I load: token to piece cache size = 0.2984 MB
0.00.112.053 I print_info: arch             = gptneox
0.00.112.053 I print_info: vocab_only       = 0
0.00.112.054 I print_info: n_ctx_train      = 2048
0.00.112.054 I print_info: n_embd           = 2048
0.00.112.054 I print_info: n_layer          = 24
0.00.112.060 I print_info: n_head           = 16
0.00.112.064 I print_info: n_head_kv        = 16
0.00.112.064 I print_info: n_rot            = 32
0.00.112.064 I print_info: n_swa            = 0
0.00.112.064 I print_info: n_embd_head_k    = 128
0.00.112.064 I print_info: n_embd_head_v    = 128
0.00.112.065 I print_info: n_gqa            = 1
0.00.112.066 I print_info: n_embd_k_gqa     = 2048
0.00.112.067 I print_info: n_embd_v_gqa     = 2048
0.00.112.068 I print_info: f_norm_eps       = 1.0e-05
0.00.112.070 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.112.070 I print_info: f_clamp_kqv      = 0.0e+00
0.00.112.070 I print_info: f_max_alibi_bias = 0.0e+00
0.00.112.070 I print_info: f_logit_scale    = 0.0e+00
0.00.112.071 I print_info: n_ff             = 8192
0.00.112.071 I print_info: n_expert         = 0
0.00.112.072 I print_info: n_expert_used    = 0
0.00.112.072 I print_info: causal attn      = 1
0.00.112.072 I print_info: pooling type     = 0
0.00.112.072 I print_info: rope type        = 2
0.00.112.072 I print_info: rope scaling     = linear
0.00.112.074 I print_info: freq_base_train  = 10000.0
0.00.112.075 I print_info: freq_scale_train = 1
0.00.112.075 I print_info: n_ctx_orig_yarn  = 2048
0.00.112.075 I print_info: rope_finetuned   = unknown
0.00.112.075 I print_info: ssm_d_conv       = 0
0.00.112.075 I print_info: ssm_d_inner      = 0
0.00.112.076 I print_info: ssm_d_state      = 0
0.00.112.076 I print_info: ssm_dt_rank      = 0
0.00.112.076 I print_info: ssm_dt_b_c_rms   = 0
0.00.112.076 I print_info: model type       = 1.4B
0.00.112.077 I print_info: model params     = 1.41 B
0.00.112.077 I print_info: general.name     = 1.4B
0.00.112.078 I print_info: vocab type       = BPE
0.00.112.078 I print_info: n_vocab          = 50304
0.00.112.078 I print_info: n_merges         = 50009
0.00.112.079 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.112.079 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.112.079 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.112.079 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.112.081 I print_info: LF token         = 187 'Ċ'
0.00.112.081 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.112.082 I print_info: max token length = 1024
0.00.112.082 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.153.340 I load_tensors: offloading 24 repeating layers to GPU
0.00.153.344 I load_tensors: offloading output layer to GPU
0.00.153.345 I load_tensors: offloaded 25/25 layers to GPU
0.00.153.368 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.153.370 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.153.782 I llama_init_from_model: n_seq_max     = 1
0.00.153.783 I llama_init_from_model: n_ctx         = 2048
0.00.153.783 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.153.783 I llama_init_from_model: n_batch       = 2048
0.00.153.783 I llama_init_from_model: n_ubatch      = 512
0.00.153.784 I llama_init_from_model: flash_attn    = 0
0.00.153.784 I llama_init_from_model: freq_base     = 10000.0
0.00.153.784 I llama_init_from_model: freq_scale    = 1
0.00.153.785 I ggml_metal_init: allocating
0.00.153.820 I ggml_metal_init: found device: Apple M4
0.00.153.827 I ggml_metal_init: picking default device: Apple M4
0.00.154.471 I ggml_metal_init: using embedded metal library
0.00.170.254 I ggml_metal_init: GPU name:   Apple M4
0.00.170.256 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.170.256 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.170.257 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.170.257 I ggml_metal_init: simdgroup reduction   = true
0.00.170.257 I ggml_metal_init: simdgroup matrix mul. = true
0.00.170.257 I ggml_metal_init: has residency sets    = true
0.00.170.257 I ggml_metal_init: has bfloat            = true
0.00.170.257 I ggml_metal_init: use bfloat            = true
0.00.170.258 I ggml_metal_init: hasUnifiedMemory      = true
0.00.170.259 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.206.650 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.234.640 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.234.646 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.234.671 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.238.239 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.238.241 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.238.242 I llama_init_from_model: graph nodes  = 967
0.00.238.242 I llama_init_from_model: graph splits = 2
0.00.238.246 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.238.379 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.238.379 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.293.717 I main: llama threadpool init, n_threads = 4
0.00.293.758 I 
0.00.293.772 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.293.773 I 
0.00.293.817 I sampler seed: 1234
0.00.293.822 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.293.846 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.293.846 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.293.846 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.136.489 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58921.16 tokens per second)
0.02.136.491 I llama_perf_context_print:        load time =     238.44 ms
0.02.136.492 I llama_perf_context_print: prompt eval time =      43.72 ms /     7 tokens (    6.25 ms per token,   160.12 tokens per second)
0.02.136.493 I llama_perf_context_print:        eval time =    1796.12 ms /    63 runs   (   28.51 ms per token,    35.08 tokens per second)
0.02.136.493 I llama_perf_context_print:       total time =    1843.57 ms /    70 tokens
0.02.136.752 I ggml_metal_free: deallocating

real	0m2.466s
user	0m0.134s
sys	0m0.124s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.058 I build: 4702 (a394039d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.093 I main: llama backend init
0.00.000.095 I main: load the model and apply lora adapter, if any
0.00.009.881 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.490 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.496 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.497 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.498 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.498 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.499 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.499 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.500 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.501 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.501 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.501 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.503 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.504 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.504 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.506 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.506 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.507 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.474 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.442 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.225 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.227 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.227 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.228 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.228 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.228 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.229 I llama_model_loader: - type  f32:  194 tensors
0.00.035.229 I llama_model_loader: - type q8_0:   98 tensors
0.00.035.230 I print_info: file format = GGUF V3 (latest)
0.00.035.231 I print_info: file type   = Q8_0
0.00.035.232 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.044.278 I load: special tokens cache size = 25
0.00.051.190 I load: token to piece cache size = 0.2984 MB
0.00.051.194 I print_info: arch             = gptneox
0.00.051.194 I print_info: vocab_only       = 0
0.00.051.195 I print_info: n_ctx_train      = 2048
0.00.051.195 I print_info: n_embd           = 2048
0.00.051.195 I print_info: n_layer          = 24
0.00.051.202 I print_info: n_head           = 16
0.00.051.203 I print_info: n_head_kv        = 16
0.00.051.203 I print_info: n_rot            = 32
0.00.051.203 I print_info: n_swa            = 0
0.00.051.203 I print_info: n_embd_head_k    = 128
0.00.051.204 I print_info: n_embd_head_v    = 128
0.00.051.204 I print_info: n_gqa            = 1
0.00.051.205 I print_info: n_embd_k_gqa     = 2048
0.00.051.206 I print_info: n_embd_v_gqa     = 2048
0.00.051.206 I print_info: f_norm_eps       = 1.0e-05
0.00.051.207 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.207 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.207 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.207 I print_info: f_logit_scale    = 0.0e+00
0.00.051.208 I print_info: n_ff             = 8192
0.00.051.208 I print_info: n_expert         = 0
0.00.051.208 I print_info: n_expert_used    = 0
0.00.051.208 I print_info: causal attn      = 1
0.00.051.208 I print_info: pooling type     = 0
0.00.051.209 I print_info: rope type        = 2
0.00.051.209 I print_info: rope scaling     = linear
0.00.051.209 I print_info: freq_base_train  = 10000.0
0.00.051.210 I print_info: freq_scale_train = 1
0.00.051.210 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.210 I print_info: rope_finetuned   = unknown
0.00.051.210 I print_info: ssm_d_conv       = 0
0.00.051.210 I print_info: ssm_d_inner      = 0
0.00.051.210 I print_info: ssm_d_state      = 0
0.00.051.211 I print_info: ssm_dt_rank      = 0
0.00.051.211 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.211 I print_info: model type       = 1.4B
0.00.051.211 I print_info: model params     = 1.41 B
0.00.051.211 I print_info: general.name     = 1.4B
0.00.051.212 I print_info: vocab type       = BPE
0.00.051.214 I print_info: n_vocab          = 50304
0.00.051.215 I print_info: n_merges         = 50009
0.00.051.215 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.215 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.215 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.215 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.215 I print_info: LF token         = 187 'Ċ'
0.00.051.216 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.216 I print_info: max token length = 1024
0.00.051.217 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.179.487 I load_tensors: offloading 24 repeating layers to GPU
0.01.179.492 I load_tensors: offloading output layer to GPU
0.01.179.493 I load_tensors: offloaded 25/25 layers to GPU
0.01.179.517 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.179.523 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.180.352 I llama_init_from_model: n_seq_max     = 1
0.01.180.354 I llama_init_from_model: n_ctx         = 2048
0.01.180.354 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.180.355 I llama_init_from_model: n_batch       = 2048
0.01.180.355 I llama_init_from_model: n_ubatch      = 512
0.01.180.356 I llama_init_from_model: flash_attn    = 0
0.01.180.357 I llama_init_from_model: freq_base     = 10000.0
0.01.180.357 I llama_init_from_model: freq_scale    = 1
0.01.180.358 I ggml_metal_init: allocating
0.01.180.368 I ggml_metal_init: found device: Apple M4
0.01.180.375 I ggml_metal_init: picking default device: Apple M4
0.01.181.563 I ggml_metal_init: using embedded metal library
0.01.186.860 I ggml_metal_init: GPU name:   Apple M4
0.01.186.863 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.186.864 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.186.864 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.186.865 I ggml_metal_init: simdgroup reduction   = true
0.01.186.865 I ggml_metal_init: simdgroup matrix mul. = true
0.01.186.865 I ggml_metal_init: has residency sets    = true
0.01.186.866 I ggml_metal_init: has bfloat            = true
0.01.186.866 I ggml_metal_init: use bfloat            = true
0.01.186.867 I ggml_metal_init: hasUnifiedMemory      = true
0.01.186.868 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.202.302 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.245.666 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.245.674 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.245.702 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.249.741 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.249.743 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.249.743 I llama_init_from_model: graph nodes  = 967
0.01.249.743 I llama_init_from_model: graph splits = 2
0.01.249.750 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.249.874 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.249.875 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.303.377 I main: llama threadpool init, n_threads = 4
0.01.303.420 I 
0.01.303.436 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.303.437 I 
0.01.303.589 I sampler seed: 1234
0.01.303.593 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.303.604 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.303.604 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.303.605 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.397.033 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54406.13 tokens per second)
0.02.397.033 I llama_perf_context_print:        load time =    1292.80 ms
0.02.397.034 I llama_perf_context_print: prompt eval time =      48.53 ms /     7 tokens (    6.93 ms per token,   144.23 tokens per second)
0.02.397.035 I llama_perf_context_print:        eval time =    1041.97 ms /    63 runs   (   16.54 ms per token,    60.46 tokens per second)
0.02.397.035 I llama_perf_context_print:       total time =    1094.35 ms /    70 tokens
0.02.397.286 I ggml_metal_free: deallocating

real	0m2.419s
user	0m0.110s
sys	0m0.244s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.058 I build: 4702 (a394039d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.095 I main: llama backend init
0.00.000.097 I main: load the model and apply lora adapter, if any
0.00.012.484 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.179 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.020.184 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.186 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.186 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.187 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.187 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.187 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.188 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.189 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.189 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.189 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.190 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.190 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.191 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.192 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.192 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.193 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.059 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.084 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.990 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.992 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.992 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.993 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.993 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.993 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.028.994 I llama_model_loader: - type  f32:  194 tensors
0.00.028.994 I llama_model_loader: - type q4_0:   97 tensors
0.00.028.994 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.995 I print_info: file format = GGUF V3 (latest)
0.00.028.998 I print_info: file type   = Q4_0
0.00.028.999 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.037.099 I load: special tokens cache size = 25
0.00.043.222 I load: token to piece cache size = 0.2984 MB
0.00.043.226 I print_info: arch             = gptneox
0.00.043.226 I print_info: vocab_only       = 0
0.00.043.226 I print_info: n_ctx_train      = 2048
0.00.043.226 I print_info: n_embd           = 2048
0.00.043.226 I print_info: n_layer          = 24
0.00.043.232 I print_info: n_head           = 16
0.00.043.233 I print_info: n_head_kv        = 16
0.00.043.233 I print_info: n_rot            = 32
0.00.043.233 I print_info: n_swa            = 0
0.00.043.233 I print_info: n_embd_head_k    = 128
0.00.043.235 I print_info: n_embd_head_v    = 128
0.00.043.236 I print_info: n_gqa            = 1
0.00.043.237 I print_info: n_embd_k_gqa     = 2048
0.00.043.238 I print_info: n_embd_v_gqa     = 2048
0.00.043.238 I print_info: f_norm_eps       = 1.0e-05
0.00.043.239 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.043.239 I print_info: f_clamp_kqv      = 0.0e+00
0.00.043.239 I print_info: f_max_alibi_bias = 0.0e+00
0.00.043.239 I print_info: f_logit_scale    = 0.0e+00
0.00.043.240 I print_info: n_ff             = 8192
0.00.043.240 I print_info: n_expert         = 0
0.00.043.240 I print_info: n_expert_used    = 0
0.00.043.240 I print_info: causal attn      = 1
0.00.043.240 I print_info: pooling type     = 0
0.00.043.241 I print_info: rope type        = 2
0.00.043.241 I print_info: rope scaling     = linear
0.00.043.241 I print_info: freq_base_train  = 10000.0
0.00.043.242 I print_info: freq_scale_train = 1
0.00.043.242 I print_info: n_ctx_orig_yarn  = 2048
0.00.043.242 I print_info: rope_finetuned   = unknown
0.00.043.242 I print_info: ssm_d_conv       = 0
0.00.043.242 I print_info: ssm_d_inner      = 0
0.00.043.243 I print_info: ssm_d_state      = 0
0.00.043.243 I print_info: ssm_dt_rank      = 0
0.00.043.243 I print_info: ssm_dt_b_c_rms   = 0
0.00.043.243 I print_info: model type       = 1.4B
0.00.043.243 I print_info: model params     = 1.41 B
0.00.043.243 I print_info: general.name     = 1.4B
0.00.043.244 I print_info: vocab type       = BPE
0.00.043.245 I print_info: n_vocab          = 50304
0.00.043.245 I print_info: n_merges         = 50009
0.00.043.245 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.043.246 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.043.246 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.043.246 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.043.246 I print_info: LF token         = 187 'Ċ'
0.00.043.246 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.043.246 I print_info: max token length = 1024
0.00.043.247 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.606.251 I load_tensors: offloading 24 repeating layers to GPU
0.00.606.265 I load_tensors: offloading output layer to GPU
0.00.606.266 I load_tensors: offloaded 25/25 layers to GPU
0.00.606.299 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.606.300 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.607.941 I llama_init_from_model: n_seq_max     = 1
0.00.607.943 I llama_init_from_model: n_ctx         = 2048
0.00.607.944 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.607.944 I llama_init_from_model: n_batch       = 2048
0.00.607.945 I llama_init_from_model: n_ubatch      = 512
0.00.607.945 I llama_init_from_model: flash_attn    = 0
0.00.607.947 I llama_init_from_model: freq_base     = 10000.0
0.00.607.947 I llama_init_from_model: freq_scale    = 1
0.00.607.950 I ggml_metal_init: allocating
0.00.608.026 I ggml_metal_init: found device: Apple M4
0.00.608.040 I ggml_metal_init: picking default device: Apple M4
0.00.609.877 I ggml_metal_init: using embedded metal library
0.00.615.429 I ggml_metal_init: GPU name:   Apple M4
0.00.615.434 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.615.434 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.615.435 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.615.436 I ggml_metal_init: simdgroup reduction   = true
0.00.615.436 I ggml_metal_init: simdgroup matrix mul. = true
0.00.615.437 I ggml_metal_init: has residency sets    = true
0.00.615.437 I ggml_metal_init: has bfloat            = true
0.00.615.437 I ggml_metal_init: use bfloat            = true
0.00.615.438 I ggml_metal_init: hasUnifiedMemory      = true
0.00.615.440 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.635.424 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.689.726 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.689.734 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.689.763 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.693.923 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.693.925 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.693.925 I llama_init_from_model: graph nodes  = 967
0.00.693.926 I llama_init_from_model: graph splits = 2
0.00.693.931 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.694.063 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.694.064 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.747.603 I main: llama threadpool init, n_threads = 4
0.00.747.644 I 
0.00.747.660 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.747.661 I 
0.00.747.838 I sampler seed: 1234
0.00.747.842 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.747.884 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.747.887 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.747.888 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.437.059 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51226.55 tokens per second)
0.01.437.060 I llama_perf_context_print:        load time =     734.38 ms
0.01.437.060 I llama_perf_context_print: prompt eval time =      49.31 ms /     7 tokens (    7.04 ms per token,   141.95 tokens per second)
0.01.437.061 I llama_perf_context_print:        eval time =     636.99 ms /    63 runs   (   10.11 ms per token,    98.90 tokens per second)
0.01.437.061 I llama_perf_context_print:       total time =     690.19 ms /    70 tokens
0.01.437.290 I ggml_metal_free: deallocating

real	0m1.456s
user	0m0.111s
sys	0m0.211s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4702 (a394039d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.396 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.362 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.367 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.369 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.369 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.370 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.370 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.370 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.373 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.373 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.374 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.375 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.376 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.376 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.376 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.379 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.380 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.381 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.122 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.109 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.893 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.895 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.895 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.895 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.895 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.896 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.896 I llama_model_loader: - type  f32:  194 tensors
0.00.025.897 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.897 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.897 I print_info: file format = GGUF V3 (latest)
0.00.025.898 I print_info: file type   = Q4_1
0.00.025.899 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.766 I load: special tokens cache size = 25
0.00.039.911 I load: token to piece cache size = 0.2984 MB
0.00.039.914 I print_info: arch             = gptneox
0.00.039.915 I print_info: vocab_only       = 0
0.00.039.915 I print_info: n_ctx_train      = 2048
0.00.039.915 I print_info: n_embd           = 2048
0.00.039.915 I print_info: n_layer          = 24
0.00.039.918 I print_info: n_head           = 16
0.00.039.919 I print_info: n_head_kv        = 16
0.00.039.919 I print_info: n_rot            = 32
0.00.039.921 I print_info: n_swa            = 0
0.00.039.921 I print_info: n_embd_head_k    = 128
0.00.039.921 I print_info: n_embd_head_v    = 128
0.00.039.922 I print_info: n_gqa            = 1
0.00.039.923 I print_info: n_embd_k_gqa     = 2048
0.00.039.924 I print_info: n_embd_v_gqa     = 2048
0.00.039.924 I print_info: f_norm_eps       = 1.0e-05
0.00.039.924 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.925 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.925 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.925 I print_info: f_logit_scale    = 0.0e+00
0.00.039.926 I print_info: n_ff             = 8192
0.00.039.926 I print_info: n_expert         = 0
0.00.039.926 I print_info: n_expert_used    = 0
0.00.039.926 I print_info: causal attn      = 1
0.00.039.926 I print_info: pooling type     = 0
0.00.039.928 I print_info: rope type        = 2
0.00.039.929 I print_info: rope scaling     = linear
0.00.039.930 I print_info: freq_base_train  = 10000.0
0.00.039.930 I print_info: freq_scale_train = 1
0.00.039.930 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.930 I print_info: rope_finetuned   = unknown
0.00.039.930 I print_info: ssm_d_conv       = 0
0.00.039.931 I print_info: ssm_d_inner      = 0
0.00.039.931 I print_info: ssm_d_state      = 0
0.00.039.931 I print_info: ssm_dt_rank      = 0
0.00.039.931 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.931 I print_info: model type       = 1.4B
0.00.039.932 I print_info: model params     = 1.41 B
0.00.039.932 I print_info: general.name     = 1.4B
0.00.039.932 I print_info: vocab type       = BPE
0.00.039.932 I print_info: n_vocab          = 50304
0.00.039.932 I print_info: n_merges         = 50009
0.00.039.933 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.933 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.937 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.937 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.937 I print_info: LF token         = 187 'Ċ'
0.00.039.941 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.942 I print_info: max token length = 1024
0.00.039.942 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.092.865 I load_tensors: offloading 24 repeating layers to GPU
0.01.092.874 I load_tensors: offloading output layer to GPU
0.01.092.875 I load_tensors: offloaded 25/25 layers to GPU
0.01.092.893 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.01.092.894 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.01.093.676 I llama_init_from_model: n_seq_max     = 1
0.01.093.679 I llama_init_from_model: n_ctx         = 2048
0.01.093.680 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.093.680 I llama_init_from_model: n_batch       = 2048
0.01.093.680 I llama_init_from_model: n_ubatch      = 512
0.01.093.681 I llama_init_from_model: flash_attn    = 0
0.01.093.682 I llama_init_from_model: freq_base     = 10000.0
0.01.093.683 I llama_init_from_model: freq_scale    = 1
0.01.093.684 I ggml_metal_init: allocating
0.01.093.722 I ggml_metal_init: found device: Apple M4
0.01.093.732 I ggml_metal_init: picking default device: Apple M4
0.01.094.790 I ggml_metal_init: using embedded metal library
0.01.099.086 I ggml_metal_init: GPU name:   Apple M4
0.01.099.094 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.099.094 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.099.095 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.099.095 I ggml_metal_init: simdgroup reduction   = true
0.01.099.096 I ggml_metal_init: simdgroup matrix mul. = true
0.01.099.096 I ggml_metal_init: has residency sets    = true
0.01.099.096 I ggml_metal_init: has bfloat            = true
0.01.099.097 I ggml_metal_init: use bfloat            = true
0.01.099.098 I ggml_metal_init: hasUnifiedMemory      = true
0.01.099.100 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.114.476 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.146.366 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.146.373 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.146.396 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.150.926 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.150.928 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.150.929 I llama_init_from_model: graph nodes  = 967
0.01.150.929 I llama_init_from_model: graph splits = 2
0.01.150.932 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.151.077 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.151.078 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.206.074 I main: llama threadpool init, n_threads = 4
0.01.206.118 I 
0.01.206.133 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.206.133 I 
0.01.206.289 I sampler seed: 1234
0.01.206.294 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.206.305 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.206.305 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.206.305 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.955.299 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 50070.52 tokens per second)
0.01.955.300 I llama_perf_context_print:        load time =    1195.97 ms
0.01.955.303 I llama_perf_context_print: prompt eval time =      48.79 ms /     7 tokens (    6.97 ms per token,   143.48 tokens per second)
0.01.955.304 I llama_perf_context_print:        eval time =     697.79 ms /    63 runs   (   11.08 ms per token,    90.28 tokens per second)
0.01.955.304 I llama_perf_context_print:       total time =     749.93 ms /    70 tokens
0.01.955.554 I ggml_metal_free: deallocating

real	0m1.972s
user	0m0.104s
sys	0m0.166s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.059 I build: 4702 (a394039d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.093 I main: llama backend init
0.00.000.096 I main: load the model and apply lora adapter, if any
0.00.009.872 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.266 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.273 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.274 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.277 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.277 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.277 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.278 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.279 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.279 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.280 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.280 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.280 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.281 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.281 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.283 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.283 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.284 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.229 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.273 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.095 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.096 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.096 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.097 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.097 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.097 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.098 I llama_model_loader: - type  f32:  194 tensors
0.00.026.098 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.099 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.099 I print_info: file format = GGUF V3 (latest)
0.00.026.100 I print_info: file type   = Q5_0
0.00.026.101 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.135 I load: special tokens cache size = 25
0.00.040.394 I load: token to piece cache size = 0.2984 MB
0.00.040.398 I print_info: arch             = gptneox
0.00.040.399 I print_info: vocab_only       = 0
0.00.040.399 I print_info: n_ctx_train      = 2048
0.00.040.399 I print_info: n_embd           = 2048
0.00.040.399 I print_info: n_layer          = 24
0.00.040.403 I print_info: n_head           = 16
0.00.040.404 I print_info: n_head_kv        = 16
0.00.040.405 I print_info: n_rot            = 32
0.00.040.405 I print_info: n_swa            = 0
0.00.040.405 I print_info: n_embd_head_k    = 128
0.00.040.405 I print_info: n_embd_head_v    = 128
0.00.040.406 I print_info: n_gqa            = 1
0.00.040.406 I print_info: n_embd_k_gqa     = 2048
0.00.040.407 I print_info: n_embd_v_gqa     = 2048
0.00.040.408 I print_info: f_norm_eps       = 1.0e-05
0.00.040.408 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.408 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.408 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.409 I print_info: f_logit_scale    = 0.0e+00
0.00.040.409 I print_info: n_ff             = 8192
0.00.040.409 I print_info: n_expert         = 0
0.00.040.410 I print_info: n_expert_used    = 0
0.00.040.410 I print_info: causal attn      = 1
0.00.040.410 I print_info: pooling type     = 0
0.00.040.410 I print_info: rope type        = 2
0.00.040.410 I print_info: rope scaling     = linear
0.00.040.414 I print_info: freq_base_train  = 10000.0
0.00.040.414 I print_info: freq_scale_train = 1
0.00.040.414 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.415 I print_info: rope_finetuned   = unknown
0.00.040.415 I print_info: ssm_d_conv       = 0
0.00.040.415 I print_info: ssm_d_inner      = 0
0.00.040.415 I print_info: ssm_d_state      = 0
0.00.040.415 I print_info: ssm_dt_rank      = 0
0.00.040.415 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.415 I print_info: model type       = 1.4B
0.00.040.416 I print_info: model params     = 1.41 B
0.00.040.417 I print_info: general.name     = 1.4B
0.00.040.419 I print_info: vocab type       = BPE
0.00.040.419 I print_info: n_vocab          = 50304
0.00.040.419 I print_info: n_merges         = 50009
0.00.040.419 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.419 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.419 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.420 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.420 I print_info: LF token         = 187 'Ċ'
0.00.040.420 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.420 I print_info: max token length = 1024
0.00.040.421 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.661.749 I load_tensors: offloading 24 repeating layers to GPU
0.00.661.766 I load_tensors: offloading output layer to GPU
0.00.661.767 I load_tensors: offloaded 25/25 layers to GPU
0.00.661.800 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.661.801 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.663.267 I llama_init_from_model: n_seq_max     = 1
0.00.663.269 I llama_init_from_model: n_ctx         = 2048
0.00.663.270 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.663.271 I llama_init_from_model: n_batch       = 2048
0.00.663.271 I llama_init_from_model: n_ubatch      = 512
0.00.663.272 I llama_init_from_model: flash_attn    = 0
0.00.663.274 I llama_init_from_model: freq_base     = 10000.0
0.00.663.275 I llama_init_from_model: freq_scale    = 1
0.00.663.278 I ggml_metal_init: allocating
0.00.663.370 I ggml_metal_init: found device: Apple M4
0.00.663.385 I ggml_metal_init: picking default device: Apple M4
0.00.665.219 I ggml_metal_init: using embedded metal library
0.00.671.876 I ggml_metal_init: GPU name:   Apple M4
0.00.671.882 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.671.883 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.671.884 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.671.884 I ggml_metal_init: simdgroup reduction   = true
0.00.671.885 I ggml_metal_init: simdgroup matrix mul. = true
0.00.671.885 I ggml_metal_init: has residency sets    = true
0.00.671.885 I ggml_metal_init: has bfloat            = true
0.00.671.885 I ggml_metal_init: use bfloat            = true
0.00.671.886 I ggml_metal_init: hasUnifiedMemory      = true
0.00.671.891 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.689.284 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.751.283 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.751.290 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.751.317 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.756.123 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.756.125 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.756.125 I llama_init_from_model: graph nodes  = 967
0.00.756.126 I llama_init_from_model: graph splits = 2
0.00.756.131 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.756.245 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.756.245 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.815.618 I main: llama threadpool init, n_threads = 4
0.00.815.665 I 
0.00.815.680 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.815.680 I 
0.00.815.832 I sampler seed: 1234
0.00.815.837 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.815.881 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.815.885 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.815.885 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.612.095 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51449.28 tokens per second)
0.01.612.096 I llama_perf_context_print:        load time =     805.05 ms
0.01.612.097 I llama_perf_context_print: prompt eval time =      51.13 ms /     7 tokens (    7.30 ms per token,   136.91 tokens per second)
0.01.612.099 I llama_perf_context_print:        eval time =     742.12 ms /    63 runs   (   11.78 ms per token,    84.89 tokens per second)
0.01.612.099 I llama_perf_context_print:       total time =     797.17 ms /    70 tokens
0.01.612.378 I ggml_metal_free: deallocating

real	0m1.631s
user	0m0.109s
sys	0m0.212s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4702 (a394039d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.008.842 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.979 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.984 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.986 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.991 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.991 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.992 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.992 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.993 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.993 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.994 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.994 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.994 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.995 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.995 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.999 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.000 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.000 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.773 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.757 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.498 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.499 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.500 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.500 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.500 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.501 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.501 I llama_model_loader: - type  f32:  194 tensors
0.00.025.501 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.502 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.502 I print_info: file format = GGUF V3 (latest)
0.00.025.503 I print_info: file type   = Q5_1
0.00.025.503 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.251 I load: special tokens cache size = 25
0.00.039.453 I load: token to piece cache size = 0.2984 MB
0.00.039.456 I print_info: arch             = gptneox
0.00.039.456 I print_info: vocab_only       = 0
0.00.039.456 I print_info: n_ctx_train      = 2048
0.00.039.456 I print_info: n_embd           = 2048
0.00.039.457 I print_info: n_layer          = 24
0.00.039.459 I print_info: n_head           = 16
0.00.039.460 I print_info: n_head_kv        = 16
0.00.039.460 I print_info: n_rot            = 32
0.00.039.461 I print_info: n_swa            = 0
0.00.039.461 I print_info: n_embd_head_k    = 128
0.00.039.462 I print_info: n_embd_head_v    = 128
0.00.039.463 I print_info: n_gqa            = 1
0.00.039.463 I print_info: n_embd_k_gqa     = 2048
0.00.039.465 I print_info: n_embd_v_gqa     = 2048
0.00.039.466 I print_info: f_norm_eps       = 1.0e-05
0.00.039.466 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.466 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.468 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.468 I print_info: f_logit_scale    = 0.0e+00
0.00.039.469 I print_info: n_ff             = 8192
0.00.039.469 I print_info: n_expert         = 0
0.00.039.470 I print_info: n_expert_used    = 0
0.00.039.470 I print_info: causal attn      = 1
0.00.039.470 I print_info: pooling type     = 0
0.00.039.472 I print_info: rope type        = 2
0.00.039.472 I print_info: rope scaling     = linear
0.00.039.476 I print_info: freq_base_train  = 10000.0
0.00.039.477 I print_info: freq_scale_train = 1
0.00.039.477 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.477 I print_info: rope_finetuned   = unknown
0.00.039.477 I print_info: ssm_d_conv       = 0
0.00.039.481 I print_info: ssm_d_inner      = 0
0.00.039.483 I print_info: ssm_d_state      = 0
0.00.039.483 I print_info: ssm_dt_rank      = 0
0.00.039.483 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.483 I print_info: model type       = 1.4B
0.00.039.484 I print_info: model params     = 1.41 B
0.00.039.484 I print_info: general.name     = 1.4B
0.00.039.485 I print_info: vocab type       = BPE
0.00.039.485 I print_info: n_vocab          = 50304
0.00.039.485 I print_info: n_merges         = 50009
0.00.039.485 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.486 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.486 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.486 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.486 I print_info: LF token         = 187 'Ċ'
0.00.039.486 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.487 I print_info: max token length = 1024
0.00.039.487 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.661.771 I load_tensors: offloading 24 repeating layers to GPU
0.00.661.786 I load_tensors: offloading output layer to GPU
0.00.661.787 I load_tensors: offloaded 25/25 layers to GPU
0.00.661.820 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.661.821 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.663.473 I llama_init_from_model: n_seq_max     = 1
0.00.663.476 I llama_init_from_model: n_ctx         = 2048
0.00.663.476 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.663.476 I llama_init_from_model: n_batch       = 2048
0.00.663.477 I llama_init_from_model: n_ubatch      = 512
0.00.663.477 I llama_init_from_model: flash_attn    = 0
0.00.663.479 I llama_init_from_model: freq_base     = 10000.0
0.00.663.479 I llama_init_from_model: freq_scale    = 1
0.00.663.480 I ggml_metal_init: allocating
0.00.663.499 I ggml_metal_init: found device: Apple M4
0.00.663.510 I ggml_metal_init: picking default device: Apple M4
0.00.665.042 I ggml_metal_init: using embedded metal library
0.00.671.169 I ggml_metal_init: GPU name:   Apple M4
0.00.671.174 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.671.175 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.671.176 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.671.177 I ggml_metal_init: simdgroup reduction   = true
0.00.671.177 I ggml_metal_init: simdgroup matrix mul. = true
0.00.671.177 I ggml_metal_init: has residency sets    = true
0.00.671.177 I ggml_metal_init: has bfloat            = true
0.00.671.178 I ggml_metal_init: use bfloat            = true
0.00.671.178 I ggml_metal_init: hasUnifiedMemory      = true
0.00.671.180 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.688.916 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.745.954 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.745.962 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.746.031 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.750.647 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.750.649 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.750.650 I llama_init_from_model: graph nodes  = 967
0.00.750.650 I llama_init_from_model: graph splits = 2
0.00.750.655 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.750.783 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.750.784 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.807.713 I main: llama threadpool init, n_threads = 4
0.00.807.759 I 
0.00.807.799 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.807.804 I 
0.00.808.070 I sampler seed: 1234
0.00.808.080 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.808.097 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.808.099 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.808.099 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.643.538 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52205.88 tokens per second)
0.01.643.539 I llama_perf_context_print:        load time =     798.12 ms
0.01.643.542 I llama_perf_context_print: prompt eval time =      42.24 ms /     7 tokens (    6.03 ms per token,   165.71 tokens per second)
0.01.643.542 I llama_perf_context_print:        eval time =     790.30 ms /    63 runs   (   12.54 ms per token,    79.72 tokens per second)
0.01.643.543 I llama_perf_context_print:       total time =     836.57 ms /    70 tokens
0.01.643.820 I ggml_metal_free: deallocating

real	0m1.659s
user	0m0.110s
sys	0m0.207s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4702 (a394039d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.009.948 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.736 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.741 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.747 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.748 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.748 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.749 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.749 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.750 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.750 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.751 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.753 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.753 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.753 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.754 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.755 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.755 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.757 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.626 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.618 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.452 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.453 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.453 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.453 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.453 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.454 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.454 I llama_model_loader: - type  f32:  194 tensors
0.00.025.454 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.454 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.455 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.455 I print_info: file format = GGUF V3 (latest)
0.00.025.456 I print_info: file type   = Q2_K - Medium
0.00.025.459 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.251 I load: special tokens cache size = 25
0.00.039.312 I load: token to piece cache size = 0.2984 MB
0.00.039.315 I print_info: arch             = gptneox
0.00.039.315 I print_info: vocab_only       = 0
0.00.039.315 I print_info: n_ctx_train      = 2048
0.00.039.315 I print_info: n_embd           = 2048
0.00.039.316 I print_info: n_layer          = 24
0.00.039.318 I print_info: n_head           = 16
0.00.039.319 I print_info: n_head_kv        = 16
0.00.039.319 I print_info: n_rot            = 32
0.00.039.319 I print_info: n_swa            = 0
0.00.039.321 I print_info: n_embd_head_k    = 128
0.00.039.321 I print_info: n_embd_head_v    = 128
0.00.039.322 I print_info: n_gqa            = 1
0.00.039.323 I print_info: n_embd_k_gqa     = 2048
0.00.039.323 I print_info: n_embd_v_gqa     = 2048
0.00.039.324 I print_info: f_norm_eps       = 1.0e-05
0.00.039.329 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.329 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.330 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.330 I print_info: f_logit_scale    = 0.0e+00
0.00.039.330 I print_info: n_ff             = 8192
0.00.039.331 I print_info: n_expert         = 0
0.00.039.331 I print_info: n_expert_used    = 0
0.00.039.331 I print_info: causal attn      = 1
0.00.039.332 I print_info: pooling type     = 0
0.00.039.333 I print_info: rope type        = 2
0.00.039.333 I print_info: rope scaling     = linear
0.00.039.333 I print_info: freq_base_train  = 10000.0
0.00.039.334 I print_info: freq_scale_train = 1
0.00.039.334 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.334 I print_info: rope_finetuned   = unknown
0.00.039.334 I print_info: ssm_d_conv       = 0
0.00.039.335 I print_info: ssm_d_inner      = 0
0.00.039.336 I print_info: ssm_d_state      = 0
0.00.039.336 I print_info: ssm_dt_rank      = 0
0.00.039.336 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.336 I print_info: model type       = 1.4B
0.00.039.337 I print_info: model params     = 1.41 B
0.00.039.337 I print_info: general.name     = 1.4B
0.00.039.338 I print_info: vocab type       = BPE
0.00.039.338 I print_info: n_vocab          = 50304
0.00.039.338 I print_info: n_merges         = 50009
0.00.039.339 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.339 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.339 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.339 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.339 I print_info: LF token         = 187 'Ċ'
0.00.039.340 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.340 I print_info: max token length = 1024
0.00.039.340 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.342.940 I load_tensors: offloading 24 repeating layers to GPU
0.00.342.953 I load_tensors: offloading output layer to GPU
0.00.342.954 I load_tensors: offloaded 25/25 layers to GPU
0.00.342.988 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.342.990 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.344.394 I llama_init_from_model: n_seq_max     = 1
0.00.344.396 I llama_init_from_model: n_ctx         = 2048
0.00.344.397 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.344.398 I llama_init_from_model: n_batch       = 2048
0.00.344.398 I llama_init_from_model: n_ubatch      = 512
0.00.344.399 I llama_init_from_model: flash_attn    = 0
0.00.344.401 I llama_init_from_model: freq_base     = 10000.0
0.00.344.401 I llama_init_from_model: freq_scale    = 1
0.00.344.407 I ggml_metal_init: allocating
0.00.344.521 I ggml_metal_init: found device: Apple M4
0.00.344.538 I ggml_metal_init: picking default device: Apple M4
0.00.346.534 I ggml_metal_init: using embedded metal library
0.00.352.094 I ggml_metal_init: GPU name:   Apple M4
0.00.352.108 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.352.109 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.352.110 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.352.111 I ggml_metal_init: simdgroup reduction   = true
0.00.352.111 I ggml_metal_init: simdgroup matrix mul. = true
0.00.352.111 I ggml_metal_init: has residency sets    = true
0.00.352.112 I ggml_metal_init: has bfloat            = true
0.00.352.112 I ggml_metal_init: use bfloat            = true
0.00.352.114 I ggml_metal_init: hasUnifiedMemory      = true
0.00.352.118 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.373.299 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.434.572 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.434.581 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.434.606 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.438.918 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.438.921 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.438.921 I llama_init_from_model: graph nodes  = 967
0.00.438.921 I llama_init_from_model: graph splits = 2
0.00.438.926 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.439.062 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.439.062 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.496.414 I main: llama threadpool init, n_threads = 4
0.00.496.455 I 
0.00.496.470 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.496.471 I 
0.00.496.625 I sampler seed: 1234
0.00.496.629 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.496.671 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.496.674 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.496.674 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.167.804 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51337.67 tokens per second)
0.01.167.805 I llama_perf_context_print:        load time =     485.78 ms
0.01.167.805 I llama_perf_context_print: prompt eval time =      35.73 ms /     7 tokens (    5.10 ms per token,   195.89 tokens per second)
0.01.167.807 I llama_perf_context_print:        eval time =     632.46 ms /    63 runs   (   10.04 ms per token,    99.61 tokens per second)
0.01.167.808 I llama_perf_context_print:       total time =     672.08 ms /    70 tokens
0.01.168.032 I ggml_metal_free: deallocating

real	0m1.186s
user	0m0.111s
sys	0m0.172s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4702 (a394039d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.008.774 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.267 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.272 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.274 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.274 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.274 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.275 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.275 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.276 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.276 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.277 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.277 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.279 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.280 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.280 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.284 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.284 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.285 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.053 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.073 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.824 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.825 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.825 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.826 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.826 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.826 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.827 I llama_model_loader: - type  f32:  194 tensors
0.00.024.827 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.827 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.828 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.828 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.829 I print_info: file format = GGUF V3 (latest)
0.00.024.829 I print_info: file type   = Q3_K - Medium
0.00.024.830 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.605 I load: special tokens cache size = 25
0.00.038.629 I load: token to piece cache size = 0.2984 MB
0.00.038.631 I print_info: arch             = gptneox
0.00.038.632 I print_info: vocab_only       = 0
0.00.038.632 I print_info: n_ctx_train      = 2048
0.00.038.632 I print_info: n_embd           = 2048
0.00.038.632 I print_info: n_layer          = 24
0.00.038.635 I print_info: n_head           = 16
0.00.038.635 I print_info: n_head_kv        = 16
0.00.038.635 I print_info: n_rot            = 32
0.00.038.636 I print_info: n_swa            = 0
0.00.038.636 I print_info: n_embd_head_k    = 128
0.00.038.636 I print_info: n_embd_head_v    = 128
0.00.038.637 I print_info: n_gqa            = 1
0.00.038.637 I print_info: n_embd_k_gqa     = 2048
0.00.038.638 I print_info: n_embd_v_gqa     = 2048
0.00.038.641 I print_info: f_norm_eps       = 1.0e-05
0.00.038.641 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.642 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.642 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.642 I print_info: f_logit_scale    = 0.0e+00
0.00.038.642 I print_info: n_ff             = 8192
0.00.038.643 I print_info: n_expert         = 0
0.00.038.643 I print_info: n_expert_used    = 0
0.00.038.645 I print_info: causal attn      = 1
0.00.038.646 I print_info: pooling type     = 0
0.00.038.646 I print_info: rope type        = 2
0.00.038.646 I print_info: rope scaling     = linear
0.00.038.647 I print_info: freq_base_train  = 10000.0
0.00.038.647 I print_info: freq_scale_train = 1
0.00.038.647 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.647 I print_info: rope_finetuned   = unknown
0.00.038.648 I print_info: ssm_d_conv       = 0
0.00.038.648 I print_info: ssm_d_inner      = 0
0.00.038.648 I print_info: ssm_d_state      = 0
0.00.038.648 I print_info: ssm_dt_rank      = 0
0.00.038.648 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.648 I print_info: model type       = 1.4B
0.00.038.649 I print_info: model params     = 1.41 B
0.00.038.650 I print_info: general.name     = 1.4B
0.00.038.651 I print_info: vocab type       = BPE
0.00.038.651 I print_info: n_vocab          = 50304
0.00.038.652 I print_info: n_merges         = 50009
0.00.038.652 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.652 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.652 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.656 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.657 I print_info: LF token         = 187 'Ċ'
0.00.038.657 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.657 I print_info: max token length = 1024
0.00.038.658 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.459.879 I load_tensors: offloading 24 repeating layers to GPU
0.00.459.893 I load_tensors: offloading output layer to GPU
0.00.459.894 I load_tensors: offloaded 25/25 layers to GPU
0.00.459.923 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.459.924 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.461.416 I llama_init_from_model: n_seq_max     = 1
0.00.461.420 I llama_init_from_model: n_ctx         = 2048
0.00.461.421 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.461.421 I llama_init_from_model: n_batch       = 2048
0.00.461.422 I llama_init_from_model: n_ubatch      = 512
0.00.461.422 I llama_init_from_model: flash_attn    = 0
0.00.461.424 I llama_init_from_model: freq_base     = 10000.0
0.00.461.424 I llama_init_from_model: freq_scale    = 1
0.00.461.427 I ggml_metal_init: allocating
0.00.461.482 I ggml_metal_init: found device: Apple M4
0.00.461.494 I ggml_metal_init: picking default device: Apple M4
0.00.463.255 I ggml_metal_init: using embedded metal library
0.00.468.855 I ggml_metal_init: GPU name:   Apple M4
0.00.468.862 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.468.863 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.468.864 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.468.864 I ggml_metal_init: simdgroup reduction   = true
0.00.468.865 I ggml_metal_init: simdgroup matrix mul. = true
0.00.468.865 I ggml_metal_init: has residency sets    = true
0.00.468.865 I ggml_metal_init: has bfloat            = true
0.00.468.865 I ggml_metal_init: use bfloat            = true
0.00.468.870 I ggml_metal_init: hasUnifiedMemory      = true
0.00.468.874 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.489.862 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.546.581 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.546.589 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.546.624 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.550.888 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.550.890 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.550.891 I llama_init_from_model: graph nodes  = 967
0.00.550.891 I llama_init_from_model: graph splits = 2
0.00.550.896 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.551.012 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.551.012 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.607.653 I main: llama threadpool init, n_threads = 4
0.00.607.695 I 
0.00.607.711 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.607.711 I 
0.00.607.872 I sampler seed: 1234
0.00.607.876 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.607.896 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.607.896 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.607.896 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.341.905 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52553.66 tokens per second)
0.01.341.906 I llama_perf_context_print:        load time =     598.19 ms
0.01.341.907 I llama_perf_context_print: prompt eval time =      40.34 ms /     7 tokens (    5.76 ms per token,   173.51 tokens per second)
0.01.341.910 I llama_perf_context_print:        eval time =     690.75 ms /    63 runs   (   10.96 ms per token,    91.21 tokens per second)
0.01.341.911 I llama_perf_context_print:       total time =     734.94 ms /    70 tokens
0.01.342.126 I ggml_metal_free: deallocating

real	0m1.358s
user	0m0.110s
sys	0m0.199s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4702 (a394039d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.827 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.276 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.287 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.289 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.290 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.290 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.291 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.291 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.292 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.292 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.295 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.296 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.296 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.297 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.297 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.299 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.299 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.300 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.157 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.109 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.839 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.840 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.841 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.841 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.841 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.842 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.842 I llama_model_loader: - type  f32:  194 tensors
0.00.024.843 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.843 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.843 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.843 I print_info: file format = GGUF V3 (latest)
0.00.024.844 I print_info: file type   = Q4_K - Medium
0.00.024.845 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.971 I load: special tokens cache size = 25
0.00.039.146 I load: token to piece cache size = 0.2984 MB
0.00.039.148 I print_info: arch             = gptneox
0.00.039.149 I print_info: vocab_only       = 0
0.00.039.149 I print_info: n_ctx_train      = 2048
0.00.039.149 I print_info: n_embd           = 2048
0.00.039.149 I print_info: n_layer          = 24
0.00.039.152 I print_info: n_head           = 16
0.00.039.152 I print_info: n_head_kv        = 16
0.00.039.153 I print_info: n_rot            = 32
0.00.039.153 I print_info: n_swa            = 0
0.00.039.153 I print_info: n_embd_head_k    = 128
0.00.039.153 I print_info: n_embd_head_v    = 128
0.00.039.154 I print_info: n_gqa            = 1
0.00.039.155 I print_info: n_embd_k_gqa     = 2048
0.00.039.155 I print_info: n_embd_v_gqa     = 2048
0.00.039.156 I print_info: f_norm_eps       = 1.0e-05
0.00.039.156 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.156 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.157 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.157 I print_info: f_logit_scale    = 0.0e+00
0.00.039.157 I print_info: n_ff             = 8192
0.00.039.158 I print_info: n_expert         = 0
0.00.039.158 I print_info: n_expert_used    = 0
0.00.039.158 I print_info: causal attn      = 1
0.00.039.158 I print_info: pooling type     = 0
0.00.039.158 I print_info: rope type        = 2
0.00.039.158 I print_info: rope scaling     = linear
0.00.039.159 I print_info: freq_base_train  = 10000.0
0.00.039.159 I print_info: freq_scale_train = 1
0.00.039.159 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.160 I print_info: rope_finetuned   = unknown
0.00.039.160 I print_info: ssm_d_conv       = 0
0.00.039.160 I print_info: ssm_d_inner      = 0
0.00.039.160 I print_info: ssm_d_state      = 0
0.00.039.160 I print_info: ssm_dt_rank      = 0
0.00.039.160 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.160 I print_info: model type       = 1.4B
0.00.039.161 I print_info: model params     = 1.41 B
0.00.039.161 I print_info: general.name     = 1.4B
0.00.039.162 I print_info: vocab type       = BPE
0.00.039.162 I print_info: n_vocab          = 50304
0.00.039.162 I print_info: n_merges         = 50009
0.00.039.162 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.162 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.164 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.164 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.164 I print_info: LF token         = 187 'Ċ'
0.00.039.164 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.165 I print_info: max token length = 1024
0.00.039.165 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.534.273 I load_tensors: offloading 24 repeating layers to GPU
0.00.534.285 I load_tensors: offloading output layer to GPU
0.00.534.286 I load_tensors: offloaded 25/25 layers to GPU
0.00.534.319 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.534.320 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.535.771 I llama_init_from_model: n_seq_max     = 1
0.00.535.774 I llama_init_from_model: n_ctx         = 2048
0.00.535.774 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.535.775 I llama_init_from_model: n_batch       = 2048
0.00.535.775 I llama_init_from_model: n_ubatch      = 512
0.00.535.776 I llama_init_from_model: flash_attn    = 0
0.00.535.778 I llama_init_from_model: freq_base     = 10000.0
0.00.535.778 I llama_init_from_model: freq_scale    = 1
0.00.535.780 I ggml_metal_init: allocating
0.00.535.845 I ggml_metal_init: found device: Apple M4
0.00.535.859 I ggml_metal_init: picking default device: Apple M4
0.00.537.652 I ggml_metal_init: using embedded metal library
0.00.544.469 I ggml_metal_init: GPU name:   Apple M4
0.00.544.474 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.544.475 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.544.475 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.544.476 I ggml_metal_init: simdgroup reduction   = true
0.00.544.476 I ggml_metal_init: simdgroup matrix mul. = true
0.00.544.477 I ggml_metal_init: has residency sets    = true
0.00.544.477 I ggml_metal_init: has bfloat            = true
0.00.544.477 I ggml_metal_init: use bfloat            = true
0.00.544.478 I ggml_metal_init: hasUnifiedMemory      = true
0.00.544.480 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.562.198 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.618.385 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.618.391 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.618.412 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.622.469 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.622.471 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.622.471 I llama_init_from_model: graph nodes  = 967
0.00.622.472 I llama_init_from_model: graph splits = 2
0.00.622.477 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.622.607 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.622.607 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.680.958 I main: llama threadpool init, n_threads = 4
0.00.681.001 I 
0.00.681.018 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.681.018 I 
0.00.681.171 I sampler seed: 1234
0.00.681.176 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.681.196 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.681.196 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.681.196 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.448.524 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51337.67 tokens per second)
0.01.448.525 I llama_perf_context_print:        load time =     671.43 ms
0.01.448.525 I llama_perf_context_print: prompt eval time =      56.19 ms /     7 tokens (    8.03 ms per token,   124.57 tokens per second)
0.01.448.526 I llama_perf_context_print:        eval time =     708.19 ms /    63 runs   (   11.24 ms per token,    88.96 tokens per second)
0.01.448.526 I llama_perf_context_print:       total time =     768.27 ms /    70 tokens
0.01.448.781 I ggml_metal_free: deallocating

real	0m1.466s
user	0m0.110s
sys	0m0.205s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4702 (a394039d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.011.459 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.897 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.902 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.904 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.904 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.905 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.905 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.905 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.906 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.907 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.907 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.907 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.908 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.908 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.908 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.910 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.910 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.911 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.618 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.613 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.341 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.342 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.342 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.342 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.343 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.343 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.343 I llama_model_loader: - type  f32:  194 tensors
0.00.026.344 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.344 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.345 I print_info: file format = GGUF V3 (latest)
0.00.026.345 I print_info: file type   = Q5_K - Medium
0.00.026.346 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.145 I load: special tokens cache size = 25
0.00.040.255 I load: token to piece cache size = 0.2984 MB
0.00.040.257 I print_info: arch             = gptneox
0.00.040.258 I print_info: vocab_only       = 0
0.00.040.258 I print_info: n_ctx_train      = 2048
0.00.040.258 I print_info: n_embd           = 2048
0.00.040.258 I print_info: n_layer          = 24
0.00.040.261 I print_info: n_head           = 16
0.00.040.262 I print_info: n_head_kv        = 16
0.00.040.262 I print_info: n_rot            = 32
0.00.040.262 I print_info: n_swa            = 0
0.00.040.262 I print_info: n_embd_head_k    = 128
0.00.040.263 I print_info: n_embd_head_v    = 128
0.00.040.263 I print_info: n_gqa            = 1
0.00.040.264 I print_info: n_embd_k_gqa     = 2048
0.00.040.265 I print_info: n_embd_v_gqa     = 2048
0.00.040.265 I print_info: f_norm_eps       = 1.0e-05
0.00.040.266 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.266 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.266 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.266 I print_info: f_logit_scale    = 0.0e+00
0.00.040.267 I print_info: n_ff             = 8192
0.00.040.267 I print_info: n_expert         = 0
0.00.040.267 I print_info: n_expert_used    = 0
0.00.040.267 I print_info: causal attn      = 1
0.00.040.268 I print_info: pooling type     = 0
0.00.040.268 I print_info: rope type        = 2
0.00.040.268 I print_info: rope scaling     = linear
0.00.040.268 I print_info: freq_base_train  = 10000.0
0.00.040.269 I print_info: freq_scale_train = 1
0.00.040.269 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.269 I print_info: rope_finetuned   = unknown
0.00.040.269 I print_info: ssm_d_conv       = 0
0.00.040.270 I print_info: ssm_d_inner      = 0
0.00.040.270 I print_info: ssm_d_state      = 0
0.00.040.270 I print_info: ssm_dt_rank      = 0
0.00.040.270 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.270 I print_info: model type       = 1.4B
0.00.040.271 I print_info: model params     = 1.41 B
0.00.040.271 I print_info: general.name     = 1.4B
0.00.040.271 I print_info: vocab type       = BPE
0.00.040.272 I print_info: n_vocab          = 50304
0.00.040.272 I print_info: n_merges         = 50009
0.00.040.272 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.272 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.272 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.273 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.274 I print_info: LF token         = 187 'Ċ'
0.00.040.276 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.276 I print_info: max token length = 1024
0.00.040.277 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.592.599 I load_tensors: offloading 24 repeating layers to GPU
0.00.592.616 I load_tensors: offloading output layer to GPU
0.00.592.617 I load_tensors: offloaded 25/25 layers to GPU
0.00.592.654 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.592.655 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.594.276 I llama_init_from_model: n_seq_max     = 1
0.00.594.278 I llama_init_from_model: n_ctx         = 2048
0.00.594.279 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.594.279 I llama_init_from_model: n_batch       = 2048
0.00.594.280 I llama_init_from_model: n_ubatch      = 512
0.00.594.280 I llama_init_from_model: flash_attn    = 0
0.00.594.281 I llama_init_from_model: freq_base     = 10000.0
0.00.594.282 I llama_init_from_model: freq_scale    = 1
0.00.594.286 I ggml_metal_init: allocating
0.00.594.381 I ggml_metal_init: found device: Apple M4
0.00.594.396 I ggml_metal_init: picking default device: Apple M4
0.00.596.053 I ggml_metal_init: using embedded metal library
0.00.602.115 I ggml_metal_init: GPU name:   Apple M4
0.00.602.119 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.602.119 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.602.120 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.602.121 I ggml_metal_init: simdgroup reduction   = true
0.00.602.121 I ggml_metal_init: simdgroup matrix mul. = true
0.00.602.121 I ggml_metal_init: has residency sets    = true
0.00.602.122 I ggml_metal_init: has bfloat            = true
0.00.602.122 I ggml_metal_init: use bfloat            = true
0.00.602.123 I ggml_metal_init: hasUnifiedMemory      = true
0.00.602.125 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.619.666 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.676.550 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.676.560 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.676.633 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.681.204 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.681.206 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.681.207 I llama_init_from_model: graph nodes  = 967
0.00.681.207 I llama_init_from_model: graph splits = 2
0.00.681.212 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.681.340 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.681.341 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.747.244 I main: llama threadpool init, n_threads = 4
0.00.747.288 I 
0.00.747.304 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.747.304 I 
0.00.747.476 I sampler seed: 1234
0.00.747.480 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.747.492 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.747.492 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.747.492 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.601.865 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54239.88 tokens per second)
0.01.601.866 I llama_perf_context_print:        load time =     735.09 ms
0.01.601.867 I llama_perf_context_print: prompt eval time =      55.54 ms /     7 tokens (    7.93 ms per token,   126.04 tokens per second)
0.01.601.867 I llama_perf_context_print:        eval time =     795.97 ms /    63 runs   (   12.63 ms per token,    79.15 tokens per second)
0.01.601.868 I llama_perf_context_print:       total time =     855.32 ms /    70 tokens
0.01.602.108 I ggml_metal_free: deallocating

real	0m1.620s
user	0m0.108s
sys	0m0.207s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4702 (a394039d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.008.762 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.272 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.276 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.282 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.283 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.283 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.284 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.284 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.285 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.285 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.286 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.286 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.286 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.287 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.287 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.288 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.289 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.289 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.140 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.156 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.951 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.952 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.952 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.953 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.953 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.953 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.954 I llama_model_loader: - type  f32:  194 tensors
0.00.023.954 I llama_model_loader: - type q6_K:   98 tensors
0.00.023.955 I print_info: file format = GGUF V3 (latest)
0.00.023.955 I print_info: file type   = Q6_K
0.00.023.956 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.031.798 I load: special tokens cache size = 25
0.00.037.885 I load: token to piece cache size = 0.2984 MB
0.00.037.888 I print_info: arch             = gptneox
0.00.037.888 I print_info: vocab_only       = 0
0.00.037.888 I print_info: n_ctx_train      = 2048
0.00.037.888 I print_info: n_embd           = 2048
0.00.037.888 I print_info: n_layer          = 24
0.00.037.891 I print_info: n_head           = 16
0.00.037.892 I print_info: n_head_kv        = 16
0.00.037.892 I print_info: n_rot            = 32
0.00.037.892 I print_info: n_swa            = 0
0.00.037.892 I print_info: n_embd_head_k    = 128
0.00.037.892 I print_info: n_embd_head_v    = 128
0.00.037.893 I print_info: n_gqa            = 1
0.00.037.894 I print_info: n_embd_k_gqa     = 2048
0.00.037.895 I print_info: n_embd_v_gqa     = 2048
0.00.037.895 I print_info: f_norm_eps       = 1.0e-05
0.00.037.895 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.896 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.896 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.896 I print_info: f_logit_scale    = 0.0e+00
0.00.037.897 I print_info: n_ff             = 8192
0.00.037.897 I print_info: n_expert         = 0
0.00.037.897 I print_info: n_expert_used    = 0
0.00.037.897 I print_info: causal attn      = 1
0.00.037.897 I print_info: pooling type     = 0
0.00.037.897 I print_info: rope type        = 2
0.00.037.898 I print_info: rope scaling     = linear
0.00.037.898 I print_info: freq_base_train  = 10000.0
0.00.037.898 I print_info: freq_scale_train = 1
0.00.037.898 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.899 I print_info: rope_finetuned   = unknown
0.00.037.901 I print_info: ssm_d_conv       = 0
0.00.037.901 I print_info: ssm_d_inner      = 0
0.00.037.901 I print_info: ssm_d_state      = 0
0.00.037.901 I print_info: ssm_dt_rank      = 0
0.00.037.902 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.902 I print_info: model type       = 1.4B
0.00.037.902 I print_info: model params     = 1.41 B
0.00.037.902 I print_info: general.name     = 1.4B
0.00.037.903 I print_info: vocab type       = BPE
0.00.037.903 I print_info: n_vocab          = 50304
0.00.037.903 I print_info: n_merges         = 50009
0.00.037.903 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.904 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.905 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.905 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.905 I print_info: LF token         = 187 'Ċ'
0.00.037.905 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.906 I print_info: max token length = 1024
0.00.037.906 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.638.105 I load_tensors: offloading 24 repeating layers to GPU
0.00.638.122 I load_tensors: offloading output layer to GPU
0.00.638.123 I load_tensors: offloaded 25/25 layers to GPU
0.00.638.158 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.638.160 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.639.853 I llama_init_from_model: n_seq_max     = 1
0.00.639.855 I llama_init_from_model: n_ctx         = 2048
0.00.639.855 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.639.856 I llama_init_from_model: n_batch       = 2048
0.00.639.857 I llama_init_from_model: n_ubatch      = 512
0.00.639.857 I llama_init_from_model: flash_attn    = 0
0.00.639.858 I llama_init_from_model: freq_base     = 10000.0
0.00.639.859 I llama_init_from_model: freq_scale    = 1
0.00.639.860 I ggml_metal_init: allocating
0.00.639.905 I ggml_metal_init: found device: Apple M4
0.00.639.917 I ggml_metal_init: picking default device: Apple M4
0.00.641.441 I ggml_metal_init: using embedded metal library
0.00.647.524 I ggml_metal_init: GPU name:   Apple M4
0.00.647.527 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.647.528 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.647.529 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.647.530 I ggml_metal_init: simdgroup reduction   = true
0.00.647.530 I ggml_metal_init: simdgroup matrix mul. = true
0.00.647.530 I ggml_metal_init: has residency sets    = true
0.00.647.531 I ggml_metal_init: has bfloat            = true
0.00.647.531 I ggml_metal_init: use bfloat            = true
0.00.647.532 I ggml_metal_init: hasUnifiedMemory      = true
0.00.647.533 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.664.164 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.717.820 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.717.829 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.717.858 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.722.413 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.722.415 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.722.415 I llama_init_from_model: graph nodes  = 967
0.00.722.416 I llama_init_from_model: graph splits = 2
0.00.722.426 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.722.540 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.722.541 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.786.611 I main: llama threadpool init, n_threads = 4
0.00.786.655 I 
0.00.786.670 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.786.671 I 
0.00.786.845 I sampler seed: 1234
0.00.786.850 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.786.871 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.786.872 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.786.872 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.668.016 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52788.10 tokens per second)
0.01.668.017 I llama_perf_context_print:        load time =     777.15 ms
0.01.668.017 I llama_perf_context_print: prompt eval time =      54.06 ms /     7 tokens (    7.72 ms per token,   129.49 tokens per second)
0.01.668.018 I llama_perf_context_print:        eval time =     824.13 ms /    63 runs   (   13.08 ms per token,    76.44 tokens per second)
0.01.668.018 I llama_perf_context_print:       total time =     882.10 ms /    70 tokens
0.01.668.287 I ggml_metal_free: deallocating

real	0m1.684s
user	0m0.107s
sys	0m0.213s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.746 I build: 4702 (a394039d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.806 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.690 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.696 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.698 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.704 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.705 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.705 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.706 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.707 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.708 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.709 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.709 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.709 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.710 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.711 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.715 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.716 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.716 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.028 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.904 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.045 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.047 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.048 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.048 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.049 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.049 I llama_model_loader: - type  f32:  194 tensors
0.00.055.050 I llama_model_loader: - type  f16:   98 tensors
0.00.055.051 I print_info: file format = GGUF V3 (latest)
0.00.055.051 I print_info: file type   = all F32 (guessed)
0.00.055.053 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.067.681 I load: special tokens cache size = 25
0.00.075.857 I load: token to piece cache size = 0.2984 MB
0.00.075.860 I print_info: arch             = gptneox
0.00.075.861 I print_info: vocab_only       = 0
0.00.075.861 I print_info: n_ctx_train      = 2048
0.00.075.861 I print_info: n_embd           = 2048
0.00.075.861 I print_info: n_layer          = 24
0.00.075.864 I print_info: n_head           = 16
0.00.075.866 I print_info: n_head_kv        = 16
0.00.075.866 I print_info: n_rot            = 32
0.00.075.866 I print_info: n_swa            = 0
0.00.075.866 I print_info: n_embd_head_k    = 128
0.00.075.866 I print_info: n_embd_head_v    = 128
0.00.075.867 I print_info: n_gqa            = 1
0.00.075.868 I print_info: n_embd_k_gqa     = 2048
0.00.075.869 I print_info: n_embd_v_gqa     = 2048
0.00.075.869 I print_info: f_norm_eps       = 1.0e-05
0.00.075.870 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.075.870 I print_info: f_clamp_kqv      = 0.0e+00
0.00.075.870 I print_info: f_max_alibi_bias = 0.0e+00
0.00.075.870 I print_info: f_logit_scale    = 0.0e+00
0.00.075.871 I print_info: n_ff             = 8192
0.00.075.871 I print_info: n_expert         = 0
0.00.075.871 I print_info: n_expert_used    = 0
0.00.075.871 I print_info: causal attn      = 1
0.00.075.873 I print_info: pooling type     = 0
0.00.075.873 I print_info: rope type        = 2
0.00.075.874 I print_info: rope scaling     = linear
0.00.075.874 I print_info: freq_base_train  = 10000.0
0.00.075.876 I print_info: freq_scale_train = 1
0.00.075.876 I print_info: n_ctx_orig_yarn  = 2048
0.00.075.877 I print_info: rope_finetuned   = unknown
0.00.075.877 I print_info: ssm_d_conv       = 0
0.00.075.877 I print_info: ssm_d_inner      = 0
0.00.075.877 I print_info: ssm_d_state      = 0
0.00.075.877 I print_info: ssm_dt_rank      = 0
0.00.075.877 I print_info: ssm_dt_b_c_rms   = 0
0.00.075.877 I print_info: model type       = 1.4B
0.00.075.878 I print_info: model params     = 1.41 B
0.00.075.878 I print_info: general.name     = 1.4B
0.00.075.882 I print_info: vocab type       = BPE
0.00.075.882 I print_info: n_vocab          = 50304
0.00.075.883 I print_info: n_merges         = 50009
0.00.075.883 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.075.883 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.075.883 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.075.883 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.075.884 I print_info: LF token         = 187 'Ċ'
0.00.075.885 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.075.885 I print_info: max token length = 1024
0.00.075.885 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.163.821 I load_tensors: offloading 24 repeating layers to GPU
0.01.163.825 I load_tensors: offloading output layer to GPU
0.01.163.826 I load_tensors: offloaded 25/25 layers to GPU
0.01.163.850 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.163.854 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.164.696 I llama_init_from_model: n_seq_max     = 1
0.01.164.697 I llama_init_from_model: n_ctx         = 128
0.01.164.697 I llama_init_from_model: n_ctx_per_seq = 128
0.01.164.698 I llama_init_from_model: n_batch       = 128
0.01.164.698 I llama_init_from_model: n_ubatch      = 128
0.01.164.698 I llama_init_from_model: flash_attn    = 0
0.01.164.699 I llama_init_from_model: freq_base     = 10000.0
0.01.164.699 I llama_init_from_model: freq_scale    = 1
0.01.164.699 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.164.700 I ggml_metal_init: allocating
0.01.164.756 I ggml_metal_init: found device: Apple M4
0.01.164.763 I ggml_metal_init: picking default device: Apple M4
0.01.165.838 I ggml_metal_init: using embedded metal library
0.01.169.716 I ggml_metal_init: GPU name:   Apple M4
0.01.169.718 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.169.719 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.169.719 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.169.720 I ggml_metal_init: simdgroup reduction   = true
0.01.169.720 I ggml_metal_init: simdgroup matrix mul. = true
0.01.169.720 I ggml_metal_init: has residency sets    = true
0.01.169.720 I ggml_metal_init: has bfloat            = true
0.01.169.720 I ggml_metal_init: use bfloat            = true
0.01.169.721 I ggml_metal_init: hasUnifiedMemory      = true
0.01.169.722 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.180.523 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.182.225 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.182.227 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.182.241 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.183.907 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.183.908 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.183.908 I llama_init_from_model: graph nodes  = 967
0.01.183.908 I llama_init_from_model: graph splits = 2
0.01.183.910 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.183.910 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.217.632 I 
0.01.217.657 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.217.680 I perplexity: tokenizing the input ..
0.01.222.428 I perplexity: tokenization took 4.747 ms
0.01.222.448 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.340.866 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.342.473 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.342.488 I llama_perf_context_print:        load time =    1193.82 ms
0.01.342.489 I llama_perf_context_print: prompt eval time =     118.17 ms /   128 tokens (    0.92 ms per token,  1083.21 tokens per second)
0.01.342.489 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.342.490 I llama_perf_context_print:       total time =     124.85 ms /   129 tokens
0.01.342.845 I ggml_metal_free: deallocating

real	0m1.530s
user	0m0.096s
sys	0m0.219s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4702 (a394039d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.074 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.035 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.039 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.046 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.047 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.047 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.047 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.048 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.049 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.049 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.050 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.050 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.050 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.050 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.051 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.053 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.054 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.054 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.054 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.116 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.923 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.924 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.925 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.925 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.925 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.926 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.926 I llama_model_loader: - type  f32:  194 tensors
0.00.025.926 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.927 I print_info: file format = GGUF V3 (latest)
0.00.025.927 I print_info: file type   = Q8_0
0.00.025.932 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.807 I load: special tokens cache size = 25
0.00.040.102 I load: token to piece cache size = 0.2984 MB
0.00.040.104 I print_info: arch             = gptneox
0.00.040.105 I print_info: vocab_only       = 0
0.00.040.105 I print_info: n_ctx_train      = 2048
0.00.040.105 I print_info: n_embd           = 2048
0.00.040.105 I print_info: n_layer          = 24
0.00.040.109 I print_info: n_head           = 16
0.00.040.109 I print_info: n_head_kv        = 16
0.00.040.110 I print_info: n_rot            = 32
0.00.040.110 I print_info: n_swa            = 0
0.00.040.110 I print_info: n_embd_head_k    = 128
0.00.040.110 I print_info: n_embd_head_v    = 128
0.00.040.112 I print_info: n_gqa            = 1
0.00.040.113 I print_info: n_embd_k_gqa     = 2048
0.00.040.113 I print_info: n_embd_v_gqa     = 2048
0.00.040.114 I print_info: f_norm_eps       = 1.0e-05
0.00.040.114 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.116 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.116 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.116 I print_info: f_logit_scale    = 0.0e+00
0.00.040.117 I print_info: n_ff             = 8192
0.00.040.117 I print_info: n_expert         = 0
0.00.040.117 I print_info: n_expert_used    = 0
0.00.040.118 I print_info: causal attn      = 1
0.00.040.118 I print_info: pooling type     = 0
0.00.040.118 I print_info: rope type        = 2
0.00.040.119 I print_info: rope scaling     = linear
0.00.040.119 I print_info: freq_base_train  = 10000.0
0.00.040.119 I print_info: freq_scale_train = 1
0.00.040.119 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.119 I print_info: rope_finetuned   = unknown
0.00.040.120 I print_info: ssm_d_conv       = 0
0.00.040.120 I print_info: ssm_d_inner      = 0
0.00.040.120 I print_info: ssm_d_state      = 0
0.00.040.120 I print_info: ssm_dt_rank      = 0
0.00.040.120 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.120 I print_info: model type       = 1.4B
0.00.040.121 I print_info: model params     = 1.41 B
0.00.040.121 I print_info: general.name     = 1.4B
0.00.040.121 I print_info: vocab type       = BPE
0.00.040.121 I print_info: n_vocab          = 50304
0.00.040.121 I print_info: n_merges         = 50009
0.00.040.126 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.126 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.126 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.126 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.126 I print_info: LF token         = 187 'Ċ'
0.00.040.127 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.127 I print_info: max token length = 1024
0.00.040.127 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.844.443 I load_tensors: offloading 24 repeating layers to GPU
0.00.844.446 I load_tensors: offloading output layer to GPU
0.00.844.446 I load_tensors: offloaded 25/25 layers to GPU
0.00.844.464 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.844.465 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.845.057 I llama_init_from_model: n_seq_max     = 1
0.00.845.058 I llama_init_from_model: n_ctx         = 128
0.00.845.059 I llama_init_from_model: n_ctx_per_seq = 128
0.00.845.059 I llama_init_from_model: n_batch       = 128
0.00.845.059 I llama_init_from_model: n_ubatch      = 128
0.00.845.059 I llama_init_from_model: flash_attn    = 0
0.00.845.063 I llama_init_from_model: freq_base     = 10000.0
0.00.845.063 I llama_init_from_model: freq_scale    = 1
0.00.845.063 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.845.064 I ggml_metal_init: allocating
0.00.845.088 I ggml_metal_init: found device: Apple M4
0.00.845.094 I ggml_metal_init: picking default device: Apple M4
0.00.845.663 I ggml_metal_init: using embedded metal library
0.00.848.137 I ggml_metal_init: GPU name:   Apple M4
0.00.848.139 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.848.139 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.848.140 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.848.141 I ggml_metal_init: simdgroup reduction   = true
0.00.848.141 I ggml_metal_init: simdgroup matrix mul. = true
0.00.848.142 I ggml_metal_init: has residency sets    = true
0.00.848.142 I ggml_metal_init: has bfloat            = true
0.00.848.142 I ggml_metal_init: use bfloat            = true
0.00.848.142 I ggml_metal_init: hasUnifiedMemory      = true
0.00.848.145 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.857.701 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.859.299 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.859.302 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.859.317 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.861.001 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.861.002 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.861.002 I llama_init_from_model: graph nodes  = 967
0.00.861.002 I llama_init_from_model: graph splits = 2
0.00.861.004 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.861.004 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.887.027 I 
0.00.887.052 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.887.065 I perplexity: tokenizing the input ..
0.00.890.850 I perplexity: tokenization took 3.783 ms
0.00.890.860 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.025.611 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.027.182 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.027.200 I llama_perf_context_print:        load time =     876.95 ms
0.01.027.202 I llama_perf_context_print: prompt eval time =     134.53 ms /   128 tokens (    1.05 ms per token,   951.49 tokens per second)
0.01.027.202 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.027.203 I llama_perf_context_print:       total time =     140.17 ms /   129 tokens
0.01.027.550 I ggml_metal_free: deallocating

real	0m1.042s
user	0m0.064s
sys	0m0.141s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4702 (a394039d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.994 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.386 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.391 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.399 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.400 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.400 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.400 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.401 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.402 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.402 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.402 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.403 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.405 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.405 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.405 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.407 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.408 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.408 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.490 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.524 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.502 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.503 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.504 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.504 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.504 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.505 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.505 I llama_model_loader: - type  f32:  194 tensors
0.00.026.506 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.506 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.507 I print_info: file format = GGUF V3 (latest)
0.00.026.507 I print_info: file type   = Q4_0
0.00.026.509 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.035.096 I load: special tokens cache size = 25
0.00.041.282 I load: token to piece cache size = 0.2984 MB
0.00.041.287 I print_info: arch             = gptneox
0.00.041.287 I print_info: vocab_only       = 0
0.00.041.287 I print_info: n_ctx_train      = 2048
0.00.041.287 I print_info: n_embd           = 2048
0.00.041.288 I print_info: n_layer          = 24
0.00.041.292 I print_info: n_head           = 16
0.00.041.293 I print_info: n_head_kv        = 16
0.00.041.293 I print_info: n_rot            = 32
0.00.041.293 I print_info: n_swa            = 0
0.00.041.293 I print_info: n_embd_head_k    = 128
0.00.041.294 I print_info: n_embd_head_v    = 128
0.00.041.294 I print_info: n_gqa            = 1
0.00.041.295 I print_info: n_embd_k_gqa     = 2048
0.00.041.296 I print_info: n_embd_v_gqa     = 2048
0.00.041.296 I print_info: f_norm_eps       = 1.0e-05
0.00.041.297 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.297 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.297 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.298 I print_info: f_logit_scale    = 0.0e+00
0.00.041.298 I print_info: n_ff             = 8192
0.00.041.301 I print_info: n_expert         = 0
0.00.041.301 I print_info: n_expert_used    = 0
0.00.041.301 I print_info: causal attn      = 1
0.00.041.301 I print_info: pooling type     = 0
0.00.041.302 I print_info: rope type        = 2
0.00.041.302 I print_info: rope scaling     = linear
0.00.041.302 I print_info: freq_base_train  = 10000.0
0.00.041.302 I print_info: freq_scale_train = 1
0.00.041.303 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.303 I print_info: rope_finetuned   = unknown
0.00.041.303 I print_info: ssm_d_conv       = 0
0.00.041.303 I print_info: ssm_d_inner      = 0
0.00.041.303 I print_info: ssm_d_state      = 0
0.00.041.305 I print_info: ssm_dt_rank      = 0
0.00.041.305 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.305 I print_info: model type       = 1.4B
0.00.041.306 I print_info: model params     = 1.41 B
0.00.041.306 I print_info: general.name     = 1.4B
0.00.041.306 I print_info: vocab type       = BPE
0.00.041.306 I print_info: n_vocab          = 50304
0.00.041.307 I print_info: n_merges         = 50009
0.00.041.307 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.307 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.307 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.307 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.307 I print_info: LF token         = 187 'Ċ'
0.00.041.308 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.308 I print_info: max token length = 1024
0.00.041.308 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.602.045 I load_tensors: offloading 24 repeating layers to GPU
0.00.602.059 I load_tensors: offloading output layer to GPU
0.00.602.060 I load_tensors: offloaded 25/25 layers to GPU
0.00.602.092 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.602.094 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.603.462 I llama_init_from_model: n_seq_max     = 1
0.00.603.473 I llama_init_from_model: n_ctx         = 128
0.00.603.473 I llama_init_from_model: n_ctx_per_seq = 128
0.00.603.474 I llama_init_from_model: n_batch       = 128
0.00.603.474 I llama_init_from_model: n_ubatch      = 128
0.00.603.474 I llama_init_from_model: flash_attn    = 0
0.00.603.476 I llama_init_from_model: freq_base     = 10000.0
0.00.603.476 I llama_init_from_model: freq_scale    = 1
0.00.603.477 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.603.480 I ggml_metal_init: allocating
0.00.603.535 I ggml_metal_init: found device: Apple M4
0.00.603.547 I ggml_metal_init: picking default device: Apple M4
0.00.605.306 I ggml_metal_init: using embedded metal library
0.00.612.439 I ggml_metal_init: GPU name:   Apple M4
0.00.612.447 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.612.447 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.612.448 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.612.449 I ggml_metal_init: simdgroup reduction   = true
0.00.612.449 I ggml_metal_init: simdgroup matrix mul. = true
0.00.612.449 I ggml_metal_init: has residency sets    = true
0.00.612.450 I ggml_metal_init: has bfloat            = true
0.00.612.450 I ggml_metal_init: use bfloat            = true
0.00.612.451 I ggml_metal_init: hasUnifiedMemory      = true
0.00.612.455 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.631.452 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.635.163 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.635.167 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.635.195 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.638.756 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.638.758 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.638.759 I llama_init_from_model: graph nodes  = 967
0.00.638.759 I llama_init_from_model: graph splits = 2
0.00.638.763 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.638.764 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.665.013 I 
0.00.665.070 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.665.098 I perplexity: tokenizing the input ..
0.00.671.784 I perplexity: tokenization took 6.684 ms
0.00.671.802 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.801.430 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.802.832 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.802.852 I llama_perf_context_print:        load time =     655.01 ms
0.00.802.853 I llama_perf_context_print: prompt eval time =     128.86 ms /   128 tokens (    1.01 ms per token,   993.30 tokens per second)
0.00.802.853 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.802.854 I llama_perf_context_print:       total time =     137.84 ms /   129 tokens
0.00.803.244 I ggml_metal_free: deallocating

real	0m0.820s
user	0m0.081s
sys	0m0.130s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4702 (a394039d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.441 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.762 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.768 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.770 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.770 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.771 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.771 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.771 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.772 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.773 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.773 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.773 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.774 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.774 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.774 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.776 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.777 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.777 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.675 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.685 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.576 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.577 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.577 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.578 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.578 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.578 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.579 I llama_model_loader: - type  f32:  194 tensors
0.00.025.579 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.580 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.580 I print_info: file format = GGUF V3 (latest)
0.00.025.581 I print_info: file type   = Q4_1
0.00.025.582 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.873 I load: special tokens cache size = 25
0.00.040.220 I load: token to piece cache size = 0.2984 MB
0.00.040.226 I print_info: arch             = gptneox
0.00.040.226 I print_info: vocab_only       = 0
0.00.040.226 I print_info: n_ctx_train      = 2048
0.00.040.226 I print_info: n_embd           = 2048
0.00.040.227 I print_info: n_layer          = 24
0.00.040.231 I print_info: n_head           = 16
0.00.040.232 I print_info: n_head_kv        = 16
0.00.040.232 I print_info: n_rot            = 32
0.00.040.232 I print_info: n_swa            = 0
0.00.040.232 I print_info: n_embd_head_k    = 128
0.00.040.232 I print_info: n_embd_head_v    = 128
0.00.040.233 I print_info: n_gqa            = 1
0.00.040.233 I print_info: n_embd_k_gqa     = 2048
0.00.040.234 I print_info: n_embd_v_gqa     = 2048
0.00.040.235 I print_info: f_norm_eps       = 1.0e-05
0.00.040.235 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.235 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.235 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.235 I print_info: f_logit_scale    = 0.0e+00
0.00.040.236 I print_info: n_ff             = 8192
0.00.040.236 I print_info: n_expert         = 0
0.00.040.236 I print_info: n_expert_used    = 0
0.00.040.236 I print_info: causal attn      = 1
0.00.040.236 I print_info: pooling type     = 0
0.00.040.237 I print_info: rope type        = 2
0.00.040.237 I print_info: rope scaling     = linear
0.00.040.237 I print_info: freq_base_train  = 10000.0
0.00.040.237 I print_info: freq_scale_train = 1
0.00.040.237 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.238 I print_info: rope_finetuned   = unknown
0.00.040.238 I print_info: ssm_d_conv       = 0
0.00.040.238 I print_info: ssm_d_inner      = 0
0.00.040.238 I print_info: ssm_d_state      = 0
0.00.040.238 I print_info: ssm_dt_rank      = 0
0.00.040.238 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.238 I print_info: model type       = 1.4B
0.00.040.239 I print_info: model params     = 1.41 B
0.00.040.239 I print_info: general.name     = 1.4B
0.00.040.239 I print_info: vocab type       = BPE
0.00.040.239 I print_info: n_vocab          = 50304
0.00.040.239 I print_info: n_merges         = 50009
0.00.040.240 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.240 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.240 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.240 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.240 I print_info: LF token         = 187 'Ċ'
0.00.040.240 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.241 I print_info: max token length = 1024
0.00.040.241 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.612.027 I load_tensors: offloading 24 repeating layers to GPU
0.00.612.030 I load_tensors: offloading output layer to GPU
0.00.612.031 I load_tensors: offloaded 25/25 layers to GPU
0.00.612.053 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.612.054 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.613.051 I llama_init_from_model: n_seq_max     = 1
0.00.613.055 I llama_init_from_model: n_ctx         = 128
0.00.613.055 I llama_init_from_model: n_ctx_per_seq = 128
0.00.613.055 I llama_init_from_model: n_batch       = 128
0.00.613.056 I llama_init_from_model: n_ubatch      = 128
0.00.613.056 I llama_init_from_model: flash_attn    = 0
0.00.613.057 I llama_init_from_model: freq_base     = 10000.0
0.00.613.058 I llama_init_from_model: freq_scale    = 1
0.00.613.058 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.613.059 I ggml_metal_init: allocating
0.00.613.108 I ggml_metal_init: found device: Apple M4
0.00.613.121 I ggml_metal_init: picking default device: Apple M4
0.00.614.156 I ggml_metal_init: using embedded metal library
0.00.620.502 I ggml_metal_init: GPU name:   Apple M4
0.00.620.510 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.620.510 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.620.511 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.620.511 I ggml_metal_init: simdgroup reduction   = true
0.00.620.512 I ggml_metal_init: simdgroup matrix mul. = true
0.00.620.512 I ggml_metal_init: has residency sets    = true
0.00.620.512 I ggml_metal_init: has bfloat            = true
0.00.620.512 I ggml_metal_init: use bfloat            = true
0.00.620.514 I ggml_metal_init: hasUnifiedMemory      = true
0.00.620.525 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.643.029 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.645.214 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.645.219 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.645.239 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.646.918 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.646.919 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.646.919 I llama_init_from_model: graph nodes  = 967
0.00.646.919 I llama_init_from_model: graph splits = 2
0.00.646.921 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.646.921 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.672.598 I 
0.00.672.623 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.672.635 I perplexity: tokenizing the input ..
0.00.676.472 I perplexity: tokenization took 3.835 ms
0.00.676.483 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.807.799 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.809.152 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.809.165 I llama_perf_context_print:        load time =     663.15 ms
0.00.809.168 I llama_perf_context_print: prompt eval time =     131.07 ms /   128 tokens (    1.02 ms per token,   976.54 tokens per second)
0.00.809.169 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.809.169 I llama_perf_context_print:       total time =     136.57 ms /   129 tokens
0.00.809.516 I ggml_metal_free: deallocating

real	0m0.824s
user	0m0.074s
sys	0m0.097s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4702 (a394039d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.507 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.728 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.020.732 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.734 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.734 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.734 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.735 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.735 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.736 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.738 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.739 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.739 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.739 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.740 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.740 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.742 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.742 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.743 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.703 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.694 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.598 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.599 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.600 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.600 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.600 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.601 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.029.601 I llama_model_loader: - type  f32:  194 tensors
0.00.029.601 I llama_model_loader: - type q5_0:   97 tensors
0.00.029.602 I llama_model_loader: - type q6_K:    1 tensors
0.00.029.602 I print_info: file format = GGUF V3 (latest)
0.00.029.603 I print_info: file type   = Q5_0
0.00.029.604 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.037.832 I load: special tokens cache size = 25
0.00.043.963 I load: token to piece cache size = 0.2984 MB
0.00.043.970 I print_info: arch             = gptneox
0.00.043.970 I print_info: vocab_only       = 0
0.00.043.971 I print_info: n_ctx_train      = 2048
0.00.043.971 I print_info: n_embd           = 2048
0.00.043.971 I print_info: n_layer          = 24
0.00.043.974 I print_info: n_head           = 16
0.00.043.975 I print_info: n_head_kv        = 16
0.00.043.976 I print_info: n_rot            = 32
0.00.043.976 I print_info: n_swa            = 0
0.00.043.976 I print_info: n_embd_head_k    = 128
0.00.043.976 I print_info: n_embd_head_v    = 128
0.00.043.977 I print_info: n_gqa            = 1
0.00.043.977 I print_info: n_embd_k_gqa     = 2048
0.00.043.978 I print_info: n_embd_v_gqa     = 2048
0.00.043.979 I print_info: f_norm_eps       = 1.0e-05
0.00.043.979 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.043.979 I print_info: f_clamp_kqv      = 0.0e+00
0.00.043.981 I print_info: f_max_alibi_bias = 0.0e+00
0.00.043.981 I print_info: f_logit_scale    = 0.0e+00
0.00.043.981 I print_info: n_ff             = 8192
0.00.043.982 I print_info: n_expert         = 0
0.00.043.982 I print_info: n_expert_used    = 0
0.00.043.982 I print_info: causal attn      = 1
0.00.043.982 I print_info: pooling type     = 0
0.00.043.982 I print_info: rope type        = 2
0.00.043.982 I print_info: rope scaling     = linear
0.00.043.983 I print_info: freq_base_train  = 10000.0
0.00.043.983 I print_info: freq_scale_train = 1
0.00.043.983 I print_info: n_ctx_orig_yarn  = 2048
0.00.043.985 I print_info: rope_finetuned   = unknown
0.00.043.985 I print_info: ssm_d_conv       = 0
0.00.043.985 I print_info: ssm_d_inner      = 0
0.00.043.986 I print_info: ssm_d_state      = 0
0.00.043.986 I print_info: ssm_dt_rank      = 0
0.00.043.986 I print_info: ssm_dt_b_c_rms   = 0
0.00.043.986 I print_info: model type       = 1.4B
0.00.043.986 I print_info: model params     = 1.41 B
0.00.043.987 I print_info: general.name     = 1.4B
0.00.043.987 I print_info: vocab type       = BPE
0.00.043.987 I print_info: n_vocab          = 50304
0.00.043.987 I print_info: n_merges         = 50009
0.00.043.987 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.043.988 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.043.988 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.043.988 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.043.988 I print_info: LF token         = 187 'Ċ'
0.00.043.989 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.043.989 I print_info: max token length = 1024
0.00.043.994 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.663.460 I load_tensors: offloading 24 repeating layers to GPU
0.00.663.475 I load_tensors: offloading output layer to GPU
0.00.663.476 I load_tensors: offloaded 25/25 layers to GPU
0.00.663.524 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.663.526 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.664.977 I llama_init_from_model: n_seq_max     = 1
0.00.664.979 I llama_init_from_model: n_ctx         = 128
0.00.664.980 I llama_init_from_model: n_ctx_per_seq = 128
0.00.664.980 I llama_init_from_model: n_batch       = 128
0.00.664.980 I llama_init_from_model: n_ubatch      = 128
0.00.664.981 I llama_init_from_model: flash_attn    = 0
0.00.664.983 I llama_init_from_model: freq_base     = 10000.0
0.00.664.984 I llama_init_from_model: freq_scale    = 1
0.00.664.984 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.664.987 I ggml_metal_init: allocating
0.00.665.064 I ggml_metal_init: found device: Apple M4
0.00.665.078 I ggml_metal_init: picking default device: Apple M4
0.00.666.771 I ggml_metal_init: using embedded metal library
0.00.673.308 I ggml_metal_init: GPU name:   Apple M4
0.00.673.314 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.673.315 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.673.315 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.673.316 I ggml_metal_init: simdgroup reduction   = true
0.00.673.316 I ggml_metal_init: simdgroup matrix mul. = true
0.00.673.316 I ggml_metal_init: has residency sets    = true
0.00.673.317 I ggml_metal_init: has bfloat            = true
0.00.673.317 I ggml_metal_init: use bfloat            = true
0.00.673.318 I ggml_metal_init: hasUnifiedMemory      = true
0.00.673.322 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.690.687 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.694.333 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.694.337 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.694.371 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.697.759 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.697.761 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.697.761 I llama_init_from_model: graph nodes  = 967
0.00.697.761 I llama_init_from_model: graph splits = 2
0.00.697.764 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.697.764 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.726.100 I 
0.00.726.166 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.726.197 I perplexity: tokenizing the input ..
0.00.733.580 I perplexity: tokenization took 7.38 ms
0.00.733.607 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.869.821 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.871.163 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.871.172 I llama_perf_context_print:        load time =     716.58 ms
0.00.871.175 I llama_perf_context_print: prompt eval time =     135.25 ms /   128 tokens (    1.06 ms per token,   946.42 tokens per second)
0.00.871.176 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.871.176 I llama_perf_context_print:       total time =     145.08 ms /   129 tokens
0.00.871.541 I ggml_metal_free: deallocating

real	0m0.887s
user	0m0.080s
sys	0m0.149s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4702 (a394039d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.035 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.075 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.081 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.082 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.084 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.084 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.085 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.085 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.086 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.086 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.087 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.087 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.087 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.088 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.088 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.090 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.091 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.091 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.971 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.031 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.922 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.923 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.924 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.924 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.924 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.925 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.925 I llama_model_loader: - type  f32:  194 tensors
0.00.024.925 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.926 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.926 I print_info: file format = GGUF V3 (latest)
0.00.024.927 I print_info: file type   = Q5_1
0.00.024.928 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.287 I load: special tokens cache size = 25
0.00.039.321 I load: token to piece cache size = 0.2984 MB
0.00.039.325 I print_info: arch             = gptneox
0.00.039.325 I print_info: vocab_only       = 0
0.00.039.325 I print_info: n_ctx_train      = 2048
0.00.039.325 I print_info: n_embd           = 2048
0.00.039.326 I print_info: n_layer          = 24
0.00.039.329 I print_info: n_head           = 16
0.00.039.330 I print_info: n_head_kv        = 16
0.00.039.331 I print_info: n_rot            = 32
0.00.039.331 I print_info: n_swa            = 0
0.00.039.331 I print_info: n_embd_head_k    = 128
0.00.039.331 I print_info: n_embd_head_v    = 128
0.00.039.332 I print_info: n_gqa            = 1
0.00.039.332 I print_info: n_embd_k_gqa     = 2048
0.00.039.333 I print_info: n_embd_v_gqa     = 2048
0.00.039.334 I print_info: f_norm_eps       = 1.0e-05
0.00.039.334 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.334 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.334 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.336 I print_info: f_logit_scale    = 0.0e+00
0.00.039.337 I print_info: n_ff             = 8192
0.00.039.337 I print_info: n_expert         = 0
0.00.039.337 I print_info: n_expert_used    = 0
0.00.039.337 I print_info: causal attn      = 1
0.00.039.337 I print_info: pooling type     = 0
0.00.039.339 I print_info: rope type        = 2
0.00.039.340 I print_info: rope scaling     = linear
0.00.039.340 I print_info: freq_base_train  = 10000.0
0.00.039.340 I print_info: freq_scale_train = 1
0.00.039.341 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.341 I print_info: rope_finetuned   = unknown
0.00.039.341 I print_info: ssm_d_conv       = 0
0.00.039.341 I print_info: ssm_d_inner      = 0
0.00.039.341 I print_info: ssm_d_state      = 0
0.00.039.342 I print_info: ssm_dt_rank      = 0
0.00.039.342 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.342 I print_info: model type       = 1.4B
0.00.039.342 I print_info: model params     = 1.41 B
0.00.039.343 I print_info: general.name     = 1.4B
0.00.039.343 I print_info: vocab type       = BPE
0.00.039.343 I print_info: n_vocab          = 50304
0.00.039.343 I print_info: n_merges         = 50009
0.00.039.343 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.344 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.344 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.344 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.344 I print_info: LF token         = 187 'Ċ'
0.00.039.345 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.345 I print_info: max token length = 1024
0.00.039.345 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.656.344 I load_tensors: offloading 24 repeating layers to GPU
0.00.656.353 I load_tensors: offloading output layer to GPU
0.00.656.353 I load_tensors: offloaded 25/25 layers to GPU
0.00.656.387 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.656.389 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.657.997 I llama_init_from_model: n_seq_max     = 1
0.00.658.000 I llama_init_from_model: n_ctx         = 128
0.00.658.000 I llama_init_from_model: n_ctx_per_seq = 128
0.00.658.001 I llama_init_from_model: n_batch       = 128
0.00.658.001 I llama_init_from_model: n_ubatch      = 128
0.00.658.001 I llama_init_from_model: flash_attn    = 0
0.00.658.004 I llama_init_from_model: freq_base     = 10000.0
0.00.658.004 I llama_init_from_model: freq_scale    = 1
0.00.658.005 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.658.007 I ggml_metal_init: allocating
0.00.658.086 I ggml_metal_init: found device: Apple M4
0.00.658.101 I ggml_metal_init: picking default device: Apple M4
0.00.659.907 I ggml_metal_init: using embedded metal library
0.00.666.694 I ggml_metal_init: GPU name:   Apple M4
0.00.666.701 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.666.701 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.666.702 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.666.703 I ggml_metal_init: simdgroup reduction   = true
0.00.666.703 I ggml_metal_init: simdgroup matrix mul. = true
0.00.666.703 I ggml_metal_init: has residency sets    = true
0.00.666.704 I ggml_metal_init: has bfloat            = true
0.00.666.704 I ggml_metal_init: use bfloat            = true
0.00.666.705 I ggml_metal_init: hasUnifiedMemory      = true
0.00.666.707 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.684.901 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.688.613 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.688.618 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.688.646 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.691.857 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.691.860 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.691.860 I llama_init_from_model: graph nodes  = 967
0.00.691.861 I llama_init_from_model: graph splits = 2
0.00.691.863 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.691.863 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.720.447 I 
0.00.720.494 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.720.522 I perplexity: tokenizing the input ..
0.00.728.630 I perplexity: tokenization took 8.102 ms
0.00.728.663 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.865.028 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.866.368 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.866.382 I llama_perf_context_print:        load time =     711.40 ms
0.00.866.383 I llama_perf_context_print: prompt eval time =     135.42 ms /   128 tokens (    1.06 ms per token,   945.22 tokens per second)
0.00.866.384 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.866.384 I llama_perf_context_print:       total time =     145.94 ms /   129 tokens
0.00.866.771 I ggml_metal_free: deallocating

real	0m0.881s
user	0m0.082s
sys	0m0.130s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4702 (a394039d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.910 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.630 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.636 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.638 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.643 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.644 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.644 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.644 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.645 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.646 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.646 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.646 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.647 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.650 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.650 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.652 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.652 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.653 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.541 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.593 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.497 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.499 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.499 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.499 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.500 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.500 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.501 I llama_model_loader: - type  f32:  194 tensors
0.00.025.501 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.501 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.502 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.502 I print_info: file format = GGUF V3 (latest)
0.00.025.503 I print_info: file type   = Q2_K - Medium
0.00.025.504 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.654 I load: special tokens cache size = 25
0.00.039.927 I load: token to piece cache size = 0.2984 MB
0.00.039.930 I print_info: arch             = gptneox
0.00.039.931 I print_info: vocab_only       = 0
0.00.039.931 I print_info: n_ctx_train      = 2048
0.00.039.931 I print_info: n_embd           = 2048
0.00.039.931 I print_info: n_layer          = 24
0.00.039.935 I print_info: n_head           = 16
0.00.039.936 I print_info: n_head_kv        = 16
0.00.039.936 I print_info: n_rot            = 32
0.00.039.936 I print_info: n_swa            = 0
0.00.039.939 I print_info: n_embd_head_k    = 128
0.00.039.939 I print_info: n_embd_head_v    = 128
0.00.039.940 I print_info: n_gqa            = 1
0.00.039.941 I print_info: n_embd_k_gqa     = 2048
0.00.039.942 I print_info: n_embd_v_gqa     = 2048
0.00.039.942 I print_info: f_norm_eps       = 1.0e-05
0.00.039.942 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.943 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.943 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.943 I print_info: f_logit_scale    = 0.0e+00
0.00.039.945 I print_info: n_ff             = 8192
0.00.039.945 I print_info: n_expert         = 0
0.00.039.945 I print_info: n_expert_used    = 0
0.00.039.945 I print_info: causal attn      = 1
0.00.039.947 I print_info: pooling type     = 0
0.00.039.947 I print_info: rope type        = 2
0.00.039.947 I print_info: rope scaling     = linear
0.00.039.947 I print_info: freq_base_train  = 10000.0
0.00.039.948 I print_info: freq_scale_train = 1
0.00.039.948 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.948 I print_info: rope_finetuned   = unknown
0.00.039.948 I print_info: ssm_d_conv       = 0
0.00.039.948 I print_info: ssm_d_inner      = 0
0.00.039.948 I print_info: ssm_d_state      = 0
0.00.039.948 I print_info: ssm_dt_rank      = 0
0.00.039.948 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.949 I print_info: model type       = 1.4B
0.00.039.949 I print_info: model params     = 1.41 B
0.00.039.949 I print_info: general.name     = 1.4B
0.00.039.950 I print_info: vocab type       = BPE
0.00.039.950 I print_info: n_vocab          = 50304
0.00.039.950 I print_info: n_merges         = 50009
0.00.039.950 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.951 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.951 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.951 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.952 I print_info: LF token         = 187 'Ċ'
0.00.039.953 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.953 I print_info: max token length = 1024
0.00.039.954 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.338.319 I load_tensors: offloading 24 repeating layers to GPU
0.00.338.331 I load_tensors: offloading output layer to GPU
0.00.338.332 I load_tensors: offloaded 25/25 layers to GPU
0.00.338.362 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.338.364 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.340.039 I llama_init_from_model: n_seq_max     = 1
0.00.340.042 I llama_init_from_model: n_ctx         = 128
0.00.340.042 I llama_init_from_model: n_ctx_per_seq = 128
0.00.340.043 I llama_init_from_model: n_batch       = 128
0.00.340.043 I llama_init_from_model: n_ubatch      = 128
0.00.340.044 I llama_init_from_model: flash_attn    = 0
0.00.340.047 I llama_init_from_model: freq_base     = 10000.0
0.00.340.047 I llama_init_from_model: freq_scale    = 1
0.00.340.048 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.340.050 I ggml_metal_init: allocating
0.00.340.113 I ggml_metal_init: found device: Apple M4
0.00.340.126 I ggml_metal_init: picking default device: Apple M4
0.00.341.873 I ggml_metal_init: using embedded metal library
0.00.347.310 I ggml_metal_init: GPU name:   Apple M4
0.00.347.321 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.347.322 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.347.323 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.347.323 I ggml_metal_init: simdgroup reduction   = true
0.00.347.324 I ggml_metal_init: simdgroup matrix mul. = true
0.00.347.324 I ggml_metal_init: has residency sets    = true
0.00.347.324 I ggml_metal_init: has bfloat            = true
0.00.347.325 I ggml_metal_init: use bfloat            = true
0.00.347.326 I ggml_metal_init: hasUnifiedMemory      = true
0.00.347.331 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.368.683 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.372.267 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.372.271 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.372.308 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.375.574 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.375.576 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.375.577 I llama_init_from_model: graph nodes  = 967
0.00.375.577 I llama_init_from_model: graph splits = 2
0.00.375.580 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.375.581 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.404.820 I 
0.00.404.884 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.404.915 I perplexity: tokenizing the input ..
0.00.412.069 I perplexity: tokenization took 7.149 ms
0.00.412.095 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.545.865 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.547.339 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.547.360 I llama_perf_context_print:        load time =     394.90 ms
0.00.547.361 I llama_perf_context_print: prompt eval time =     132.81 ms /   128 tokens (    1.04 ms per token,   963.76 tokens per second)
0.00.547.362 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.547.362 I llama_perf_context_print:       total time =     142.54 ms /   129 tokens
0.00.547.726 I ggml_metal_free: deallocating

real	0m0.563s
user	0m0.082s
sys	0m0.092s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4702 (a394039d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.058 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.142 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.148 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.150 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.151 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.151 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.151 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.152 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.153 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.153 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.153 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.154 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.154 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.154 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.155 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.157 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.157 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.157 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.022 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.054 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.880 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.881 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.881 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.882 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.882 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.882 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.883 I llama_model_loader: - type  f32:  194 tensors
0.00.024.883 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.884 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.884 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.884 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.885 I print_info: file format = GGUF V3 (latest)
0.00.024.890 I print_info: file type   = Q3_K - Medium
0.00.024.891 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.889 I load: special tokens cache size = 25
0.00.039.009 I load: token to piece cache size = 0.2984 MB
0.00.039.013 I print_info: arch             = gptneox
0.00.039.013 I print_info: vocab_only       = 0
0.00.039.014 I print_info: n_ctx_train      = 2048
0.00.039.014 I print_info: n_embd           = 2048
0.00.039.014 I print_info: n_layer          = 24
0.00.039.018 I print_info: n_head           = 16
0.00.039.021 I print_info: n_head_kv        = 16
0.00.039.021 I print_info: n_rot            = 32
0.00.039.022 I print_info: n_swa            = 0
0.00.039.022 I print_info: n_embd_head_k    = 128
0.00.039.022 I print_info: n_embd_head_v    = 128
0.00.039.023 I print_info: n_gqa            = 1
0.00.039.023 I print_info: n_embd_k_gqa     = 2048
0.00.039.024 I print_info: n_embd_v_gqa     = 2048
0.00.039.025 I print_info: f_norm_eps       = 1.0e-05
0.00.039.025 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.025 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.025 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.025 I print_info: f_logit_scale    = 0.0e+00
0.00.039.026 I print_info: n_ff             = 8192
0.00.039.026 I print_info: n_expert         = 0
0.00.039.026 I print_info: n_expert_used    = 0
0.00.039.027 I print_info: causal attn      = 1
0.00.039.027 I print_info: pooling type     = 0
0.00.039.029 I print_info: rope type        = 2
0.00.039.029 I print_info: rope scaling     = linear
0.00.039.029 I print_info: freq_base_train  = 10000.0
0.00.039.030 I print_info: freq_scale_train = 1
0.00.039.030 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.030 I print_info: rope_finetuned   = unknown
0.00.039.030 I print_info: ssm_d_conv       = 0
0.00.039.030 I print_info: ssm_d_inner      = 0
0.00.039.030 I print_info: ssm_d_state      = 0
0.00.039.031 I print_info: ssm_dt_rank      = 0
0.00.039.031 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.031 I print_info: model type       = 1.4B
0.00.039.031 I print_info: model params     = 1.41 B
0.00.039.031 I print_info: general.name     = 1.4B
0.00.039.032 I print_info: vocab type       = BPE
0.00.039.032 I print_info: n_vocab          = 50304
0.00.039.032 I print_info: n_merges         = 50009
0.00.039.033 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.033 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.033 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.033 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.034 I print_info: LF token         = 187 'Ċ'
0.00.039.037 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.037 I print_info: max token length = 1024
0.00.039.038 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.453.430 I load_tensors: offloading 24 repeating layers to GPU
0.00.453.444 I load_tensors: offloading output layer to GPU
0.00.453.445 I load_tensors: offloaded 25/25 layers to GPU
0.00.453.480 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.453.481 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.454.958 I llama_init_from_model: n_seq_max     = 1
0.00.454.961 I llama_init_from_model: n_ctx         = 128
0.00.454.961 I llama_init_from_model: n_ctx_per_seq = 128
0.00.454.962 I llama_init_from_model: n_batch       = 128
0.00.454.962 I llama_init_from_model: n_ubatch      = 128
0.00.454.963 I llama_init_from_model: flash_attn    = 0
0.00.454.965 I llama_init_from_model: freq_base     = 10000.0
0.00.454.965 I llama_init_from_model: freq_scale    = 1
0.00.454.966 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.454.968 I ggml_metal_init: allocating
0.00.455.061 I ggml_metal_init: found device: Apple M4
0.00.455.075 I ggml_metal_init: picking default device: Apple M4
0.00.456.866 I ggml_metal_init: using embedded metal library
0.00.462.213 I ggml_metal_init: GPU name:   Apple M4
0.00.462.224 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.462.224 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.462.225 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.462.226 I ggml_metal_init: simdgroup reduction   = true
0.00.462.226 I ggml_metal_init: simdgroup matrix mul. = true
0.00.462.226 I ggml_metal_init: has residency sets    = true
0.00.462.227 I ggml_metal_init: has bfloat            = true
0.00.462.227 I ggml_metal_init: use bfloat            = true
0.00.462.228 I ggml_metal_init: hasUnifiedMemory      = true
0.00.462.233 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.482.461 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.486.055 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.486.059 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.486.089 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.489.418 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.489.420 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.489.420 I llama_init_from_model: graph nodes  = 967
0.00.489.421 I llama_init_from_model: graph splits = 2
0.00.489.424 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.489.424 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.519.956 I 
0.00.520.016 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.520.047 I perplexity: tokenizing the input ..
0.00.527.143 I perplexity: tokenization took 7.093 ms
0.00.527.166 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.668.865 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.670.202 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.670.216 I llama_perf_context_print:        load time =     510.89 ms
0.00.670.217 I llama_perf_context_print: prompt eval time =     140.75 ms /   128 tokens (    1.10 ms per token,   909.42 tokens per second)
0.00.670.217 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.670.218 I llama_perf_context_print:       total time =     150.27 ms /   129 tokens
0.00.670.628 I ggml_metal_free: deallocating

real	0m0.684s
user	0m0.080s
sys	0m0.120s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4702 (a394039d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.210 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.186 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.192 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.194 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.195 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.195 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.196 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.196 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.197 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.197 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.198 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.198 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.200 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.202 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.202 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.204 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.205 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.205 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.988 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.959 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.811 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.812 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.813 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.813 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.813 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.814 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.814 I llama_model_loader: - type  f32:  194 tensors
0.00.024.815 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.815 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.815 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.816 I print_info: file format = GGUF V3 (latest)
0.00.024.816 I print_info: file type   = Q4_K - Medium
0.00.024.818 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.644 I load: special tokens cache size = 25
0.00.038.744 I load: token to piece cache size = 0.2984 MB
0.00.038.747 I print_info: arch             = gptneox
0.00.038.747 I print_info: vocab_only       = 0
0.00.038.747 I print_info: n_ctx_train      = 2048
0.00.038.747 I print_info: n_embd           = 2048
0.00.038.748 I print_info: n_layer          = 24
0.00.038.751 I print_info: n_head           = 16
0.00.038.752 I print_info: n_head_kv        = 16
0.00.038.754 I print_info: n_rot            = 32
0.00.038.755 I print_info: n_swa            = 0
0.00.038.755 I print_info: n_embd_head_k    = 128
0.00.038.755 I print_info: n_embd_head_v    = 128
0.00.038.756 I print_info: n_gqa            = 1
0.00.038.756 I print_info: n_embd_k_gqa     = 2048
0.00.038.757 I print_info: n_embd_v_gqa     = 2048
0.00.038.758 I print_info: f_norm_eps       = 1.0e-05
0.00.038.759 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.759 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.759 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.759 I print_info: f_logit_scale    = 0.0e+00
0.00.038.760 I print_info: n_ff             = 8192
0.00.038.760 I print_info: n_expert         = 0
0.00.038.760 I print_info: n_expert_used    = 0
0.00.038.760 I print_info: causal attn      = 1
0.00.038.761 I print_info: pooling type     = 0
0.00.038.761 I print_info: rope type        = 2
0.00.038.761 I print_info: rope scaling     = linear
0.00.038.761 I print_info: freq_base_train  = 10000.0
0.00.038.762 I print_info: freq_scale_train = 1
0.00.038.763 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.763 I print_info: rope_finetuned   = unknown
0.00.038.764 I print_info: ssm_d_conv       = 0
0.00.038.764 I print_info: ssm_d_inner      = 0
0.00.038.764 I print_info: ssm_d_state      = 0
0.00.038.764 I print_info: ssm_dt_rank      = 0
0.00.038.764 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.764 I print_info: model type       = 1.4B
0.00.038.765 I print_info: model params     = 1.41 B
0.00.038.766 I print_info: general.name     = 1.4B
0.00.038.767 I print_info: vocab type       = BPE
0.00.038.767 I print_info: n_vocab          = 50304
0.00.038.767 I print_info: n_merges         = 50009
0.00.038.767 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.767 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.768 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.768 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.768 I print_info: LF token         = 187 'Ċ'
0.00.038.768 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.769 I print_info: max token length = 1024
0.00.038.769 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.533.091 I load_tensors: offloading 24 repeating layers to GPU
0.00.533.109 I load_tensors: offloading output layer to GPU
0.00.533.109 I load_tensors: offloaded 25/25 layers to GPU
0.00.533.141 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.533.142 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.534.825 I llama_init_from_model: n_seq_max     = 1
0.00.534.828 I llama_init_from_model: n_ctx         = 128
0.00.534.829 I llama_init_from_model: n_ctx_per_seq = 128
0.00.534.829 I llama_init_from_model: n_batch       = 128
0.00.534.830 I llama_init_from_model: n_ubatch      = 128
0.00.534.830 I llama_init_from_model: flash_attn    = 0
0.00.534.832 I llama_init_from_model: freq_base     = 10000.0
0.00.534.833 I llama_init_from_model: freq_scale    = 1
0.00.534.833 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.534.840 I ggml_metal_init: allocating
0.00.534.914 I ggml_metal_init: found device: Apple M4
0.00.534.927 I ggml_metal_init: picking default device: Apple M4
0.00.536.757 I ggml_metal_init: using embedded metal library
0.00.543.316 I ggml_metal_init: GPU name:   Apple M4
0.00.543.322 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.543.323 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.543.323 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.543.324 I ggml_metal_init: simdgroup reduction   = true
0.00.543.325 I ggml_metal_init: simdgroup matrix mul. = true
0.00.543.325 I ggml_metal_init: has residency sets    = true
0.00.543.325 I ggml_metal_init: has bfloat            = true
0.00.543.325 I ggml_metal_init: use bfloat            = true
0.00.543.326 I ggml_metal_init: hasUnifiedMemory      = true
0.00.543.329 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.561.371 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.564.832 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.564.838 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.564.888 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.568.077 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.568.079 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.568.080 I llama_init_from_model: graph nodes  = 967
0.00.568.080 I llama_init_from_model: graph splits = 2
0.00.568.083 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.568.083 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.598.903 I 
0.00.598.964 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.598.993 I perplexity: tokenizing the input ..
0.00.606.278 I perplexity: tokenization took 7.283 ms
0.00.606.305 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.750.713 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.752.051 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.752.065 I llama_perf_context_print:        load time =     589.68 ms
0.00.752.066 I llama_perf_context_print: prompt eval time =     143.53 ms /   128 tokens (    1.12 ms per token,   891.77 tokens per second)
0.00.752.066 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.752.067 I llama_perf_context_print:       total time =     153.17 ms /   129 tokens
0.00.752.444 I ggml_metal_free: deallocating

real	0m0.768s
user	0m0.079s
sys	0m0.132s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.108 I build: 4702 (a394039d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.916 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.644 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.650 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.656 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.656 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.657 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.657 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.657 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.658 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.659 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.659 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.659 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.660 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.660 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.660 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.662 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.663 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.663 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.492 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.483 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.320 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.321 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.322 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.322 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.323 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.323 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.323 I llama_model_loader: - type  f32:  194 tensors
0.00.025.324 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.324 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.325 I print_info: file format = GGUF V3 (latest)
0.00.025.325 I print_info: file type   = Q5_K - Medium
0.00.025.327 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.610 I load: special tokens cache size = 25
0.00.039.532 I load: token to piece cache size = 0.2984 MB
0.00.039.535 I print_info: arch             = gptneox
0.00.039.536 I print_info: vocab_only       = 0
0.00.039.536 I print_info: n_ctx_train      = 2048
0.00.039.536 I print_info: n_embd           = 2048
0.00.039.536 I print_info: n_layer          = 24
0.00.039.541 I print_info: n_head           = 16
0.00.039.541 I print_info: n_head_kv        = 16
0.00.039.542 I print_info: n_rot            = 32
0.00.039.542 I print_info: n_swa            = 0
0.00.039.542 I print_info: n_embd_head_k    = 128
0.00.039.542 I print_info: n_embd_head_v    = 128
0.00.039.543 I print_info: n_gqa            = 1
0.00.039.544 I print_info: n_embd_k_gqa     = 2048
0.00.039.544 I print_info: n_embd_v_gqa     = 2048
0.00.039.547 I print_info: f_norm_eps       = 1.0e-05
0.00.039.548 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.548 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.548 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.548 I print_info: f_logit_scale    = 0.0e+00
0.00.039.549 I print_info: n_ff             = 8192
0.00.039.549 I print_info: n_expert         = 0
0.00.039.549 I print_info: n_expert_used    = 0
0.00.039.549 I print_info: causal attn      = 1
0.00.039.549 I print_info: pooling type     = 0
0.00.039.549 I print_info: rope type        = 2
0.00.039.550 I print_info: rope scaling     = linear
0.00.039.550 I print_info: freq_base_train  = 10000.0
0.00.039.551 I print_info: freq_scale_train = 1
0.00.039.551 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.551 I print_info: rope_finetuned   = unknown
0.00.039.551 I print_info: ssm_d_conv       = 0
0.00.039.551 I print_info: ssm_d_inner      = 0
0.00.039.552 I print_info: ssm_d_state      = 0
0.00.039.552 I print_info: ssm_dt_rank      = 0
0.00.039.552 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.552 I print_info: model type       = 1.4B
0.00.039.553 I print_info: model params     = 1.41 B
0.00.039.553 I print_info: general.name     = 1.4B
0.00.039.553 I print_info: vocab type       = BPE
0.00.039.553 I print_info: n_vocab          = 50304
0.00.039.554 I print_info: n_merges         = 50009
0.00.039.555 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.555 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.556 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.556 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.556 I print_info: LF token         = 187 'Ċ'
0.00.039.556 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.557 I print_info: max token length = 1024
0.00.039.557 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.588.983 I load_tensors: offloading 24 repeating layers to GPU
0.00.588.991 I load_tensors: offloading output layer to GPU
0.00.588.992 I load_tensors: offloaded 25/25 layers to GPU
0.00.589.024 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.589.026 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.590.492 I llama_init_from_model: n_seq_max     = 1
0.00.590.495 I llama_init_from_model: n_ctx         = 128
0.00.590.495 I llama_init_from_model: n_ctx_per_seq = 128
0.00.590.496 I llama_init_from_model: n_batch       = 128
0.00.590.496 I llama_init_from_model: n_ubatch      = 128
0.00.590.496 I llama_init_from_model: flash_attn    = 0
0.00.590.498 I llama_init_from_model: freq_base     = 10000.0
0.00.590.499 I llama_init_from_model: freq_scale    = 1
0.00.590.500 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.590.504 I ggml_metal_init: allocating
0.00.590.564 I ggml_metal_init: found device: Apple M4
0.00.590.576 I ggml_metal_init: picking default device: Apple M4
0.00.592.424 I ggml_metal_init: using embedded metal library
0.00.598.997 I ggml_metal_init: GPU name:   Apple M4
0.00.599.001 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.599.002 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.599.003 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.599.003 I ggml_metal_init: simdgroup reduction   = true
0.00.599.003 I ggml_metal_init: simdgroup matrix mul. = true
0.00.599.004 I ggml_metal_init: has residency sets    = true
0.00.599.004 I ggml_metal_init: has bfloat            = true
0.00.599.004 I ggml_metal_init: use bfloat            = true
0.00.599.005 I ggml_metal_init: hasUnifiedMemory      = true
0.00.599.006 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.616.053 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.619.554 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.619.559 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.619.599 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.622.809 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.622.811 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.622.811 I llama_init_from_model: graph nodes  = 967
0.00.622.812 I llama_init_from_model: graph splits = 2
0.00.622.814 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.622.814 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.655.531 I 
0.00.655.590 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.655.619 I perplexity: tokenizing the input ..
0.00.660.357 I perplexity: tokenization took 4.737 ms
0.00.660.371 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.800.228 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.801.643 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.801.658 I llama_perf_context_print:        load time =     645.61 ms
0.00.801.661 I llama_perf_context_print: prompt eval time =     139.62 ms /   128 tokens (    1.09 ms per token,   916.74 tokens per second)
0.00.801.664 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.801.665 I llama_perf_context_print:       total time =     146.13 ms /   129 tokens
0.00.802.026 I ggml_metal_free: deallocating

real	0m0.818s
user	0m0.075s
sys	0m0.130s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4702 (a394039d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.971 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.594 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.600 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.601 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.603 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.608 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.608 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.608 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.609 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.610 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.610 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.611 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.612 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.612 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.612 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.614 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.614 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.614 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.495 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.511 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.361 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.362 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.362 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.362 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.363 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.363 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.364 I llama_model_loader: - type  f32:  194 tensors
0.00.024.364 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.365 I print_info: file format = GGUF V3 (latest)
0.00.024.366 I print_info: file type   = Q6_K
0.00.024.366 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.558 I load: special tokens cache size = 25
0.00.038.949 I load: token to piece cache size = 0.2984 MB
0.00.038.952 I print_info: arch             = gptneox
0.00.038.952 I print_info: vocab_only       = 0
0.00.038.953 I print_info: n_ctx_train      = 2048
0.00.038.953 I print_info: n_embd           = 2048
0.00.038.953 I print_info: n_layer          = 24
0.00.038.957 I print_info: n_head           = 16
0.00.038.959 I print_info: n_head_kv        = 16
0.00.038.959 I print_info: n_rot            = 32
0.00.038.959 I print_info: n_swa            = 0
0.00.038.959 I print_info: n_embd_head_k    = 128
0.00.038.959 I print_info: n_embd_head_v    = 128
0.00.038.962 I print_info: n_gqa            = 1
0.00.038.963 I print_info: n_embd_k_gqa     = 2048
0.00.038.964 I print_info: n_embd_v_gqa     = 2048
0.00.038.964 I print_info: f_norm_eps       = 1.0e-05
0.00.038.964 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.965 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.965 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.965 I print_info: f_logit_scale    = 0.0e+00
0.00.038.966 I print_info: n_ff             = 8192
0.00.038.966 I print_info: n_expert         = 0
0.00.038.966 I print_info: n_expert_used    = 0
0.00.038.972 I print_info: causal attn      = 1
0.00.038.973 I print_info: pooling type     = 0
0.00.038.973 I print_info: rope type        = 2
0.00.038.973 I print_info: rope scaling     = linear
0.00.038.974 I print_info: freq_base_train  = 10000.0
0.00.038.974 I print_info: freq_scale_train = 1
0.00.038.974 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.975 I print_info: rope_finetuned   = unknown
0.00.038.976 I print_info: ssm_d_conv       = 0
0.00.038.976 I print_info: ssm_d_inner      = 0
0.00.038.976 I print_info: ssm_d_state      = 0
0.00.038.976 I print_info: ssm_dt_rank      = 0
0.00.038.976 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.977 I print_info: model type       = 1.4B
0.00.038.977 I print_info: model params     = 1.41 B
0.00.038.977 I print_info: general.name     = 1.4B
0.00.038.978 I print_info: vocab type       = BPE
0.00.038.978 I print_info: n_vocab          = 50304
0.00.038.978 I print_info: n_merges         = 50009
0.00.038.978 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.979 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.979 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.979 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.979 I print_info: LF token         = 187 'Ċ'
0.00.038.980 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.980 I print_info: max token length = 1024
0.00.038.980 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.402.589 I load_tensors: offloading 24 repeating layers to GPU
0.00.402.597 I load_tensors: offloading output layer to GPU
0.00.402.598 I load_tensors: offloaded 25/25 layers to GPU
0.00.402.622 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.402.625 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.404.095 I llama_init_from_model: n_seq_max     = 1
0.00.404.098 I llama_init_from_model: n_ctx         = 128
0.00.404.098 I llama_init_from_model: n_ctx_per_seq = 128
0.00.404.098 I llama_init_from_model: n_batch       = 128
0.00.404.099 I llama_init_from_model: n_ubatch      = 128
0.00.404.099 I llama_init_from_model: flash_attn    = 0
0.00.404.100 I llama_init_from_model: freq_base     = 10000.0
0.00.404.101 I llama_init_from_model: freq_scale    = 1
0.00.404.102 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.404.103 I ggml_metal_init: allocating
0.00.404.130 I ggml_metal_init: found device: Apple M4
0.00.404.139 I ggml_metal_init: picking default device: Apple M4
0.00.405.440 I ggml_metal_init: using embedded metal library
0.00.411.156 I ggml_metal_init: GPU name:   Apple M4
0.00.411.159 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.411.160 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.411.161 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.411.162 I ggml_metal_init: simdgroup reduction   = true
0.00.411.162 I ggml_metal_init: simdgroup matrix mul. = true
0.00.411.162 I ggml_metal_init: has residency sets    = true
0.00.411.162 I ggml_metal_init: has bfloat            = true
0.00.411.163 I ggml_metal_init: use bfloat            = true
0.00.411.163 I ggml_metal_init: hasUnifiedMemory      = true
0.00.411.165 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.427.952 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.431.419 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.431.425 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.431.479 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.434.464 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.434.466 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.434.466 I llama_init_from_model: graph nodes  = 967
0.00.434.467 I llama_init_from_model: graph splits = 2
0.00.434.469 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.434.469 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.471.795 I 
0.00.471.871 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.471.897 I perplexity: tokenizing the input ..
0.00.478.872 I perplexity: tokenization took 6.973 ms
0.00.478.884 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.618.069 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.619.414 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.619.428 I llama_perf_context_print:        load time =     462.82 ms
0.00.619.429 I llama_perf_context_print: prompt eval time =     138.92 ms /   128 tokens (    1.09 ms per token,   921.41 tokens per second)
0.00.619.430 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.619.430 I llama_perf_context_print:       total time =     147.64 ms /   129 tokens
0.00.619.804 I ggml_metal_free: deallocating

real	0m0.634s
user	0m0.077s
sys	0m0.112s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.272 I build: 4702 (a394039d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.353 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.563 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.567 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.571 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.572 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.572 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.573 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.573 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.574 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.575 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.577 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.577 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.577 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.578 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.578 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.580 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.586 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.586 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.772 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.556 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.000 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.052.002 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.003 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.052.003 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.052.003 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.052.004 I llama_model_loader: - type  f32:  194 tensors
0.00.052.004 I llama_model_loader: - type  f16:   98 tensors
0.00.052.005 I print_info: file format = GGUF V3 (latest)
0.00.052.006 I print_info: file type   = all F32 (guessed)
0.00.052.007 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.063.376 I load: special tokens cache size = 25
0.00.070.887 I load: token to piece cache size = 0.2984 MB
0.00.070.891 I print_info: arch             = gptneox
0.00.070.891 I print_info: vocab_only       = 0
0.00.070.891 I print_info: n_ctx_train      = 2048
0.00.070.891 I print_info: n_embd           = 2048
0.00.070.891 I print_info: n_layer          = 24
0.00.070.894 I print_info: n_head           = 16
0.00.070.895 I print_info: n_head_kv        = 16
0.00.070.896 I print_info: n_rot            = 32
0.00.070.896 I print_info: n_swa            = 0
0.00.070.896 I print_info: n_embd_head_k    = 128
0.00.070.896 I print_info: n_embd_head_v    = 128
0.00.070.897 I print_info: n_gqa            = 1
0.00.070.897 I print_info: n_embd_k_gqa     = 2048
0.00.070.898 I print_info: n_embd_v_gqa     = 2048
0.00.070.899 I print_info: f_norm_eps       = 1.0e-05
0.00.070.899 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.070.899 I print_info: f_clamp_kqv      = 0.0e+00
0.00.070.899 I print_info: f_max_alibi_bias = 0.0e+00
0.00.070.900 I print_info: f_logit_scale    = 0.0e+00
0.00.070.900 I print_info: n_ff             = 8192
0.00.070.900 I print_info: n_expert         = 0
0.00.070.900 I print_info: n_expert_used    = 0
0.00.070.901 I print_info: causal attn      = 1
0.00.070.902 I print_info: pooling type     = 0
0.00.070.902 I print_info: rope type        = 2
0.00.070.903 I print_info: rope scaling     = linear
0.00.070.903 I print_info: freq_base_train  = 10000.0
0.00.070.903 I print_info: freq_scale_train = 1
0.00.070.903 I print_info: n_ctx_orig_yarn  = 2048
0.00.070.904 I print_info: rope_finetuned   = unknown
0.00.070.904 I print_info: ssm_d_conv       = 0
0.00.070.904 I print_info: ssm_d_inner      = 0
0.00.070.905 I print_info: ssm_d_state      = 0
0.00.070.906 I print_info: ssm_dt_rank      = 0
0.00.070.906 I print_info: ssm_dt_b_c_rms   = 0
0.00.070.906 I print_info: model type       = 1.4B
0.00.070.906 I print_info: model params     = 1.41 B
0.00.070.906 I print_info: general.name     = 1.4B
0.00.070.907 I print_info: vocab type       = BPE
0.00.070.907 I print_info: n_vocab          = 50304
0.00.070.907 I print_info: n_merges         = 50009
0.00.070.907 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.070.908 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.070.908 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.070.908 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.070.912 I print_info: LF token         = 187 'Ċ'
0.00.070.913 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.070.913 I print_info: max token length = 1024
0.00.070.918 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.326.908 I load_tensors: offloading 24 repeating layers to GPU
0.01.326.912 I load_tensors: offloading output layer to GPU
0.01.326.913 I load_tensors: offloaded 25/25 layers to GPU
0.01.326.936 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.326.938 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.327.732 I llama_init_from_model: n_seq_max     = 1
0.01.327.733 I llama_init_from_model: n_ctx         = 128
0.01.327.734 I llama_init_from_model: n_ctx_per_seq = 128
0.01.327.734 I llama_init_from_model: n_batch       = 128
0.01.327.734 I llama_init_from_model: n_ubatch      = 128
0.01.327.734 I llama_init_from_model: flash_attn    = 0
0.01.327.735 I llama_init_from_model: freq_base     = 10000.0
0.01.327.735 I llama_init_from_model: freq_scale    = 1
0.01.327.736 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.327.737 I ggml_metal_init: allocating
0.01.327.794 I ggml_metal_init: found device: Apple M4
0.01.327.802 I ggml_metal_init: picking default device: Apple M4
0.01.328.860 I ggml_metal_init: using embedded metal library
0.01.332.585 I ggml_metal_init: GPU name:   Apple M4
0.01.332.588 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.332.588 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.332.589 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.332.589 I ggml_metal_init: simdgroup reduction   = true
0.01.332.589 I ggml_metal_init: simdgroup matrix mul. = true
0.01.332.590 I ggml_metal_init: has residency sets    = true
0.01.332.590 I ggml_metal_init: has bfloat            = true
0.01.332.590 I ggml_metal_init: use bfloat            = true
0.01.332.590 I ggml_metal_init: hasUnifiedMemory      = true
0.01.332.591 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.344.042 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.345.712 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.345.715 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.345.728 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.347.490 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.347.491 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.347.491 I llama_init_from_model: graph nodes  = 967
0.01.347.492 I llama_init_from_model: graph splits = 2
0.01.347.493 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.347.493 I 
0.01.347.521 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.347.522 I compute_imatrix: tokenizing the input ..
0.01.351.626 I compute_imatrix: tokenization took 4.103 ms
0.01.351.628 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.613.311 I compute_imatrix: 0.26 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.616.124 I llama_perf_context_print:        load time =    1591.95 ms
0.01.616.125 I llama_perf_context_print: prompt eval time =     259.94 ms /   128 tokens (    2.03 ms per token,   492.42 tokens per second)
0.01.616.126 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.616.127 I llama_perf_context_print:       total time =    1594.77 ms /   129 tokens
0.01.616.713 I ggml_metal_free: deallocating

real	0m1.802s
user	0m0.124s
sys	0m0.246s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4702 (a394039d)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12f608060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12f608520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12f608ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12f609080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12f609630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12f609be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12f60a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12f60a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12f60acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12f60b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12f60b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12f60bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12f60c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12f60cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12f60d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12f60ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12f60e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12f60ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12f60f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12f60fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12f610240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12f610960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12f611080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12f611920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12f612040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12f612300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12f612910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12f613580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12f613ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12f613d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12f614220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12f6144e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12f614d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12f6152b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12f615570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12f615a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12f615eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12f616350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12f6167f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12f616c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12f617130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12f6175d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12f617a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12f617f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12f6181d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12f6187e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12f618df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12f619710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12f619d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12f61a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12f61a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12f61af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12f61b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12f61bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12f61c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12f61c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12f61cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12f61cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12f61d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12f61dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12f61e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12f61e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12f61e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12f61ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12f61f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12f61f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12f61fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12f620080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12f620520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12f6209c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12f620e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12f621300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12f6217a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12f621cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12f622240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12f622790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12f622ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12f623230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12f623780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12f623cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12f624220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12f624770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12f624cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12f625210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12f625760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12f625cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12f626200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12f626750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12f626ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12f6271f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12f627740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12f627c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12f6281e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12f628730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12f628c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12f6291d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12f629720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12f619400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12f629b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12f62a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12f62a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12f62ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12f62b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12f62b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12f62bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12f62c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12f62c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12f62cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12f62d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12f62d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12f62ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12f62e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12f62e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12f62ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12f62f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12f62f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12f62fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12f62ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12f630410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12f6308b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12f630d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12f6311f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12f631690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12f631b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12f631fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12f632470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12f632910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12f632db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12f633250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12f6336f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12f633b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12f634030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12f6344d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12f634970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12f634e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12f6352b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12f635750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12f635bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12f636090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12f636530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12f6369d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12f636e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12f637310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12f6377b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12f637c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12f6380f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12f638590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12f638a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12f638ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12f639370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12f639810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12f639cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12f63a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12f63a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12f63aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12f63af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12f63b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12f63b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12f63bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12f63c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12f63c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12f63caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12f63cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12f63d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12f63d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12f63dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12f63e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12f63e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12f63eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12f63eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12f63f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12f63f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12f63fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12f640270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12f640710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12f640bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12f641050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12f6414f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12f641990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12f641e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12f6422d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12f642770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12f642c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12f6430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12f643550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12f6439f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12f643e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12f644330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12f6447d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12f644c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12f645110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12f6455b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12f645a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12f645fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12f6464f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12f646a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12f646f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12f647250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12f647860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12f647e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12f648480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12f648c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12f649110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12f6493d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12f6499e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12f649ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12f64a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12f64ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12f64b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12f64b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12f64bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12f64c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12f64c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12f64cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12f64d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12f64d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12f64dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12f64e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12f64e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12f64ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12f64f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12f64f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12f64fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12f650280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12f6507d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12f650d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12f651270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12f6517c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12f651d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12f652260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12f6527b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12f652d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12f653250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12f6537a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12f653cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12f654240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12f654790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12f654ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12f655230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12f655780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12f655cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12f656220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12f656770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12f656cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12f657210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12f657760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12f657cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12f658200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12f658750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12f658ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12f6591f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12f659740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12f659c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12f65a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12f65a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12f65ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12f65b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12f65b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12f65bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12f65c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12f65c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12f65cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12f65d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12f65d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12f65dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12f65e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12f65e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12f65eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12f65f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12f65f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12f65f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12f65fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12f6602b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12f660750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12f660bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12f661090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12f661530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12f6619d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12f661e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12f662310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12f6627b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12f662c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12f6631a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12f6638c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12f663fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12f664700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12f664e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12f6650e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12f6658d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12f665b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12f6661a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.725.952 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.725.956 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14b504dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14b505240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14b5056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14b505b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14b505f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14b506400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14b506870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14b506ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14b507150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14b5075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14b507a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14b508120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14b508c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14b5093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14b509c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14b50a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14b50aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14b50b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14b50b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14b50bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14b50c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14b50cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14b50d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14b50dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14b50e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14b50e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14b50e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14b50ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14b50f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14b50f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14b50fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14b50ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14b510430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14b5106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14b510b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14b510fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14b511440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14b5118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14b511d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14b512190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14b512600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14b512a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14b512ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14b513350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14b5137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14b513c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14b5140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14b514510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14b514980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14b514df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14b515260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14b5156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14b515b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14b515fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14b516420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14b516890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14b516e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14b517300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14b517770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14b517be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14b518050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14b5184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14b518930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14b518da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14b519210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14b519680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14b519af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14b519f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14b51a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14b51a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14b51acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14b51b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14b51b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14b51ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14b51be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14b51c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14b51c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14b51cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14b51d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14b51d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14b51d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14b51dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14b51e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14b51e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14b51ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14b51ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14b51f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14b51f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14b51fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14b520100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14b520570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14b5209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14b520e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14b5212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14b521730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14b521ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14b522010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14b522480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14b5228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14b522d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14b5231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14b523640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14b523ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14b523f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14b524390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14b524800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14b524c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14b5250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14b525550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14b5259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14b525e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14b5262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14b526710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14b526b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14b526ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14b527460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14b5278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14b527d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14b5281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14b528620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14b528a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14b528f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14b529370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14b5297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14b529c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14b52a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14b52a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14b52a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14b52ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14b52b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14b52b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14b52bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14b52bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14b52c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14b52c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14b52cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14b52d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14b52d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14b52da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14b52dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14b52e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14b52e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14b52ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14b52f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14b52f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14b52f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14b52fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14b530260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14b5306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14b530b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14b530fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14b531420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14b531890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14b531d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14b532170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14b5325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14b532a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14b532ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14b533330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14b5337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14b533c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14b534080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14b5344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14b534960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14b534dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14b535240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14b535e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14b536130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14b5363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14b536860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14b536cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14b537140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14b5375b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14b537a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14b537e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14b538300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14b538770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14b538be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14b539050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14b5394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14b539930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14b539da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14b53a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14b53a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14b53aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14b53af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14b53b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14b53b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14b53bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14b53c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14b53c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14b53ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14b53ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14b53d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14b53d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14b53dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14b53e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14b53e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14b53e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14b53ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14b53f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14b53f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14b53fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14b5400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14b540540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14b5409b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14b540e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14b541290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14b5417b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14b541cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14b542830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14b542af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14b5430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14b543670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14b543c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14b5441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14b5447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14b544d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14b545330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14b5458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14b545eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14b546470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14b546a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14b546ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14b5475b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14b547b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14b548130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14b5486f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14b548cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14b549270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14b549830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14b549df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14b54a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14b54a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14b54af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14b54b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14b54bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14b54c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14b54c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14b54cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14b54d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14b54d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14b54dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14b54e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14b54e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14b54ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14b54f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14b54f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14b54ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14b550570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14b550b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14b5510f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14b5516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14b551c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14b552230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14b5527f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14b552db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14b553370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14b553930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14b553ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14b5544b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14b554a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14b555030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14b5555f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14b555bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14b556170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14b556730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14b556cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14b5571f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14b5576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14b557bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14b5580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14b5585f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14b558af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14b558ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14b5594f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14b5599f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14b559ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14b55a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14b55a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14b55adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14b55b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14b55b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14b55c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14b55c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14b55d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14b55d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14b55da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14b55e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14b55e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14b55eae0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14d6044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14d604950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14d604dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14d605230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14d6056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14d605b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14d605f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14d6063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14d606860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14d606db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14d607220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14d6078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14d6083c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14d608b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14d609380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14d609aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14d60a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14d60a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14d60b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14d60b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14d60bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14d60c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14d60cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14d60d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14d60db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14d60de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14d60e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14d60e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14d60e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14d60ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14d60f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14d60f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14d60fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14d60ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14d610380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14d6107f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14d610c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14d6110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14d611540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14d6119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14d611e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14d612290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14d612700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14d612b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14d612fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14d613450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14d6138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14d613d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14d6141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14d614610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14d614a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14d614ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14d615360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14d6157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14d615c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14d6160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14d616620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14d616b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14d616f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14d617400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14d617870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14d617ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14d618150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14d6185c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14d618a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14d618ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14d619310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14d619780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14d619bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14d61a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14d61a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14d61a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14d61adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14d61b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14d61b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14d61bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14d61bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14d61c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14d61c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14d61ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14d61d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14d61d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14d61da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14d61de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14d61e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14d61e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14d61ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14d61f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14d61f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14d61f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14d61fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14d620200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14d620670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14d620ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14d620f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14d6213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14d621830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14d621ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14d622110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14d622580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14d6229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14d622e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14d6232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14d623b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14d623e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14d624290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14d624700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14d624b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14d624fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14d625450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14d6258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14d625d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14d6261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14d626610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14d626a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14d626ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14d627360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14d6277d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14d627c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14d6280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14d628520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14d628990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14d628e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14d629270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14d6296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14d629b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14d629fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14d62a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14d62a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14d62ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14d62b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14d62b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14d62ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14d62bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14d62c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14d62c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14d62cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14d62d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14d62d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14d62d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14d62dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14d62e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14d62e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14d62eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14d62efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14d62f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14d62f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14d62fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14d630160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14d6305d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14d630a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14d630eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14d631320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14d631790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14d631c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14d632070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14d6324e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14d632950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14d632dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14d633230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14d6336a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14d633b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14d633f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14d6343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14d634860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14d634cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14d635140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14d6355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14d635a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14d635e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14d636300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14d636770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14d636be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14d637050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14d6374c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14d637930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14d637da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14d638210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14d638680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14d638af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14d638f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14d6393d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14d639840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14d639cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14d63a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14d63a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14d63aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14d63ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14d63b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14d63b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14d63bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14d63c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14d63c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14d63c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14d63cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14d63d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14d63d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14d63dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14d63df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14d804230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14d8046a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14d804b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14d804f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14d8053f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14d805860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14d805cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14d806140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14d8066c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14d806b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14d806fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14d807af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14d807db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14d808070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14d8084e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14d808950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14d808dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14d809230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14d8096a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14d809b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14d809f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14d80a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14d80a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14d80acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14d80b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14d80b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14d80ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14d80be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14d80c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14d80c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14d80cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14d80d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14d80d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14d80d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14d80dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14d80e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14d80e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14d80eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14d80ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14d80f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14d80f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14d80fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14d810120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14d810590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14d810a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14d810e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14d8112e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14d811750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14d811bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14d812030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14d8124a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14d812910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14d812d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14d8131f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14d813660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14d813ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14d813f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14d8143b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14d814820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14d814c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14d815100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14d815570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14d8159e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14d815e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14d8162c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14d816730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14d816ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14d817010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14d817480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14d8178f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14d817d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14d8181d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14d818640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14d818ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14d818f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14d819390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14d819800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14d819c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14d81a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14d81a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14d81a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14d81ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14d81b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14d81b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14d81c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14d81c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14d81cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14d81d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14d81d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14d81de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14d81e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14d81ea20 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.759s
user	0m0.282s
sys	0m0.321s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4702 (a394039d)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12170b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12170b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12170bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12170c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12170c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12170cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12170d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12170d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12170df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12170e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12170e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12170ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12170f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1217100d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1217108e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x121711000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x121711720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x121711e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x121712560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x121712d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x121713450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x121713b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x121714290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x121714b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x121715250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x121715510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x121715b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x121716790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x121716cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x121716f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x121717430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1217176f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x121717f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1217184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x121718780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x121718c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1217190c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x121719560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x121719a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x121719ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12171a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12171a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12171ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12171b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12171b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12171b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12171c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12171c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12171cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12171d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12171db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12171e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12171e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12171ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12171f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12171fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12171feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x121720170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x121720780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x121720f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x121721230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1217216d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x121721b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x121722010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1217224b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x121722950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x121722df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x121723290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x121723730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x121723bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x121724070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x121724510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1217249b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x121724f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x121725450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1217259a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x121725ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x121726440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x121726990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x121726ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x121727430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x121727980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x121727ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x121728420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x121728970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x121728ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x121729410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x121729960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x121729eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12172a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12172a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12172aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12172b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12172b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12172be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12172c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12172c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12171c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12172cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12172d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12172daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12172dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12172e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12172ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12172efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12172f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12172fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12172ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x121730520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x121730a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x121730fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x121731510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x121731a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x121731f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1217323a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x121732840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x121732ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x121733180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x121733620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x121733ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x121733f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x121734400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1217348a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x121734d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1217351e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x121735680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x121735b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x121735fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x121736460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x121736900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x121736da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x121737240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1217376e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x121737b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x121738020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1217384c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x121738960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x121738e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1217392a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x121739740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x121739be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12173a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12173a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12173a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12173ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12173b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12173b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12173bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12173c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12173c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12173ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12173cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12173d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12173d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12173dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12173e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12173e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12173ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12173ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12173f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12173f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12173fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1217401a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x121740640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x121740ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x121740f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x121741420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1217418c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x121741d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x121742200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1217426a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x121742b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x121742fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x121743480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x121743920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x121743dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x121744260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x121744700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x121744ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x121745040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1217454e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x121745980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x121745e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1217462c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x121746760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x121746c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1217470a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x121747540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1217479e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x121747e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x121748320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1217487c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x121748c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1217491b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x121749700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x121749c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12174a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12174a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12174aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12174b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12174b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12174be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12174c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12174c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x122804080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1228044f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x122804960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x122804dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x122805240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1228056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x122806220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1228064e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x122806b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x122807150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x122807700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x122807cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x122808260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x122808810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x122808dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x122809370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x122809920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x122809ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12280a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12280aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12280afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12280b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12280bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12280c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12280c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12280cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12280d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12280d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12280dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12280e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12280e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12280ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12280f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12280f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12280ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x122810530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x122810ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x122811090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x122811640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x122811bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1228121a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x122812750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x122812d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1228132b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x122813860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x122813e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1228143c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x122814970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x122814f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1228154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x122815a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x122816030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1228165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x122816b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x122817140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1228176f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x122817ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x122818250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x122818800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x122818db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x122819360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x122819910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x122819ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12281a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12281a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12281ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12281b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12281b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12281bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12281c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12281c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12281cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12281d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12281d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12281db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12281e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12281e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12281ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12281ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12281f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1228200a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1228207c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x122820ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1228211a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x122821990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x122821c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x122822260 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.096.784 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.096.788 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12280da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12280fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12280a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x122812a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12280d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x122813b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x122810da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12280c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x122819070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x122808520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x122807410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x122819620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12281f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12280cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x122813570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1228151e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x122815790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x122808ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x122818510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12280e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x122823230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x122823950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x122824070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x122824790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x122824eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x122825170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x122825780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x122825d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1228263a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x122826b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x122827030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1228272f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x122817400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x122827ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1228281c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x122828660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x122828b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x122828fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x122829440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1228298e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x122829d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12282a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12282a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12282ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12282ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12282b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12282ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12282c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12282c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12282cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12282d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12282d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12282dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12282e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12282eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12282f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12282f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12282f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12282feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1228306a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x122830b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x122830fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x122831480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x122831920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x122831dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x122832260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x122832700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x122832ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x122833040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1228334e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x122833980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x122833e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1228342c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x122834810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x122834d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1228352b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x122835800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x122835d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1228362a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1228367f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x122836d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x122837290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1228377e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x122837d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x122838280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1228387d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x122838d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x122839270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1228397c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x122839d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12283a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12283a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12283ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12283b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12283b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12283bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12283c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12283c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12283cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12283d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12283d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12283dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12283e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12283e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12283ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12283f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12283f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12283fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x122840200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x122840750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x122840ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1228411f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x122841740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x122841be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x122842080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x122842520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1228429c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x122842e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x122843300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1228437a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x122843c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1228440e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x122844580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x122844a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x122844ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x122845360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x122845800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x122845ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x122846140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1228465e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x122846a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x122846f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1228473c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x122847860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x122847d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1228481a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x122848640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x122848ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x122848f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x122849420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1228498c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x122849d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12284a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12284a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12284ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12284afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12284b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12284b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12284bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12284c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12284c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12284cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12284d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12284d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12284d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12284de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12284e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12284e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12284ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12284f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12284f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12284f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12284fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x122850320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1228507c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x122850c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x122851100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1228515a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x122851a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x122851ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x122852380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x122852820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x122852cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x122853160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x122853600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x122853aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x122853f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1228543e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x122854880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x122854d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1228551c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x122855660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x122855b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x122855fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x122856440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1228568e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x122856d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x122857220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1228576c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x122857b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x122858000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1228584a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x122858940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x122858e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1228593e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x122859930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x122859e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12285a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12285a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12285ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12285b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12285bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12285c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12285c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12285c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12285cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12285d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12285db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12285e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12285e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12285ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12285f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12285f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12285fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1228601a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1228606f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x122860c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x122861190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1228616e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x122861c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x122862180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1228626d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x122862c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x122863170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1228636c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x122863c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x122864160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1228646b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x122864c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x122865150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1228656a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x122865bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x122866140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x122866690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x122866be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x122867130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x122867680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x122867bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x122868120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x122868670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x122868bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x122869110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x122869660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x122869bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12286a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12286a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12286aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12286b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12286b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12286bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12286c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12286c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12286cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12286d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12286d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12286db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12286e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12286e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12286eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12286f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12286f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12286fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1228700a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1228705f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x122870b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x122871090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1228715e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x122871a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x122871f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1228723c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x122872860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x122872d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1228731a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x122873640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x122873ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x122873f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x122874420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1228748c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x122874d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x122875200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1228756a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x122875b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x122876090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1228767b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x122876ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1228775f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x122877d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x122877fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1228787c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x122878a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x122879090 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12174ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12174c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12174a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12174b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12171e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12171de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x121720430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1217157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12171c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12171cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12171d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12171bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12171ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1217147d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12170a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12171f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x121720a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12172d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1217179b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x121717c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12174b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x121715de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1217160a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x121716360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12174cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12174ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12174d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12174d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12174d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12174d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12174dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12174df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12174e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12174e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12174e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12174ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12174ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12174efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12174f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12174f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12174f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12174fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12174fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x121750030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1217502f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1217505b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x121750870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x121750b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x121750df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1217510b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x121751370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x121751630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1217518f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x121751bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x121751e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x121752130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1217523f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1217526b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x121752970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x121752c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x121752ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1217531b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x121753470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x121753730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1217539f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x121753cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x121753f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x121754230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1217544f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1217547b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x121754a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x121754d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x121754ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1217552b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x121755570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x121755830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x121755af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x121755db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x121756070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x121756330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1217565f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1217568b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x121756b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x121756e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1217570f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1217573b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x121757670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x121757930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x121757bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x121757eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x121758170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x121758430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1217586f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1217589b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x121758c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x121758f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1217591f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1217594b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x121759770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x121759a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x121759cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x121759fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12175a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12175a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12175a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12175aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12175ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12175b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12175b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12175b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12175b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12175bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12175bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12175c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12175c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12175c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12175c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12175cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12175ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12175d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12175d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12175d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12175d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12175dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12175def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12175e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12175e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12175e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12175e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12175ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12175ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12175f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12175f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12175f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12175fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12175fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12175fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1217602b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x121760570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x121760830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x121760af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x121760db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x121761070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x121761330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1217615f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1217618b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x121761b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x121761e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1217620f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1217623b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x121762670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x121762930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x121762bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x121762eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x121763170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x121763430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1217636f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1217639b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x121763c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x121763f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1217641f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1217644b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x121764770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x121764b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x121764e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x121765330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x121765830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x121765d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x121766230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x121766730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x121766c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x121767130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x121767630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x121767b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x121768030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x121768530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x121768a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x121768f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x121769430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x121769930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x121769e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12176a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12176a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12176ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12176b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12176b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12176bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12176c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12176c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12176cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12176d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12176d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12176da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12176df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12176e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12176ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12176f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12176f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12176fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x121770210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x121770820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x121771010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1217714b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x121771770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x121771d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x121772390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x121772b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x121773020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1217734c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x121773960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x121774110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x121774660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x121774bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x121775100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x121775650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x121775ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1217760f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x121776640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x121776b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1217770e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x121777630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x121777b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1217780d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x121778620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x121778b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1217790c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x121779610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x121779b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12177a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12177a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12177ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12177b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12177b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12177bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12177c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12177c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12177cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12177d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12177d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12177db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12177e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12177e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12177eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12177f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12177f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12177fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x121780050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1217805a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x121780af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x121781040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x121781590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x121781ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x121782030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x121782580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x121782ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x121783020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x121783570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x121783ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x121784010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x121784560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x121784ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x121785000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x121785550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x121785aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x121785ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x121786540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x121786a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x121786f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1217873d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x121787870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x121787d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1217881b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x121788650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x121788af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x121788f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x121789430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1217898d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x121789d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12178a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12178a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12178ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12178aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12178b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12178bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12178c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12178caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12178d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12178d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12178dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12178df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12178e540 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.939s
user	0m0.231s
sys	0m0.185s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
