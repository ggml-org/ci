Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.586s
user	0m0.908s
sys	0m1.234s
++ nproc
+ make -j10
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Built target sha256
[  4%] Built target build_info
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target xxhash
[  5%] Built target sha1
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 11%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 12%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 16%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 16%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama-gguf
[ 25%] Built target llama
[ 26%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 26%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 29%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 29%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-simple
[ 31%] Linking CXX executable ../../bin/llama-simple-chat
[ 31%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 32%] Linking C executable ../bin/test-c
[ 32%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Built target llava
[ 35%] Linking CXX static library libcommon.a
[ 35%] Built target llama-simple-chat
[ 35%] Built target llama-simple
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Built target test-c
[ 36%] Linking CXX static library libllava_static.a
[ 36%] Built target llama-quantize-stats
[ 36%] Built target common
[ 36%] Built target llava_static
[ 36%] Built target llava_shared
[ 36%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-0
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 45%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-grammar-integration
[ 45%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Linking CXX executable ../bin/test-grammar-parser
[ 47%] Linking CXX executable ../bin/test-sampling
[ 47%] Linking CXX executable ../bin/test-chat
[ 48%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-llama-grammar
[ 48%] Built target test-sampling
[ 48%] Built target test-json-schema-to-grammar
[ 48%] Built target test-grammar-integration
[ 48%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 49%] Built target test-chat
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Built target test-log
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-chat-template
[ 57%] Linking CXX executable ../bin/test-arg-parser
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-gguf
[ 58%] Linking CXX executable ../bin/test-model-load-cancel
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-backend-ops
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 60%] Linking CXX executable ../bin/test-barrier
[ 61%] Linking CXX executable ../bin/test-autorelease
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-model-load-cancel
[ 63%] Built target test-gguf
[ 63%] Built target test-arg-parser
[ 63%] Built target test-chat-template
[ 63%] Built target test-backend-ops
[ 63%] Built target test-autorelease
[ 63%] Built target test-barrier
[ 63%] Built target test-quantize-perf
[ 63%] Built target test-quantize-fns
[ 63%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 63%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 63%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 65%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 66%] Linking CXX executable ../../bin/llama-batched
[ 67%] Linking CXX executable ../../bin/llama-batched-bench
[ 68%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 68%] Built target test-rope
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-embedding
[ 71%] Linking CXX executable ../../bin/llama-eval-callback
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Built target llama-gbnf-validator
[ 71%] Built target llama-batched-bench
[ 71%] Built target llama-batched
[ 72%] Linking CXX executable ../../bin/llama-bench
[ 72%] Built target llama-embedding
[ 72%] Built target llama-eval-callback
[ 72%] Built target llama-gguf-split
[ 72%] Built target llama-imatrix
[ 72%] Built target llama-infill
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 73%] Built target llama-gritlm
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 73%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 73%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 74%] Linking CXX executable ../../bin/llama-lookahead
[ 74%] Linking CXX executable ../../bin/llama-lookup
[ 74%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 75%] Linking CXX executable ../../bin/llama-lookup-create
[ 75%] Built target llama-bench
[ 75%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 78%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Linking CXX executable ../../bin/llama-cli
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Built target llama-lookup
[ 81%] Built target llama-lookahead
[ 81%] Built target llama-lookup-create
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 81%] Built target llama-lookup-merge
[ 81%] Built target llama-lookup-stats
[ 82%] Generating loading.html.hpp
[ 82%] Built target llama-passkey
[ 82%] Built target llama-parallel
[ 82%] Built target llama-cli
[ 83%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 83%] Generating index.html.gz.hpp
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 83%] Built target llama-perplexity
[ 84%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 87%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 88%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-retrieval
[ 89%] Linking CXX executable ../../bin/llama-save-load-state
[ 89%] Linking CXX executable ../../bin/llama-speculative
[ 90%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 90%] Built target llama-quantize
[ 90%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-speculative-simple
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 91%] Linking CXX executable ../../bin/llama-run
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Built target llama-retrieval
[ 91%] Built target llama-speculative
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Built target llama-tokenize
[ 92%] Built target llama-tts
[ 92%] Built target llama-save-load-state
[ 92%] Built target llama-speculative-simple
[ 92%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Built target llama-gen-docs
[ 92%] Built target llama-run
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-export-lora
[ 97%] Linking CXX executable ../../bin/llama-cvector-generator
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Linking CXX executable ../../bin/llama-llava-cli
[ 98%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 98%] Built target llama-convert-llama2c-to-ggml
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.199s
user	0m6.472s
sys	0m10.029s

main: quantize time =  5497.15 ms
main:    total time =  5497.15 ms

main: quantize time =  3008.18 ms
main:    total time =  3008.18 ms

main: quantize time =  3274.01 ms
main:    total time =  3274.01 ms

main: quantize time =  2173.12 ms
main:    total time =  2173.12 ms

main: quantize time =  3916.43 ms
main:    total time =  3916.43 ms

main: quantize time =  5208.52 ms
main:    total time =  5208.52 ms

main: quantize time =  6187.08 ms
main:    total time =  6187.08 ms

main: quantize time =  7178.14 ms
main:    total time =  7178.14 ms

main: quantize time =  6197.57 ms
main:    total time =  6197.57 ms

main: quantize time =  4687.23 ms
main:    total time =  4687.23 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.179 I build: 4613 (ff227703) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.388 I main: llama backend init
0.00.000.401 I main: load the model and apply lora adapter, if any
0.00.033.197 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.047.393 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.047.405 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.047.408 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.047.409 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.047.418 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.047.418 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.047.419 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.047.421 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.047.421 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.047.422 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.047.422 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.047.423 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.047.424 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.047.425 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.047.428 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.047.429 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.047.429 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.055.319 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.057.430 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.064.220 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.064.222 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.064.222 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.064.223 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.064.223 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.064.225 I llama_model_loader: - type  f32:  194 tensors
0.00.064.225 I llama_model_loader: - type  f16:   98 tensors
0.00.064.226 I print_info: file format = GGUF V3 (latest)
0.00.064.227 I print_info: file type   = all F32 (guessed)
0.00.064.228 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.076.627 I load: special tokens cache size = 25
0.00.084.376 I load: token to piece cache size = 0.2984 MB
0.00.084.379 I print_info: arch             = gptneox
0.00.084.380 I print_info: vocab_only       = 0
0.00.084.380 I print_info: n_ctx_train      = 2048
0.00.084.380 I print_info: n_embd           = 2048
0.00.084.380 I print_info: n_layer          = 24
0.00.084.383 I print_info: n_head           = 16
0.00.084.384 I print_info: n_head_kv        = 16
0.00.084.385 I print_info: n_rot            = 32
0.00.084.385 I print_info: n_swa            = 0
0.00.084.385 I print_info: n_embd_head_k    = 128
0.00.084.385 I print_info: n_embd_head_v    = 128
0.00.084.386 I print_info: n_gqa            = 1
0.00.084.387 I print_info: n_embd_k_gqa     = 2048
0.00.084.387 I print_info: n_embd_v_gqa     = 2048
0.00.084.388 I print_info: f_norm_eps       = 1.0e-05
0.00.084.388 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.084.388 I print_info: f_clamp_kqv      = 0.0e+00
0.00.084.390 I print_info: f_max_alibi_bias = 0.0e+00
0.00.084.390 I print_info: f_logit_scale    = 0.0e+00
0.00.084.392 I print_info: n_ff             = 8192
0.00.084.392 I print_info: n_expert         = 0
0.00.084.392 I print_info: n_expert_used    = 0
0.00.084.393 I print_info: causal attn      = 1
0.00.084.393 I print_info: pooling type     = 0
0.00.084.393 I print_info: rope type        = 2
0.00.084.393 I print_info: rope scaling     = linear
0.00.084.394 I print_info: freq_base_train  = 10000.0
0.00.084.394 I print_info: freq_scale_train = 1
0.00.084.394 I print_info: n_ctx_orig_yarn  = 2048
0.00.084.395 I print_info: rope_finetuned   = unknown
0.00.084.395 I print_info: ssm_d_conv       = 0
0.00.084.395 I print_info: ssm_d_inner      = 0
0.00.084.395 I print_info: ssm_d_state      = 0
0.00.084.395 I print_info: ssm_dt_rank      = 0
0.00.084.397 I print_info: ssm_dt_b_c_rms   = 0
0.00.084.397 I print_info: model type       = 1.4B
0.00.084.398 I print_info: model params     = 1.41 B
0.00.084.398 I print_info: general.name     = 1.4B
0.00.084.398 I print_info: vocab type       = BPE
0.00.084.398 I print_info: n_vocab          = 50304
0.00.084.400 I print_info: n_merges         = 50009
0.00.084.400 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.084.400 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.084.400 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.084.400 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.084.401 I print_info: LF token         = 187 'Ċ'
0.00.084.401 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.084.401 I print_info: max token length = 1024
0.00.133.860 I load_tensors: offloading 24 repeating layers to GPU
0.00.133.864 I load_tensors: offloading output layer to GPU
0.00.133.864 I load_tensors: offloaded 25/25 layers to GPU
0.00.133.889 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.133.891 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.134.346 I llama_init_from_model: n_seq_max     = 1
0.00.134.346 I llama_init_from_model: n_ctx         = 2048
0.00.134.347 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.134.347 I llama_init_from_model: n_batch       = 2048
0.00.134.347 I llama_init_from_model: n_ubatch      = 512
0.00.134.347 I llama_init_from_model: flash_attn    = 0
0.00.134.348 I llama_init_from_model: freq_base     = 10000.0
0.00.134.348 I llama_init_from_model: freq_scale    = 1
0.00.134.348 I ggml_metal_init: allocating
0.00.134.375 I ggml_metal_init: found device: Apple M4
0.00.134.381 I ggml_metal_init: picking default device: Apple M4
0.00.135.038 I ggml_metal_init: using embedded metal library
0.00.144.914 I ggml_metal_init: GPU name:   Apple M4
0.00.144.915 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.144.916 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.144.916 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.144.917 I ggml_metal_init: simdgroup reduction   = true
0.00.144.917 I ggml_metal_init: simdgroup matrix mul. = true
0.00.144.917 I ggml_metal_init: has residency sets    = true
0.00.144.917 I ggml_metal_init: has bfloat            = true
0.00.144.917 I ggml_metal_init: use bfloat            = true
0.00.144.918 I ggml_metal_init: hasUnifiedMemory      = true
0.00.144.918 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.171.748 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.201.169 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.201.176 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.201.219 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.206.131 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.206.133 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.206.134 I llama_init_from_model: graph nodes  = 967
0.00.206.134 I llama_init_from_model: graph splits = 2
0.00.206.142 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.206.257 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.206.258 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.273.554 I main: llama threadpool init, n_threads = 4
0.00.273.606 I 
0.00.273.642 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.273.643 I 
0.00.273.820 I sampler seed: 1234
0.00.273.825 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.273.849 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.273.850 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.273.850 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.103.406 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59117.40 tokens per second)
0.02.103.406 I llama_perf_context_print:        load time =     239.25 ms
0.02.103.407 I llama_perf_context_print: prompt eval time =      43.63 ms /     7 tokens (    6.23 ms per token,   160.43 tokens per second)
0.02.103.408 I llama_perf_context_print:        eval time =    1783.02 ms /    63 runs   (   28.30 ms per token,    35.33 tokens per second)
0.02.103.410 I llama_perf_context_print:       total time =    1830.95 ms /    70 tokens
0.02.103.649 I ggml_metal_free: deallocating

real	0m2.412s
user	0m0.129s
sys	0m0.143s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4613 (ff227703) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.009.896 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.964 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.970 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.972 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.972 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.973 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.973 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.973 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.975 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.975 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.975 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.976 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.976 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.976 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.977 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.979 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.979 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.979 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.745 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.763 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.621 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.623 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.623 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.624 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.624 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.624 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.625 I llama_model_loader: - type  f32:  194 tensors
0.00.035.625 I llama_model_loader: - type q8_0:   98 tensors
0.00.035.626 I print_info: file format = GGUF V3 (latest)
0.00.035.627 I print_info: file type   = Q8_0
0.00.035.628 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.044.234 I load: special tokens cache size = 25
0.00.050.999 I load: token to piece cache size = 0.2984 MB
0.00.051.004 I print_info: arch             = gptneox
0.00.051.004 I print_info: vocab_only       = 0
0.00.051.005 I print_info: n_ctx_train      = 2048
0.00.051.005 I print_info: n_embd           = 2048
0.00.051.009 I print_info: n_layer          = 24
0.00.051.015 I print_info: n_head           = 16
0.00.051.016 I print_info: n_head_kv        = 16
0.00.051.016 I print_info: n_rot            = 32
0.00.051.016 I print_info: n_swa            = 0
0.00.051.016 I print_info: n_embd_head_k    = 128
0.00.051.016 I print_info: n_embd_head_v    = 128
0.00.051.017 I print_info: n_gqa            = 1
0.00.051.018 I print_info: n_embd_k_gqa     = 2048
0.00.051.018 I print_info: n_embd_v_gqa     = 2048
0.00.051.019 I print_info: f_norm_eps       = 1.0e-05
0.00.051.019 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.020 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.020 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.020 I print_info: f_logit_scale    = 0.0e+00
0.00.051.021 I print_info: n_ff             = 8192
0.00.051.021 I print_info: n_expert         = 0
0.00.051.021 I print_info: n_expert_used    = 0
0.00.051.021 I print_info: causal attn      = 1
0.00.051.021 I print_info: pooling type     = 0
0.00.051.022 I print_info: rope type        = 2
0.00.051.022 I print_info: rope scaling     = linear
0.00.051.023 I print_info: freq_base_train  = 10000.0
0.00.051.023 I print_info: freq_scale_train = 1
0.00.051.023 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.024 I print_info: rope_finetuned   = unknown
0.00.051.025 I print_info: ssm_d_conv       = 0
0.00.051.025 I print_info: ssm_d_inner      = 0
0.00.051.025 I print_info: ssm_d_state      = 0
0.00.051.025 I print_info: ssm_dt_rank      = 0
0.00.051.025 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.026 I print_info: model type       = 1.4B
0.00.051.026 I print_info: model params     = 1.41 B
0.00.051.035 I print_info: general.name     = 1.4B
0.00.051.039 I print_info: vocab type       = BPE
0.00.051.039 I print_info: n_vocab          = 50304
0.00.051.039 I print_info: n_merges         = 50009
0.00.051.040 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.040 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.040 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.040 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.041 I print_info: LF token         = 187 'Ċ'
0.00.051.041 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.041 I print_info: max token length = 1024
0.00.993.518 I load_tensors: offloading 24 repeating layers to GPU
0.00.993.523 I load_tensors: offloading output layer to GPU
0.00.993.525 I load_tensors: offloaded 25/25 layers to GPU
0.00.993.548 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.993.550 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.994.279 I llama_init_from_model: n_seq_max     = 1
0.00.994.280 I llama_init_from_model: n_ctx         = 2048
0.00.994.281 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.994.281 I llama_init_from_model: n_batch       = 2048
0.00.994.282 I llama_init_from_model: n_ubatch      = 512
0.00.994.282 I llama_init_from_model: flash_attn    = 0
0.00.994.283 I llama_init_from_model: freq_base     = 10000.0
0.00.994.283 I llama_init_from_model: freq_scale    = 1
0.00.994.285 I ggml_metal_init: allocating
0.00.994.304 I ggml_metal_init: found device: Apple M4
0.00.994.316 I ggml_metal_init: picking default device: Apple M4
0.00.995.498 I ggml_metal_init: using embedded metal library
0.01.001.246 I ggml_metal_init: GPU name:   Apple M4
0.01.001.250 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.001.251 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.001.252 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.001.252 I ggml_metal_init: simdgroup reduction   = true
0.01.001.252 I ggml_metal_init: simdgroup matrix mul. = true
0.01.001.252 I ggml_metal_init: has residency sets    = true
0.01.001.253 I ggml_metal_init: has bfloat            = true
0.01.001.253 I ggml_metal_init: use bfloat            = true
0.01.001.254 I ggml_metal_init: hasUnifiedMemory      = true
0.01.001.255 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.016.904 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.072.416 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.072.424 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.072.458 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.077.153 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.077.156 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.077.156 I llama_init_from_model: graph nodes  = 967
0.01.077.156 I llama_init_from_model: graph splits = 2
0.01.077.163 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.077.291 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.077.292 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.136.593 I main: llama threadpool init, n_threads = 4
0.01.136.641 I 
0.01.136.662 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.136.663 I 
0.01.136.817 I sampler seed: 1234
0.01.136.822 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.136.833 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.136.833 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.136.835 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.222.953 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55861.53 tokens per second)
0.02.222.954 I llama_perf_context_print:        load time =    1125.77 ms
0.02.222.955 I llama_perf_context_print: prompt eval time =      49.30 ms /     7 tokens (    7.04 ms per token,   141.99 tokens per second)
0.02.222.957 I llama_perf_context_print:        eval time =    1033.89 ms /    63 runs   (   16.41 ms per token,    60.93 tokens per second)
0.02.222.957 I llama_perf_context_print:       total time =    1087.29 ms /    70 tokens
0.02.223.190 I ggml_metal_free: deallocating

real	0m2.240s
user	0m0.108s
sys	0m0.265s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4613 (ff227703) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.015.258 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.031.961 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.031.968 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.970 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.971 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.971 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.971 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.971 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.973 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.973 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.973 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.974 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.974 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.974 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.975 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.977 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.977 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.977 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.036.198 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.037.302 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.041.382 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.041.383 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.041.384 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.041.384 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.041.384 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.041.385 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.041.385 I llama_model_loader: - type  f32:  194 tensors
0.00.041.386 I llama_model_loader: - type q4_0:   97 tensors
0.00.041.386 I llama_model_loader: - type q6_K:    1 tensors
0.00.041.386 I print_info: file format = GGUF V3 (latest)
0.00.041.387 I print_info: file type   = Q4_0
0.00.041.388 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.051.042 I load: special tokens cache size = 25
0.00.058.411 I load: token to piece cache size = 0.2984 MB
0.00.058.414 I print_info: arch             = gptneox
0.00.058.415 I print_info: vocab_only       = 0
0.00.058.415 I print_info: n_ctx_train      = 2048
0.00.058.415 I print_info: n_embd           = 2048
0.00.058.415 I print_info: n_layer          = 24
0.00.058.419 I print_info: n_head           = 16
0.00.058.420 I print_info: n_head_kv        = 16
0.00.058.420 I print_info: n_rot            = 32
0.00.058.424 I print_info: n_swa            = 0
0.00.058.424 I print_info: n_embd_head_k    = 128
0.00.058.424 I print_info: n_embd_head_v    = 128
0.00.058.425 I print_info: n_gqa            = 1
0.00.058.426 I print_info: n_embd_k_gqa     = 2048
0.00.058.427 I print_info: n_embd_v_gqa     = 2048
0.00.058.427 I print_info: f_norm_eps       = 1.0e-05
0.00.058.428 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.058.428 I print_info: f_clamp_kqv      = 0.0e+00
0.00.058.430 I print_info: f_max_alibi_bias = 0.0e+00
0.00.058.430 I print_info: f_logit_scale    = 0.0e+00
0.00.058.431 I print_info: n_ff             = 8192
0.00.058.431 I print_info: n_expert         = 0
0.00.058.431 I print_info: n_expert_used    = 0
0.00.058.431 I print_info: causal attn      = 1
0.00.058.431 I print_info: pooling type     = 0
0.00.058.431 I print_info: rope type        = 2
0.00.058.432 I print_info: rope scaling     = linear
0.00.058.432 I print_info: freq_base_train  = 10000.0
0.00.058.432 I print_info: freq_scale_train = 1
0.00.058.432 I print_info: n_ctx_orig_yarn  = 2048
0.00.058.433 I print_info: rope_finetuned   = unknown
0.00.058.433 I print_info: ssm_d_conv       = 0
0.00.058.433 I print_info: ssm_d_inner      = 0
0.00.058.433 I print_info: ssm_d_state      = 0
0.00.058.433 I print_info: ssm_dt_rank      = 0
0.00.058.433 I print_info: ssm_dt_b_c_rms   = 0
0.00.058.438 I print_info: model type       = 1.4B
0.00.058.438 I print_info: model params     = 1.41 B
0.00.058.438 I print_info: general.name     = 1.4B
0.00.058.439 I print_info: vocab type       = BPE
0.00.058.439 I print_info: n_vocab          = 50304
0.00.058.440 I print_info: n_merges         = 50009
0.00.058.440 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.058.440 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.058.440 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.058.441 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.058.441 I print_info: LF token         = 187 'Ċ'
0.00.058.441 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.058.441 I print_info: max token length = 1024
0.00.613.393 I load_tensors: offloading 24 repeating layers to GPU
0.00.613.406 I load_tensors: offloading output layer to GPU
0.00.613.407 I load_tensors: offloaded 25/25 layers to GPU
0.00.613.441 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.613.443 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.614.939 I llama_init_from_model: n_seq_max     = 1
0.00.614.944 I llama_init_from_model: n_ctx         = 2048
0.00.614.945 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.614.945 I llama_init_from_model: n_batch       = 2048
0.00.614.946 I llama_init_from_model: n_ubatch      = 512
0.00.614.946 I llama_init_from_model: flash_attn    = 0
0.00.614.948 I llama_init_from_model: freq_base     = 10000.0
0.00.614.949 I llama_init_from_model: freq_scale    = 1
0.00.614.952 I ggml_metal_init: allocating
0.00.615.018 I ggml_metal_init: found device: Apple M4
0.00.615.033 I ggml_metal_init: picking default device: Apple M4
0.00.616.765 I ggml_metal_init: using embedded metal library
0.00.623.191 I ggml_metal_init: GPU name:   Apple M4
0.00.623.197 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.623.198 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.623.198 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.623.199 I ggml_metal_init: simdgroup reduction   = true
0.00.623.199 I ggml_metal_init: simdgroup matrix mul. = true
0.00.623.200 I ggml_metal_init: has residency sets    = true
0.00.623.200 I ggml_metal_init: has bfloat            = true
0.00.623.200 I ggml_metal_init: use bfloat            = true
0.00.623.202 I ggml_metal_init: hasUnifiedMemory      = true
0.00.623.203 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.642.824 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.696.140 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.696.147 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.696.194 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.700.899 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.700.901 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.700.901 I llama_init_from_model: graph nodes  = 967
0.00.700.901 I llama_init_from_model: graph splits = 2
0.00.700.907 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.701.035 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.701.036 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.754.345 I main: llama threadpool init, n_threads = 4
0.00.754.390 I 
0.00.754.416 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.754.418 I 
0.00.754.594 I sampler seed: 1234
0.00.754.598 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.754.619 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.754.619 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.754.620 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.423.569 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52052.79 tokens per second)
0.01.423.569 I llama_perf_context_print:        load time =     738.12 ms
0.01.423.570 I llama_perf_context_print: prompt eval time =      39.40 ms /     7 tokens (    5.63 ms per token,   177.68 tokens per second)
0.01.423.571 I llama_perf_context_print:        eval time =     626.68 ms /    63 runs   (    9.95 ms per token,   100.53 tokens per second)
0.01.423.571 I llama_perf_context_print:       total time =     670.19 ms /    70 tokens
0.01.423.786 I ggml_metal_free: deallocating

real	0m1.441s
user	0m0.115s
sys	0m0.212s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4613 (ff227703) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.775 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.657 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.661 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.667 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.668 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.668 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.668 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.669 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.670 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.670 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.670 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.671 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.671 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.672 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.672 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.673 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.674 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.674 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.794 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.863 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.920 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.922 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.922 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.922 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.923 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.923 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.923 I llama_model_loader: - type  f32:  194 tensors
0.00.025.924 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.924 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.925 I print_info: file format = GGUF V3 (latest)
0.00.025.925 I print_info: file type   = Q4_1
0.00.025.926 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.839 I load: special tokens cache size = 25
0.00.039.793 I load: token to piece cache size = 0.2984 MB
0.00.039.796 I print_info: arch             = gptneox
0.00.039.796 I print_info: vocab_only       = 0
0.00.039.796 I print_info: n_ctx_train      = 2048
0.00.039.796 I print_info: n_embd           = 2048
0.00.039.796 I print_info: n_layer          = 24
0.00.039.800 I print_info: n_head           = 16
0.00.039.800 I print_info: n_head_kv        = 16
0.00.039.801 I print_info: n_rot            = 32
0.00.039.801 I print_info: n_swa            = 0
0.00.039.801 I print_info: n_embd_head_k    = 128
0.00.039.801 I print_info: n_embd_head_v    = 128
0.00.039.802 I print_info: n_gqa            = 1
0.00.039.803 I print_info: n_embd_k_gqa     = 2048
0.00.039.803 I print_info: n_embd_v_gqa     = 2048
0.00.039.804 I print_info: f_norm_eps       = 1.0e-05
0.00.039.805 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.806 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.806 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.806 I print_info: f_logit_scale    = 0.0e+00
0.00.039.807 I print_info: n_ff             = 8192
0.00.039.807 I print_info: n_expert         = 0
0.00.039.807 I print_info: n_expert_used    = 0
0.00.039.807 I print_info: causal attn      = 1
0.00.039.807 I print_info: pooling type     = 0
0.00.039.808 I print_info: rope type        = 2
0.00.039.808 I print_info: rope scaling     = linear
0.00.039.810 I print_info: freq_base_train  = 10000.0
0.00.039.810 I print_info: freq_scale_train = 1
0.00.039.811 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.811 I print_info: rope_finetuned   = unknown
0.00.039.811 I print_info: ssm_d_conv       = 0
0.00.039.811 I print_info: ssm_d_inner      = 0
0.00.039.811 I print_info: ssm_d_state      = 0
0.00.039.811 I print_info: ssm_dt_rank      = 0
0.00.039.811 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.812 I print_info: model type       = 1.4B
0.00.039.812 I print_info: model params     = 1.41 B
0.00.039.812 I print_info: general.name     = 1.4B
0.00.039.813 I print_info: vocab type       = BPE
0.00.039.813 I print_info: n_vocab          = 50304
0.00.039.813 I print_info: n_merges         = 50009
0.00.039.813 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.814 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.814 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.814 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.818 I print_info: LF token         = 187 'Ċ'
0.00.039.818 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.819 I print_info: max token length = 1024
0.00.615.564 I load_tensors: offloading 24 repeating layers to GPU
0.00.615.575 I load_tensors: offloading output layer to GPU
0.00.615.576 I load_tensors: offloaded 25/25 layers to GPU
0.00.615.603 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.615.605 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.616.855 I llama_init_from_model: n_seq_max     = 1
0.00.616.868 I llama_init_from_model: n_ctx         = 2048
0.00.616.868 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.616.869 I llama_init_from_model: n_batch       = 2048
0.00.616.869 I llama_init_from_model: n_ubatch      = 512
0.00.616.869 I llama_init_from_model: flash_attn    = 0
0.00.616.871 I llama_init_from_model: freq_base     = 10000.0
0.00.616.871 I llama_init_from_model: freq_scale    = 1
0.00.616.874 I ggml_metal_init: allocating
0.00.616.920 I ggml_metal_init: found device: Apple M4
0.00.616.931 I ggml_metal_init: picking default device: Apple M4
0.00.618.635 I ggml_metal_init: using embedded metal library
0.00.624.335 I ggml_metal_init: GPU name:   Apple M4
0.00.624.340 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.624.341 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.624.342 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.624.343 I ggml_metal_init: simdgroup reduction   = true
0.00.624.343 I ggml_metal_init: simdgroup matrix mul. = true
0.00.624.343 I ggml_metal_init: has residency sets    = true
0.00.624.344 I ggml_metal_init: has bfloat            = true
0.00.624.344 I ggml_metal_init: use bfloat            = true
0.00.624.345 I ggml_metal_init: hasUnifiedMemory      = true
0.00.624.347 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.643.242 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.697.845 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.697.854 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.697.887 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.703.243 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.703.245 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.703.245 I llama_init_from_model: graph nodes  = 967
0.00.703.246 I llama_init_from_model: graph splits = 2
0.00.703.252 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.703.376 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.703.376 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.759.965 I main: llama threadpool init, n_threads = 4
0.00.760.005 I 
0.00.760.029 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.760.029 I 
0.00.760.183 I sampler seed: 1234
0.00.760.187 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.760.226 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.760.229 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.760.229 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.486.772 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57211.93 tokens per second)
0.01.486.773 I llama_perf_context_print:        load time =     750.25 ms
0.01.486.773 I llama_perf_context_print: prompt eval time =      50.18 ms /     7 tokens (    7.17 ms per token,   139.49 tokens per second)
0.01.486.776 I llama_perf_context_print:        eval time =     673.59 ms /    63 runs   (   10.69 ms per token,    93.53 tokens per second)
0.01.486.776 I llama_perf_context_print:       total time =     727.74 ms /    70 tokens
0.01.487.009 I ggml_metal_free: deallocating

real	0m1.504s
user	0m0.109s
sys	0m0.200s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4613 (ff227703) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.011.831 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.198 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.019.204 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.206 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.207 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.207 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.207 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.208 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.208 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.209 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.209 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.210 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.210 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.210 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.211 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.213 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.213 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.213 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.276 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.305 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.285 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.287 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.287 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.287 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.288 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.288 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.028.288 I llama_model_loader: - type  f32:  194 tensors
0.00.028.289 I llama_model_loader: - type q5_0:   97 tensors
0.00.028.289 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.289 I print_info: file format = GGUF V3 (latest)
0.00.028.290 I print_info: file type   = Q5_0
0.00.028.296 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.036.208 I load: special tokens cache size = 25
0.00.042.220 I load: token to piece cache size = 0.2984 MB
0.00.042.223 I print_info: arch             = gptneox
0.00.042.223 I print_info: vocab_only       = 0
0.00.042.223 I print_info: n_ctx_train      = 2048
0.00.042.224 I print_info: n_embd           = 2048
0.00.042.224 I print_info: n_layer          = 24
0.00.042.226 I print_info: n_head           = 16
0.00.042.227 I print_info: n_head_kv        = 16
0.00.042.227 I print_info: n_rot            = 32
0.00.042.228 I print_info: n_swa            = 0
0.00.042.230 I print_info: n_embd_head_k    = 128
0.00.042.230 I print_info: n_embd_head_v    = 128
0.00.042.231 I print_info: n_gqa            = 1
0.00.042.231 I print_info: n_embd_k_gqa     = 2048
0.00.042.232 I print_info: n_embd_v_gqa     = 2048
0.00.042.233 I print_info: f_norm_eps       = 1.0e-05
0.00.042.233 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.233 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.233 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.233 I print_info: f_logit_scale    = 0.0e+00
0.00.042.234 I print_info: n_ff             = 8192
0.00.042.234 I print_info: n_expert         = 0
0.00.042.234 I print_info: n_expert_used    = 0
0.00.042.235 I print_info: causal attn      = 1
0.00.042.235 I print_info: pooling type     = 0
0.00.042.235 I print_info: rope type        = 2
0.00.042.240 I print_info: rope scaling     = linear
0.00.042.243 I print_info: freq_base_train  = 10000.0
0.00.042.244 I print_info: freq_scale_train = 1
0.00.042.244 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.244 I print_info: rope_finetuned   = unknown
0.00.042.244 I print_info: ssm_d_conv       = 0
0.00.042.244 I print_info: ssm_d_inner      = 0
0.00.042.245 I print_info: ssm_d_state      = 0
0.00.042.245 I print_info: ssm_dt_rank      = 0
0.00.042.245 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.245 I print_info: model type       = 1.4B
0.00.042.246 I print_info: model params     = 1.41 B
0.00.042.246 I print_info: general.name     = 1.4B
0.00.042.252 I print_info: vocab type       = BPE
0.00.042.253 I print_info: n_vocab          = 50304
0.00.042.254 I print_info: n_merges         = 50009
0.00.042.254 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.254 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.254 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.255 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.256 I print_info: LF token         = 187 'Ċ'
0.00.042.257 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.258 I print_info: max token length = 1024
0.00.639.351 I load_tensors: offloading 24 repeating layers to GPU
0.00.639.367 I load_tensors: offloading output layer to GPU
0.00.639.367 I load_tensors: offloaded 25/25 layers to GPU
0.00.639.400 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.639.402 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.640.713 I llama_init_from_model: n_seq_max     = 1
0.00.640.717 I llama_init_from_model: n_ctx         = 2048
0.00.640.718 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.640.718 I llama_init_from_model: n_batch       = 2048
0.00.640.719 I llama_init_from_model: n_ubatch      = 512
0.00.640.719 I llama_init_from_model: flash_attn    = 0
0.00.640.721 I llama_init_from_model: freq_base     = 10000.0
0.00.640.722 I llama_init_from_model: freq_scale    = 1
0.00.640.737 I ggml_metal_init: allocating
0.00.640.818 I ggml_metal_init: found device: Apple M4
0.00.640.833 I ggml_metal_init: picking default device: Apple M4
0.00.642.616 I ggml_metal_init: using embedded metal library
0.00.649.288 I ggml_metal_init: GPU name:   Apple M4
0.00.649.292 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.649.293 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.649.294 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.649.295 I ggml_metal_init: simdgroup reduction   = true
0.00.649.295 I ggml_metal_init: simdgroup matrix mul. = true
0.00.649.296 I ggml_metal_init: has residency sets    = true
0.00.649.296 I ggml_metal_init: has bfloat            = true
0.00.649.296 I ggml_metal_init: use bfloat            = true
0.00.649.297 I ggml_metal_init: hasUnifiedMemory      = true
0.00.649.307 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.667.447 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.722.456 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.722.463 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.722.498 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.727.665 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.727.667 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.727.667 I llama_init_from_model: graph nodes  = 967
0.00.727.667 I llama_init_from_model: graph splits = 2
0.00.727.672 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.727.787 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.727.788 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.787.146 I main: llama threadpool init, n_threads = 4
0.00.787.186 I 
0.00.787.211 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.787.212 I 
0.00.787.364 I sampler seed: 1234
0.00.787.369 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.787.407 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.787.410 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.787.410 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.583.623 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54699.54 tokens per second)
0.01.583.624 I llama_perf_context_print:        load time =     774.39 ms
0.01.583.624 I llama_perf_context_print: prompt eval time =      51.00 ms /     7 tokens (    7.29 ms per token,   137.24 tokens per second)
0.01.583.626 I llama_perf_context_print:        eval time =     742.37 ms /    63 runs   (   11.78 ms per token,    84.86 tokens per second)
0.01.583.627 I llama_perf_context_print:       total time =     797.40 ms /    70 tokens
0.01.583.907 I ggml_metal_free: deallocating

real	0m1.602s
user	0m0.110s
sys	0m0.208s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4613 (ff227703) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.009.142 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.627 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.018.631 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.633 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.638 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.638 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.639 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.639 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.640 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.640 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.641 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.641 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.642 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.642 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.642 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.647 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.647 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.647 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.537 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.658 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.509 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.511 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.511 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.511 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.512 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.512 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.027.513 I llama_model_loader: - type  f32:  194 tensors
0.00.027.513 I llama_model_loader: - type q5_1:   97 tensors
0.00.027.513 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.514 I print_info: file format = GGUF V3 (latest)
0.00.027.514 I print_info: file type   = Q5_1
0.00.027.516 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.035.678 I load: special tokens cache size = 25
0.00.041.809 I load: token to piece cache size = 0.2984 MB
0.00.041.811 I print_info: arch             = gptneox
0.00.041.811 I print_info: vocab_only       = 0
0.00.041.812 I print_info: n_ctx_train      = 2048
0.00.041.812 I print_info: n_embd           = 2048
0.00.041.812 I print_info: n_layer          = 24
0.00.041.815 I print_info: n_head           = 16
0.00.041.816 I print_info: n_head_kv        = 16
0.00.041.816 I print_info: n_rot            = 32
0.00.041.816 I print_info: n_swa            = 0
0.00.041.817 I print_info: n_embd_head_k    = 128
0.00.041.817 I print_info: n_embd_head_v    = 128
0.00.041.817 I print_info: n_gqa            = 1
0.00.041.818 I print_info: n_embd_k_gqa     = 2048
0.00.041.819 I print_info: n_embd_v_gqa     = 2048
0.00.041.819 I print_info: f_norm_eps       = 1.0e-05
0.00.041.820 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.820 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.820 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.820 I print_info: f_logit_scale    = 0.0e+00
0.00.041.821 I print_info: n_ff             = 8192
0.00.041.821 I print_info: n_expert         = 0
0.00.041.821 I print_info: n_expert_used    = 0
0.00.041.821 I print_info: causal attn      = 1
0.00.041.821 I print_info: pooling type     = 0
0.00.041.823 I print_info: rope type        = 2
0.00.041.825 I print_info: rope scaling     = linear
0.00.041.825 I print_info: freq_base_train  = 10000.0
0.00.041.825 I print_info: freq_scale_train = 1
0.00.041.826 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.826 I print_info: rope_finetuned   = unknown
0.00.041.826 I print_info: ssm_d_conv       = 0
0.00.041.826 I print_info: ssm_d_inner      = 0
0.00.041.826 I print_info: ssm_d_state      = 0
0.00.041.827 I print_info: ssm_dt_rank      = 0
0.00.041.827 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.827 I print_info: model type       = 1.4B
0.00.041.829 I print_info: model params     = 1.41 B
0.00.041.829 I print_info: general.name     = 1.4B
0.00.041.829 I print_info: vocab type       = BPE
0.00.041.830 I print_info: n_vocab          = 50304
0.00.041.830 I print_info: n_merges         = 50009
0.00.041.830 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.830 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.830 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.831 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.831 I print_info: LF token         = 187 'Ċ'
0.00.041.831 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.831 I print_info: max token length = 1024
0.00.699.046 I load_tensors: offloading 24 repeating layers to GPU
0.00.699.054 I load_tensors: offloading output layer to GPU
0.00.699.055 I load_tensors: offloaded 25/25 layers to GPU
0.00.699.089 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.699.096 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.700.570 I llama_init_from_model: n_seq_max     = 1
0.00.700.574 I llama_init_from_model: n_ctx         = 2048
0.00.700.574 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.700.574 I llama_init_from_model: n_batch       = 2048
0.00.700.575 I llama_init_from_model: n_ubatch      = 512
0.00.700.576 I llama_init_from_model: flash_attn    = 0
0.00.700.577 I llama_init_from_model: freq_base     = 10000.0
0.00.700.577 I llama_init_from_model: freq_scale    = 1
0.00.700.582 I ggml_metal_init: allocating
0.00.700.601 I ggml_metal_init: found device: Apple M4
0.00.700.610 I ggml_metal_init: picking default device: Apple M4
0.00.702.078 I ggml_metal_init: using embedded metal library
0.00.708.339 I ggml_metal_init: GPU name:   Apple M4
0.00.708.343 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.708.344 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.708.345 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.708.345 I ggml_metal_init: simdgroup reduction   = true
0.00.708.346 I ggml_metal_init: simdgroup matrix mul. = true
0.00.708.346 I ggml_metal_init: has residency sets    = true
0.00.708.346 I ggml_metal_init: has bfloat            = true
0.00.708.347 I ggml_metal_init: use bfloat            = true
0.00.708.347 I ggml_metal_init: hasUnifiedMemory      = true
0.00.708.352 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.725.951 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.777.457 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.777.463 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.777.501 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.781.661 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.781.663 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.781.663 I llama_init_from_model: graph nodes  = 967
0.00.781.663 I llama_init_from_model: graph splits = 2
0.00.781.670 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.781.791 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.781.792 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.830.644 I main: llama threadpool init, n_threads = 4
0.00.830.685 I 
0.00.830.708 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.830.708 I 
0.00.830.831 I sampler seed: 1234
0.00.830.835 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.830.852 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.830.853 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.830.853 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.669.097 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54573.41 tokens per second)
0.01.669.098 I llama_perf_context_print:        load time =     820.55 ms
0.01.669.100 I llama_perf_context_print: prompt eval time =      41.96 ms /     7 tokens (    5.99 ms per token,   166.83 tokens per second)
0.01.669.101 I llama_perf_context_print:        eval time =     793.47 ms /    63 runs   (   12.59 ms per token,    79.40 tokens per second)
0.01.669.101 I llama_perf_context_print:       total time =     839.40 ms /    70 tokens
0.01.669.362 I ggml_metal_free: deallocating

real	0m1.686s
user	0m0.109s
sys	0m0.197s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4613 (ff227703) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.009.778 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.368 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.373 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.375 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.376 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.376 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.376 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.377 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.378 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.378 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.378 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.379 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.379 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.379 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.380 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.381 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.382 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.382 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.343 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.364 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.274 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.275 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.276 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.276 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.276 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.277 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.277 I llama_model_loader: - type  f32:  194 tensors
0.00.025.278 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.278 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.278 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.279 I print_info: file format = GGUF V3 (latest)
0.00.025.279 I print_info: file type   = Q2_K - Medium
0.00.025.280 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.148 I load: special tokens cache size = 25
0.00.039.262 I load: token to piece cache size = 0.2984 MB
0.00.039.265 I print_info: arch             = gptneox
0.00.039.265 I print_info: vocab_only       = 0
0.00.039.265 I print_info: n_ctx_train      = 2048
0.00.039.265 I print_info: n_embd           = 2048
0.00.039.265 I print_info: n_layer          = 24
0.00.039.268 I print_info: n_head           = 16
0.00.039.269 I print_info: n_head_kv        = 16
0.00.039.269 I print_info: n_rot            = 32
0.00.039.269 I print_info: n_swa            = 0
0.00.039.272 I print_info: n_embd_head_k    = 128
0.00.039.272 I print_info: n_embd_head_v    = 128
0.00.039.272 I print_info: n_gqa            = 1
0.00.039.273 I print_info: n_embd_k_gqa     = 2048
0.00.039.278 I print_info: n_embd_v_gqa     = 2048
0.00.039.279 I print_info: f_norm_eps       = 1.0e-05
0.00.039.280 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.280 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.280 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.280 I print_info: f_logit_scale    = 0.0e+00
0.00.039.281 I print_info: n_ff             = 8192
0.00.039.281 I print_info: n_expert         = 0
0.00.039.281 I print_info: n_expert_used    = 0
0.00.039.281 I print_info: causal attn      = 1
0.00.039.281 I print_info: pooling type     = 0
0.00.039.282 I print_info: rope type        = 2
0.00.039.284 I print_info: rope scaling     = linear
0.00.039.285 I print_info: freq_base_train  = 10000.0
0.00.039.285 I print_info: freq_scale_train = 1
0.00.039.285 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.285 I print_info: rope_finetuned   = unknown
0.00.039.285 I print_info: ssm_d_conv       = 0
0.00.039.286 I print_info: ssm_d_inner      = 0
0.00.039.286 I print_info: ssm_d_state      = 0
0.00.039.286 I print_info: ssm_dt_rank      = 0
0.00.039.286 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.286 I print_info: model type       = 1.4B
0.00.039.287 I print_info: model params     = 1.41 B
0.00.039.287 I print_info: general.name     = 1.4B
0.00.039.287 I print_info: vocab type       = BPE
0.00.039.288 I print_info: n_vocab          = 50304
0.00.039.288 I print_info: n_merges         = 50009
0.00.039.288 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.288 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.288 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.289 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.289 I print_info: LF token         = 187 'Ċ'
0.00.039.289 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.289 I print_info: max token length = 1024
0.00.363.272 I load_tensors: offloading 24 repeating layers to GPU
0.00.363.285 I load_tensors: offloading output layer to GPU
0.00.363.285 I load_tensors: offloaded 25/25 layers to GPU
0.00.363.320 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.363.321 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.364.937 I llama_init_from_model: n_seq_max     = 1
0.00.364.945 I llama_init_from_model: n_ctx         = 2048
0.00.364.945 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.364.946 I llama_init_from_model: n_batch       = 2048
0.00.364.946 I llama_init_from_model: n_ubatch      = 512
0.00.364.947 I llama_init_from_model: flash_attn    = 0
0.00.364.948 I llama_init_from_model: freq_base     = 10000.0
0.00.364.954 I llama_init_from_model: freq_scale    = 1
0.00.364.956 I ggml_metal_init: allocating
0.00.365.041 I ggml_metal_init: found device: Apple M4
0.00.365.055 I ggml_metal_init: picking default device: Apple M4
0.00.366.853 I ggml_metal_init: using embedded metal library
0.00.372.354 I ggml_metal_init: GPU name:   Apple M4
0.00.372.363 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.372.364 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.372.365 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.372.365 I ggml_metal_init: simdgroup reduction   = true
0.00.372.365 I ggml_metal_init: simdgroup matrix mul. = true
0.00.372.366 I ggml_metal_init: has residency sets    = true
0.00.372.366 I ggml_metal_init: has bfloat            = true
0.00.372.366 I ggml_metal_init: use bfloat            = true
0.00.372.368 I ggml_metal_init: hasUnifiedMemory      = true
0.00.372.373 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.394.471 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.450.365 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.450.374 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.450.410 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.454.638 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.454.640 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.454.640 I llama_init_from_model: graph nodes  = 967
0.00.454.640 I llama_init_from_model: graph splits = 2
0.00.454.645 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.454.775 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.454.776 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.515.045 I main: llama threadpool init, n_threads = 4
0.00.515.079 I 
0.00.515.101 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.515.101 I 
0.00.515.285 I sampler seed: 1234
0.00.515.290 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.515.301 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.515.301 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.515.301 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.201.245 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53064.28 tokens per second)
0.01.201.247 I llama_perf_context_print:        load time =     504.32 ms
0.01.201.247 I llama_perf_context_print: prompt eval time =      44.17 ms /     7 tokens (    6.31 ms per token,   158.46 tokens per second)
0.01.201.248 I llama_perf_context_print:        eval time =     638.84 ms /    63 runs   (   10.14 ms per token,    98.62 tokens per second)
0.01.201.248 I llama_perf_context_print:       total time =     687.14 ms /    70 tokens
0.01.201.464 I ggml_metal_free: deallocating

real	0m1.220s
user	0m0.112s
sys	0m0.174s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4613 (ff227703) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.008.703 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.346 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.352 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.353 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.354 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.354 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.355 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.355 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.356 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.356 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.357 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.357 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.357 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.360 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.360 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.363 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.363 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.363 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.420 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.458 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.467 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.468 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.468 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.468 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.469 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.469 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.470 I llama_model_loader: - type  f32:  194 tensors
0.00.025.470 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.470 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.470 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.471 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.471 I print_info: file format = GGUF V3 (latest)
0.00.025.472 I print_info: file type   = Q3_K - Medium
0.00.025.473 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.331 I load: special tokens cache size = 25
0.00.039.312 I load: token to piece cache size = 0.2984 MB
0.00.039.315 I print_info: arch             = gptneox
0.00.039.315 I print_info: vocab_only       = 0
0.00.039.315 I print_info: n_ctx_train      = 2048
0.00.039.316 I print_info: n_embd           = 2048
0.00.039.316 I print_info: n_layer          = 24
0.00.039.319 I print_info: n_head           = 16
0.00.039.319 I print_info: n_head_kv        = 16
0.00.039.320 I print_info: n_rot            = 32
0.00.039.320 I print_info: n_swa            = 0
0.00.039.320 I print_info: n_embd_head_k    = 128
0.00.039.320 I print_info: n_embd_head_v    = 128
0.00.039.322 I print_info: n_gqa            = 1
0.00.039.323 I print_info: n_embd_k_gqa     = 2048
0.00.039.323 I print_info: n_embd_v_gqa     = 2048
0.00.039.324 I print_info: f_norm_eps       = 1.0e-05
0.00.039.324 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.325 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.325 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.325 I print_info: f_logit_scale    = 0.0e+00
0.00.039.326 I print_info: n_ff             = 8192
0.00.039.327 I print_info: n_expert         = 0
0.00.039.328 I print_info: n_expert_used    = 0
0.00.039.329 I print_info: causal attn      = 1
0.00.039.329 I print_info: pooling type     = 0
0.00.039.330 I print_info: rope type        = 2
0.00.039.331 I print_info: rope scaling     = linear
0.00.039.331 I print_info: freq_base_train  = 10000.0
0.00.039.331 I print_info: freq_scale_train = 1
0.00.039.331 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.332 I print_info: rope_finetuned   = unknown
0.00.039.332 I print_info: ssm_d_conv       = 0
0.00.039.332 I print_info: ssm_d_inner      = 0
0.00.039.332 I print_info: ssm_d_state      = 0
0.00.039.332 I print_info: ssm_dt_rank      = 0
0.00.039.332 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.333 I print_info: model type       = 1.4B
0.00.039.333 I print_info: model params     = 1.41 B
0.00.039.333 I print_info: general.name     = 1.4B
0.00.039.334 I print_info: vocab type       = BPE
0.00.039.334 I print_info: n_vocab          = 50304
0.00.039.334 I print_info: n_merges         = 50009
0.00.039.334 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.335 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.335 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.335 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.335 I print_info: LF token         = 187 'Ċ'
0.00.039.335 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.336 I print_info: max token length = 1024
0.00.441.701 I load_tensors: offloading 24 repeating layers to GPU
0.00.441.718 I load_tensors: offloading output layer to GPU
0.00.441.719 I load_tensors: offloaded 25/25 layers to GPU
0.00.441.752 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.441.753 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.443.316 I llama_init_from_model: n_seq_max     = 1
0.00.443.321 I llama_init_from_model: n_ctx         = 2048
0.00.443.321 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.443.322 I llama_init_from_model: n_batch       = 2048
0.00.443.322 I llama_init_from_model: n_ubatch      = 512
0.00.443.322 I llama_init_from_model: flash_attn    = 0
0.00.443.329 I llama_init_from_model: freq_base     = 10000.0
0.00.443.334 I llama_init_from_model: freq_scale    = 1
0.00.443.336 I ggml_metal_init: allocating
0.00.443.411 I ggml_metal_init: found device: Apple M4
0.00.443.426 I ggml_metal_init: picking default device: Apple M4
0.00.445.260 I ggml_metal_init: using embedded metal library
0.00.450.966 I ggml_metal_init: GPU name:   Apple M4
0.00.450.971 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.450.972 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.450.973 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.450.974 I ggml_metal_init: simdgroup reduction   = true
0.00.450.974 I ggml_metal_init: simdgroup matrix mul. = true
0.00.450.974 I ggml_metal_init: has residency sets    = true
0.00.450.975 I ggml_metal_init: has bfloat            = true
0.00.450.975 I ggml_metal_init: use bfloat            = true
0.00.450.976 I ggml_metal_init: hasUnifiedMemory      = true
0.00.450.977 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.470.374 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.528.379 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.528.385 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.528.425 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.532.533 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.532.535 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.532.535 I llama_init_from_model: graph nodes  = 967
0.00.532.536 I llama_init_from_model: graph splits = 2
0.00.532.542 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.532.658 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.532.659 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.590.825 I main: llama threadpool init, n_threads = 4
0.00.590.871 I 
0.00.590.895 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.590.896 I 
0.00.591.069 I sampler seed: 1234
0.00.591.073 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.591.095 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.591.095 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.591.095 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.342.162 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54657.43 tokens per second)
0.01.342.162 I llama_perf_context_print:        load time =     581.19 ms
0.01.342.163 I llama_perf_context_print: prompt eval time =      49.23 ms /     7 tokens (    7.03 ms per token,   142.20 tokens per second)
0.01.342.165 I llama_perf_context_print:        eval time =     698.99 ms /    63 runs   (   11.10 ms per token,    90.13 tokens per second)
0.01.342.165 I llama_perf_context_print:       total time =     752.27 ms /    70 tokens
0.01.342.452 I ggml_metal_free: deallocating

real	0m1.359s
user	0m0.110s
sys	0m0.187s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4613 (ff227703) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.008.869 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.597 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.602 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.608 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.609 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.609 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.610 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.610 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.613 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.613 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.614 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.614 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.614 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.615 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.615 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.617 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.617 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.618 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.761 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.890 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.943 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.945 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.945 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.945 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.946 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.946 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.947 I llama_model_loader: - type  f32:  194 tensors
0.00.025.947 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.947 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.947 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.948 I print_info: file format = GGUF V3 (latest)
0.00.025.948 I print_info: file type   = Q4_K - Medium
0.00.025.949 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.187 I load: special tokens cache size = 25
0.00.040.277 I load: token to piece cache size = 0.2984 MB
0.00.040.279 I print_info: arch             = gptneox
0.00.040.280 I print_info: vocab_only       = 0
0.00.040.280 I print_info: n_ctx_train      = 2048
0.00.040.280 I print_info: n_embd           = 2048
0.00.040.280 I print_info: n_layer          = 24
0.00.040.283 I print_info: n_head           = 16
0.00.040.284 I print_info: n_head_kv        = 16
0.00.040.284 I print_info: n_rot            = 32
0.00.040.284 I print_info: n_swa            = 0
0.00.040.285 I print_info: n_embd_head_k    = 128
0.00.040.285 I print_info: n_embd_head_v    = 128
0.00.040.286 I print_info: n_gqa            = 1
0.00.040.287 I print_info: n_embd_k_gqa     = 2048
0.00.040.288 I print_info: n_embd_v_gqa     = 2048
0.00.040.290 I print_info: f_norm_eps       = 1.0e-05
0.00.040.290 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.290 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.291 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.291 I print_info: f_logit_scale    = 0.0e+00
0.00.040.292 I print_info: n_ff             = 8192
0.00.040.292 I print_info: n_expert         = 0
0.00.040.292 I print_info: n_expert_used    = 0
0.00.040.292 I print_info: causal attn      = 1
0.00.040.292 I print_info: pooling type     = 0
0.00.040.292 I print_info: rope type        = 2
0.00.040.294 I print_info: rope scaling     = linear
0.00.040.294 I print_info: freq_base_train  = 10000.0
0.00.040.295 I print_info: freq_scale_train = 1
0.00.040.295 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.295 I print_info: rope_finetuned   = unknown
0.00.040.295 I print_info: ssm_d_conv       = 0
0.00.040.295 I print_info: ssm_d_inner      = 0
0.00.040.295 I print_info: ssm_d_state      = 0
0.00.040.295 I print_info: ssm_dt_rank      = 0
0.00.040.296 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.296 I print_info: model type       = 1.4B
0.00.040.296 I print_info: model params     = 1.41 B
0.00.040.296 I print_info: general.name     = 1.4B
0.00.040.297 I print_info: vocab type       = BPE
0.00.040.297 I print_info: n_vocab          = 50304
0.00.040.297 I print_info: n_merges         = 50009
0.00.040.298 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.301 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.301 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.302 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.302 I print_info: LF token         = 187 'Ċ'
0.00.040.302 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.302 I print_info: max token length = 1024
0.00.516.862 I load_tensors: offloading 24 repeating layers to GPU
0.00.516.879 I load_tensors: offloading output layer to GPU
0.00.516.880 I load_tensors: offloaded 25/25 layers to GPU
0.00.516.913 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.516.915 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.518.212 I llama_init_from_model: n_seq_max     = 1
0.00.518.216 I llama_init_from_model: n_ctx         = 2048
0.00.518.217 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.518.217 I llama_init_from_model: n_batch       = 2048
0.00.518.217 I llama_init_from_model: n_ubatch      = 512
0.00.518.218 I llama_init_from_model: flash_attn    = 0
0.00.518.220 I llama_init_from_model: freq_base     = 10000.0
0.00.518.220 I llama_init_from_model: freq_scale    = 1
0.00.518.223 I ggml_metal_init: allocating
0.00.518.306 I ggml_metal_init: found device: Apple M4
0.00.518.320 I ggml_metal_init: picking default device: Apple M4
0.00.520.085 I ggml_metal_init: using embedded metal library
0.00.526.643 I ggml_metal_init: GPU name:   Apple M4
0.00.526.648 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.526.649 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.526.649 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.526.650 I ggml_metal_init: simdgroup reduction   = true
0.00.526.650 I ggml_metal_init: simdgroup matrix mul. = true
0.00.526.651 I ggml_metal_init: has residency sets    = true
0.00.526.651 I ggml_metal_init: has bfloat            = true
0.00.526.651 I ggml_metal_init: use bfloat            = true
0.00.526.653 I ggml_metal_init: hasUnifiedMemory      = true
0.00.526.654 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.544.782 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.602.737 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.602.744 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.602.779 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.607.598 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.607.600 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.607.600 I llama_init_from_model: graph nodes  = 967
0.00.607.600 I llama_init_from_model: graph splits = 2
0.00.607.607 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.607.722 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.607.722 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.666.485 I main: llama threadpool init, n_threads = 4
0.00.666.528 I 
0.00.666.553 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.666.555 I 
0.00.666.711 I sampler seed: 1234
0.00.666.715 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.666.736 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.666.736 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.666.736 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.436.829 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49408.49 tokens per second)
0.01.436.829 I llama_perf_context_print:        load time =     656.69 ms
0.01.436.830 I llama_perf_context_print: prompt eval time =      54.83 ms /     7 tokens (    7.83 ms per token,   127.66 tokens per second)
0.01.436.831 I llama_perf_context_print:        eval time =     712.25 ms /    63 runs   (   11.31 ms per token,    88.45 tokens per second)
0.01.436.831 I llama_perf_context_print:       total time =     771.27 ms /    70 tokens
0.01.437.124 I ggml_metal_free: deallocating

real	0m1.453s
user	0m0.111s
sys	0m0.196s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4613 (ff227703) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.010.125 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.880 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.885 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.887 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.888 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.888 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.889 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.889 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.890 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.890 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.891 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.891 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.891 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.892 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.892 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.896 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.896 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.897 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.887 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.900 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.817 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.818 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.818 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.818 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.819 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.819 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.819 I llama_model_loader: - type  f32:  194 tensors
0.00.026.820 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.820 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.821 I print_info: file format = GGUF V3 (latest)
0.00.026.821 I print_info: file type   = Q5_K - Medium
0.00.026.822 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.669 I load: special tokens cache size = 25
0.00.040.563 I load: token to piece cache size = 0.2984 MB
0.00.040.566 I print_info: arch             = gptneox
0.00.040.566 I print_info: vocab_only       = 0
0.00.040.566 I print_info: n_ctx_train      = 2048
0.00.040.566 I print_info: n_embd           = 2048
0.00.040.566 I print_info: n_layer          = 24
0.00.040.569 I print_info: n_head           = 16
0.00.040.570 I print_info: n_head_kv        = 16
0.00.040.570 I print_info: n_rot            = 32
0.00.040.572 I print_info: n_swa            = 0
0.00.040.572 I print_info: n_embd_head_k    = 128
0.00.040.573 I print_info: n_embd_head_v    = 128
0.00.040.573 I print_info: n_gqa            = 1
0.00.040.574 I print_info: n_embd_k_gqa     = 2048
0.00.040.580 I print_info: n_embd_v_gqa     = 2048
0.00.040.580 I print_info: f_norm_eps       = 1.0e-05
0.00.040.580 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.581 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.582 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.582 I print_info: f_logit_scale    = 0.0e+00
0.00.040.583 I print_info: n_ff             = 8192
0.00.040.583 I print_info: n_expert         = 0
0.00.040.584 I print_info: n_expert_used    = 0
0.00.040.584 I print_info: causal attn      = 1
0.00.040.584 I print_info: pooling type     = 0
0.00.040.585 I print_info: rope type        = 2
0.00.040.587 I print_info: rope scaling     = linear
0.00.040.587 I print_info: freq_base_train  = 10000.0
0.00.040.588 I print_info: freq_scale_train = 1
0.00.040.588 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.588 I print_info: rope_finetuned   = unknown
0.00.040.588 I print_info: ssm_d_conv       = 0
0.00.040.588 I print_info: ssm_d_inner      = 0
0.00.040.589 I print_info: ssm_d_state      = 0
0.00.040.589 I print_info: ssm_dt_rank      = 0
0.00.040.589 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.589 I print_info: model type       = 1.4B
0.00.040.589 I print_info: model params     = 1.41 B
0.00.040.591 I print_info: general.name     = 1.4B
0.00.040.591 I print_info: vocab type       = BPE
0.00.040.591 I print_info: n_vocab          = 50304
0.00.040.591 I print_info: n_merges         = 50009
0.00.040.591 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.592 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.592 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.592 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.592 I print_info: LF token         = 187 'Ċ'
0.00.040.592 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.592 I print_info: max token length = 1024
0.00.584.514 I load_tensors: offloading 24 repeating layers to GPU
0.00.584.532 I load_tensors: offloading output layer to GPU
0.00.584.533 I load_tensors: offloaded 25/25 layers to GPU
0.00.584.570 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.584.571 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.586.006 I llama_init_from_model: n_seq_max     = 1
0.00.586.013 I llama_init_from_model: n_ctx         = 2048
0.00.586.013 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.586.014 I llama_init_from_model: n_batch       = 2048
0.00.586.014 I llama_init_from_model: n_ubatch      = 512
0.00.586.015 I llama_init_from_model: flash_attn    = 0
0.00.586.017 I llama_init_from_model: freq_base     = 10000.0
0.00.586.018 I llama_init_from_model: freq_scale    = 1
0.00.586.025 I ggml_metal_init: allocating
0.00.586.106 I ggml_metal_init: found device: Apple M4
0.00.586.121 I ggml_metal_init: picking default device: Apple M4
0.00.588.022 I ggml_metal_init: using embedded metal library
0.00.594.839 I ggml_metal_init: GPU name:   Apple M4
0.00.594.844 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.594.845 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.594.846 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.594.846 I ggml_metal_init: simdgroup reduction   = true
0.00.594.847 I ggml_metal_init: simdgroup matrix mul. = true
0.00.594.847 I ggml_metal_init: has residency sets    = true
0.00.594.847 I ggml_metal_init: has bfloat            = true
0.00.594.847 I ggml_metal_init: use bfloat            = true
0.00.594.849 I ggml_metal_init: hasUnifiedMemory      = true
0.00.594.851 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.612.729 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.697.248 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.697.258 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.697.307 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.702.065 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.702.070 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.702.070 I llama_init_from_model: graph nodes  = 967
0.00.702.071 I llama_init_from_model: graph splits = 2
0.00.702.076 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.702.204 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.702.205 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.767.778 I main: llama threadpool init, n_threads = 4
0.00.767.821 I 
0.00.767.847 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.767.849 I 
0.00.768.019 I sampler seed: 1234
0.00.768.024 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.768.035 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.768.035 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.768.035 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.618.247 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56259.90 tokens per second)
0.01.618.248 I llama_perf_context_print:        load time =     756.65 ms
0.01.618.249 I llama_perf_context_print: prompt eval time =      51.56 ms /     7 tokens (    7.37 ms per token,   135.76 tokens per second)
0.01.618.249 I llama_perf_context_print:        eval time =     795.84 ms /    63 runs   (   12.63 ms per token,    79.16 tokens per second)
0.01.618.250 I llama_perf_context_print:       total time =     851.47 ms /    70 tokens
0.01.618.536 I ggml_metal_free: deallocating

real	0m1.635s
user	0m0.113s
sys	0m0.228s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4613 (ff227703) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.009.168 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.630 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.635 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.637 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.638 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.638 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.638 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.639 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.639 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.640 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.640 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.641 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.641 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.641 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.642 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.643 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.644 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.644 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.565 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.542 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.362 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.363 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.364 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.364 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.365 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.365 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.365 I llama_model_loader: - type  f32:  194 tensors
0.00.026.366 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.366 I print_info: file format = GGUF V3 (latest)
0.00.026.367 I print_info: file type   = Q6_K
0.00.026.367 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.034.187 I load: special tokens cache size = 25
0.00.040.185 I load: token to piece cache size = 0.2984 MB
0.00.040.187 I print_info: arch             = gptneox
0.00.040.188 I print_info: vocab_only       = 0
0.00.040.188 I print_info: n_ctx_train      = 2048
0.00.040.188 I print_info: n_embd           = 2048
0.00.040.188 I print_info: n_layer          = 24
0.00.040.191 I print_info: n_head           = 16
0.00.040.192 I print_info: n_head_kv        = 16
0.00.040.192 I print_info: n_rot            = 32
0.00.040.192 I print_info: n_swa            = 0
0.00.040.192 I print_info: n_embd_head_k    = 128
0.00.040.194 I print_info: n_embd_head_v    = 128
0.00.040.195 I print_info: n_gqa            = 1
0.00.040.196 I print_info: n_embd_k_gqa     = 2048
0.00.040.196 I print_info: n_embd_v_gqa     = 2048
0.00.040.197 I print_info: f_norm_eps       = 1.0e-05
0.00.040.197 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.197 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.198 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.198 I print_info: f_logit_scale    = 0.0e+00
0.00.040.198 I print_info: n_ff             = 8192
0.00.040.199 I print_info: n_expert         = 0
0.00.040.199 I print_info: n_expert_used    = 0
0.00.040.199 I print_info: causal attn      = 1
0.00.040.199 I print_info: pooling type     = 0
0.00.040.199 I print_info: rope type        = 2
0.00.040.199 I print_info: rope scaling     = linear
0.00.040.200 I print_info: freq_base_train  = 10000.0
0.00.040.201 I print_info: freq_scale_train = 1
0.00.040.201 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.202 I print_info: rope_finetuned   = unknown
0.00.040.203 I print_info: ssm_d_conv       = 0
0.00.040.203 I print_info: ssm_d_inner      = 0
0.00.040.203 I print_info: ssm_d_state      = 0
0.00.040.203 I print_info: ssm_dt_rank      = 0
0.00.040.203 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.203 I print_info: model type       = 1.4B
0.00.040.204 I print_info: model params     = 1.41 B
0.00.040.204 I print_info: general.name     = 1.4B
0.00.040.206 I print_info: vocab type       = BPE
0.00.040.206 I print_info: n_vocab          = 50304
0.00.040.206 I print_info: n_merges         = 50009
0.00.040.206 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.206 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.207 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.207 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.207 I print_info: LF token         = 187 'Ċ'
0.00.040.211 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.212 I print_info: max token length = 1024
0.00.646.651 I load_tensors: offloading 24 repeating layers to GPU
0.00.646.654 I load_tensors: offloading output layer to GPU
0.00.646.655 I load_tensors: offloaded 25/25 layers to GPU
0.00.646.687 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.646.692 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.648.165 I llama_init_from_model: n_seq_max     = 1
0.00.648.167 I llama_init_from_model: n_ctx         = 2048
0.00.648.168 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.648.168 I llama_init_from_model: n_batch       = 2048
0.00.648.168 I llama_init_from_model: n_ubatch      = 512
0.00.648.169 I llama_init_from_model: flash_attn    = 0
0.00.648.170 I llama_init_from_model: freq_base     = 10000.0
0.00.648.170 I llama_init_from_model: freq_scale    = 1
0.00.648.172 I ggml_metal_init: allocating
0.00.648.228 I ggml_metal_init: found device: Apple M4
0.00.648.240 I ggml_metal_init: picking default device: Apple M4
0.00.649.626 I ggml_metal_init: using embedded metal library
0.00.655.278 I ggml_metal_init: GPU name:   Apple M4
0.00.655.282 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.655.282 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.655.283 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.655.283 I ggml_metal_init: simdgroup reduction   = true
0.00.655.284 I ggml_metal_init: simdgroup matrix mul. = true
0.00.655.284 I ggml_metal_init: has residency sets    = true
0.00.655.284 I ggml_metal_init: has bfloat            = true
0.00.655.284 I ggml_metal_init: use bfloat            = true
0.00.655.285 I ggml_metal_init: hasUnifiedMemory      = true
0.00.655.286 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.671.412 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.721.598 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.721.605 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.721.683 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.725.825 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.725.827 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.725.828 I llama_init_from_model: graph nodes  = 967
0.00.725.828 I llama_init_from_model: graph splits = 2
0.00.725.834 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.725.960 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.725.961 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.789.934 I main: llama threadpool init, n_threads = 4
0.00.789.974 I 
0.00.789.999 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.789.999 I 
0.00.790.174 I sampler seed: 1234
0.00.790.178 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.790.198 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.790.198 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.790.198 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.670.332 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52985.07 tokens per second)
0.01.670.333 I llama_perf_context_print:        load time =     779.80 ms
0.01.670.334 I llama_perf_context_print: prompt eval time =      54.11 ms /     7 tokens (    7.73 ms per token,   129.37 tokens per second)
0.01.670.335 I llama_perf_context_print:        eval time =     823.10 ms /    63 runs   (   13.07 ms per token,    76.54 tokens per second)
0.01.670.336 I llama_perf_context_print:       total time =     881.36 ms /    70 tokens
0.01.670.589 I ggml_metal_free: deallocating

real	0m1.688s
user	0m0.106s
sys	0m0.216s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.518 I build: 4613 (ff227703) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.707 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.458 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.465 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.466 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.472 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.472 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.473 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.473 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.475 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.475 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.476 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.476 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.477 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.482 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.483 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.486 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.487 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.488 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.497 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.635 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.853 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.855 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.856 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.856 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.857 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.857 I llama_model_loader: - type  f32:  194 tensors
0.00.054.858 I llama_model_loader: - type  f16:   98 tensors
0.00.054.859 I print_info: file format = GGUF V3 (latest)
0.00.054.860 I print_info: file type   = all F32 (guessed)
0.00.054.861 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.067.332 I load: special tokens cache size = 25
0.00.075.670 I load: token to piece cache size = 0.2984 MB
0.00.075.673 I print_info: arch             = gptneox
0.00.075.673 I print_info: vocab_only       = 0
0.00.075.674 I print_info: n_ctx_train      = 2048
0.00.075.674 I print_info: n_embd           = 2048
0.00.075.674 I print_info: n_layer          = 24
0.00.075.677 I print_info: n_head           = 16
0.00.075.678 I print_info: n_head_kv        = 16
0.00.075.678 I print_info: n_rot            = 32
0.00.075.678 I print_info: n_swa            = 0
0.00.075.679 I print_info: n_embd_head_k    = 128
0.00.075.679 I print_info: n_embd_head_v    = 128
0.00.075.680 I print_info: n_gqa            = 1
0.00.075.681 I print_info: n_embd_k_gqa     = 2048
0.00.075.681 I print_info: n_embd_v_gqa     = 2048
0.00.075.682 I print_info: f_norm_eps       = 1.0e-05
0.00.075.682 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.075.682 I print_info: f_clamp_kqv      = 0.0e+00
0.00.075.683 I print_info: f_max_alibi_bias = 0.0e+00
0.00.075.683 I print_info: f_logit_scale    = 0.0e+00
0.00.075.683 I print_info: n_ff             = 8192
0.00.075.684 I print_info: n_expert         = 0
0.00.075.684 I print_info: n_expert_used    = 0
0.00.075.684 I print_info: causal attn      = 1
0.00.075.687 I print_info: pooling type     = 0
0.00.075.687 I print_info: rope type        = 2
0.00.075.687 I print_info: rope scaling     = linear
0.00.075.687 I print_info: freq_base_train  = 10000.0
0.00.075.688 I print_info: freq_scale_train = 1
0.00.075.688 I print_info: n_ctx_orig_yarn  = 2048
0.00.075.688 I print_info: rope_finetuned   = unknown
0.00.075.688 I print_info: ssm_d_conv       = 0
0.00.075.688 I print_info: ssm_d_inner      = 0
0.00.075.689 I print_info: ssm_d_state      = 0
0.00.075.689 I print_info: ssm_dt_rank      = 0
0.00.075.689 I print_info: ssm_dt_b_c_rms   = 0
0.00.075.689 I print_info: model type       = 1.4B
0.00.075.690 I print_info: model params     = 1.41 B
0.00.075.690 I print_info: general.name     = 1.4B
0.00.075.690 I print_info: vocab type       = BPE
0.00.075.690 I print_info: n_vocab          = 50304
0.00.075.691 I print_info: n_merges         = 50009
0.00.075.692 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.075.693 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.075.693 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.075.693 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.075.693 I print_info: LF token         = 187 'Ċ'
0.00.075.694 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.075.698 I print_info: max token length = 1024
0.01.238.719 I load_tensors: offloading 24 repeating layers to GPU
0.01.238.724 I load_tensors: offloading output layer to GPU
0.01.238.724 I load_tensors: offloaded 25/25 layers to GPU
0.01.238.750 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.238.753 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.239.647 I llama_init_from_model: n_seq_max     = 1
0.01.239.649 I llama_init_from_model: n_ctx         = 128
0.01.239.649 I llama_init_from_model: n_ctx_per_seq = 128
0.01.239.649 I llama_init_from_model: n_batch       = 128
0.01.239.649 I llama_init_from_model: n_ubatch      = 128
0.01.239.650 I llama_init_from_model: flash_attn    = 0
0.01.239.650 I llama_init_from_model: freq_base     = 10000.0
0.01.239.650 I llama_init_from_model: freq_scale    = 1
0.01.239.651 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.239.655 I ggml_metal_init: allocating
0.01.239.715 I ggml_metal_init: found device: Apple M4
0.01.239.722 I ggml_metal_init: picking default device: Apple M4
0.01.240.763 I ggml_metal_init: using embedded metal library
0.01.244.663 I ggml_metal_init: GPU name:   Apple M4
0.01.244.666 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.244.666 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.244.667 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.244.667 I ggml_metal_init: simdgroup reduction   = true
0.01.244.667 I ggml_metal_init: simdgroup matrix mul. = true
0.01.244.668 I ggml_metal_init: has residency sets    = true
0.01.244.668 I ggml_metal_init: has bfloat            = true
0.01.244.669 I ggml_metal_init: use bfloat            = true
0.01.244.669 I ggml_metal_init: hasUnifiedMemory      = true
0.01.244.670 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.255.490 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.257.261 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.257.263 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.257.287 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.259.053 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.259.054 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.259.054 I llama_init_from_model: graph nodes  = 967
0.01.259.055 I llama_init_from_model: graph splits = 2
0.01.259.056 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.259.056 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.293.674 I 
0.01.293.711 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.293.715 I perplexity: tokenizing the input ..
0.01.298.665 I perplexity: tokenization took 4.948 ms
0.01.298.669 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.416.694 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.418.106 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.418.135 I llama_perf_context_print:        load time =    1269.96 ms
0.01.418.136 I llama_perf_context_print: prompt eval time =     117.76 ms /   128 tokens (    0.92 ms per token,  1086.98 tokens per second)
0.01.418.137 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.418.137 I llama_perf_context_print:       total time =     124.46 ms /   129 tokens
0.01.418.499 I ggml_metal_free: deallocating

real	0m1.604s
user	0m0.097s
sys	0m0.230s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4613 (ff227703) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.050 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.904 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.015.909 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.910 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.911 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.911 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.912 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.918 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.919 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.920 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.920 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.920 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.922 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.922 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.922 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.927 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.927 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.927 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.868 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.990 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.989 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.993 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.993 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.993 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.993 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.994 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.024.994 I llama_model_loader: - type  f32:  194 tensors
0.00.024.995 I llama_model_loader: - type q8_0:   98 tensors
0.00.024.995 I print_info: file format = GGUF V3 (latest)
0.00.024.996 I print_info: file type   = Q8_0
0.00.024.997 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.327 I load: special tokens cache size = 25
0.00.039.664 I load: token to piece cache size = 0.2984 MB
0.00.039.670 I print_info: arch             = gptneox
0.00.039.670 I print_info: vocab_only       = 0
0.00.039.671 I print_info: n_ctx_train      = 2048
0.00.039.671 I print_info: n_embd           = 2048
0.00.039.671 I print_info: n_layer          = 24
0.00.039.676 I print_info: n_head           = 16
0.00.039.677 I print_info: n_head_kv        = 16
0.00.039.677 I print_info: n_rot            = 32
0.00.039.677 I print_info: n_swa            = 0
0.00.039.677 I print_info: n_embd_head_k    = 128
0.00.039.678 I print_info: n_embd_head_v    = 128
0.00.039.679 I print_info: n_gqa            = 1
0.00.039.679 I print_info: n_embd_k_gqa     = 2048
0.00.039.680 I print_info: n_embd_v_gqa     = 2048
0.00.039.681 I print_info: f_norm_eps       = 1.0e-05
0.00.039.681 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.681 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.682 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.682 I print_info: f_logit_scale    = 0.0e+00
0.00.039.683 I print_info: n_ff             = 8192
0.00.039.683 I print_info: n_expert         = 0
0.00.039.683 I print_info: n_expert_used    = 0
0.00.039.683 I print_info: causal attn      = 1
0.00.039.684 I print_info: pooling type     = 0
0.00.039.684 I print_info: rope type        = 2
0.00.039.684 I print_info: rope scaling     = linear
0.00.039.684 I print_info: freq_base_train  = 10000.0
0.00.039.685 I print_info: freq_scale_train = 1
0.00.039.686 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.688 I print_info: rope_finetuned   = unknown
0.00.039.688 I print_info: ssm_d_conv       = 0
0.00.039.689 I print_info: ssm_d_inner      = 0
0.00.039.689 I print_info: ssm_d_state      = 0
0.00.039.689 I print_info: ssm_dt_rank      = 0
0.00.039.689 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.689 I print_info: model type       = 1.4B
0.00.039.690 I print_info: model params     = 1.41 B
0.00.039.690 I print_info: general.name     = 1.4B
0.00.039.690 I print_info: vocab type       = BPE
0.00.039.690 I print_info: n_vocab          = 50304
0.00.039.692 I print_info: n_merges         = 50009
0.00.039.693 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.693 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.693 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.693 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.693 I print_info: LF token         = 187 'Ċ'
0.00.039.694 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.694 I print_info: max token length = 1024
0.00.851.335 I load_tensors: offloading 24 repeating layers to GPU
0.00.851.341 I load_tensors: offloading output layer to GPU
0.00.851.341 I load_tensors: offloaded 25/25 layers to GPU
0.00.851.360 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.851.362 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.852.041 I llama_init_from_model: n_seq_max     = 1
0.00.852.044 I llama_init_from_model: n_ctx         = 128
0.00.852.045 I llama_init_from_model: n_ctx_per_seq = 128
0.00.852.045 I llama_init_from_model: n_batch       = 128
0.00.852.045 I llama_init_from_model: n_ubatch      = 128
0.00.852.046 I llama_init_from_model: flash_attn    = 0
0.00.852.047 I llama_init_from_model: freq_base     = 10000.0
0.00.852.047 I llama_init_from_model: freq_scale    = 1
0.00.852.048 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.852.049 I ggml_metal_init: allocating
0.00.852.084 I ggml_metal_init: found device: Apple M4
0.00.852.095 I ggml_metal_init: picking default device: Apple M4
0.00.853.063 I ggml_metal_init: using embedded metal library
0.00.857.087 I ggml_metal_init: GPU name:   Apple M4
0.00.857.097 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.857.097 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.857.098 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.857.098 I ggml_metal_init: simdgroup reduction   = true
0.00.857.099 I ggml_metal_init: simdgroup matrix mul. = true
0.00.857.099 I ggml_metal_init: has residency sets    = true
0.00.857.099 I ggml_metal_init: has bfloat            = true
0.00.857.100 I ggml_metal_init: use bfloat            = true
0.00.857.101 I ggml_metal_init: hasUnifiedMemory      = true
0.00.857.104 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.868.546 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.870.108 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.870.110 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.870.140 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.871.816 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.871.817 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.871.818 I llama_init_from_model: graph nodes  = 967
0.00.871.818 I llama_init_from_model: graph splits = 2
0.00.871.819 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.871.819 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.895.732 I 
0.00.895.768 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.895.771 I perplexity: tokenizing the input ..
0.00.899.740 I perplexity: tokenization took 3.967 ms
0.00.899.744 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.037.296 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.038.454 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.038.481 I llama_perf_context_print:        load time =     886.68 ms
0.01.038.481 I llama_perf_context_print: prompt eval time =     137.32 ms /   128 tokens (    1.07 ms per token,   932.14 tokens per second)
0.01.038.482 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.038.483 I llama_perf_context_print:       total time =     142.75 ms /   129 tokens
0.01.038.925 I ggml_metal_free: deallocating

real	0m1.058s
user	0m0.067s
sys	0m0.147s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4613 (ff227703) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.218 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.529 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.534 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.541 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.541 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.542 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.544 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.544 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.545 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.545 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.545 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.546 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.546 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.546 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.547 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.549 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.549 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.549 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.551 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.671 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.670 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.671 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.671 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.671 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.672 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.672 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.672 I llama_model_loader: - type  f32:  194 tensors
0.00.026.673 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.673 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.674 I print_info: file format = GGUF V3 (latest)
0.00.026.674 I print_info: file type   = Q4_0
0.00.026.675 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.792 I load: special tokens cache size = 25
0.00.040.827 I load: token to piece cache size = 0.2984 MB
0.00.040.830 I print_info: arch             = gptneox
0.00.040.831 I print_info: vocab_only       = 0
0.00.040.831 I print_info: n_ctx_train      = 2048
0.00.040.831 I print_info: n_embd           = 2048
0.00.040.831 I print_info: n_layer          = 24
0.00.040.834 I print_info: n_head           = 16
0.00.040.835 I print_info: n_head_kv        = 16
0.00.040.837 I print_info: n_rot            = 32
0.00.040.837 I print_info: n_swa            = 0
0.00.040.837 I print_info: n_embd_head_k    = 128
0.00.040.838 I print_info: n_embd_head_v    = 128
0.00.040.838 I print_info: n_gqa            = 1
0.00.040.839 I print_info: n_embd_k_gqa     = 2048
0.00.040.840 I print_info: n_embd_v_gqa     = 2048
0.00.040.840 I print_info: f_norm_eps       = 1.0e-05
0.00.040.841 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.841 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.841 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.841 I print_info: f_logit_scale    = 0.0e+00
0.00.040.842 I print_info: n_ff             = 8192
0.00.040.842 I print_info: n_expert         = 0
0.00.040.842 I print_info: n_expert_used    = 0
0.00.040.842 I print_info: causal attn      = 1
0.00.040.842 I print_info: pooling type     = 0
0.00.040.842 I print_info: rope type        = 2
0.00.040.847 I print_info: rope scaling     = linear
0.00.040.847 I print_info: freq_base_train  = 10000.0
0.00.040.848 I print_info: freq_scale_train = 1
0.00.040.849 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.849 I print_info: rope_finetuned   = unknown
0.00.040.849 I print_info: ssm_d_conv       = 0
0.00.040.849 I print_info: ssm_d_inner      = 0
0.00.040.849 I print_info: ssm_d_state      = 0
0.00.040.849 I print_info: ssm_dt_rank      = 0
0.00.040.849 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.850 I print_info: model type       = 1.4B
0.00.040.850 I print_info: model params     = 1.41 B
0.00.040.850 I print_info: general.name     = 1.4B
0.00.040.851 I print_info: vocab type       = BPE
0.00.040.851 I print_info: n_vocab          = 50304
0.00.040.851 I print_info: n_merges         = 50009
0.00.040.851 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.851 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.852 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.855 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.855 I print_info: LF token         = 187 'Ċ'
0.00.040.855 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.855 I print_info: max token length = 1024
0.00.538.237 I load_tensors: offloading 24 repeating layers to GPU
0.00.538.253 I load_tensors: offloading output layer to GPU
0.00.538.254 I load_tensors: offloaded 25/25 layers to GPU
0.00.538.288 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.538.289 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.539.832 I llama_init_from_model: n_seq_max     = 1
0.00.539.837 I llama_init_from_model: n_ctx         = 128
0.00.539.837 I llama_init_from_model: n_ctx_per_seq = 128
0.00.539.838 I llama_init_from_model: n_batch       = 128
0.00.539.838 I llama_init_from_model: n_ubatch      = 128
0.00.539.839 I llama_init_from_model: flash_attn    = 0
0.00.539.841 I llama_init_from_model: freq_base     = 10000.0
0.00.539.841 I llama_init_from_model: freq_scale    = 1
0.00.539.842 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.539.845 I ggml_metal_init: allocating
0.00.539.914 I ggml_metal_init: found device: Apple M4
0.00.539.928 I ggml_metal_init: picking default device: Apple M4
0.00.541.634 I ggml_metal_init: using embedded metal library
0.00.548.001 I ggml_metal_init: GPU name:   Apple M4
0.00.548.007 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.548.008 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.548.009 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.548.010 I ggml_metal_init: simdgroup reduction   = true
0.00.548.010 I ggml_metal_init: simdgroup matrix mul. = true
0.00.548.010 I ggml_metal_init: has residency sets    = true
0.00.548.011 I ggml_metal_init: has bfloat            = true
0.00.548.011 I ggml_metal_init: use bfloat            = true
0.00.548.012 I ggml_metal_init: hasUnifiedMemory      = true
0.00.548.014 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.567.360 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.570.881 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.570.885 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.570.928 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.574.184 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.574.185 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.574.186 I llama_init_from_model: graph nodes  = 967
0.00.574.186 I llama_init_from_model: graph splits = 2
0.00.574.189 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.574.189 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.603.734 I 
0.00.603.799 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.603.807 I perplexity: tokenizing the input ..
0.00.610.915 I perplexity: tokenization took 7.106 ms
0.00.610.920 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.745.905 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.747.262 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.747.281 I llama_perf_context_print:        load time =     593.51 ms
0.00.747.282 I llama_perf_context_print: prompt eval time =     134.75 ms /   128 tokens (    1.05 ms per token,   949.89 tokens per second)
0.00.747.284 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.747.285 I llama_perf_context_print:       total time =     143.55 ms /   129 tokens
0.00.747.672 I ggml_metal_free: deallocating

real	0m0.761s
user	0m0.079s
sys	0m0.118s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4613 (ff227703) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.855 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.038 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.044 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.051 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.052 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.052 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.052 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.053 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.055 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.055 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.056 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.056 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.056 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.057 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.057 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.059 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.059 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.059 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.076 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.192 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.162 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.163 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.163 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.164 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.164 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.164 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.165 I llama_model_loader: - type  f32:  194 tensors
0.00.025.165 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.165 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.166 I print_info: file format = GGUF V3 (latest)
0.00.025.166 I print_info: file type   = Q4_1
0.00.025.171 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.083 I load: special tokens cache size = 25
0.00.038.995 I load: token to piece cache size = 0.2984 MB
0.00.038.998 I print_info: arch             = gptneox
0.00.038.998 I print_info: vocab_only       = 0
0.00.038.998 I print_info: n_ctx_train      = 2048
0.00.038.998 I print_info: n_embd           = 2048
0.00.038.999 I print_info: n_layer          = 24
0.00.039.002 I print_info: n_head           = 16
0.00.039.002 I print_info: n_head_kv        = 16
0.00.039.003 I print_info: n_rot            = 32
0.00.039.003 I print_info: n_swa            = 0
0.00.039.003 I print_info: n_embd_head_k    = 128
0.00.039.003 I print_info: n_embd_head_v    = 128
0.00.039.004 I print_info: n_gqa            = 1
0.00.039.005 I print_info: n_embd_k_gqa     = 2048
0.00.039.005 I print_info: n_embd_v_gqa     = 2048
0.00.039.006 I print_info: f_norm_eps       = 1.0e-05
0.00.039.006 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.006 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.007 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.007 I print_info: f_logit_scale    = 0.0e+00
0.00.039.008 I print_info: n_ff             = 8192
0.00.039.008 I print_info: n_expert         = 0
0.00.039.008 I print_info: n_expert_used    = 0
0.00.039.008 I print_info: causal attn      = 1
0.00.039.008 I print_info: pooling type     = 0
0.00.039.010 I print_info: rope type        = 2
0.00.039.010 I print_info: rope scaling     = linear
0.00.039.011 I print_info: freq_base_train  = 10000.0
0.00.039.011 I print_info: freq_scale_train = 1
0.00.039.012 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.012 I print_info: rope_finetuned   = unknown
0.00.039.012 I print_info: ssm_d_conv       = 0
0.00.039.013 I print_info: ssm_d_inner      = 0
0.00.039.013 I print_info: ssm_d_state      = 0
0.00.039.013 I print_info: ssm_dt_rank      = 0
0.00.039.013 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.013 I print_info: model type       = 1.4B
0.00.039.014 I print_info: model params     = 1.41 B
0.00.039.014 I print_info: general.name     = 1.4B
0.00.039.014 I print_info: vocab type       = BPE
0.00.039.014 I print_info: n_vocab          = 50304
0.00.039.014 I print_info: n_merges         = 50009
0.00.039.015 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.015 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.015 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.015 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.016 I print_info: LF token         = 187 'Ċ'
0.00.039.016 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.020 I print_info: max token length = 1024
0.00.621.339 I load_tensors: offloading 24 repeating layers to GPU
0.00.621.352 I load_tensors: offloading output layer to GPU
0.00.621.353 I load_tensors: offloaded 25/25 layers to GPU
0.00.621.382 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.621.383 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.622.829 I llama_init_from_model: n_seq_max     = 1
0.00.622.838 I llama_init_from_model: n_ctx         = 128
0.00.622.838 I llama_init_from_model: n_ctx_per_seq = 128
0.00.622.839 I llama_init_from_model: n_batch       = 128
0.00.622.839 I llama_init_from_model: n_ubatch      = 128
0.00.622.840 I llama_init_from_model: flash_attn    = 0
0.00.622.841 I llama_init_from_model: freq_base     = 10000.0
0.00.622.841 I llama_init_from_model: freq_scale    = 1
0.00.622.842 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.622.846 I ggml_metal_init: allocating
0.00.622.897 I ggml_metal_init: found device: Apple M4
0.00.622.910 I ggml_metal_init: picking default device: Apple M4
0.00.624.528 I ggml_metal_init: using embedded metal library
0.00.630.572 I ggml_metal_init: GPU name:   Apple M4
0.00.630.577 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.630.578 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.630.579 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.630.583 I ggml_metal_init: simdgroup reduction   = true
0.00.630.583 I ggml_metal_init: simdgroup matrix mul. = true
0.00.630.584 I ggml_metal_init: has residency sets    = true
0.00.630.584 I ggml_metal_init: has bfloat            = true
0.00.630.584 I ggml_metal_init: use bfloat            = true
0.00.630.586 I ggml_metal_init: hasUnifiedMemory      = true
0.00.630.591 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.650.110 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.653.712 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.653.716 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.653.763 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.657.141 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.657.143 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.657.143 I llama_init_from_model: graph nodes  = 967
0.00.657.143 I llama_init_from_model: graph splits = 2
0.00.657.146 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.657.146 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.683.703 I 
0.00.683.773 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.683.783 I perplexity: tokenizing the input ..
0.00.690.261 I perplexity: tokenization took 6.477 ms
0.00.690.276 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.825.017 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.826.382 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.826.404 I llama_perf_context_print:        load time =     674.84 ms
0.00.826.404 I llama_perf_context_print: prompt eval time =     134.26 ms /   128 tokens (    1.05 ms per token,   953.38 tokens per second)
0.00.826.405 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.826.405 I llama_perf_context_print:       total time =     142.71 ms /   129 tokens
0.00.826.808 I ggml_metal_free: deallocating

real	0m0.841s
user	0m0.078s
sys	0m0.130s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4613 (ff227703) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.889 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.898 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.903 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.910 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.910 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.911 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.911 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.911 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.912 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.913 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.913 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.914 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.914 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.914 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.915 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.917 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.917 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.918 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.726 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.782 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.597 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.598 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.598 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.599 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.599 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.599 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.600 I llama_model_loader: - type  f32:  194 tensors
0.00.025.600 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.600 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.601 I print_info: file format = GGUF V3 (latest)
0.00.025.601 I print_info: file type   = Q5_0
0.00.025.602 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.459 I load: special tokens cache size = 25
0.00.039.414 I load: token to piece cache size = 0.2984 MB
0.00.039.417 I print_info: arch             = gptneox
0.00.039.417 I print_info: vocab_only       = 0
0.00.039.417 I print_info: n_ctx_train      = 2048
0.00.039.417 I print_info: n_embd           = 2048
0.00.039.417 I print_info: n_layer          = 24
0.00.039.420 I print_info: n_head           = 16
0.00.039.421 I print_info: n_head_kv        = 16
0.00.039.421 I print_info: n_rot            = 32
0.00.039.422 I print_info: n_swa            = 0
0.00.039.422 I print_info: n_embd_head_k    = 128
0.00.039.422 I print_info: n_embd_head_v    = 128
0.00.039.423 I print_info: n_gqa            = 1
0.00.039.423 I print_info: n_embd_k_gqa     = 2048
0.00.039.424 I print_info: n_embd_v_gqa     = 2048
0.00.039.425 I print_info: f_norm_eps       = 1.0e-05
0.00.039.430 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.430 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.430 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.430 I print_info: f_logit_scale    = 0.0e+00
0.00.039.431 I print_info: n_ff             = 8192
0.00.039.431 I print_info: n_expert         = 0
0.00.039.433 I print_info: n_expert_used    = 0
0.00.039.433 I print_info: causal attn      = 1
0.00.039.434 I print_info: pooling type     = 0
0.00.039.434 I print_info: rope type        = 2
0.00.039.434 I print_info: rope scaling     = linear
0.00.039.434 I print_info: freq_base_train  = 10000.0
0.00.039.435 I print_info: freq_scale_train = 1
0.00.039.435 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.435 I print_info: rope_finetuned   = unknown
0.00.039.435 I print_info: ssm_d_conv       = 0
0.00.039.435 I print_info: ssm_d_inner      = 0
0.00.039.435 I print_info: ssm_d_state      = 0
0.00.039.435 I print_info: ssm_dt_rank      = 0
0.00.039.436 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.436 I print_info: model type       = 1.4B
0.00.039.436 I print_info: model params     = 1.41 B
0.00.039.439 I print_info: general.name     = 1.4B
0.00.039.440 I print_info: vocab type       = BPE
0.00.039.440 I print_info: n_vocab          = 50304
0.00.039.440 I print_info: n_merges         = 50009
0.00.039.440 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.441 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.441 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.441 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.441 I print_info: LF token         = 187 'Ċ'
0.00.039.442 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.442 I print_info: max token length = 1024
0.00.639.530 I load_tensors: offloading 24 repeating layers to GPU
0.00.639.540 I load_tensors: offloading output layer to GPU
0.00.639.541 I load_tensors: offloaded 25/25 layers to GPU
0.00.639.573 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.639.574 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.641.103 I llama_init_from_model: n_seq_max     = 1
0.00.641.107 I llama_init_from_model: n_ctx         = 128
0.00.641.107 I llama_init_from_model: n_ctx_per_seq = 128
0.00.641.108 I llama_init_from_model: n_batch       = 128
0.00.641.108 I llama_init_from_model: n_ubatch      = 128
0.00.641.109 I llama_init_from_model: flash_attn    = 0
0.00.641.111 I llama_init_from_model: freq_base     = 10000.0
0.00.641.111 I llama_init_from_model: freq_scale    = 1
0.00.641.112 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.641.114 I ggml_metal_init: allocating
0.00.641.179 I ggml_metal_init: found device: Apple M4
0.00.641.193 I ggml_metal_init: picking default device: Apple M4
0.00.642.837 I ggml_metal_init: using embedded metal library
0.00.649.311 I ggml_metal_init: GPU name:   Apple M4
0.00.649.315 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.649.316 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.649.318 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.649.318 I ggml_metal_init: simdgroup reduction   = true
0.00.649.318 I ggml_metal_init: simdgroup matrix mul. = true
0.00.649.319 I ggml_metal_init: has residency sets    = true
0.00.649.319 I ggml_metal_init: has bfloat            = true
0.00.649.319 I ggml_metal_init: use bfloat            = true
0.00.649.320 I ggml_metal_init: hasUnifiedMemory      = true
0.00.649.322 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.666.887 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.670.490 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.670.496 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.670.563 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.673.857 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.673.859 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.673.859 I llama_init_from_model: graph nodes  = 967
0.00.673.860 I llama_init_from_model: graph splits = 2
0.00.673.863 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.673.863 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.704.419 I 
0.00.704.508 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.704.516 I perplexity: tokenizing the input ..
0.00.711.291 I perplexity: tokenization took 6.771 ms
0.00.711.299 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.853.599 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.854.946 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.854.973 I llama_perf_context_print:        load time =     694.52 ms
0.00.854.975 I llama_perf_context_print: prompt eval time =     141.41 ms /   128 tokens (    1.10 ms per token,   905.14 tokens per second)
0.00.854.976 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.854.976 I llama_perf_context_print:       total time =     150.56 ms /   129 tokens
0.00.855.358 I ggml_metal_free: deallocating

real	0m0.871s
user	0m0.079s
sys	0m0.134s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4613 (ff227703) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.836 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.319 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.325 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.331 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.332 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.332 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.332 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.333 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.334 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.334 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.335 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.335 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.335 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.336 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.336 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.338 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.338 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.338 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.432 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.450 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.517 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.518 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.518 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.519 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.519 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.519 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.520 I llama_model_loader: - type  f32:  194 tensors
0.00.025.520 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.520 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.521 I print_info: file format = GGUF V3 (latest)
0.00.025.522 I print_info: file type   = Q5_1
0.00.025.523 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.721 I load: special tokens cache size = 25
0.00.039.989 I load: token to piece cache size = 0.2984 MB
0.00.039.992 I print_info: arch             = gptneox
0.00.039.993 I print_info: vocab_only       = 0
0.00.039.993 I print_info: n_ctx_train      = 2048
0.00.039.993 I print_info: n_embd           = 2048
0.00.039.993 I print_info: n_layer          = 24
0.00.039.996 I print_info: n_head           = 16
0.00.039.997 I print_info: n_head_kv        = 16
0.00.039.997 I print_info: n_rot            = 32
0.00.039.997 I print_info: n_swa            = 0
0.00.039.997 I print_info: n_embd_head_k    = 128
0.00.039.998 I print_info: n_embd_head_v    = 128
0.00.039.998 I print_info: n_gqa            = 1
0.00.039.999 I print_info: n_embd_k_gqa     = 2048
0.00.040.001 I print_info: n_embd_v_gqa     = 2048
0.00.040.001 I print_info: f_norm_eps       = 1.0e-05
0.00.040.002 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.003 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.003 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.003 I print_info: f_logit_scale    = 0.0e+00
0.00.040.004 I print_info: n_ff             = 8192
0.00.040.004 I print_info: n_expert         = 0
0.00.040.004 I print_info: n_expert_used    = 0
0.00.040.004 I print_info: causal attn      = 1
0.00.040.004 I print_info: pooling type     = 0
0.00.040.005 I print_info: rope type        = 2
0.00.040.005 I print_info: rope scaling     = linear
0.00.040.005 I print_info: freq_base_train  = 10000.0
0.00.040.006 I print_info: freq_scale_train = 1
0.00.040.006 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.006 I print_info: rope_finetuned   = unknown
0.00.040.006 I print_info: ssm_d_conv       = 0
0.00.040.006 I print_info: ssm_d_inner      = 0
0.00.040.006 I print_info: ssm_d_state      = 0
0.00.040.007 I print_info: ssm_dt_rank      = 0
0.00.040.007 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.007 I print_info: model type       = 1.4B
0.00.040.007 I print_info: model params     = 1.41 B
0.00.040.007 I print_info: general.name     = 1.4B
0.00.040.008 I print_info: vocab type       = BPE
0.00.040.008 I print_info: n_vocab          = 50304
0.00.040.008 I print_info: n_merges         = 50009
0.00.040.009 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.009 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.009 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.009 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.010 I print_info: LF token         = 187 'Ċ'
0.00.040.010 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.010 I print_info: max token length = 1024
0.00.696.208 I load_tensors: offloading 24 repeating layers to GPU
0.00.696.213 I load_tensors: offloading output layer to GPU
0.00.696.215 I load_tensors: offloaded 25/25 layers to GPU
0.00.696.238 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.696.241 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.697.623 I llama_init_from_model: n_seq_max     = 1
0.00.697.625 I llama_init_from_model: n_ctx         = 128
0.00.697.625 I llama_init_from_model: n_ctx_per_seq = 128
0.00.697.626 I llama_init_from_model: n_batch       = 128
0.00.697.626 I llama_init_from_model: n_ubatch      = 128
0.00.697.627 I llama_init_from_model: flash_attn    = 0
0.00.697.627 I llama_init_from_model: freq_base     = 10000.0
0.00.697.628 I llama_init_from_model: freq_scale    = 1
0.00.697.629 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.697.630 I ggml_metal_init: allocating
0.00.697.648 I ggml_metal_init: found device: Apple M4
0.00.697.657 I ggml_metal_init: picking default device: Apple M4
0.00.698.998 I ggml_metal_init: using embedded metal library
0.00.705.050 I ggml_metal_init: GPU name:   Apple M4
0.00.705.054 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.705.054 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.705.055 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.705.056 I ggml_metal_init: simdgroup reduction   = true
0.00.705.056 I ggml_metal_init: simdgroup matrix mul. = true
0.00.705.056 I ggml_metal_init: has residency sets    = true
0.00.705.057 I ggml_metal_init: has bfloat            = true
0.00.705.057 I ggml_metal_init: use bfloat            = true
0.00.705.058 I ggml_metal_init: hasUnifiedMemory      = true
0.00.705.070 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.721.314 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.724.703 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.724.708 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.724.757 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.727.930 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.727.932 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.727.933 I llama_init_from_model: graph nodes  = 967
0.00.727.933 I llama_init_from_model: graph splits = 2
0.00.727.936 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.727.936 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.755.690 I 
0.00.755.761 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.755.771 I perplexity: tokenizing the input ..
0.00.763.280 I perplexity: tokenization took 7.508 ms
0.00.763.289 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.899.553 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.900.983 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.901.005 I llama_perf_context_print:        load time =     746.85 ms
0.00.901.006 I llama_perf_context_print: prompt eval time =     135.34 ms /   128 tokens (    1.06 ms per token,   945.77 tokens per second)
0.00.901.006 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.901.007 I llama_perf_context_print:       total time =     145.32 ms /   129 tokens
0.00.901.360 I ggml_metal_free: deallocating

real	0m0.916s
user	0m0.079s
sys	0m0.136s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4613 (ff227703) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.972 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.861 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.866 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.868 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.869 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.869 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.869 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.870 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.871 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.871 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.872 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.872 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.874 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.874 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.875 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.876 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.877 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.877 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.758 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.885 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.743 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.744 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.745 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.745 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.745 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.746 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.746 I llama_model_loader: - type  f32:  194 tensors
0.00.025.747 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.747 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.747 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.748 I print_info: file format = GGUF V3 (latest)
0.00.025.748 I print_info: file type   = Q2_K - Medium
0.00.025.749 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.968 I load: special tokens cache size = 25
0.00.039.827 I load: token to piece cache size = 0.2984 MB
0.00.039.830 I print_info: arch             = gptneox
0.00.039.830 I print_info: vocab_only       = 0
0.00.039.830 I print_info: n_ctx_train      = 2048
0.00.039.831 I print_info: n_embd           = 2048
0.00.039.831 I print_info: n_layer          = 24
0.00.039.834 I print_info: n_head           = 16
0.00.039.835 I print_info: n_head_kv        = 16
0.00.039.835 I print_info: n_rot            = 32
0.00.039.835 I print_info: n_swa            = 0
0.00.039.835 I print_info: n_embd_head_k    = 128
0.00.039.836 I print_info: n_embd_head_v    = 128
0.00.039.836 I print_info: n_gqa            = 1
0.00.039.837 I print_info: n_embd_k_gqa     = 2048
0.00.039.838 I print_info: n_embd_v_gqa     = 2048
0.00.039.838 I print_info: f_norm_eps       = 1.0e-05
0.00.039.839 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.839 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.839 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.839 I print_info: f_logit_scale    = 0.0e+00
0.00.039.840 I print_info: n_ff             = 8192
0.00.039.840 I print_info: n_expert         = 0
0.00.039.840 I print_info: n_expert_used    = 0
0.00.039.840 I print_info: causal attn      = 1
0.00.039.840 I print_info: pooling type     = 0
0.00.039.841 I print_info: rope type        = 2
0.00.039.843 I print_info: rope scaling     = linear
0.00.039.844 I print_info: freq_base_train  = 10000.0
0.00.039.844 I print_info: freq_scale_train = 1
0.00.039.844 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.844 I print_info: rope_finetuned   = unknown
0.00.039.845 I print_info: ssm_d_conv       = 0
0.00.039.845 I print_info: ssm_d_inner      = 0
0.00.039.845 I print_info: ssm_d_state      = 0
0.00.039.845 I print_info: ssm_dt_rank      = 0
0.00.039.845 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.845 I print_info: model type       = 1.4B
0.00.039.846 I print_info: model params     = 1.41 B
0.00.039.846 I print_info: general.name     = 1.4B
0.00.039.847 I print_info: vocab type       = BPE
0.00.039.847 I print_info: n_vocab          = 50304
0.00.039.848 I print_info: n_merges         = 50009
0.00.039.852 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.852 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.852 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.852 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.852 I print_info: LF token         = 187 'Ċ'
0.00.039.853 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.853 I print_info: max token length = 1024
0.00.374.259 I load_tensors: offloading 24 repeating layers to GPU
0.00.374.267 I load_tensors: offloading output layer to GPU
0.00.374.268 I load_tensors: offloaded 25/25 layers to GPU
0.00.374.299 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.374.305 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.375.804 I llama_init_from_model: n_seq_max     = 1
0.00.375.807 I llama_init_from_model: n_ctx         = 128
0.00.375.808 I llama_init_from_model: n_ctx_per_seq = 128
0.00.375.808 I llama_init_from_model: n_batch       = 128
0.00.375.809 I llama_init_from_model: n_ubatch      = 128
0.00.375.809 I llama_init_from_model: flash_attn    = 0
0.00.375.811 I llama_init_from_model: freq_base     = 10000.0
0.00.375.812 I llama_init_from_model: freq_scale    = 1
0.00.375.812 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.375.818 I ggml_metal_init: allocating
0.00.375.893 I ggml_metal_init: found device: Apple M4
0.00.375.906 I ggml_metal_init: picking default device: Apple M4
0.00.377.595 I ggml_metal_init: using embedded metal library
0.00.383.095 I ggml_metal_init: GPU name:   Apple M4
0.00.383.111 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.383.112 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.383.113 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.383.113 I ggml_metal_init: simdgroup reduction   = true
0.00.383.114 I ggml_metal_init: simdgroup matrix mul. = true
0.00.383.114 I ggml_metal_init: has residency sets    = true
0.00.383.114 I ggml_metal_init: has bfloat            = true
0.00.383.114 I ggml_metal_init: use bfloat            = true
0.00.383.126 I ggml_metal_init: hasUnifiedMemory      = true
0.00.383.130 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.404.250 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.407.937 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.407.943 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.408.032 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.411.589 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.411.591 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.411.592 I llama_init_from_model: graph nodes  = 967
0.00.411.592 I llama_init_from_model: graph splits = 2
0.00.411.596 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.411.596 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.445.549 I 
0.00.445.636 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.445.644 I perplexity: tokenizing the input ..
0.00.452.086 I perplexity: tokenization took 6.439 ms
0.00.452.091 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.591.390 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.592.736 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.592.759 I llama_perf_context_print:        load time =     435.57 ms
0.00.592.760 I llama_perf_context_print: prompt eval time =     138.75 ms /   128 tokens (    1.08 ms per token,   922.50 tokens per second)
0.00.592.760 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.592.761 I llama_perf_context_print:       total time =     147.22 ms /   129 tokens
0.00.593.133 I ggml_metal_free: deallocating

real	0m0.612s
user	0m0.081s
sys	0m0.110s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4613 (ff227703) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.894 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.029 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.034 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.036 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.036 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.037 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.037 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.037 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.038 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.039 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.039 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.039 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.040 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.040 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.041 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.042 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.043 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.043 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.071 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.140 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.039 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.040 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.040 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.041 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.041 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.041 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.042 I llama_model_loader: - type  f32:  194 tensors
0.00.025.042 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.043 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.043 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.043 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.044 I print_info: file format = GGUF V3 (latest)
0.00.025.044 I print_info: file type   = Q3_K - Medium
0.00.025.045 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.183 I load: special tokens cache size = 25
0.00.039.171 I load: token to piece cache size = 0.2984 MB
0.00.039.174 I print_info: arch             = gptneox
0.00.039.174 I print_info: vocab_only       = 0
0.00.039.175 I print_info: n_ctx_train      = 2048
0.00.039.175 I print_info: n_embd           = 2048
0.00.039.175 I print_info: n_layer          = 24
0.00.039.178 I print_info: n_head           = 16
0.00.039.179 I print_info: n_head_kv        = 16
0.00.039.179 I print_info: n_rot            = 32
0.00.039.179 I print_info: n_swa            = 0
0.00.039.180 I print_info: n_embd_head_k    = 128
0.00.039.180 I print_info: n_embd_head_v    = 128
0.00.039.181 I print_info: n_gqa            = 1
0.00.039.181 I print_info: n_embd_k_gqa     = 2048
0.00.039.182 I print_info: n_embd_v_gqa     = 2048
0.00.039.183 I print_info: f_norm_eps       = 1.0e-05
0.00.039.183 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.183 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.183 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.183 I print_info: f_logit_scale    = 0.0e+00
0.00.039.184 I print_info: n_ff             = 8192
0.00.039.186 I print_info: n_expert         = 0
0.00.039.186 I print_info: n_expert_used    = 0
0.00.039.186 I print_info: causal attn      = 1
0.00.039.186 I print_info: pooling type     = 0
0.00.039.187 I print_info: rope type        = 2
0.00.039.187 I print_info: rope scaling     = linear
0.00.039.189 I print_info: freq_base_train  = 10000.0
0.00.039.189 I print_info: freq_scale_train = 1
0.00.039.190 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.190 I print_info: rope_finetuned   = unknown
0.00.039.190 I print_info: ssm_d_conv       = 0
0.00.039.190 I print_info: ssm_d_inner      = 0
0.00.039.190 I print_info: ssm_d_state      = 0
0.00.039.190 I print_info: ssm_dt_rank      = 0
0.00.039.190 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.191 I print_info: model type       = 1.4B
0.00.039.191 I print_info: model params     = 1.41 B
0.00.039.191 I print_info: general.name     = 1.4B
0.00.039.192 I print_info: vocab type       = BPE
0.00.039.192 I print_info: n_vocab          = 50304
0.00.039.192 I print_info: n_merges         = 50009
0.00.039.193 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.193 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.193 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.193 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.193 I print_info: LF token         = 187 'Ċ'
0.00.039.194 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.194 I print_info: max token length = 1024
0.00.441.752 I load_tensors: offloading 24 repeating layers to GPU
0.00.441.767 I load_tensors: offloading output layer to GPU
0.00.441.768 I load_tensors: offloaded 25/25 layers to GPU
0.00.441.802 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.441.808 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.443.208 I llama_init_from_model: n_seq_max     = 1
0.00.443.213 I llama_init_from_model: n_ctx         = 128
0.00.443.214 I llama_init_from_model: n_ctx_per_seq = 128
0.00.443.214 I llama_init_from_model: n_batch       = 128
0.00.443.215 I llama_init_from_model: n_ubatch      = 128
0.00.443.215 I llama_init_from_model: flash_attn    = 0
0.00.443.217 I llama_init_from_model: freq_base     = 10000.0
0.00.443.218 I llama_init_from_model: freq_scale    = 1
0.00.443.218 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.443.224 I ggml_metal_init: allocating
0.00.443.310 I ggml_metal_init: found device: Apple M4
0.00.443.324 I ggml_metal_init: picking default device: Apple M4
0.00.445.108 I ggml_metal_init: using embedded metal library
0.00.450.719 I ggml_metal_init: GPU name:   Apple M4
0.00.450.732 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.450.732 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.450.733 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.450.734 I ggml_metal_init: simdgroup reduction   = true
0.00.450.734 I ggml_metal_init: simdgroup matrix mul. = true
0.00.450.734 I ggml_metal_init: has residency sets    = true
0.00.450.735 I ggml_metal_init: has bfloat            = true
0.00.450.735 I ggml_metal_init: use bfloat            = true
0.00.450.737 I ggml_metal_init: hasUnifiedMemory      = true
0.00.450.742 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.470.895 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.474.384 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.474.391 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.474.443 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.477.696 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.477.698 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.477.698 I llama_init_from_model: graph nodes  = 967
0.00.477.699 I llama_init_from_model: graph splits = 2
0.00.477.701 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.477.701 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.505.595 I 
0.00.505.690 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.505.699 I perplexity: tokenizing the input ..
0.00.512.545 I perplexity: tokenization took 6.843 ms
0.00.512.551 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.654.277 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.655.599 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.655.621 I llama_perf_context_print:        load time =     496.69 ms
0.00.655.622 I llama_perf_context_print: prompt eval time =     141.34 ms /   128 tokens (    1.10 ms per token,   905.60 tokens per second)
0.00.655.623 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.655.623 I llama_perf_context_print:       total time =     150.03 ms /   129 tokens
0.00.655.995 I ggml_metal_free: deallocating

real	0m0.669s
user	0m0.080s
sys	0m0.110s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4613 (ff227703) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.736 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.994 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.999 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.005 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.006 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.006 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.007 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.007 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.008 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.008 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.009 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.009 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.009 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.010 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.010 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.012 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.012 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.013 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.999 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.055 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.959 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.960 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.960 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.960 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.960 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.961 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.961 I llama_model_loader: - type  f32:  194 tensors
0.00.024.962 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.962 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.962 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.963 I print_info: file format = GGUF V3 (latest)
0.00.024.963 I print_info: file type   = Q4_K - Medium
0.00.024.964 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.798 I load: special tokens cache size = 25
0.00.038.801 I load: token to piece cache size = 0.2984 MB
0.00.038.804 I print_info: arch             = gptneox
0.00.038.804 I print_info: vocab_only       = 0
0.00.038.805 I print_info: n_ctx_train      = 2048
0.00.038.805 I print_info: n_embd           = 2048
0.00.038.805 I print_info: n_layer          = 24
0.00.038.808 I print_info: n_head           = 16
0.00.038.809 I print_info: n_head_kv        = 16
0.00.038.809 I print_info: n_rot            = 32
0.00.038.809 I print_info: n_swa            = 0
0.00.038.810 I print_info: n_embd_head_k    = 128
0.00.038.810 I print_info: n_embd_head_v    = 128
0.00.038.810 I print_info: n_gqa            = 1
0.00.038.811 I print_info: n_embd_k_gqa     = 2048
0.00.038.812 I print_info: n_embd_v_gqa     = 2048
0.00.038.812 I print_info: f_norm_eps       = 1.0e-05
0.00.038.813 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.813 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.813 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.817 I print_info: f_logit_scale    = 0.0e+00
0.00.038.818 I print_info: n_ff             = 8192
0.00.038.818 I print_info: n_expert         = 0
0.00.038.818 I print_info: n_expert_used    = 0
0.00.038.819 I print_info: causal attn      = 1
0.00.038.819 I print_info: pooling type     = 0
0.00.038.819 I print_info: rope type        = 2
0.00.038.819 I print_info: rope scaling     = linear
0.00.038.820 I print_info: freq_base_train  = 10000.0
0.00.038.820 I print_info: freq_scale_train = 1
0.00.038.820 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.820 I print_info: rope_finetuned   = unknown
0.00.038.820 I print_info: ssm_d_conv       = 0
0.00.038.822 I print_info: ssm_d_inner      = 0
0.00.038.823 I print_info: ssm_d_state      = 0
0.00.038.823 I print_info: ssm_dt_rank      = 0
0.00.038.823 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.823 I print_info: model type       = 1.4B
0.00.038.823 I print_info: model params     = 1.41 B
0.00.038.824 I print_info: general.name     = 1.4B
0.00.038.824 I print_info: vocab type       = BPE
0.00.038.824 I print_info: n_vocab          = 50304
0.00.038.824 I print_info: n_merges         = 50009
0.00.038.825 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.825 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.825 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.825 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.828 I print_info: LF token         = 187 'Ċ'
0.00.038.828 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.828 I print_info: max token length = 1024
0.00.516.347 I load_tensors: offloading 24 repeating layers to GPU
0.00.516.360 I load_tensors: offloading output layer to GPU
0.00.516.361 I load_tensors: offloaded 25/25 layers to GPU
0.00.516.393 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.516.394 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.517.925 I llama_init_from_model: n_seq_max     = 1
0.00.517.930 I llama_init_from_model: n_ctx         = 128
0.00.517.930 I llama_init_from_model: n_ctx_per_seq = 128
0.00.517.930 I llama_init_from_model: n_batch       = 128
0.00.517.931 I llama_init_from_model: n_ubatch      = 128
0.00.517.931 I llama_init_from_model: flash_attn    = 0
0.00.517.934 I llama_init_from_model: freq_base     = 10000.0
0.00.517.934 I llama_init_from_model: freq_scale    = 1
0.00.517.935 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.517.957 I ggml_metal_init: allocating
0.00.518.039 I ggml_metal_init: found device: Apple M4
0.00.518.066 I ggml_metal_init: picking default device: Apple M4
0.00.519.796 I ggml_metal_init: using embedded metal library
0.00.526.190 I ggml_metal_init: GPU name:   Apple M4
0.00.526.195 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.526.196 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.526.197 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.526.198 I ggml_metal_init: simdgroup reduction   = true
0.00.526.198 I ggml_metal_init: simdgroup matrix mul. = true
0.00.526.198 I ggml_metal_init: has residency sets    = true
0.00.526.199 I ggml_metal_init: has bfloat            = true
0.00.526.199 I ggml_metal_init: use bfloat            = true
0.00.526.200 I ggml_metal_init: hasUnifiedMemory      = true
0.00.526.202 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.544.465 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.547.977 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.547.981 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.548.021 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.551.528 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.551.529 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.551.530 I llama_init_from_model: graph nodes  = 967
0.00.551.530 I llama_init_from_model: graph splits = 2
0.00.551.533 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.551.534 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.581.086 I 
0.00.581.178 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.581.204 I perplexity: tokenizing the input ..
0.00.587.824 I perplexity: tokenization took 6.615 ms
0.00.587.830 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.730.744 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.732.221 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.732.244 I llama_perf_context_print:        load time =     572.34 ms
0.00.732.245 I llama_perf_context_print: prompt eval time =     141.98 ms /   128 tokens (    1.11 ms per token,   901.52 tokens per second)
0.00.732.246 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.732.246 I llama_perf_context_print:       total time =     151.16 ms /   129 tokens
0.00.732.618 I ggml_metal_free: deallocating

real	0m0.747s
user	0m0.078s
sys	0m0.120s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4613 (ff227703) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.899 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.641 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.647 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.649 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.649 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.650 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.650 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.650 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.651 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.651 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.652 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.654 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.655 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.655 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.656 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.657 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.658 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.658 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.586 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.606 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.553 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.555 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.555 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.555 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.556 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.556 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.556 I llama_model_loader: - type  f32:  194 tensors
0.00.025.557 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.557 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.558 I print_info: file format = GGUF V3 (latest)
0.00.025.558 I print_info: file type   = Q5_K - Medium
0.00.025.559 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.443 I load: special tokens cache size = 25
0.00.039.612 I load: token to piece cache size = 0.2984 MB
0.00.039.614 I print_info: arch             = gptneox
0.00.039.615 I print_info: vocab_only       = 0
0.00.039.615 I print_info: n_ctx_train      = 2048
0.00.039.615 I print_info: n_embd           = 2048
0.00.039.615 I print_info: n_layer          = 24
0.00.039.618 I print_info: n_head           = 16
0.00.039.619 I print_info: n_head_kv        = 16
0.00.039.619 I print_info: n_rot            = 32
0.00.039.619 I print_info: n_swa            = 0
0.00.039.621 I print_info: n_embd_head_k    = 128
0.00.039.621 I print_info: n_embd_head_v    = 128
0.00.039.622 I print_info: n_gqa            = 1
0.00.039.623 I print_info: n_embd_k_gqa     = 2048
0.00.039.628 I print_info: n_embd_v_gqa     = 2048
0.00.039.630 I print_info: f_norm_eps       = 1.0e-05
0.00.039.630 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.631 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.631 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.631 I print_info: f_logit_scale    = 0.0e+00
0.00.039.632 I print_info: n_ff             = 8192
0.00.039.632 I print_info: n_expert         = 0
0.00.039.632 I print_info: n_expert_used    = 0
0.00.039.632 I print_info: causal attn      = 1
0.00.039.633 I print_info: pooling type     = 0
0.00.039.633 I print_info: rope type        = 2
0.00.039.633 I print_info: rope scaling     = linear
0.00.039.637 I print_info: freq_base_train  = 10000.0
0.00.039.639 I print_info: freq_scale_train = 1
0.00.039.639 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.639 I print_info: rope_finetuned   = unknown
0.00.039.640 I print_info: ssm_d_conv       = 0
0.00.039.640 I print_info: ssm_d_inner      = 0
0.00.039.640 I print_info: ssm_d_state      = 0
0.00.039.640 I print_info: ssm_dt_rank      = 0
0.00.039.640 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.640 I print_info: model type       = 1.4B
0.00.039.641 I print_info: model params     = 1.41 B
0.00.039.641 I print_info: general.name     = 1.4B
0.00.039.641 I print_info: vocab type       = BPE
0.00.039.641 I print_info: n_vocab          = 50304
0.00.039.642 I print_info: n_merges         = 50009
0.00.039.642 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.642 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.642 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.642 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.643 I print_info: LF token         = 187 'Ċ'
0.00.039.645 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.646 I print_info: max token length = 1024
0.00.593.621 I load_tensors: offloading 24 repeating layers to GPU
0.00.593.632 I load_tensors: offloading output layer to GPU
0.00.593.633 I load_tensors: offloaded 25/25 layers to GPU
0.00.593.668 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.593.670 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.595.226 I llama_init_from_model: n_seq_max     = 1
0.00.595.230 I llama_init_from_model: n_ctx         = 128
0.00.595.230 I llama_init_from_model: n_ctx_per_seq = 128
0.00.595.231 I llama_init_from_model: n_batch       = 128
0.00.595.232 I llama_init_from_model: n_ubatch      = 128
0.00.595.232 I llama_init_from_model: flash_attn    = 0
0.00.595.234 I llama_init_from_model: freq_base     = 10000.0
0.00.595.234 I llama_init_from_model: freq_scale    = 1
0.00.595.235 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.595.237 I ggml_metal_init: allocating
0.00.595.300 I ggml_metal_init: found device: Apple M4
0.00.595.314 I ggml_metal_init: picking default device: Apple M4
0.00.596.764 I ggml_metal_init: using embedded metal library
0.00.603.196 I ggml_metal_init: GPU name:   Apple M4
0.00.603.200 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.603.201 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.603.202 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.603.202 I ggml_metal_init: simdgroup reduction   = true
0.00.603.202 I ggml_metal_init: simdgroup matrix mul. = true
0.00.603.203 I ggml_metal_init: has residency sets    = true
0.00.603.203 I ggml_metal_init: has bfloat            = true
0.00.603.203 I ggml_metal_init: use bfloat            = true
0.00.603.204 I ggml_metal_init: hasUnifiedMemory      = true
0.00.603.206 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.620.115 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.623.658 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.623.662 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.623.703 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.626.839 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.626.840 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.626.841 I llama_init_from_model: graph nodes  = 967
0.00.626.841 I llama_init_from_model: graph splits = 2
0.00.626.844 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.626.844 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.659.482 I 
0.00.659.560 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.659.566 I perplexity: tokenizing the input ..
0.00.665.482 I perplexity: tokenization took 5.911 ms
0.00.665.489 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.805.968 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.807.311 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.807.339 I llama_perf_context_print:        load time =     649.58 ms
0.00.807.340 I llama_perf_context_print: prompt eval time =     139.59 ms /   128 tokens (    1.09 ms per token,   916.94 tokens per second)
0.00.807.341 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.807.341 I llama_perf_context_print:       total time =     147.86 ms /   129 tokens
0.00.807.717 I ggml_metal_free: deallocating

real	0m0.823s
user	0m0.077s
sys	0m0.137s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4613 (ff227703) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.853 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.792 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.797 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.804 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.805 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.805 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.806 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.806 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.807 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.807 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.808 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.808 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.808 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.809 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.809 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.811 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.811 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.811 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.798 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.805 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.766 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.767 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.768 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.768 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.768 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.768 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.769 I llama_model_loader: - type  f32:  194 tensors
0.00.024.769 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.770 I print_info: file format = GGUF V3 (latest)
0.00.024.770 I print_info: file type   = Q6_K
0.00.024.771 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.626 I load: special tokens cache size = 25
0.00.038.625 I load: token to piece cache size = 0.2984 MB
0.00.038.628 I print_info: arch             = gptneox
0.00.038.628 I print_info: vocab_only       = 0
0.00.038.628 I print_info: n_ctx_train      = 2048
0.00.038.628 I print_info: n_embd           = 2048
0.00.038.629 I print_info: n_layer          = 24
0.00.038.632 I print_info: n_head           = 16
0.00.038.633 I print_info: n_head_kv        = 16
0.00.038.633 I print_info: n_rot            = 32
0.00.038.633 I print_info: n_swa            = 0
0.00.038.633 I print_info: n_embd_head_k    = 128
0.00.038.633 I print_info: n_embd_head_v    = 128
0.00.038.634 I print_info: n_gqa            = 1
0.00.038.635 I print_info: n_embd_k_gqa     = 2048
0.00.038.635 I print_info: n_embd_v_gqa     = 2048
0.00.038.636 I print_info: f_norm_eps       = 1.0e-05
0.00.038.637 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.639 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.639 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.639 I print_info: f_logit_scale    = 0.0e+00
0.00.038.640 I print_info: n_ff             = 8192
0.00.038.640 I print_info: n_expert         = 0
0.00.038.640 I print_info: n_expert_used    = 0
0.00.038.642 I print_info: causal attn      = 1
0.00.038.642 I print_info: pooling type     = 0
0.00.038.642 I print_info: rope type        = 2
0.00.038.642 I print_info: rope scaling     = linear
0.00.038.642 I print_info: freq_base_train  = 10000.0
0.00.038.643 I print_info: freq_scale_train = 1
0.00.038.643 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.644 I print_info: rope_finetuned   = unknown
0.00.038.645 I print_info: ssm_d_conv       = 0
0.00.038.645 I print_info: ssm_d_inner      = 0
0.00.038.645 I print_info: ssm_d_state      = 0
0.00.038.645 I print_info: ssm_dt_rank      = 0
0.00.038.645 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.645 I print_info: model type       = 1.4B
0.00.038.647 I print_info: model params     = 1.41 B
0.00.038.647 I print_info: general.name     = 1.4B
0.00.038.647 I print_info: vocab type       = BPE
0.00.038.651 I print_info: n_vocab          = 50304
0.00.038.651 I print_info: n_merges         = 50009
0.00.038.651 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.652 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.652 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.652 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.652 I print_info: LF token         = 187 'Ċ'
0.00.038.652 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.653 I print_info: max token length = 1024
0.00.457.753 I load_tensors: offloading 24 repeating layers to GPU
0.00.457.760 I load_tensors: offloading output layer to GPU
0.00.457.761 I load_tensors: offloaded 25/25 layers to GPU
0.00.457.786 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.457.789 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.459.229 I llama_init_from_model: n_seq_max     = 1
0.00.459.231 I llama_init_from_model: n_ctx         = 128
0.00.459.231 I llama_init_from_model: n_ctx_per_seq = 128
0.00.459.232 I llama_init_from_model: n_batch       = 128
0.00.459.232 I llama_init_from_model: n_ubatch      = 128
0.00.459.233 I llama_init_from_model: flash_attn    = 0
0.00.459.234 I llama_init_from_model: freq_base     = 10000.0
0.00.459.234 I llama_init_from_model: freq_scale    = 1
0.00.459.235 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.459.237 I ggml_metal_init: allocating
0.00.459.261 I ggml_metal_init: found device: Apple M4
0.00.459.270 I ggml_metal_init: picking default device: Apple M4
0.00.460.585 I ggml_metal_init: using embedded metal library
0.00.466.747 I ggml_metal_init: GPU name:   Apple M4
0.00.466.751 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.466.752 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.466.753 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.466.754 I ggml_metal_init: simdgroup reduction   = true
0.00.466.754 I ggml_metal_init: simdgroup matrix mul. = true
0.00.466.754 I ggml_metal_init: has residency sets    = true
0.00.466.754 I ggml_metal_init: has bfloat            = true
0.00.466.755 I ggml_metal_init: use bfloat            = true
0.00.466.756 I ggml_metal_init: hasUnifiedMemory      = true
0.00.466.757 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.483.313 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.486.805 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.486.809 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.486.849 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.490.084 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.490.086 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.490.087 I llama_init_from_model: graph nodes  = 967
0.00.490.087 I llama_init_from_model: graph splits = 2
0.00.490.089 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.490.089 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.522.055 I 
0.00.522.144 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.522.151 I perplexity: tokenizing the input ..
0.00.529.313 I perplexity: tokenization took 7.158 ms
0.00.529.321 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.670.449 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.671.800 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.671.825 I llama_perf_context_print:        load time =     513.19 ms
0.00.671.826 I llama_perf_context_print: prompt eval time =     140.26 ms /   128 tokens (    1.10 ms per token,   912.61 tokens per second)
0.00.671.827 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.671.827 I llama_perf_context_print:       total time =     149.78 ms /   129 tokens
0.00.672.199 I ggml_metal_free: deallocating

real	0m0.686s
user	0m0.078s
sys	0m0.119s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.320 I build: 4613 (ff227703) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.385 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.483 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.488 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.490 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.491 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.491 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.491 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.492 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.493 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.493 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.496 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.496 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.497 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.497 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.497 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.499 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.500 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.500 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.858 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.683 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.284 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.286 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.287 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.287 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.287 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.288 I llama_model_loader: - type  f32:  194 tensors
0.00.054.288 I llama_model_loader: - type  f16:   98 tensors
0.00.054.289 I print_info: file format = GGUF V3 (latest)
0.00.054.290 I print_info: file type   = all F32 (guessed)
0.00.054.291 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.065.649 I load: special tokens cache size = 25
0.00.072.851 I load: token to piece cache size = 0.2984 MB
0.00.072.854 I print_info: arch             = gptneox
0.00.072.854 I print_info: vocab_only       = 0
0.00.072.854 I print_info: n_ctx_train      = 2048
0.00.072.855 I print_info: n_embd           = 2048
0.00.072.855 I print_info: n_layer          = 24
0.00.072.858 I print_info: n_head           = 16
0.00.072.858 I print_info: n_head_kv        = 16
0.00.072.859 I print_info: n_rot            = 32
0.00.072.859 I print_info: n_swa            = 0
0.00.072.859 I print_info: n_embd_head_k    = 128
0.00.072.859 I print_info: n_embd_head_v    = 128
0.00.072.860 I print_info: n_gqa            = 1
0.00.072.861 I print_info: n_embd_k_gqa     = 2048
0.00.072.861 I print_info: n_embd_v_gqa     = 2048
0.00.072.862 I print_info: f_norm_eps       = 1.0e-05
0.00.072.862 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.072.862 I print_info: f_clamp_kqv      = 0.0e+00
0.00.072.862 I print_info: f_max_alibi_bias = 0.0e+00
0.00.072.863 I print_info: f_logit_scale    = 0.0e+00
0.00.072.863 I print_info: n_ff             = 8192
0.00.072.863 I print_info: n_expert         = 0
0.00.072.864 I print_info: n_expert_used    = 0
0.00.072.864 I print_info: causal attn      = 1
0.00.072.864 I print_info: pooling type     = 0
0.00.072.864 I print_info: rope type        = 2
0.00.072.866 I print_info: rope scaling     = linear
0.00.072.867 I print_info: freq_base_train  = 10000.0
0.00.072.867 I print_info: freq_scale_train = 1
0.00.072.867 I print_info: n_ctx_orig_yarn  = 2048
0.00.072.867 I print_info: rope_finetuned   = unknown
0.00.072.868 I print_info: ssm_d_conv       = 0
0.00.072.868 I print_info: ssm_d_inner      = 0
0.00.072.868 I print_info: ssm_d_state      = 0
0.00.072.868 I print_info: ssm_dt_rank      = 0
0.00.072.868 I print_info: ssm_dt_b_c_rms   = 0
0.00.072.868 I print_info: model type       = 1.4B
0.00.072.869 I print_info: model params     = 1.41 B
0.00.072.869 I print_info: general.name     = 1.4B
0.00.072.869 I print_info: vocab type       = BPE
0.00.072.869 I print_info: n_vocab          = 50304
0.00.072.870 I print_info: n_merges         = 50009
0.00.072.870 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.072.870 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.072.870 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.072.870 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.072.871 I print_info: LF token         = 187 'Ċ'
0.00.072.871 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.072.872 I print_info: max token length = 1024
0.01.350.252 I load_tensors: offloading 24 repeating layers to GPU
0.01.350.257 I load_tensors: offloading output layer to GPU
0.01.350.258 I load_tensors: offloaded 25/25 layers to GPU
0.01.350.284 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.350.286 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.351.279 I llama_init_from_model: n_seq_max     = 1
0.01.351.280 I llama_init_from_model: n_ctx         = 128
0.01.351.280 I llama_init_from_model: n_ctx_per_seq = 128
0.01.351.281 I llama_init_from_model: n_batch       = 128
0.01.351.281 I llama_init_from_model: n_ubatch      = 128
0.01.351.281 I llama_init_from_model: flash_attn    = 0
0.01.351.282 I llama_init_from_model: freq_base     = 10000.0
0.01.351.282 I llama_init_from_model: freq_scale    = 1
0.01.351.282 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.351.283 I ggml_metal_init: allocating
0.01.351.325 I ggml_metal_init: found device: Apple M4
0.01.351.331 I ggml_metal_init: picking default device: Apple M4
0.01.352.324 I ggml_metal_init: using embedded metal library
0.01.356.107 I ggml_metal_init: GPU name:   Apple M4
0.01.356.109 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.356.110 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.356.110 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.356.111 I ggml_metal_init: simdgroup reduction   = true
0.01.356.111 I ggml_metal_init: simdgroup matrix mul. = true
0.01.356.111 I ggml_metal_init: has residency sets    = true
0.01.356.111 I ggml_metal_init: has bfloat            = true
0.01.356.111 I ggml_metal_init: use bfloat            = true
0.01.356.112 I ggml_metal_init: hasUnifiedMemory      = true
0.01.356.115 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.366.698 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.368.441 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.368.443 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.368.468 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.370.171 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.370.172 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.370.173 I llama_init_from_model: graph nodes  = 967
0.01.370.173 I llama_init_from_model: graph splits = 2
0.01.370.174 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.370.175 I 
0.01.370.210 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.370.211 I compute_imatrix: tokenizing the input ..
0.01.374.364 I compute_imatrix: tokenization took 4.152 ms
0.01.374.365 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.640.366 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.642.745 I llama_perf_context_print:        load time =    1616.98 ms
0.01.642.746 I llama_perf_context_print: prompt eval time =     264.26 ms /   128 tokens (    2.06 ms per token,   484.38 tokens per second)
0.01.642.747 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.642.747 I llama_perf_context_print:       total time =    1619.35 ms /   129 tokens
0.01.643.324 I ggml_metal_free: deallocating

real	0m1.829s
user	0m0.124s
sys	0m0.255s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4613 (ff227703)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x111a04280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x111a04970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x111a04de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x111a05250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x111a056c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x111a05b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x111a05fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x111a06410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x111a06880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x111a06cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x111a07160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x111a07800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x111a08320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x111a08ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x111a092e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x111a09a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x111a0a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x111a0a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x111a0af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x111a0b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x111a0be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x111a0c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x111a0cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x111a0d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x111a0dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x111a0df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x111a0e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x111a0e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x111a0ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x111a0f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x111a0f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x111a0fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x111a0ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x111a10290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x111a10700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x111a10fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x111a11270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x111a116e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x111a11b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x111a11fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x111a12430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x111a128a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x111a12d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x111a13180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x111a135f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x111a13a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x111a13ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x111a14900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x111a14bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x111a15030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x111a154a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x111a15910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x111a15d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x111a161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x111a16660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x111a16d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x111a171b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x111a17470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x111a178e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x111a17fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x111a183b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x111a18670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x111a18b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x111a19070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x111a19570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x111a19a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x111a19f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x111a1a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x111a1a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x111a1ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x111a1b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x111a1b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x111a1bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x111a1c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x111a1c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x111a1cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x111a1d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x111a1d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x111a1dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x111a1e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x111a1ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x111a1eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x111a1f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x111a1fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x111a20100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x111a206b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x111a20c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x111a21210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x111a217c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x111a21d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x111a22320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x111a228d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x111a22e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x111a23430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x111a239e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x111a23f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x111a24540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x111a144f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x111a24ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x111a25110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x111a25580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x111a25b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x111a260e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x111a26690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x111a26c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x111a271f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x111a277a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x111a27d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x111a28300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x111a288b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x111a28e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x111a29410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x111a299c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x111a29f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x111a2a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x111a2a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x111a2ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x111a2b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x111a2b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x111a2bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x111a2c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x111a2c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x111a2cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x111a2d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x111a2d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x111a2db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x111a2e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x111a2e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x111a2ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x111a2ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x111a2f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x111a2f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x111a2fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x111a30370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x111a30870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x111a30d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x111a31270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x111a31770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x111a31c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x111a32170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x111a32670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x111a32b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x111a33070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x111a33570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x111a33a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x111a33f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x111a34470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x111a34970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x111a34e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x111a35370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x111a35870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x111a35d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x111a36270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x111a36770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x111a36c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x111a37170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x111a37670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x111a37b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x111a38070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x111a38570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x111a38a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x111a38f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x111a39470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x111a39970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x111a39e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x111a3a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x111a3a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x111a3ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x111a3b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x111a3b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x111a3bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x111a3c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x111a3c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x111a3cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x111a3d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x111a3d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x111a3da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x111a3df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x111a3e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x111a3e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x111a3ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x111a3f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x111a3f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x111a3fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x111a40270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x111a40770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x111a40c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x111a41170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x111a41670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x111a41b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x111a42070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x111a42570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x111a42a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x111a42f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x111a43520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x111a43ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x111a44080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x111a44630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x111a44c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x111a45250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x111a45860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x111a46050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x111a464f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x111a467b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x111a46dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x111a473d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x111a47bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x111a48060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x111a48500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x111a489a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x111a49150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x111a496a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x111a49bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x111a4a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x111a4a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x111a4abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x111a4b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x111a4b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x111a4bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x111a4c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x111a4c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x111a4cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x111a4d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x111a4d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x111a4dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x111a4e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x111a4e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x111a4eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x111a4f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x111a4f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x111a4fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x111a500e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x111a50630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x111a50b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x111a510d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x111a51620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x111a51b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x111a520c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x111a52610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x111a52b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x111a530b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x111a53600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x111a53b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x111a540a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x111a545f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x111a54b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x111a55090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x111a555e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x111a55b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x111a56080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x111a565d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x111a56b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x111a57070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x111a575c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x111a57b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x111a58060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x111a585b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x111a58b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x111a59050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x111a595a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x111a59af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x111a5a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x111a5a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x111a5aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x111a5b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x111a5b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x111a5bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x111a5bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x111a5c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x111a5c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x111a5cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x111a5d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x111a5d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x111a5db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x111a5dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x111a5e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x111a5e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x111a5edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x111a5f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x111a5f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x111a5fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x111a60030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x111a60580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x111a60ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x111a613c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x111a61ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x111a62200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x111a624c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x111a62cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x111a62f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x111a63580 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.675.055 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.675.059 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x111f091b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x111f09620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x111f09a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x111f09f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x111f0a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x111f0a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x111f0ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x111f0b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x111f0b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x111f0b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x111f0be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x111f0c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x111f0cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x111f0d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x111f0dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x111f0e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x111f0edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x111f0f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x111f0fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x111f10400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x111f10b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x111f11240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x111f11960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x111f12080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x111f127a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x111f12a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x111f12d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x111f13190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x111f13600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x111f13a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x111f13f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x111f14480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x111f148f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x111f14bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x111f15020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x111f15490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x111f159f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x111f15ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x111f163f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x111f168f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x111f16df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x111f172f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x111f177f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x111f17cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x111f181f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x111f18660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x111f18ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x111f18f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x111f193b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x111f19820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x111f19c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x111f1a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x111f1a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x111f1a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x111f1ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x111f1b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x111f1bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x111f1bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x111f1c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x111f1cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x111f1d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x111f1d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x111f1d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x111f1de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x111f1e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x111f1e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x111f1ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x111f1f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x111f1f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x111f1f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x111f1fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x111f20300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x111f207a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x111f20cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x111f21240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x111f21790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x111f21ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x111f22230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x111f22780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x111f22cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x111f23220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x111f23770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x111f23cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x111f24210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x111f24760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x111f24cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x111f25200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x111f25750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x111f25ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x111f261f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x111f26740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x111f26c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x111f271e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x111f27730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x111f27c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x111f281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x111f28720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x111f28c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x111f291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x111f29710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x111f29c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x111f2a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x111f2a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x111f2ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x111f2b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x111f2b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x111f2bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x111f2c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x111f2c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x111f2cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x111f2d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x111f2d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x111f2dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x111f2e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x111f2e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x111f2ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x111f2eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x111f2f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x111f2f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x111f2fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x111f30120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x111f305c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x111f30a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x111f30f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x111f313a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x111f31840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x111f31ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x111f32180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x111f32620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x111f32ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x111f32f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x111f33400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x111f338a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x111f33d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x111f341e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x111f34680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x111f34b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x111f34fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x111f35460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x111f35900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x111f35da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x111f36240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x111f366e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x111f36b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x111f37020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x111f374c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x111f37960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x111f37e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x111f382a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x111f38740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x111f38be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x111f39080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x111f39520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x111f399c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x111f39e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x111f3a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x111f3a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x111f3ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x111f3b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x111f3b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x111f3ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x111f3bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x111f3c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x111f3c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x111f3cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x111f3d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x111f3d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x111f3da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x111f3df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x111f3e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x111f3e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x111f3ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x111f3f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x111f3f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x111f3fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x111f3ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x111f40420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x111f408c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x111f40d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x111f41200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x111f416a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x111f41b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x111f41fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x111f42480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x111f42920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x111f42dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x111f43260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x111f43700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x111f43ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x111f44040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x111f444e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x111f44980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x111f44e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x111f45370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x111f458c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x111f45e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x111f46360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x111f46620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x111f46c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x111f47240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x111f47850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x111f48040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x111f484e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x111f487a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x111f48db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x111f493c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x111f49bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x111f4a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x111f4a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x111f4a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x111f4b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x111f4b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x111f4bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x111f4c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x111f4c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x111f4cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x111f4d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x111f4d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x111f4dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x111f4e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x111f4e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x111f4ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x111f4f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x111f4f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x111f4fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x111f500f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x111f50640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x111f50b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x111f510e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x111f51630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x111f51b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x111f520d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x111f52620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x111f52b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x111f530c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x111f53610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x111f53b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x111f540b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x111f54600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x111f54b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x111f550a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x111f555f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x111f55b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x111f56090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x111f565e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x111f56b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x111f57080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x111f575d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x111f57b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x111f58070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x111f585c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x111f58b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x111f59060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x111f595b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x111f59b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x111f5a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x111f5a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x111f5aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x111f5b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x111f5b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x111f5bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x111f5c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x111f5c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x111f5cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x111f5d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x111f5d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x111f5dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x111f5df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x111f5e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x111f5e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x111f5ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x111f5f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x111f5f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x111f5fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x111f5ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x111f60460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x111f60900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x111f60da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x111f61240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x111f616e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x111f61b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x111f62020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x111f62570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x111f62c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x111f633b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x111f63ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x111f641f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x111f644b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x111f64ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x111f64f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x111f65570 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x111f65220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x111f46ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x111f468e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x111f47500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x111f1c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x111f49070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x111f0c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x111f08d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x111f1c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x111f64770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x111f1b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x111f49680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x111f0c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x111f65ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x111f66310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x111f665d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x111f66890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x111f66b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x111f66e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x111f670d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x111f67390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x111f67650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x111f67910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x111f67bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x111f67e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x111f68150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x111f68410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x111f686d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x111f68990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x111f68c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x111f68f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x111f691d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x111f69490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x111f69750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x111f69a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x111f69cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x111f69f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x111f6a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x111f6a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x111f6a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x111f6aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x111f6ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x111f6b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x111f6b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x111f6b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x111f6b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x111f6bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x111f6bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x111f6c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x111f6c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x111f6c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x111f6c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x111f6cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x111f6ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x111f6d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x111f6d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x111f6d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x111f6d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x111f6dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x111f6ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x111f6e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x111f6e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x111f6e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x111f6e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x111f6ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x111f6ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x111f6f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x111f6f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x111f6f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x111f6fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x111f6fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x111f6ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x111f70290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x111f70550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x111f70810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x111f70ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x111f70d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x111f71050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x111f71310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x111f715d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x111f71890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x111f71b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x111f71e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x111f720d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x111f72390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x111f72650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x111f72910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x111f72bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x111f72e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x111f73150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x111f73410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x111f736d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x111f73990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x111f73c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x111f73f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x111f741d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x111f74490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x111f74750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x111f74a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x111f74cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x111f74f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x111f75250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x111f75510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x111f757d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x111f75a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x111f75d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x111f76010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x111f762d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x111f76590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x111f76850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x111f76b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x111f76dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x111f77090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x111f77350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x111f77610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x111f778d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x111f77b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x111f77e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x111f78110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x111f783d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x111f78690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x111f78950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x111f78c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x111f78ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x111f79190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x111f79450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x111f79710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x111f799d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x111f79c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x111f79f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x111f7a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x111f7a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x111f7a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x111f7aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x111f7ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x111f7afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x111f7b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x111f7b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x111f7b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x111f7bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x111f7bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x111f7c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x111f7c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x111f7c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x111f7c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x111f7cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x111f7ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x111f7d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x111f7d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x111f7d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x111f7d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x111f7dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x111f7de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x111f7e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x111f7e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x111f7e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x111f7e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x111f7ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x111f7ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x111f7f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x111f7f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x111f7f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x111f7fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x111f7fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x111f7ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x111f80250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x111f80510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x111f807d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x111f80a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x111f80d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x111f81010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x111f812d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x111f81590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x111f81850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x111f81b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x111f81dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x111f82090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x111f82350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x111f82610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x111f828d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x111f82b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x111f82e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x111f83110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x111f833d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x111f83690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x111f83950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x111f83c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x111f83ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x111f84190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x111f84450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x111f84710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x111f849d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x111f84c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x111f84f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x111f85210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x111f854d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x111f85790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x111f85a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x111f85d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x111f85fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x111f86290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x111f86550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x111f86810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x111f86ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x111f86d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x111f87050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x111f87310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x111f875d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x111f87890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x111f87b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x111f88120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x111f883e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x111f886a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x111f88960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x111f88c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x111f88ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x111f891a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x111f89460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x111f89720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x111f899e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x111f89ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x111f89f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x111f8a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x111f8a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x111f8a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x111f8aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x111f8ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x111f8afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x111f8b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x111f8b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x111f8b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x111f8bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x111f8bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x111f8c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x111f8c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x111f8c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x111f8c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x111f8cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x111f8ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x111f8d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x111f8d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x111f8d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x111f8d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x111f8dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x111f8dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x111f8e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x111f8e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x111f8e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x111f8e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x111f8ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x111f8ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x111f8f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x111f8f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x111f8f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x111f8fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x111f8ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x111f904c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x111f90a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x111f90f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x111f914b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x111f91a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x111f91f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x111f924a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x111f929f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x111f92f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x111f93490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x111f939e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x111f93ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x111f93f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x111f94460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x111f94960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x111f94e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x111f95360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x111f95860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x111f95d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x111f96260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x111f96760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x111f96c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x111f97160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x111f97660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x111f97b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x111f98060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x111f98560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x111f98f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x111f99690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x111f99db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x111f9a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x111f9a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x111f9af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x111f9b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x111f9b850 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.709s
user	0m0.284s
sys	0m0.314s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4613 (ff227703)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12ee07650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12ee07d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12ee08310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12ee088c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12ee08e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12ee09420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12ee099d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12ee09f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12ee0a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12ee0aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12ee0af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12ee0b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12ee0bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12ee0c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12ee0cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12ee0d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12ee0dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12ee0e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12ee0eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12ee0f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12ee0fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12ee101a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12ee108c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12ee11160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12ee11880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12ee11b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12ee12150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12ee12dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12ee13300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12ee135c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12ee13a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12ee13d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12ee145b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12ee14af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12ee14db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12ee15250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12ee156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12ee15b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12ee16030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12ee164d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12ee16970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12ee16e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12ee172b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12ee17750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12ee17a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12ee18020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12ee18630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12ee18f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12ee19560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12ee19b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12ee1a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12ee1a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12ee1ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12ee1b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12ee1bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12ee1c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12ee1c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12ee1c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12ee1cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12ee1d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12ee1d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12ee1dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12ee1e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12ee1e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12ee1eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12ee1ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12ee1f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12ee1f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12ee1fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12ee20200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12ee206a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12ee20b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12ee20fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12ee21530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12ee21a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12ee21fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12ee22520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12ee22a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12ee22fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12ee23510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12ee23a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12ee23fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12ee24500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12ee24a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12ee24fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12ee254f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12ee25a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12ee25f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12ee264e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12ee26a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12ee26f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12ee274d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12ee27a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12ee27f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12ee284c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12ee28a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12ee28f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12ee18c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12ee293d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12ee29b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12ee2a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12ee2a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12ee2ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12ee2b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12ee2b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12ee2bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12ee2c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12ee2c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12ee2cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12ee2d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12ee2d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12ee2db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12ee2e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12ee2e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12ee2e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12ee2ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12ee2f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12ee2f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12ee2fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12ee300f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12ee30590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12ee30a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12ee30ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12ee31370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12ee31810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12ee31cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12ee32150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12ee325f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12ee32a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12ee32f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12ee333d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12ee33870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12ee33d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12ee341b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12ee34650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12ee34af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12ee34f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12ee35430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12ee358d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12ee35d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12ee36210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12ee366b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12ee36b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12ee36ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12ee37490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12ee37930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12ee37dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12ee38270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12ee38710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12ee38bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12ee39050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12ee394f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12ee39990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12ee39e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12ee3a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12ee3a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12ee3ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12ee3b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12ee3b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12ee3b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12ee3be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12ee3c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12ee3c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12ee3cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12ee3d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12ee3d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12ee3da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12ee3def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12ee3e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12ee3e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12ee3ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12ee3f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12ee3f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12ee3fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12ee3ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12ee403f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12ee40890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12ee40d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12ee411d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12ee41670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12ee41b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12ee41fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12ee42450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12ee428f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12ee42d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12ee43230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12ee436d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12ee43b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12ee44010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12ee444b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12ee44950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12ee44df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12ee45290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12ee457e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12ee45d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12ee46280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12ee467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12ee46a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12ee470a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12ee476b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12ee47cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12ee484b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12ee48950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12ee48c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12ee49220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12ee49830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12ee4a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12ee4a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12ee4a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12ee4ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12ee4b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12ee4bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12ee4c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12ee4c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12ee4caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12ee4d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12ee4d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12ee4dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12ee4e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12ee4e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12ee4ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12ee4f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12ee4f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12ee4fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12ee50010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12ee50560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12ee50ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12ee51000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12ee51550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12ee51aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12ee51ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12ee52540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12ee52a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12ee52fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12ee53530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12ee53a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12ee53fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12ee54520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12ee54a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12ee54fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12ee55510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12ee55a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12ee55fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12ee56500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12ee56a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12ee56fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12ee574f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12ee57a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12ee57f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12ee584e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12ee58a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12ee58f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12ee594d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12ee59a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12ee59f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12ee5a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12ee5aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12ee5af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12ee5b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12ee5ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12ee5bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12ee5c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12ee5c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12ee5cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12ee5d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12ee5d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12ee5df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12ee5e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12ee5e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12ee5ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12ee5f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12ee5f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12ee5faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12ee5ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12ee60430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12ee608d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12ee60d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12ee61210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12ee616b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12ee61b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12ee61ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12ee62490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12ee629e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12ee63100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12ee63820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12ee63f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12ee64660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12ee64920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12ee65110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12ee653d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12ee659e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.099.941 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.099.945 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x140804d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x140805190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x140805600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x140805a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x140805ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x140806350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1408067c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x140806c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1408070a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1408075e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x140807a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1408080d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x140808bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1408093a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x140809bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14080a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14080a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14080b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14080b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14080c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14080c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14080ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14080d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14080dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14080e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14080e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14080e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14080ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14080f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14080f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14080fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x140810010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x140810480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x140810740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x140810bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x140811020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x140811490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x140811900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x140811d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1408121e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x140812650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x140812ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x140812f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1408133a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x140813810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x140813c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1408140f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x140814560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1408149d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x140814e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1408152b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x140815720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x140815b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x140816000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x140816470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1408168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x140816e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x140817350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1408177c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x140817c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1408180a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x140818510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x140818980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x140818df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x140819260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1408196d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x140819b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x140819fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14081a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14081a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14081ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14081b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14081b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14081ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14081bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14081c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14081c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14081cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14081d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14081d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14081d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14081ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14081e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14081e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14081eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14081ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14081f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14081f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14081fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x140820150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1408205c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x140820a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x140820ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x140821310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x140821780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x140821bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x140822060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1408224d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x140822940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x140822db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x140823220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x140823690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x140823b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x140823f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1408243e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x140824850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x140824cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x140825130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1408255a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x140825a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x140825e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1408262f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x140826760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x140826bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x140827040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1408274b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x140827920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x140827d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x140828200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x140828670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x140828ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x140828f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1408293c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x140829830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x140829ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14082a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14082a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14082a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14082ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14082b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14082b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14082bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14082c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14082c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14082c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14082cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14082d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14082d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14082dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14082df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14082e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14082e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14082ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14082f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14082f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14082f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14082fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1408302b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x140830720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x140830b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x140831000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x140831470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1408318e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x140831d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1408321c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x140832630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x140832aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x140832f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x140833380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1408337f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x140833c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1408340d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x140834540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1408349b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x140834e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x140835290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x140835ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x140836180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x140836440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1408368b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x140836d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x140837190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x140837600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x140837a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x140837ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x140838350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1408387c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x140838c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1408390a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x140839510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x140839980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x140839df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14083a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14083a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14083ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14083afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14083b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14083b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14083bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14083c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14083c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14083ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14083cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14083d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14083d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14083dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14083e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14083e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14083e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14083edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14083f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14083f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14083fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x140840120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x140840590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x140840a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x140840e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1408412e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x140841800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x140841d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x140842880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x140842b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x140843100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1408436c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x140843c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x140844240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x140844800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x140844dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x140845380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x140845940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x140845f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1408464c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x140846a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x140847040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x140847600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x140847bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x140848180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x140848740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x140848d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1408492c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x140849880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x140849e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14084a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14084a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14084af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14084b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14084bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14084c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14084c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14084cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14084d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14084d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14084dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14084e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14084e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14084eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14084f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14084fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x140850000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1408505c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x140850b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x140851140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x140851700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x140851cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x140852280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x140852840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x140852e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1408533c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x140853980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x140853f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x140854500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x140854ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x140855080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x140855640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x140855c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1408561c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x140856780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x140856d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x140857240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x140857740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x140857c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x140858140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x140858640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x140858b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x140859040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x140859540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x140859a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x140859f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14085a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14085a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14085ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14085b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14085b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14085c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14085c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14085d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14085d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14085da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14085e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14085e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14085eb30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12ee65690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12ee47360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12ee46d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12ee47970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12ee1aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12ee1a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12ee1ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12ee494e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12ee11e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12ee188f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12ee19210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12ee19820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12ee17cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12ee19e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12ee10e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12ee1d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12ee29690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12ee64be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12ee13fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12ee142a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12ee49af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12ee47f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12ee12410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12ee126d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12ee12990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12ee65e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12ee66100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12ee663c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12ee66680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12ee66940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12ee66c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12ee66ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12ee67180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12ee67440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12ee67700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12ee679c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12ee67c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12ee67f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12ee68200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12ee684c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12ee68780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12ee68a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12ee68d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12ee68fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12ee69280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12ee69540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12ee69800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12ee69ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12ee69d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12ee6a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12ee6a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12ee6a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12ee6a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12ee6ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12ee6ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12ee6b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12ee6b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12ee6b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12ee6b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12ee6bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12ee6be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12ee6c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12ee6c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12ee6c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12ee6c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12ee6cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12ee6cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12ee6d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12ee6d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12ee6d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12ee6da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12ee6dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12ee6df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12ee6e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12ee6e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12ee6e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12ee6ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12ee6ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12ee6f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12ee6f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12ee6f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12ee6f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12ee6fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12ee6fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12ee70080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12ee70340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12ee70600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12ee708c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12ee70b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12ee70e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12ee71100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12ee713c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12ee71680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12ee71940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12ee71c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12ee71ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12ee72180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12ee72440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12ee72700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12ee729c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12ee72c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12ee72f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12ee73200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12ee734c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12ee73780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12ee73a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12ee73d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12ee73fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12ee74280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12ee74540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12ee74800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12ee74ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12ee74d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12ee75040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12ee75300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12ee755c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12ee75880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12ee75b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12ee75e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12ee760c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12ee76380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12ee76640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12ee76900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12ee76bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12ee76e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12ee77140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12ee77400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12ee776c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12ee77980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12ee77c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12ee77f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12ee781c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12ee78480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12ee78740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12ee78a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12ee78cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12ee78f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12ee79240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12ee79500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12ee797c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12ee79a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12ee79d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12ee7a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12ee7a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12ee7a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12ee7a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12ee7ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12ee7adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12ee7b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12ee7b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12ee7b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12ee7b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12ee7bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12ee7be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12ee7c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12ee7c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12ee7c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12ee7c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12ee7cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12ee7cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12ee7d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12ee7d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12ee7d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12ee7d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12ee7dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12ee7df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12ee7e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12ee7e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12ee7e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12ee7ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12ee7ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12ee7efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12ee7f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12ee7f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12ee7f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12ee7fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12ee7fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12ee80040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12ee80300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12ee805c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12ee80880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12ee80b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12ee80e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12ee810c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12ee81380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12ee81640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12ee81900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12ee81bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12ee81e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12ee82140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12ee82400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12ee826c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12ee82980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12ee82c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12ee82f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12ee831c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12ee83480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12ee83740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12ee83a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12ee83cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12ee83f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12ee84240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12ee84500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12ee847c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12ee84a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12ee84d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12ee85000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12ee852c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12ee856c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12ee85b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12ee86310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12ee865d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12ee86890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12ee86d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12ee87170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12ee875e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12ee87a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12ee87ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12ee88330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12ee887a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12ee88c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12ee89080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12ee894f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12ee89960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12ee89dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12ee8a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12ee8a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12ee8ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12ee8af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12ee8b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12ee8b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12ee8bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12ee8c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12ee8c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12ee8ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12ee8cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12ee8d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12ee8d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12ee8dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12ee8e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12ee8e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12ee8e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12ee8edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12ee8f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12ee8f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12ee8fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12ee8ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12ee903e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12ee90850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12ee90cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12ee91130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12ee915a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12ee91a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12ee91e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12ee922f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12ee92760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12ee92bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12ee93040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12ee934b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12ee93920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12ee93d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12ee94200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12ee94670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12ee94ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12ee94f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12ee953c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12ee95830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12ee95ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12ee96110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12ee96580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12ee969f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12ee96e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12ee972d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12ee97740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12ee97bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12ee98020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12ee98490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12ee98900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12ee98d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12ee991e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12ee99650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12ee99ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12ee99f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12ee9a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12ee9b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12ee9b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12ee9bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12ee9c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12ee9c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12ee9cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12ee9d280 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.966s
user	0m0.236s
sys	0m0.193s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
