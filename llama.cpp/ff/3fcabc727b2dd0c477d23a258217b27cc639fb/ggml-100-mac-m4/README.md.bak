### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.21 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.86 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.69 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.43 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.34 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.46 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.34 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    1.00 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.33 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.33 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.14 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.22 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.03 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.23 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.25 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.20 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.93 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  177.82 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.90 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   26.04 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.34 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 221.32 sec*proc (28 tests)

Total Test time (real) = 221.33 sec

real	3m41.363s
user	7m35.812s
sys	0m6.203s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.21 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.31 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.05 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.08 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.23 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.16 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.90 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.14 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.32 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.20 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.42 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   28.99 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.27 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.14 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.21 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  51.14 sec*proc (28 tests)

Total Test time (real) =  51.15 sec

real	0m51.165s
user	1m11.076s
sys	0m5.380s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.074 I build: 4459 (ff3fcabc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.013.216 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.609 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.015.614 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.617 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.015.618 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.618 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.015.618 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.015.619 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.015.619 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.015.620 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.015.620 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.015.621 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.015.621 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.015.623 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.015.623 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.015.624 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.015.624 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.015.625 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.015.625 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.015.625 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.017.829 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.018.409 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.018.410 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.018.410 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.018.411 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.018.411 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.018.411 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.018.412 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.018.412 I llama_model_loader: - type  f32:  124 tensors
0.00.018.412 I llama_model_loader: - type  f16:   73 tensors
0.00.020.653 I llm_load_vocab: special tokens cache size = 5
0.00.021.884 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.021.887 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.021.887 I llm_load_print_meta: arch             = bert
0.00.021.888 I llm_load_print_meta: vocab type       = WPM
0.00.021.888 I llm_load_print_meta: n_vocab          = 30522
0.00.021.888 I llm_load_print_meta: n_merges         = 0
0.00.021.888 I llm_load_print_meta: vocab_only       = 0
0.00.021.888 I llm_load_print_meta: n_ctx_train      = 512
0.00.021.889 I llm_load_print_meta: n_embd           = 384
0.00.021.889 I llm_load_print_meta: n_layer          = 12
0.00.021.892 I llm_load_print_meta: n_head           = 12
0.00.021.893 I llm_load_print_meta: n_head_kv        = 12
0.00.021.893 I llm_load_print_meta: n_rot            = 32
0.00.021.893 I llm_load_print_meta: n_swa            = 0
0.00.021.894 I llm_load_print_meta: n_embd_head_k    = 32
0.00.021.894 I llm_load_print_meta: n_embd_head_v    = 32
0.00.021.894 I llm_load_print_meta: n_gqa            = 1
0.00.021.895 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.021.895 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.021.896 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.021.896 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.021.896 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.021.897 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.021.897 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.021.897 I llm_load_print_meta: n_ff             = 1536
0.00.021.897 I llm_load_print_meta: n_expert         = 0
0.00.021.898 I llm_load_print_meta: n_expert_used    = 0
0.00.021.898 I llm_load_print_meta: causal attn      = 0
0.00.021.898 I llm_load_print_meta: pooling type     = 2
0.00.021.898 I llm_load_print_meta: rope type        = 2
0.00.021.898 I llm_load_print_meta: rope scaling     = linear
0.00.021.899 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.021.899 I llm_load_print_meta: freq_scale_train = 1
0.00.021.899 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.021.899 I llm_load_print_meta: rope_finetuned   = unknown
0.00.021.899 I llm_load_print_meta: ssm_d_conv       = 0
0.00.021.899 I llm_load_print_meta: ssm_d_inner      = 0
0.00.021.900 I llm_load_print_meta: ssm_d_state      = 0
0.00.021.900 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.021.900 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.021.900 I llm_load_print_meta: model type       = 33M
0.00.021.919 I llm_load_print_meta: model ftype      = F16
0.00.021.922 I llm_load_print_meta: model params     = 33.21 M
0.00.021.922 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.021.922 I llm_load_print_meta: general.name     = Bge Small
0.00.021.922 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.021.923 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.021.923 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.021.923 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.021.923 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.021.923 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.021.924 I llm_load_print_meta: max token length = 21
0.00.023.375 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.023.376 I llm_load_tensors: offloading output layer to GPU
0.00.023.376 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.023.405 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.023.406 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.023.582 I llama_new_context_with_model: n_seq_max     = 1
0.00.023.583 I llama_new_context_with_model: n_ctx         = 512
0.00.023.583 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.023.583 I llama_new_context_with_model: n_batch       = 2048
0.00.023.584 I llama_new_context_with_model: n_ubatch      = 2048
0.00.023.584 I llama_new_context_with_model: flash_attn    = 0
0.00.023.584 I llama_new_context_with_model: freq_base     = 10000.0
0.00.023.585 I llama_new_context_with_model: freq_scale    = 1
0.00.023.585 I ggml_metal_init: allocating
0.00.023.589 I ggml_metal_init: found device: Apple M4
0.00.023.592 I ggml_metal_init: picking default device: Apple M4
0.00.024.183 I ggml_metal_init: using embedded metal library
0.00.026.801 I ggml_metal_init: GPU name:   Apple M4
0.00.026.803 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.026.804 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.026.804 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.026.804 I ggml_metal_init: simdgroup reduction   = true
0.00.026.804 I ggml_metal_init: simdgroup matrix mul. = true
0.00.026.805 I ggml_metal_init: has bfloat            = true
0.00.026.805 I ggml_metal_init: use bfloat            = true
0.00.026.805 I ggml_metal_init: hasUnifiedMemory      = true
0.00.026.806 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.035.963 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.036.480 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.036.482 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.036.483 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.037.134 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.037.135 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.037.135 I llama_new_context_with_model: graph nodes  = 429
0.00.037.136 I llama_new_context_with_model: graph splits = 2
0.00.037.137 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.037.137 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.042.201 I 
0.00.042.219 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.042.808 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.047.176 I llama_perf_context_print:        load time =      28.98 ms
0.00.047.177 I llama_perf_context_print: prompt eval time =       4.29 ms /     9 tokens (    0.48 ms per token,  2099.86 tokens per second)
0.00.047.177 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.047.178 I llama_perf_context_print:       total time =       4.98 ms /    10 tokens
0.00.047.393 I ggml_metal_free: deallocating

real	0m0.230s
user	0m0.032s
sys	0m0.023s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.035 I build: 4459 (ff3fcabc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.542 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.010.957 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.010.960 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.010.962 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.010.963 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.010.963 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.010.963 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.010.964 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.010.964 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.010.965 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.010.965 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.010.966 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.010.966 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.010.970 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.010.970 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.010.971 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.010.971 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.010.971 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.010.972 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.070 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.013.655 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.013.656 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.013.657 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.013.657 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.013.657 I llama_model_loader: - kv  21:                tokenizer.ggml.cls_token_id u32              = 101
0.00.013.658 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.013.658 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.013.658 I llama_model_loader: - kv  24:                          general.file_type u32              = 7
0.00.013.659 I llama_model_loader: - type  f32:  124 tensors
0.00.013.659 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.853 I llm_load_vocab: special tokens cache size = 5
0.00.017.067 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.017.069 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.017.069 I llm_load_print_meta: arch             = bert
0.00.017.070 I llm_load_print_meta: vocab type       = WPM
0.00.017.070 I llm_load_print_meta: n_vocab          = 30522
0.00.017.070 I llm_load_print_meta: n_merges         = 0
0.00.017.070 I llm_load_print_meta: vocab_only       = 0
0.00.017.070 I llm_load_print_meta: n_ctx_train      = 512
0.00.017.070 I llm_load_print_meta: n_embd           = 384
0.00.017.071 I llm_load_print_meta: n_layer          = 12
0.00.017.073 I llm_load_print_meta: n_head           = 12
0.00.017.073 I llm_load_print_meta: n_head_kv        = 12
0.00.017.073 I llm_load_print_meta: n_rot            = 32
0.00.017.074 I llm_load_print_meta: n_swa            = 0
0.00.017.074 I llm_load_print_meta: n_embd_head_k    = 32
0.00.017.074 I llm_load_print_meta: n_embd_head_v    = 32
0.00.017.075 I llm_load_print_meta: n_gqa            = 1
0.00.017.075 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.017.076 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.017.077 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.017.077 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.017.077 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.017.079 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.017.079 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.017.079 I llm_load_print_meta: n_ff             = 1536
0.00.017.080 I llm_load_print_meta: n_expert         = 0
0.00.017.080 I llm_load_print_meta: n_expert_used    = 0
0.00.017.080 I llm_load_print_meta: causal attn      = 0
0.00.017.080 I llm_load_print_meta: pooling type     = 2
0.00.017.081 I llm_load_print_meta: rope type        = 2
0.00.017.081 I llm_load_print_meta: rope scaling     = linear
0.00.017.082 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.017.082 I llm_load_print_meta: freq_scale_train = 1
0.00.017.082 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.017.082 I llm_load_print_meta: rope_finetuned   = unknown
0.00.017.083 I llm_load_print_meta: ssm_d_conv       = 0
0.00.017.083 I llm_load_print_meta: ssm_d_inner      = 0
0.00.017.083 I llm_load_print_meta: ssm_d_state      = 0
0.00.017.083 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.017.083 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.017.087 I llm_load_print_meta: model type       = 33M
0.00.017.094 I llm_load_print_meta: model ftype      = Q8_0
0.00.017.095 I llm_load_print_meta: model params     = 33.21 M
0.00.017.095 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.017.095 I llm_load_print_meta: general.name     = Bge Small
0.00.017.096 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.017.096 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.017.096 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.017.098 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.017.098 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.017.098 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.017.098 I llm_load_print_meta: max token length = 21
0.00.018.257 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.018.257 I llm_load_tensors: offloading output layer to GPU
0.00.018.261 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.018.268 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.018.270 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.018.428 I llama_new_context_with_model: n_seq_max     = 1
0.00.018.429 I llama_new_context_with_model: n_ctx         = 512
0.00.018.429 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.018.429 I llama_new_context_with_model: n_batch       = 2048
0.00.018.429 I llama_new_context_with_model: n_ubatch      = 2048
0.00.018.430 I llama_new_context_with_model: flash_attn    = 0
0.00.018.430 I llama_new_context_with_model: freq_base     = 10000.0
0.00.018.430 I llama_new_context_with_model: freq_scale    = 1
0.00.018.431 I ggml_metal_init: allocating
0.00.018.435 I ggml_metal_init: found device: Apple M4
0.00.018.437 I ggml_metal_init: picking default device: Apple M4
0.00.018.984 I ggml_metal_init: using embedded metal library
0.00.021.335 I ggml_metal_init: GPU name:   Apple M4
0.00.021.337 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.021.337 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.021.338 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.021.338 I ggml_metal_init: simdgroup reduction   = true
0.00.021.338 I ggml_metal_init: simdgroup matrix mul. = true
0.00.021.338 I ggml_metal_init: has bfloat            = true
0.00.021.338 I ggml_metal_init: use bfloat            = true
0.00.021.339 I ggml_metal_init: hasUnifiedMemory      = true
0.00.021.340 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.031.673 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.032.166 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.032.169 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.032.170 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.032.763 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.032.764 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.032.764 I llama_new_context_with_model: graph nodes  = 429
0.00.032.765 I llama_new_context_with_model: graph splits = 2
0.00.032.766 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.032.766 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.037.081 I 
0.00.037.097 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.037.617 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.042.011 I llama_perf_context_print:        load time =      28.54 ms
0.00.042.013 I llama_perf_context_print: prompt eval time =       4.27 ms /     9 tokens (    0.47 ms per token,  2109.70 tokens per second)
0.00.042.013 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.042.014 I llama_perf_context_print:       total time =       4.93 ms /    10 tokens
0.00.042.179 I ggml_metal_free: deallocating

real	0m0.053s
user	0m0.028s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.197 I build: 4459 (ff3fcabc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.628 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.039.319 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.325 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.327 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.039.328 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.331 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.039.332 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.039.333 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.039.334 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.039.335 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.039.336 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.039.336 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.039.337 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.039.340 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.039.341 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.039.342 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.039.342 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.343 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.047.291 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.049.568 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.444 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.054.446 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.447 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.054.447 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.054.448 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.054.448 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.054.448 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.054.449 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.054.449 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.054.450 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.054.450 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.054.450 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.054.451 I llama_model_loader: - type  f32:   40 tensors
0.00.054.457 I llama_model_loader: - type  f16:   30 tensors
0.00.072.678 W llm_load_vocab: empty token at index 5
0.00.077.180 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.078.513 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.078.546 I llm_load_vocab: special tokens cache size = 5
0.00.335.590 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.335.598 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.335.599 I llm_load_print_meta: arch             = jina-bert-v2
0.00.335.602 I llm_load_print_meta: vocab type       = BPE
0.00.335.602 I llm_load_print_meta: n_vocab          = 61056
0.00.335.603 I llm_load_print_meta: n_merges         = 39382
0.00.335.603 I llm_load_print_meta: vocab_only       = 0
0.00.335.605 I llm_load_print_meta: n_ctx_train      = 8192
0.00.335.605 I llm_load_print_meta: n_embd           = 384
0.00.335.606 I llm_load_print_meta: n_layer          = 4
0.00.335.613 I llm_load_print_meta: n_head           = 12
0.00.335.613 I llm_load_print_meta: n_head_kv        = 12
0.00.335.614 I llm_load_print_meta: n_rot            = 32
0.00.335.614 I llm_load_print_meta: n_swa            = 0
0.00.335.614 I llm_load_print_meta: n_embd_head_k    = 32
0.00.335.615 I llm_load_print_meta: n_embd_head_v    = 32
0.00.335.616 I llm_load_print_meta: n_gqa            = 1
0.00.335.617 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.335.617 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.335.618 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.335.619 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.335.620 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.335.620 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.335.621 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.335.622 I llm_load_print_meta: n_ff             = 1536
0.00.335.622 I llm_load_print_meta: n_expert         = 0
0.00.335.622 I llm_load_print_meta: n_expert_used    = 0
0.00.335.622 I llm_load_print_meta: causal attn      = 0
0.00.335.622 I llm_load_print_meta: pooling type     = -1
0.00.335.622 I llm_load_print_meta: rope type        = -1
0.00.335.623 I llm_load_print_meta: rope scaling     = linear
0.00.335.623 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.335.623 I llm_load_print_meta: freq_scale_train = 1
0.00.335.623 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.335.624 I llm_load_print_meta: rope_finetuned   = unknown
0.00.335.624 I llm_load_print_meta: ssm_d_conv       = 0
0.00.335.624 I llm_load_print_meta: ssm_d_inner      = 0
0.00.335.624 I llm_load_print_meta: ssm_d_state      = 0
0.00.335.624 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.335.624 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.335.624 I llm_load_print_meta: model type       = 33M
0.00.335.649 I llm_load_print_meta: model ftype      = F16
0.00.335.651 I llm_load_print_meta: model params     = 32.90 M
0.00.335.651 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.335.651 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.335.652 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.335.652 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.335.652 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.335.652 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.335.652 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.335.653 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.335.653 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.335.653 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.335.653 I llm_load_print_meta: max token length = 45
0.00.336.933 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.336.933 I llm_load_tensors: offloading output layer to GPU
0.00.336.934 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.336.964 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.336.966 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.337.313 I llama_new_context_with_model: n_seq_max     = 1
0.00.337.314 I llama_new_context_with_model: n_ctx         = 8192
0.00.337.314 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.337.315 I llama_new_context_with_model: n_batch       = 2048
0.00.337.315 I llama_new_context_with_model: n_ubatch      = 2048
0.00.337.315 I llama_new_context_with_model: flash_attn    = 0
0.00.337.316 I llama_new_context_with_model: freq_base     = 10000.0
0.00.337.316 I llama_new_context_with_model: freq_scale    = 1
0.00.337.317 I ggml_metal_init: allocating
0.00.337.324 I ggml_metal_init: found device: Apple M4
0.00.337.327 I ggml_metal_init: picking default device: Apple M4
0.00.338.359 I ggml_metal_init: using embedded metal library
0.00.341.413 I ggml_metal_init: GPU name:   Apple M4
0.00.341.415 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.341.415 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.341.415 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.341.416 I ggml_metal_init: simdgroup reduction   = true
0.00.341.416 I ggml_metal_init: simdgroup matrix mul. = true
0.00.341.416 I ggml_metal_init: has bfloat            = true
0.00.341.416 I ggml_metal_init: use bfloat            = true
0.00.341.416 I ggml_metal_init: hasUnifiedMemory      = true
0.00.341.417 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.350.744 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.353.190 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.353.192 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.353.193 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.353.744 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.353.745 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.353.745 I llama_new_context_with_model: graph nodes  = 154
0.00.353.746 I llama_new_context_with_model: graph splits = 2
0.00.353.747 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.353.747 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.365.473 I 
0.00.365.502 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.365.656 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.365.657 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.365.659 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.365.659 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.365.663 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.365.663 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.366.232 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.369.885 I llama_perf_context_print:        load time =     340.84 ms
0.00.369.886 I llama_perf_context_print: prompt eval time =       3.65 ms /    62 tokens (    0.06 ms per token, 17004.94 tokens per second)
0.00.369.887 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.369.888 I llama_perf_context_print:       total time =       4.41 ms /    63 tokens
0.00.370.134 I ggml_metal_free: deallocating

real	0m1.085s
user	0m0.341s
sys	0m0.045s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.167 I build: 4459 (ff3fcabc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.279 I main: llama backend init
0.00.000.285 I main: load the model and apply lora adapter, if any
0.00.029.910 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.042.258 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.042.278 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.042.282 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.042.283 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.042.284 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.042.284 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.042.285 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.042.288 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.042.289 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.042.289 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.042.290 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.042.290 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.042.291 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.042.291 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.042.296 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.042.301 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.042.301 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.049.913 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.052.546 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.061.181 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.061.185 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.061.185 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.061.186 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.061.186 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.061.187 I llama_model_loader: - type  f32:  194 tensors
0.00.061.188 I llama_model_loader: - type  f16:   98 tensors
0.00.093.544 I llm_load_vocab: special tokens cache size = 25
0.00.100.532 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.100.535 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.100.535 I llm_load_print_meta: arch             = gptneox
0.00.100.536 I llm_load_print_meta: vocab type       = BPE
0.00.100.536 I llm_load_print_meta: n_vocab          = 50304
0.00.100.536 I llm_load_print_meta: n_merges         = 50009
0.00.100.536 I llm_load_print_meta: vocab_only       = 0
0.00.100.537 I llm_load_print_meta: n_ctx_train      = 2048
0.00.100.537 I llm_load_print_meta: n_embd           = 2048
0.00.100.537 I llm_load_print_meta: n_layer          = 24
0.00.100.540 I llm_load_print_meta: n_head           = 16
0.00.100.540 I llm_load_print_meta: n_head_kv        = 16
0.00.100.540 I llm_load_print_meta: n_rot            = 32
0.00.100.542 I llm_load_print_meta: n_swa            = 0
0.00.100.542 I llm_load_print_meta: n_embd_head_k    = 128
0.00.100.542 I llm_load_print_meta: n_embd_head_v    = 128
0.00.100.543 I llm_load_print_meta: n_gqa            = 1
0.00.100.544 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.100.546 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.100.547 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.100.547 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.100.547 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.100.548 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.100.548 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.100.548 I llm_load_print_meta: n_ff             = 8192
0.00.100.548 I llm_load_print_meta: n_expert         = 0
0.00.100.549 I llm_load_print_meta: n_expert_used    = 0
0.00.100.550 I llm_load_print_meta: causal attn      = 1
0.00.100.550 I llm_load_print_meta: pooling type     = 0
0.00.100.550 I llm_load_print_meta: rope type        = 2
0.00.100.550 I llm_load_print_meta: rope scaling     = linear
0.00.100.551 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.100.551 I llm_load_print_meta: freq_scale_train = 1
0.00.100.551 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.100.551 I llm_load_print_meta: rope_finetuned   = unknown
0.00.100.551 I llm_load_print_meta: ssm_d_conv       = 0
0.00.100.551 I llm_load_print_meta: ssm_d_inner      = 0
0.00.100.552 I llm_load_print_meta: ssm_d_state      = 0
0.00.100.552 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.100.552 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.100.552 I llm_load_print_meta: model type       = 1.4B
0.00.100.571 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.100.572 I llm_load_print_meta: model params     = 1.41 B
0.00.100.572 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.100.572 I llm_load_print_meta: general.name     = 1.4B
0.00.100.573 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.100.573 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.100.573 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.100.573 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.100.573 I llm_load_print_meta: LF token         = 128 ''
0.00.100.574 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.100.575 I llm_load_print_meta: max token length = 1024
0.00.103.192 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.103.193 I llm_load_tensors: offloading output layer to GPU
0.00.103.193 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.103.212 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.103.213 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.103.542 I llama_new_context_with_model: n_seq_max     = 1
0.00.103.544 I llama_new_context_with_model: n_ctx         = 2048
0.00.103.544 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.103.544 I llama_new_context_with_model: n_batch       = 2048
0.00.103.544 I llama_new_context_with_model: n_ubatch      = 512
0.00.103.544 I llama_new_context_with_model: flash_attn    = 0
0.00.103.545 I llama_new_context_with_model: freq_base     = 10000.0
0.00.103.545 I llama_new_context_with_model: freq_scale    = 1
0.00.103.545 I ggml_metal_init: allocating
0.00.103.548 I ggml_metal_init: found device: Apple M4
0.00.103.551 I ggml_metal_init: picking default device: Apple M4
0.00.104.230 I ggml_metal_init: using embedded metal library
0.00.113.787 I ggml_metal_init: GPU name:   Apple M4
0.00.113.789 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.113.789 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.113.790 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.113.790 I ggml_metal_init: simdgroup reduction   = true
0.00.113.790 I ggml_metal_init: simdgroup matrix mul. = true
0.00.113.790 I ggml_metal_init: has bfloat            = true
0.00.113.790 I ggml_metal_init: use bfloat            = true
0.00.113.791 I ggml_metal_init: hasUnifiedMemory      = true
0.00.113.791 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.137.513 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.157.881 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.157.888 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.157.909 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.158.879 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.158.882 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.158.882 I llama_new_context_with_model: graph nodes  = 967
0.00.158.882 I llama_new_context_with_model: graph splits = 2
0.00.158.886 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.159.014 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.159.015 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.245.687 I main: llama threadpool init, n_threads = 4
0.00.245.737 I 
0.00.245.766 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.245.768 I 
0.00.246.002 I sampler seed: 1234
0.00.246.007 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.246.033 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.246.035 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.246.035 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.081.717 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58436.21 tokens per second)
0.02.081.718 I llama_perf_context_print:        load time =     215.76 ms
0.02.081.720 I llama_perf_context_print: prompt eval time =      43.66 ms /     7 tokens (    6.24 ms per token,   160.33 tokens per second)
0.02.081.720 I llama_perf_context_print:        eval time =    1789.12 ms /    63 runs   (   28.40 ms per token,    35.21 tokens per second)
0.02.081.721 I llama_perf_context_print:       total time =    1836.04 ms /    70 tokens
0.02.081.948 I ggml_metal_free: deallocating

real	0m2.366s
user	0m0.146s
sys	0m0.109s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.567 I build: 4459 (ff3fcabc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.130 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.038.254 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.260 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.262 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.262 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.263 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.264 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.266 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.268 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.273 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.273 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.274 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.275 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.276 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.277 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.280 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.281 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.282 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.481 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.516 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.779 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.055.782 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.782 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.783 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.783 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.784 I llama_model_loader: - type  f32:  194 tensors
0.00.055.784 I llama_model_loader: - type  f16:   98 tensors
0.00.084.614 I llm_load_vocab: special tokens cache size = 25
0.00.091.042 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.091.045 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.091.045 I llm_load_print_meta: arch             = gptneox
0.00.091.046 I llm_load_print_meta: vocab type       = BPE
0.00.091.046 I llm_load_print_meta: n_vocab          = 50304
0.00.091.046 I llm_load_print_meta: n_merges         = 50009
0.00.091.046 I llm_load_print_meta: vocab_only       = 0
0.00.091.046 I llm_load_print_meta: n_ctx_train      = 2048
0.00.091.046 I llm_load_print_meta: n_embd           = 2048
0.00.091.046 I llm_load_print_meta: n_layer          = 24
0.00.091.050 I llm_load_print_meta: n_head           = 16
0.00.091.050 I llm_load_print_meta: n_head_kv        = 16
0.00.091.050 I llm_load_print_meta: n_rot            = 32
0.00.091.052 I llm_load_print_meta: n_swa            = 0
0.00.091.053 I llm_load_print_meta: n_embd_head_k    = 128
0.00.091.053 I llm_load_print_meta: n_embd_head_v    = 128
0.00.091.053 I llm_load_print_meta: n_gqa            = 1
0.00.091.054 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.091.055 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.091.055 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.091.056 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.091.056 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.091.056 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.091.056 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.091.057 I llm_load_print_meta: n_ff             = 8192
0.00.091.057 I llm_load_print_meta: n_expert         = 0
0.00.091.057 I llm_load_print_meta: n_expert_used    = 0
0.00.091.057 I llm_load_print_meta: causal attn      = 1
0.00.091.057 I llm_load_print_meta: pooling type     = 0
0.00.091.057 I llm_load_print_meta: rope type        = 2
0.00.091.057 I llm_load_print_meta: rope scaling     = linear
0.00.091.059 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.091.059 I llm_load_print_meta: freq_scale_train = 1
0.00.091.060 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.091.060 I llm_load_print_meta: rope_finetuned   = unknown
0.00.091.060 I llm_load_print_meta: ssm_d_conv       = 0
0.00.091.060 I llm_load_print_meta: ssm_d_inner      = 0
0.00.091.060 I llm_load_print_meta: ssm_d_state      = 0
0.00.091.060 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.091.060 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.091.061 I llm_load_print_meta: model type       = 1.4B
0.00.091.078 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.091.078 I llm_load_print_meta: model params     = 1.41 B
0.00.091.079 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.091.079 I llm_load_print_meta: general.name     = 1.4B
0.00.091.079 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.091.079 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.091.079 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.091.080 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.091.080 I llm_load_print_meta: LF token         = 128 ''
0.00.091.080 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.091.081 I llm_load_print_meta: max token length = 1024
0.00.093.578 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.093.579 I llm_load_tensors: offloading output layer to GPU
0.00.093.579 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.093.590 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.093.591 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.093.966 I llama_new_context_with_model: n_seq_max     = 1
0.00.093.967 I llama_new_context_with_model: n_ctx         = 128
0.00.093.967 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.093.967 I llama_new_context_with_model: n_batch       = 128
0.00.093.967 I llama_new_context_with_model: n_ubatch      = 128
0.00.093.967 I llama_new_context_with_model: flash_attn    = 0
0.00.093.968 I llama_new_context_with_model: freq_base     = 10000.0
0.00.093.968 I llama_new_context_with_model: freq_scale    = 1
0.00.093.968 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.093.969 I ggml_metal_init: allocating
0.00.093.972 I ggml_metal_init: found device: Apple M4
0.00.093.974 I ggml_metal_init: picking default device: Apple M4
0.00.094.570 I ggml_metal_init: using embedded metal library
0.00.097.082 I ggml_metal_init: GPU name:   Apple M4
0.00.097.083 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.084 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.084 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.084 I ggml_metal_init: simdgroup reduction   = true
0.00.097.085 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.085 I ggml_metal_init: has bfloat            = true
0.00.097.085 I ggml_metal_init: use bfloat            = true
0.00.097.085 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.086 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.106.326 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.107.632 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.107.637 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.107.652 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.108.627 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.108.628 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.108.628 I llama_new_context_with_model: graph nodes  = 967
0.00.108.628 I llama_new_context_with_model: graph splits = 2
0.00.108.629 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.108.630 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.456.124 I 
0.01.456.166 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.456.204 I perplexity: tokenizing the input ..
0.01.469.507 I perplexity: tokenization took 13.297 ms
0.01.469.514 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.604.599 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.606.341 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.606.395 I llama_perf_context_print:        load time =    1431.98 ms
0.01.606.397 I llama_perf_context_print: prompt eval time =     134.12 ms /   128 tokens (    1.05 ms per token,   954.34 tokens per second)
0.01.606.398 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.606.398 I llama_perf_context_print:       total time =     150.27 ms /   129 tokens
0.01.607.057 I ggml_metal_free: deallocating

real	0m1.799s
user	0m0.124s
sys	0m0.270s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4459 (ff3fcabc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.009.932 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.039 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.044 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.052 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.053 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.053 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.054 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.055 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.056 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.057 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.057 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.058 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.058 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.058 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.059 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.060 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.061 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.061 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.745 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.889 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.969 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.971 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.971 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.972 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.972 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.972 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.973 I llama_model_loader: - type  f32:  194 tensors
0.00.034.973 I llama_model_loader: - type q8_0:   98 tensors
0.00.056.992 I llm_load_vocab: special tokens cache size = 25
0.00.063.083 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.063.088 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.063.088 I llm_load_print_meta: arch             = gptneox
0.00.063.089 I llm_load_print_meta: vocab type       = BPE
0.00.063.089 I llm_load_print_meta: n_vocab          = 50304
0.00.063.089 I llm_load_print_meta: n_merges         = 50009
0.00.063.089 I llm_load_print_meta: vocab_only       = 0
0.00.063.089 I llm_load_print_meta: n_ctx_train      = 2048
0.00.063.090 I llm_load_print_meta: n_embd           = 2048
0.00.063.090 I llm_load_print_meta: n_layer          = 24
0.00.063.095 I llm_load_print_meta: n_head           = 16
0.00.063.096 I llm_load_print_meta: n_head_kv        = 16
0.00.063.097 I llm_load_print_meta: n_rot            = 32
0.00.063.097 I llm_load_print_meta: n_swa            = 0
0.00.063.098 I llm_load_print_meta: n_embd_head_k    = 128
0.00.063.098 I llm_load_print_meta: n_embd_head_v    = 128
0.00.063.099 I llm_load_print_meta: n_gqa            = 1
0.00.063.100 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.063.100 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.063.101 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.063.101 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.063.101 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.063.102 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.063.102 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.063.103 I llm_load_print_meta: n_ff             = 8192
0.00.063.103 I llm_load_print_meta: n_expert         = 0
0.00.063.103 I llm_load_print_meta: n_expert_used    = 0
0.00.063.103 I llm_load_print_meta: causal attn      = 1
0.00.063.103 I llm_load_print_meta: pooling type     = 0
0.00.063.103 I llm_load_print_meta: rope type        = 2
0.00.063.104 I llm_load_print_meta: rope scaling     = linear
0.00.063.104 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.063.106 I llm_load_print_meta: freq_scale_train = 1
0.00.063.106 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.063.106 I llm_load_print_meta: rope_finetuned   = unknown
0.00.063.107 I llm_load_print_meta: ssm_d_conv       = 0
0.00.063.107 I llm_load_print_meta: ssm_d_inner      = 0
0.00.063.107 I llm_load_print_meta: ssm_d_state      = 0
0.00.063.107 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.063.107 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.063.107 I llm_load_print_meta: model type       = 1.4B
0.00.063.120 I llm_load_print_meta: model ftype      = Q8_0
0.00.063.121 I llm_load_print_meta: model params     = 1.41 B
0.00.063.121 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.063.122 I llm_load_print_meta: general.name     = 1.4B
0.00.063.122 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.063.122 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.063.123 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.063.123 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.063.123 I llm_load_print_meta: LF token         = 128 ''
0.00.063.123 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.063.123 I llm_load_print_meta: max token length = 1024
0.00.065.617 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.065.617 I llm_load_tensors: offloading output layer to GPU
0.00.065.617 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.065.629 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.065.630 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.066.040 I llama_new_context_with_model: n_seq_max     = 1
0.00.066.041 I llama_new_context_with_model: n_ctx         = 2048
0.00.066.041 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.066.041 I llama_new_context_with_model: n_batch       = 2048
0.00.066.041 I llama_new_context_with_model: n_ubatch      = 512
0.00.066.042 I llama_new_context_with_model: flash_attn    = 0
0.00.066.042 I llama_new_context_with_model: freq_base     = 10000.0
0.00.066.042 I llama_new_context_with_model: freq_scale    = 1
0.00.066.043 I ggml_metal_init: allocating
0.00.066.051 I ggml_metal_init: found device: Apple M4
0.00.066.053 I ggml_metal_init: picking default device: Apple M4
0.00.066.813 I ggml_metal_init: using embedded metal library
0.00.069.397 I ggml_metal_init: GPU name:   Apple M4
0.00.069.398 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.069.398 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.069.399 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.069.399 I ggml_metal_init: simdgroup reduction   = true
0.00.069.399 I ggml_metal_init: simdgroup matrix mul. = true
0.00.069.399 I ggml_metal_init: has bfloat            = true
0.00.069.399 I ggml_metal_init: use bfloat            = true
0.00.069.400 I ggml_metal_init: hasUnifiedMemory      = true
0.00.069.401 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.080.154 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.108.066 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.108.080 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.108.104 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.109.289 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.109.291 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.109.291 I llama_new_context_with_model: graph nodes  = 967
0.00.109.291 I llama_new_context_with_model: graph splits = 2
0.00.109.294 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.109.436 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.109.436 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.129.477 I main: llama threadpool init, n_threads = 4
0.01.129.516 I 
0.01.129.544 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.129.545 I 
0.01.129.805 I sampler seed: 1234
0.01.129.810 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.129.854 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.129.856 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.129.856 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.216.028 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 62008.73 tokens per second)
0.02.216.028 I llama_perf_context_print:        load time =    1119.54 ms
0.02.216.029 I llama_perf_context_print: prompt eval time =      43.49 ms /     7 tokens (    6.21 ms per token,   160.94 tokens per second)
0.02.216.029 I llama_perf_context_print:        eval time =    1039.87 ms /    63 runs   (   16.51 ms per token,    60.58 tokens per second)
0.02.216.030 I llama_perf_context_print:       total time =    1086.55 ms /    70 tokens
0.02.216.259 I ggml_metal_free: deallocating

real	0m2.235s
user	0m0.115s
sys	0m0.246s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.119 I build: 4459 (ff3fcabc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.127 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.769 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.020.774 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.777 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.778 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.778 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.778 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.779 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.780 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.780 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.781 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.781 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.781 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.782 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.783 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.785 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.785 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.785 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.800 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.099 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.048 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.032.050 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.050 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.051 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.051 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.052 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.032.052 I llama_model_loader: - type  f32:  194 tensors
0.00.032.053 I llama_model_loader: - type q8_0:   98 tensors
0.00.057.127 I llm_load_vocab: special tokens cache size = 25
0.00.063.447 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.063.450 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.063.450 I llm_load_print_meta: arch             = gptneox
0.00.063.451 I llm_load_print_meta: vocab type       = BPE
0.00.063.451 I llm_load_print_meta: n_vocab          = 50304
0.00.063.451 I llm_load_print_meta: n_merges         = 50009
0.00.063.451 I llm_load_print_meta: vocab_only       = 0
0.00.063.452 I llm_load_print_meta: n_ctx_train      = 2048
0.00.063.452 I llm_load_print_meta: n_embd           = 2048
0.00.063.452 I llm_load_print_meta: n_layer          = 24
0.00.063.456 I llm_load_print_meta: n_head           = 16
0.00.063.456 I llm_load_print_meta: n_head_kv        = 16
0.00.063.457 I llm_load_print_meta: n_rot            = 32
0.00.063.457 I llm_load_print_meta: n_swa            = 0
0.00.063.457 I llm_load_print_meta: n_embd_head_k    = 128
0.00.063.457 I llm_load_print_meta: n_embd_head_v    = 128
0.00.063.458 I llm_load_print_meta: n_gqa            = 1
0.00.063.458 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.063.459 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.063.459 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.063.460 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.063.460 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.063.460 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.063.460 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.063.461 I llm_load_print_meta: n_ff             = 8192
0.00.063.461 I llm_load_print_meta: n_expert         = 0
0.00.063.461 I llm_load_print_meta: n_expert_used    = 0
0.00.063.461 I llm_load_print_meta: causal attn      = 1
0.00.063.461 I llm_load_print_meta: pooling type     = 0
0.00.063.461 I llm_load_print_meta: rope type        = 2
0.00.063.461 I llm_load_print_meta: rope scaling     = linear
0.00.063.465 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.063.465 I llm_load_print_meta: freq_scale_train = 1
0.00.063.465 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.063.466 I llm_load_print_meta: rope_finetuned   = unknown
0.00.063.466 I llm_load_print_meta: ssm_d_conv       = 0
0.00.063.466 I llm_load_print_meta: ssm_d_inner      = 0
0.00.063.467 I llm_load_print_meta: ssm_d_state      = 0
0.00.063.467 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.063.467 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.063.467 I llm_load_print_meta: model type       = 1.4B
0.00.063.479 I llm_load_print_meta: model ftype      = Q8_0
0.00.063.480 I llm_load_print_meta: model params     = 1.41 B
0.00.063.480 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.063.480 I llm_load_print_meta: general.name     = 1.4B
0.00.063.481 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.063.481 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.063.481 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.063.481 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.063.481 I llm_load_print_meta: LF token         = 128 ''
0.00.063.482 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.063.482 I llm_load_print_meta: max token length = 1024
0.00.065.866 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.065.866 I llm_load_tensors: offloading output layer to GPU
0.00.065.866 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.065.877 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.065.878 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.066.241 I llama_new_context_with_model: n_seq_max     = 1
0.00.066.242 I llama_new_context_with_model: n_ctx         = 128
0.00.066.242 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.066.242 I llama_new_context_with_model: n_batch       = 128
0.00.066.242 I llama_new_context_with_model: n_ubatch      = 128
0.00.066.242 I llama_new_context_with_model: flash_attn    = 0
0.00.066.243 I llama_new_context_with_model: freq_base     = 10000.0
0.00.066.243 I llama_new_context_with_model: freq_scale    = 1
0.00.066.243 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.066.244 I ggml_metal_init: allocating
0.00.066.251 I ggml_metal_init: found device: Apple M4
0.00.066.253 I ggml_metal_init: picking default device: Apple M4
0.00.066.931 I ggml_metal_init: using embedded metal library
0.00.069.454 I ggml_metal_init: GPU name:   Apple M4
0.00.069.455 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.069.456 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.069.456 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.069.456 I ggml_metal_init: simdgroup reduction   = true
0.00.069.457 I ggml_metal_init: simdgroup matrix mul. = true
0.00.069.457 I ggml_metal_init: has bfloat            = true
0.00.069.457 I ggml_metal_init: use bfloat            = true
0.00.069.457 I ggml_metal_init: hasUnifiedMemory      = true
0.00.069.458 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.079.570 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.081.091 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.081.095 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.081.110 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.082.199 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.082.200 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.082.200 I llama_new_context_with_model: graph nodes  = 967
0.00.082.201 I llama_new_context_with_model: graph splits = 2
0.00.082.202 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.082.202 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.810.815 I 
0.00.810.844 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.810.854 I perplexity: tokenizing the input ..
0.00.818.489 I perplexity: tokenization took 7.633 ms
0.00.818.497 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.942.859 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.944.107 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.944.125 I llama_perf_context_print:        load time =     799.68 ms
0.00.944.126 I llama_perf_context_print: prompt eval time =     124.10 ms /   128 tokens (    0.97 ms per token,  1031.42 tokens per second)
0.00.944.127 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.944.128 I llama_perf_context_print:       total time =     133.31 ms /   129 tokens
0.00.944.566 I ggml_metal_free: deallocating

real	0m0.961s
user	0m0.091s
sys	0m0.163s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4459 (ff3fcabc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.011.936 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.517 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.019.522 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.524 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.525 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.525 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.526 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.526 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.527 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.528 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.528 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.528 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.529 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.529 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.530 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.531 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.532 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.532 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.415 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.463 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.329 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.028.331 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.331 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.332 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.332 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.333 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.028.333 I llama_model_loader: - type  f32:  194 tensors
0.00.028.333 I llama_model_loader: - type q4_0:   97 tensors
0.00.028.334 I llama_model_loader: - type q6_K:    1 tensors
0.00.049.388 I llm_load_vocab: special tokens cache size = 25
0.00.055.433 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.055.436 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.055.437 I llm_load_print_meta: arch             = gptneox
0.00.055.437 I llm_load_print_meta: vocab type       = BPE
0.00.055.437 I llm_load_print_meta: n_vocab          = 50304
0.00.055.438 I llm_load_print_meta: n_merges         = 50009
0.00.055.438 I llm_load_print_meta: vocab_only       = 0
0.00.055.438 I llm_load_print_meta: n_ctx_train      = 2048
0.00.055.438 I llm_load_print_meta: n_embd           = 2048
0.00.055.438 I llm_load_print_meta: n_layer          = 24
0.00.055.443 I llm_load_print_meta: n_head           = 16
0.00.055.444 I llm_load_print_meta: n_head_kv        = 16
0.00.055.444 I llm_load_print_meta: n_rot            = 32
0.00.055.444 I llm_load_print_meta: n_swa            = 0
0.00.055.444 I llm_load_print_meta: n_embd_head_k    = 128
0.00.055.445 I llm_load_print_meta: n_embd_head_v    = 128
0.00.055.445 I llm_load_print_meta: n_gqa            = 1
0.00.055.446 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.055.447 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.055.447 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.055.448 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.055.448 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.055.448 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.055.448 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.055.449 I llm_load_print_meta: n_ff             = 8192
0.00.055.449 I llm_load_print_meta: n_expert         = 0
0.00.055.450 I llm_load_print_meta: n_expert_used    = 0
0.00.055.450 I llm_load_print_meta: causal attn      = 1
0.00.055.450 I llm_load_print_meta: pooling type     = 0
0.00.055.452 I llm_load_print_meta: rope type        = 2
0.00.055.452 I llm_load_print_meta: rope scaling     = linear
0.00.055.453 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.055.453 I llm_load_print_meta: freq_scale_train = 1
0.00.055.453 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.055.453 I llm_load_print_meta: rope_finetuned   = unknown
0.00.055.454 I llm_load_print_meta: ssm_d_conv       = 0
0.00.055.454 I llm_load_print_meta: ssm_d_inner      = 0
0.00.055.454 I llm_load_print_meta: ssm_d_state      = 0
0.00.055.454 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.055.454 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.055.454 I llm_load_print_meta: model type       = 1.4B
0.00.055.467 I llm_load_print_meta: model ftype      = Q4_0
0.00.055.467 I llm_load_print_meta: model params     = 1.41 B
0.00.055.468 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.055.468 I llm_load_print_meta: general.name     = 1.4B
0.00.055.468 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.055.468 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.055.468 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.055.469 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.055.469 I llm_load_print_meta: LF token         = 128 ''
0.00.055.469 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.055.469 I llm_load_print_meta: max token length = 1024
0.00.057.823 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.057.823 I llm_load_tensors: offloading output layer to GPU
0.00.057.823 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.057.835 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.057.837 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.058.198 I llama_new_context_with_model: n_seq_max     = 1
0.00.058.198 I llama_new_context_with_model: n_ctx         = 2048
0.00.058.198 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.058.199 I llama_new_context_with_model: n_batch       = 2048
0.00.058.199 I llama_new_context_with_model: n_ubatch      = 512
0.00.058.199 I llama_new_context_with_model: flash_attn    = 0
0.00.058.200 I llama_new_context_with_model: freq_base     = 10000.0
0.00.058.200 I llama_new_context_with_model: freq_scale    = 1
0.00.058.201 I ggml_metal_init: allocating
0.00.058.205 I ggml_metal_init: found device: Apple M4
0.00.058.207 I ggml_metal_init: picking default device: Apple M4
0.00.058.993 I ggml_metal_init: using embedded metal library
0.00.061.507 I ggml_metal_init: GPU name:   Apple M4
0.00.061.508 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.509 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.509 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.509 I ggml_metal_init: simdgroup reduction   = true
0.00.061.510 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.510 I ggml_metal_init: has bfloat            = true
0.00.061.510 I ggml_metal_init: use bfloat            = true
0.00.061.510 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.511 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.072.052 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.095.159 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.095.168 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.095.194 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.096.391 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.096.393 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.096.394 I llama_new_context_with_model: graph nodes  = 967
0.00.096.394 I llama_new_context_with_model: graph splits = 2
0.00.096.398 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.096.526 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.096.527 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.658.482 I main: llama threadpool init, n_threads = 4
0.00.658.527 I 
0.00.658.559 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.658.560 I 
0.00.658.785 I sampler seed: 1234
0.00.658.792 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.658.834 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.658.838 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.658.838 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.333.937 I llama_perf_sampler_print:    sampling time =       1.47 ms /    71 runs   (    0.02 ms per token, 48398.09 tokens per second)
0.01.333.938 I llama_perf_context_print:        load time =     646.54 ms
0.01.333.940 I llama_perf_context_print: prompt eval time =      47.06 ms /     7 tokens (    6.72 ms per token,   148.74 tokens per second)
0.01.333.941 I llama_perf_context_print:        eval time =     625.56 ms /    63 runs   (    9.93 ms per token,   100.71 tokens per second)
0.01.333.941 I llama_perf_context_print:       total time =     675.46 ms /    70 tokens
0.01.334.236 I ggml_metal_free: deallocating

real	0m1.352s
user	0m0.112s
sys	0m0.155s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4459 (ff3fcabc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.697 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.881 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.886 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.888 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.888 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.888 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.889 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.889 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.890 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.890 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.891 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.891 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.891 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.892 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.892 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.895 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.895 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.895 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.662 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.627 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.382 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.383 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.383 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.384 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.384 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.384 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.385 I llama_model_loader: - type  f32:  194 tensors
0.00.025.385 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.386 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.252 I llm_load_vocab: special tokens cache size = 25
0.00.051.208 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.210 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.211 I llm_load_print_meta: arch             = gptneox
0.00.051.211 I llm_load_print_meta: vocab type       = BPE
0.00.051.211 I llm_load_print_meta: n_vocab          = 50304
0.00.051.211 I llm_load_print_meta: n_merges         = 50009
0.00.051.212 I llm_load_print_meta: vocab_only       = 0
0.00.051.212 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.212 I llm_load_print_meta: n_embd           = 2048
0.00.051.212 I llm_load_print_meta: n_layer          = 24
0.00.051.214 I llm_load_print_meta: n_head           = 16
0.00.051.215 I llm_load_print_meta: n_head_kv        = 16
0.00.051.215 I llm_load_print_meta: n_rot            = 32
0.00.051.215 I llm_load_print_meta: n_swa            = 0
0.00.051.216 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.216 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.216 I llm_load_print_meta: n_gqa            = 1
0.00.051.217 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.218 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.218 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.219 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.219 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.219 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.221 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.221 I llm_load_print_meta: n_ff             = 8192
0.00.051.222 I llm_load_print_meta: n_expert         = 0
0.00.051.224 I llm_load_print_meta: n_expert_used    = 0
0.00.051.224 I llm_load_print_meta: causal attn      = 1
0.00.051.224 I llm_load_print_meta: pooling type     = 0
0.00.051.224 I llm_load_print_meta: rope type        = 2
0.00.051.224 I llm_load_print_meta: rope scaling     = linear
0.00.051.226 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.226 I llm_load_print_meta: freq_scale_train = 1
0.00.051.227 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.227 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.227 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.227 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.227 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.228 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.228 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.228 I llm_load_print_meta: model type       = 1.4B
0.00.051.239 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.240 I llm_load_print_meta: model params     = 1.41 B
0.00.051.240 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.240 I llm_load_print_meta: general.name     = 1.4B
0.00.051.241 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.241 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.241 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.241 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.242 I llm_load_print_meta: LF token         = 128 ''
0.00.051.242 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.242 I llm_load_print_meta: max token length = 1024
0.00.053.221 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.221 I llm_load_tensors: offloading output layer to GPU
0.00.053.221 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.232 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.233 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.053.702 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.703 I llama_new_context_with_model: n_ctx         = 128
0.00.053.703 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.704 I llama_new_context_with_model: n_batch       = 128
0.00.053.704 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.704 I llama_new_context_with_model: flash_attn    = 0
0.00.053.704 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.705 I llama_new_context_with_model: freq_scale    = 1
0.00.053.705 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.706 I ggml_metal_init: allocating
0.00.053.713 I ggml_metal_init: found device: Apple M4
0.00.053.716 I ggml_metal_init: picking default device: Apple M4
0.00.054.294 I ggml_metal_init: using embedded metal library
0.00.056.669 I ggml_metal_init: GPU name:   Apple M4
0.00.056.670 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.670 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.671 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.671 I ggml_metal_init: simdgroup reduction   = true
0.00.056.671 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.671 I ggml_metal_init: has bfloat            = true
0.00.056.672 I ggml_metal_init: use bfloat            = true
0.00.056.672 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.673 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.077 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.366 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.370 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.387 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.242 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.243 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.243 I llama_new_context_with_model: graph nodes  = 967
0.00.068.244 I llama_new_context_with_model: graph splits = 2
0.00.068.245 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.245 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.564.517 I 
0.00.564.546 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.564.557 I perplexity: tokenizing the input ..
0.00.572.351 I perplexity: tokenization took 7.792 ms
0.00.572.355 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.694.963 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.696.139 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.696.163 I llama_perf_context_print:        load time =     554.82 ms
0.00.696.164 I llama_perf_context_print: prompt eval time =     122.38 ms /   128 tokens (    0.96 ms per token,  1045.90 tokens per second)
0.00.696.165 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.696.165 I llama_perf_context_print:       total time =     131.65 ms /   129 tokens
0.00.696.647 I ggml_metal_free: deallocating

real	0m0.711s
user	0m0.078s
sys	0m0.089s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4459 (ff3fcabc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.008.623 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.826 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.831 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.833 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.833 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.834 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.834 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.834 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.835 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.835 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.836 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.836 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.836 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.837 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.837 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.839 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.839 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.839 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.718 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.740 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.574 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.575 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.576 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.576 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.576 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.577 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.577 I llama_model_loader: - type  f32:  194 tensors
0.00.026.578 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.578 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.526 I llm_load_vocab: special tokens cache size = 25
0.00.053.516 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.523 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.523 I llm_load_print_meta: arch             = gptneox
0.00.053.523 I llm_load_print_meta: vocab type       = BPE
0.00.053.524 I llm_load_print_meta: n_vocab          = 50304
0.00.053.524 I llm_load_print_meta: n_merges         = 50009
0.00.053.524 I llm_load_print_meta: vocab_only       = 0
0.00.053.524 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.524 I llm_load_print_meta: n_embd           = 2048
0.00.053.525 I llm_load_print_meta: n_layer          = 24
0.00.053.529 I llm_load_print_meta: n_head           = 16
0.00.053.529 I llm_load_print_meta: n_head_kv        = 16
0.00.053.529 I llm_load_print_meta: n_rot            = 32
0.00.053.530 I llm_load_print_meta: n_swa            = 0
0.00.053.530 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.530 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.531 I llm_load_print_meta: n_gqa            = 1
0.00.053.531 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.532 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.532 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.533 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.533 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.533 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.533 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.534 I llm_load_print_meta: n_ff             = 8192
0.00.053.534 I llm_load_print_meta: n_expert         = 0
0.00.053.534 I llm_load_print_meta: n_expert_used    = 0
0.00.053.534 I llm_load_print_meta: causal attn      = 1
0.00.053.534 I llm_load_print_meta: pooling type     = 0
0.00.053.534 I llm_load_print_meta: rope type        = 2
0.00.053.535 I llm_load_print_meta: rope scaling     = linear
0.00.053.535 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.535 I llm_load_print_meta: freq_scale_train = 1
0.00.053.535 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.535 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.536 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.536 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.536 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.536 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.536 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.536 I llm_load_print_meta: model type       = 1.4B
0.00.053.549 I llm_load_print_meta: model ftype      = Q4_1
0.00.053.550 I llm_load_print_meta: model params     = 1.41 B
0.00.053.551 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.053.551 I llm_load_print_meta: general.name     = 1.4B
0.00.053.551 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.551 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.552 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.552 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.552 I llm_load_print_meta: LF token         = 128 ''
0.00.053.552 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.552 I llm_load_print_meta: max token length = 1024
0.00.055.546 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.546 I llm_load_tensors: offloading output layer to GPU
0.00.055.546 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.557 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.055.558 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.055.994 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.995 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.995 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.995 I llama_new_context_with_model: n_batch       = 2048
0.00.055.996 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.996 I llama_new_context_with_model: flash_attn    = 0
0.00.055.996 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.996 I llama_new_context_with_model: freq_scale    = 1
0.00.055.997 I ggml_metal_init: allocating
0.00.056.000 I ggml_metal_init: found device: Apple M4
0.00.056.002 I ggml_metal_init: picking default device: Apple M4
0.00.056.613 I ggml_metal_init: using embedded metal library
0.00.059.044 I ggml_metal_init: GPU name:   Apple M4
0.00.059.045 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.046 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.046 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.046 I ggml_metal_init: simdgroup reduction   = true
0.00.059.047 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.047 I ggml_metal_init: has bfloat            = true
0.00.059.047 I ggml_metal_init: use bfloat            = true
0.00.059.047 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.048 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.236 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.337 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.350 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.378 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.315 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.316 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.316 I llama_new_context_with_model: graph nodes  = 967
0.00.089.316 I llama_new_context_with_model: graph splits = 2
0.00.089.319 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.465 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.465 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.675.834 I main: llama threadpool init, n_threads = 4
0.00.675.877 I 
0.00.675.930 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.675.932 I 
0.00.676.155 I sampler seed: 1234
0.00.676.163 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.676.193 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.676.195 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.676.195 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.394.387 I llama_perf_sampler_print:    sampling time =       1.06 ms /    71 runs   (    0.01 ms per token, 66854.99 tokens per second)
0.01.394.387 I llama_perf_context_print:        load time =     667.20 ms
0.01.394.389 I llama_perf_context_print: prompt eval time =      43.59 ms /     7 tokens (    6.23 ms per token,   160.57 tokens per second)
0.01.394.389 I llama_perf_context_print:        eval time =     671.79 ms /    63 runs   (   10.66 ms per token,    93.78 tokens per second)
0.01.394.390 I llama_perf_context_print:       total time =     718.56 ms /    70 tokens
0.01.394.649 I ggml_metal_free: deallocating

real	0m1.413s
user	0m0.111s
sys	0m0.150s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4459 (ff3fcabc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.862 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.845 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.850 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.855 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.856 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.856 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.857 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.859 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.860 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.860 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.860 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.861 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.861 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.861 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.862 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.863 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.864 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.864 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.671 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.659 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.431 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.432 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.432 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.433 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.433 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.433 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.434 I llama_model_loader: - type  f32:  194 tensors
0.00.024.434 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.434 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.444 I llm_load_vocab: special tokens cache size = 25
0.00.050.546 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.549 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.549 I llm_load_print_meta: arch             = gptneox
0.00.050.549 I llm_load_print_meta: vocab type       = BPE
0.00.050.549 I llm_load_print_meta: n_vocab          = 50304
0.00.050.550 I llm_load_print_meta: n_merges         = 50009
0.00.050.550 I llm_load_print_meta: vocab_only       = 0
0.00.050.550 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.550 I llm_load_print_meta: n_embd           = 2048
0.00.050.550 I llm_load_print_meta: n_layer          = 24
0.00.050.553 I llm_load_print_meta: n_head           = 16
0.00.050.554 I llm_load_print_meta: n_head_kv        = 16
0.00.050.554 I llm_load_print_meta: n_rot            = 32
0.00.050.554 I llm_load_print_meta: n_swa            = 0
0.00.050.554 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.554 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.555 I llm_load_print_meta: n_gqa            = 1
0.00.050.556 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.556 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.557 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.557 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.558 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.558 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.558 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.559 I llm_load_print_meta: n_ff             = 8192
0.00.050.559 I llm_load_print_meta: n_expert         = 0
0.00.050.559 I llm_load_print_meta: n_expert_used    = 0
0.00.050.559 I llm_load_print_meta: causal attn      = 1
0.00.050.559 I llm_load_print_meta: pooling type     = 0
0.00.050.559 I llm_load_print_meta: rope type        = 2
0.00.050.560 I llm_load_print_meta: rope scaling     = linear
0.00.050.560 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.561 I llm_load_print_meta: freq_scale_train = 1
0.00.050.561 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.561 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.563 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.563 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.563 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.564 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.564 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.564 I llm_load_print_meta: model type       = 1.4B
0.00.050.576 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.576 I llm_load_print_meta: model params     = 1.41 B
0.00.050.576 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.577 I llm_load_print_meta: general.name     = 1.4B
0.00.050.577 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.577 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.577 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.577 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.577 I llm_load_print_meta: LF token         = 128 ''
0.00.050.579 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.579 I llm_load_print_meta: max token length = 1024
0.00.052.506 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.506 I llm_load_tensors: offloading output layer to GPU
0.00.052.506 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.517 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.518 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.857 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.858 I llama_new_context_with_model: n_ctx         = 128
0.00.052.858 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.858 I llama_new_context_with_model: n_batch       = 128
0.00.052.858 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.858 I llama_new_context_with_model: flash_attn    = 0
0.00.052.859 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.859 I llama_new_context_with_model: freq_scale    = 1
0.00.052.859 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.860 I ggml_metal_init: allocating
0.00.052.862 I ggml_metal_init: found device: Apple M4
0.00.052.864 I ggml_metal_init: picking default device: Apple M4
0.00.053.408 I ggml_metal_init: using embedded metal library
0.00.055.719 I ggml_metal_init: GPU name:   Apple M4
0.00.055.721 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.721 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.721 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.722 I ggml_metal_init: simdgroup reduction   = true
0.00.055.722 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.722 I ggml_metal_init: has bfloat            = true
0.00.055.722 I ggml_metal_init: use bfloat            = true
0.00.055.722 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.723 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.253 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.538 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.540 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.554 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.465 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.466 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.466 I llama_new_context_with_model: graph nodes  = 967
0.00.067.466 I llama_new_context_with_model: graph splits = 2
0.00.067.467 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.468 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.597.011 I 
0.00.597.054 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.597.067 I perplexity: tokenizing the input ..
0.00.604.999 I perplexity: tokenization took 7.93 ms
0.00.605.002 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.727.778 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.729.000 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.729.022 I llama_perf_context_print:        load time =     588.14 ms
0.00.729.022 I llama_perf_context_print: prompt eval time =     122.55 ms /   128 tokens (    0.96 ms per token,  1044.47 tokens per second)
0.00.729.023 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.729.023 I llama_perf_context_print:       total time =     132.02 ms /   129 tokens
0.00.729.483 I ggml_metal_free: deallocating

real	0m0.742s
user	0m0.078s
sys	0m0.092s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4459 (ff3fcabc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.010.087 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.353 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.358 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.364 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.365 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.365 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.365 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.366 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.368 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.368 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.368 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.369 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.369 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.369 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.370 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.371 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.372 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.372 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.143 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.130 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.867 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.868 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.869 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.869 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.870 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.870 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.870 I llama_model_loader: - type  f32:  194 tensors
0.00.025.871 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.871 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.951 I llm_load_vocab: special tokens cache size = 25
0.00.052.073 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.075 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.076 I llm_load_print_meta: arch             = gptneox
0.00.052.076 I llm_load_print_meta: vocab type       = BPE
0.00.052.076 I llm_load_print_meta: n_vocab          = 50304
0.00.052.076 I llm_load_print_meta: n_merges         = 50009
0.00.052.076 I llm_load_print_meta: vocab_only       = 0
0.00.052.077 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.077 I llm_load_print_meta: n_embd           = 2048
0.00.052.077 I llm_load_print_meta: n_layer          = 24
0.00.052.080 I llm_load_print_meta: n_head           = 16
0.00.052.080 I llm_load_print_meta: n_head_kv        = 16
0.00.052.081 I llm_load_print_meta: n_rot            = 32
0.00.052.081 I llm_load_print_meta: n_swa            = 0
0.00.052.081 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.083 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.084 I llm_load_print_meta: n_gqa            = 1
0.00.052.084 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.085 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.086 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.086 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.086 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.086 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.087 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.087 I llm_load_print_meta: n_ff             = 8192
0.00.052.087 I llm_load_print_meta: n_expert         = 0
0.00.052.087 I llm_load_print_meta: n_expert_used    = 0
0.00.052.088 I llm_load_print_meta: causal attn      = 1
0.00.052.088 I llm_load_print_meta: pooling type     = 0
0.00.052.088 I llm_load_print_meta: rope type        = 2
0.00.052.088 I llm_load_print_meta: rope scaling     = linear
0.00.052.092 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.092 I llm_load_print_meta: freq_scale_train = 1
0.00.052.093 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.093 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.093 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.093 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.094 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.094 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.094 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.094 I llm_load_print_meta: model type       = 1.4B
0.00.052.107 I llm_load_print_meta: model ftype      = Q5_0
0.00.052.108 I llm_load_print_meta: model params     = 1.41 B
0.00.052.108 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.052.108 I llm_load_print_meta: general.name     = 1.4B
0.00.052.109 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.109 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.109 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.109 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.109 I llm_load_print_meta: LF token         = 128 ''
0.00.052.110 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.110 I llm_load_print_meta: max token length = 1024
0.00.054.141 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.141 I llm_load_tensors: offloading output layer to GPU
0.00.054.142 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.152 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.153 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.567 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.567 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.567 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.568 I llama_new_context_with_model: n_batch       = 2048
0.00.054.568 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.568 I llama_new_context_with_model: flash_attn    = 0
0.00.054.568 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.568 I llama_new_context_with_model: freq_scale    = 1
0.00.054.569 I ggml_metal_init: allocating
0.00.054.572 I ggml_metal_init: found device: Apple M4
0.00.054.574 I ggml_metal_init: picking default device: Apple M4
0.00.055.144 I ggml_metal_init: using embedded metal library
0.00.057.469 I ggml_metal_init: GPU name:   Apple M4
0.00.057.470 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.470 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.471 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.471 I ggml_metal_init: simdgroup reduction   = true
0.00.057.471 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.471 I ggml_metal_init: has bfloat            = true
0.00.057.471 I ggml_metal_init: use bfloat            = true
0.00.057.472 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.472 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.101 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.561 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.566 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.585 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.606 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.607 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.607 I llama_new_context_with_model: graph nodes  = 967
0.00.087.608 I llama_new_context_with_model: graph splits = 2
0.00.087.610 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.751 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.752 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.667.033 I main: llama threadpool init, n_threads = 4
0.00.667.067 I 
0.00.667.094 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.667.094 I 
0.00.667.304 I sampler seed: 1234
0.00.667.309 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.667.334 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.667.335 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.667.335 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.450.412 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60067.68 tokens per second)
0.01.450.413 I llama_perf_context_print:        load time =     656.94 ms
0.01.450.413 I llama_perf_context_print: prompt eval time =      43.13 ms /     7 tokens (    6.16 ms per token,   162.31 tokens per second)
0.01.450.414 I llama_perf_context_print:        eval time =     736.96 ms /    63 runs   (   11.70 ms per token,    85.49 tokens per second)
0.01.450.414 I llama_perf_context_print:       total time =     783.38 ms /    70 tokens
0.01.450.643 I ggml_metal_free: deallocating

real	0m1.468s
user	0m0.110s
sys	0m0.144s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4459 (ff3fcabc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.052 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.477 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.481 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.483 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.484 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.484 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.484 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.484 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.485 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.486 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.486 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.486 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.489 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.489 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.490 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.492 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.493 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.493 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.297 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.316 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.088 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.089 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.089 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.090 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.090 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.090 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.091 I llama_model_loader: - type  f32:  194 tensors
0.00.025.091 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.091 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.157 I llm_load_vocab: special tokens cache size = 25
0.00.051.138 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.140 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.141 I llm_load_print_meta: arch             = gptneox
0.00.051.141 I llm_load_print_meta: vocab type       = BPE
0.00.051.141 I llm_load_print_meta: n_vocab          = 50304
0.00.051.141 I llm_load_print_meta: n_merges         = 50009
0.00.051.142 I llm_load_print_meta: vocab_only       = 0
0.00.051.142 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.142 I llm_load_print_meta: n_embd           = 2048
0.00.051.142 I llm_load_print_meta: n_layer          = 24
0.00.051.145 I llm_load_print_meta: n_head           = 16
0.00.051.148 I llm_load_print_meta: n_head_kv        = 16
0.00.051.148 I llm_load_print_meta: n_rot            = 32
0.00.051.148 I llm_load_print_meta: n_swa            = 0
0.00.051.149 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.149 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.157 I llm_load_print_meta: n_gqa            = 1
0.00.051.158 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.160 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.161 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.161 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.161 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.161 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.162 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.163 I llm_load_print_meta: n_ff             = 8192
0.00.051.163 I llm_load_print_meta: n_expert         = 0
0.00.051.163 I llm_load_print_meta: n_expert_used    = 0
0.00.051.163 I llm_load_print_meta: causal attn      = 1
0.00.051.164 I llm_load_print_meta: pooling type     = 0
0.00.051.164 I llm_load_print_meta: rope type        = 2
0.00.051.164 I llm_load_print_meta: rope scaling     = linear
0.00.051.165 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.165 I llm_load_print_meta: freq_scale_train = 1
0.00.051.165 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.165 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.166 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.167 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.167 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.167 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.167 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.167 I llm_load_print_meta: model type       = 1.4B
0.00.051.179 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.179 I llm_load_print_meta: model params     = 1.41 B
0.00.051.180 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.180 I llm_load_print_meta: general.name     = 1.4B
0.00.051.180 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.180 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.180 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.181 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.181 I llm_load_print_meta: LF token         = 128 ''
0.00.051.181 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.181 I llm_load_print_meta: max token length = 1024
0.00.053.159 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.159 I llm_load_tensors: offloading output layer to GPU
0.00.053.159 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.170 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.171 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.553 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.554 I llama_new_context_with_model: n_ctx         = 128
0.00.053.554 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.555 I llama_new_context_with_model: n_batch       = 128
0.00.053.555 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.555 I llama_new_context_with_model: flash_attn    = 0
0.00.053.555 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.556 I llama_new_context_with_model: freq_scale    = 1
0.00.053.556 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.557 I ggml_metal_init: allocating
0.00.053.563 I ggml_metal_init: found device: Apple M4
0.00.053.565 I ggml_metal_init: picking default device: Apple M4
0.00.054.145 I ggml_metal_init: using embedded metal library
0.00.056.473 I ggml_metal_init: GPU name:   Apple M4
0.00.056.474 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.475 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.475 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.475 I ggml_metal_init: simdgroup reduction   = true
0.00.056.476 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.476 I ggml_metal_init: has bfloat            = true
0.00.056.476 I ggml_metal_init: use bfloat            = true
0.00.056.476 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.477 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.911 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.281 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.283 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.307 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.214 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.215 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.215 I llama_new_context_with_model: graph nodes  = 967
0.00.068.215 I llama_new_context_with_model: graph splits = 2
0.00.068.217 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.217 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.615.344 I 
0.00.615.382 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.615.394 I perplexity: tokenizing the input ..
0.00.622.832 I perplexity: tokenization took 7.435 ms
0.00.622.836 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.756.788 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.758.225 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.758.249 I llama_perf_context_print:        load time =     606.29 ms
0.00.758.250 I llama_perf_context_print: prompt eval time =     133.72 ms /   128 tokens (    1.04 ms per token,   957.22 tokens per second)
0.00.758.251 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.758.251 I llama_perf_context_print:       total time =     142.91 ms /   129 tokens
0.00.758.629 I ggml_metal_free: deallocating

real	0m0.772s
user	0m0.078s
sys	0m0.095s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4459 (ff3fcabc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.741 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.177 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.188 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.190 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.191 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.191 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.191 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.192 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.194 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.194 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.194 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.195 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.195 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.196 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.196 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.198 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.198 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.200 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.024 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.006 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.729 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.730 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.731 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.731 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.731 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.732 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.732 I llama_model_loader: - type  f32:  194 tensors
0.00.025.733 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.733 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.716 I llm_load_vocab: special tokens cache size = 25
0.00.052.942 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.945 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.945 I llm_load_print_meta: arch             = gptneox
0.00.052.945 I llm_load_print_meta: vocab type       = BPE
0.00.052.945 I llm_load_print_meta: n_vocab          = 50304
0.00.052.946 I llm_load_print_meta: n_merges         = 50009
0.00.052.946 I llm_load_print_meta: vocab_only       = 0
0.00.052.946 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.946 I llm_load_print_meta: n_embd           = 2048
0.00.052.946 I llm_load_print_meta: n_layer          = 24
0.00.052.950 I llm_load_print_meta: n_head           = 16
0.00.052.950 I llm_load_print_meta: n_head_kv        = 16
0.00.052.951 I llm_load_print_meta: n_rot            = 32
0.00.052.951 I llm_load_print_meta: n_swa            = 0
0.00.052.951 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.951 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.952 I llm_load_print_meta: n_gqa            = 1
0.00.052.953 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.953 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.954 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.954 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.954 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.954 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.954 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.956 I llm_load_print_meta: n_ff             = 8192
0.00.052.957 I llm_load_print_meta: n_expert         = 0
0.00.052.957 I llm_load_print_meta: n_expert_used    = 0
0.00.052.957 I llm_load_print_meta: causal attn      = 1
0.00.052.957 I llm_load_print_meta: pooling type     = 0
0.00.052.957 I llm_load_print_meta: rope type        = 2
0.00.052.959 I llm_load_print_meta: rope scaling     = linear
0.00.052.959 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.960 I llm_load_print_meta: freq_scale_train = 1
0.00.052.960 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.960 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.960 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.960 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.962 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.962 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.962 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.962 I llm_load_print_meta: model type       = 1.4B
0.00.052.974 I llm_load_print_meta: model ftype      = Q5_1
0.00.052.975 I llm_load_print_meta: model params     = 1.41 B
0.00.052.975 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.052.975 I llm_load_print_meta: general.name     = 1.4B
0.00.052.975 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.977 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.977 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.977 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.978 I llm_load_print_meta: LF token         = 128 ''
0.00.052.978 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.978 I llm_load_print_meta: max token length = 1024
0.00.055.072 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.073 I llm_load_tensors: offloading output layer to GPU
0.00.055.073 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.083 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.055.085 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.055.460 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.461 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.461 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.461 I llama_new_context_with_model: n_batch       = 2048
0.00.055.461 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.461 I llama_new_context_with_model: flash_attn    = 0
0.00.055.462 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.462 I llama_new_context_with_model: freq_scale    = 1
0.00.055.463 I ggml_metal_init: allocating
0.00.055.469 I ggml_metal_init: found device: Apple M4
0.00.055.471 I ggml_metal_init: picking default device: Apple M4
0.00.056.079 I ggml_metal_init: using embedded metal library
0.00.058.394 I ggml_metal_init: GPU name:   Apple M4
0.00.058.396 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.396 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.397 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.397 I ggml_metal_init: simdgroup reduction   = true
0.00.058.397 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.397 I ggml_metal_init: has bfloat            = true
0.00.058.397 I ggml_metal_init: use bfloat            = true
0.00.058.398 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.398 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.948 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.605 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.611 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.628 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.661 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.662 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.663 I llama_new_context_with_model: graph nodes  = 967
0.00.089.663 I llama_new_context_with_model: graph splits = 2
0.00.089.666 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.796 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.797 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.730.466 I main: llama threadpool init, n_threads = 4
0.00.730.503 I 
0.00.730.546 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.730.547 I 
0.00.730.769 I sampler seed: 1234
0.00.730.773 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.730.814 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.730.814 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.730.814 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.569.272 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59463.99 tokens per second)
0.01.569.273 I llama_perf_context_print:        load time =     721.72 ms
0.01.569.274 I llama_perf_context_print: prompt eval time =      42.19 ms /     7 tokens (    6.03 ms per token,   165.93 tokens per second)
0.01.569.278 I llama_perf_context_print:        eval time =     793.32 ms /    63 runs   (   12.59 ms per token,    79.41 tokens per second)
0.01.569.278 I llama_perf_context_print:       total time =     838.81 ms /    70 tokens
0.01.569.508 I ggml_metal_free: deallocating

real	0m1.586s
user	0m0.109s
sys	0m0.173s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4459 (ff3fcabc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.333 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.338 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.344 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.346 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.346 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.347 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.347 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.347 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.348 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.349 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.349 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.350 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.350 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.350 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.352 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.354 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.354 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.354 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.276 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.324 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.171 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.172 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.173 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.173 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.173 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.174 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.174 I llama_model_loader: - type  f32:  194 tensors
0.00.026.174 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.175 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.584 I llm_load_vocab: special tokens cache size = 25
0.00.053.548 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.553 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.553 I llm_load_print_meta: arch             = gptneox
0.00.053.553 I llm_load_print_meta: vocab type       = BPE
0.00.053.554 I llm_load_print_meta: n_vocab          = 50304
0.00.053.554 I llm_load_print_meta: n_merges         = 50009
0.00.053.554 I llm_load_print_meta: vocab_only       = 0
0.00.053.554 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.554 I llm_load_print_meta: n_embd           = 2048
0.00.053.555 I llm_load_print_meta: n_layer          = 24
0.00.053.559 I llm_load_print_meta: n_head           = 16
0.00.053.559 I llm_load_print_meta: n_head_kv        = 16
0.00.053.560 I llm_load_print_meta: n_rot            = 32
0.00.053.560 I llm_load_print_meta: n_swa            = 0
0.00.053.560 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.560 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.561 I llm_load_print_meta: n_gqa            = 1
0.00.053.562 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.562 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.563 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.563 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.563 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.563 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.564 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.564 I llm_load_print_meta: n_ff             = 8192
0.00.053.564 I llm_load_print_meta: n_expert         = 0
0.00.053.564 I llm_load_print_meta: n_expert_used    = 0
0.00.053.566 I llm_load_print_meta: causal attn      = 1
0.00.053.566 I llm_load_print_meta: pooling type     = 0
0.00.053.566 I llm_load_print_meta: rope type        = 2
0.00.053.566 I llm_load_print_meta: rope scaling     = linear
0.00.053.567 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.567 I llm_load_print_meta: freq_scale_train = 1
0.00.053.567 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.567 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.568 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.568 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.568 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.568 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.568 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.568 I llm_load_print_meta: model type       = 1.4B
0.00.053.583 I llm_load_print_meta: model ftype      = Q5_1
0.00.053.583 I llm_load_print_meta: model params     = 1.41 B
0.00.053.584 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.053.584 I llm_load_print_meta: general.name     = 1.4B
0.00.053.584 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.584 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.584 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.584 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.585 I llm_load_print_meta: LF token         = 128 ''
0.00.053.585 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.585 I llm_load_print_meta: max token length = 1024
0.00.055.568 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.568 I llm_load_tensors: offloading output layer to GPU
0.00.055.568 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.579 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.055.581 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.055.903 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.904 I llama_new_context_with_model: n_ctx         = 128
0.00.055.904 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.905 I llama_new_context_with_model: n_batch       = 128
0.00.055.905 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.905 I llama_new_context_with_model: flash_attn    = 0
0.00.055.905 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.906 I llama_new_context_with_model: freq_scale    = 1
0.00.055.906 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.906 I ggml_metal_init: allocating
0.00.055.910 I ggml_metal_init: found device: Apple M4
0.00.055.913 I ggml_metal_init: picking default device: Apple M4
0.00.056.513 I ggml_metal_init: using embedded metal library
0.00.058.940 I ggml_metal_init: GPU name:   Apple M4
0.00.058.941 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.942 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.942 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.942 I ggml_metal_init: simdgroup reduction   = true
0.00.058.942 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.942 I ggml_metal_init: has bfloat            = true
0.00.058.943 I ggml_metal_init: use bfloat            = true
0.00.058.943 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.944 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.098 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.070.402 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.070.408 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.070.425 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.071.472 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.071.474 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.071.474 I llama_new_context_with_model: graph nodes  = 967
0.00.071.474 I llama_new_context_with_model: graph splits = 2
0.00.071.476 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.071.476 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.668.500 I 
0.00.668.531 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.668.544 I perplexity: tokenizing the input ..
0.00.675.855 I perplexity: tokenization took 7.309 ms
0.00.675.861 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.810.271 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.811.723 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.811.741 I llama_perf_context_print:        load time =     658.16 ms
0.00.811.742 I llama_perf_context_print: prompt eval time =     134.18 ms /   128 tokens (    1.05 ms per token,   953.93 tokens per second)
0.00.811.742 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.811.743 I llama_perf_context_print:       total time =     143.24 ms /   129 tokens
0.00.812.145 I ggml_metal_free: deallocating

real	0m0.829s
user	0m0.081s
sys	0m0.100s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4459 (ff3fcabc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.010.092 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.695 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.700 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.703 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.703 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.704 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.704 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.704 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.707 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.707 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.707 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.708 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.708 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.708 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.709 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.711 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.711 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.711 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.491 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.486 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.282 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.284 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.284 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.284 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.285 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.285 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.286 I llama_model_loader: - type  f32:  194 tensors
0.00.025.286 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.286 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.286 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.401 I llm_load_vocab: special tokens cache size = 25
0.00.051.439 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.442 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.442 I llm_load_print_meta: arch             = gptneox
0.00.051.443 I llm_load_print_meta: vocab type       = BPE
0.00.051.443 I llm_load_print_meta: n_vocab          = 50304
0.00.051.443 I llm_load_print_meta: n_merges         = 50009
0.00.051.443 I llm_load_print_meta: vocab_only       = 0
0.00.051.443 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.444 I llm_load_print_meta: n_embd           = 2048
0.00.051.444 I llm_load_print_meta: n_layer          = 24
0.00.051.446 I llm_load_print_meta: n_head           = 16
0.00.051.447 I llm_load_print_meta: n_head_kv        = 16
0.00.051.448 I llm_load_print_meta: n_rot            = 32
0.00.051.448 I llm_load_print_meta: n_swa            = 0
0.00.051.448 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.449 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.450 I llm_load_print_meta: n_gqa            = 1
0.00.051.450 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.451 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.452 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.452 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.452 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.452 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.452 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.453 I llm_load_print_meta: n_ff             = 8192
0.00.051.453 I llm_load_print_meta: n_expert         = 0
0.00.051.453 I llm_load_print_meta: n_expert_used    = 0
0.00.051.454 I llm_load_print_meta: causal attn      = 1
0.00.051.454 I llm_load_print_meta: pooling type     = 0
0.00.051.454 I llm_load_print_meta: rope type        = 2
0.00.051.454 I llm_load_print_meta: rope scaling     = linear
0.00.051.455 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.455 I llm_load_print_meta: freq_scale_train = 1
0.00.051.455 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.455 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.457 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.457 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.457 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.457 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.458 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.458 I llm_load_print_meta: model type       = 1.4B
0.00.051.470 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.471 I llm_load_print_meta: model params     = 1.41 B
0.00.051.471 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.471 I llm_load_print_meta: general.name     = 1.4B
0.00.051.471 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.472 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.472 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.472 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.472 I llm_load_print_meta: LF token         = 128 ''
0.00.051.472 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.472 I llm_load_print_meta: max token length = 1024
0.00.053.319 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.319 I llm_load_tensors: offloading output layer to GPU
0.00.053.319 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.330 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.331 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.654 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.655 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.655 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.655 I llama_new_context_with_model: n_batch       = 2048
0.00.053.655 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.655 I llama_new_context_with_model: flash_attn    = 0
0.00.053.656 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.656 I llama_new_context_with_model: freq_scale    = 1
0.00.053.656 I ggml_metal_init: allocating
0.00.053.663 I ggml_metal_init: found device: Apple M4
0.00.053.665 I ggml_metal_init: picking default device: Apple M4
0.00.054.291 I ggml_metal_init: using embedded metal library
0.00.056.642 I ggml_metal_init: GPU name:   Apple M4
0.00.056.644 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.644 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.644 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.645 I ggml_metal_init: simdgroup reduction   = true
0.00.056.645 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.645 I ggml_metal_init: has bfloat            = true
0.00.056.645 I ggml_metal_init: use bfloat            = true
0.00.056.646 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.646 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.197 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.798 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.804 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.823 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.863 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.864 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.864 I llama_new_context_with_model: graph nodes  = 967
0.00.085.864 I llama_new_context_with_model: graph splits = 2
0.00.085.867 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.015 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.015 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.460.417 I main: llama threadpool init, n_threads = 4
0.00.460.471 I 
0.00.460.505 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.460.505 I 
0.00.460.750 I sampler seed: 1234
0.00.460.756 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.460.776 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.460.777 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.460.777 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.139.324 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56438.79 tokens per second)
0.01.139.325 I llama_perf_context_print:        load time =     450.32 ms
0.01.139.326 I llama_perf_context_print: prompt eval time =      35.55 ms /     7 tokens (    5.08 ms per token,   196.92 tokens per second)
0.01.139.327 I llama_perf_context_print:        eval time =     640.13 ms /    63 runs   (   10.16 ms per token,    98.42 tokens per second)
0.01.139.327 I llama_perf_context_print:       total time =     678.91 ms /    70 tokens
0.01.139.629 I ggml_metal_free: deallocating

real	0m1.157s
user	0m0.108s
sys	0m0.108s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4459 (ff3fcabc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.726 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.941 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.948 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.950 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.950 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.951 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.951 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.951 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.952 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.953 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.953 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.953 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.954 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.954 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.955 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.957 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.957 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.957 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.863 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.930 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.881 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.883 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.883 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.884 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.884 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.884 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.885 I llama_model_loader: - type  f32:  194 tensors
0.00.024.885 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.886 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.886 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.973 I llm_load_vocab: special tokens cache size = 25
0.00.053.143 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.148 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.148 I llm_load_print_meta: arch             = gptneox
0.00.053.148 I llm_load_print_meta: vocab type       = BPE
0.00.053.149 I llm_load_print_meta: n_vocab          = 50304
0.00.053.149 I llm_load_print_meta: n_merges         = 50009
0.00.053.149 I llm_load_print_meta: vocab_only       = 0
0.00.053.149 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.149 I llm_load_print_meta: n_embd           = 2048
0.00.053.149 I llm_load_print_meta: n_layer          = 24
0.00.053.153 I llm_load_print_meta: n_head           = 16
0.00.053.154 I llm_load_print_meta: n_head_kv        = 16
0.00.053.154 I llm_load_print_meta: n_rot            = 32
0.00.053.155 I llm_load_print_meta: n_swa            = 0
0.00.053.155 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.155 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.156 I llm_load_print_meta: n_gqa            = 1
0.00.053.156 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.159 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.159 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.160 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.160 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.160 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.160 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.161 I llm_load_print_meta: n_ff             = 8192
0.00.053.161 I llm_load_print_meta: n_expert         = 0
0.00.053.161 I llm_load_print_meta: n_expert_used    = 0
0.00.053.161 I llm_load_print_meta: causal attn      = 1
0.00.053.161 I llm_load_print_meta: pooling type     = 0
0.00.053.162 I llm_load_print_meta: rope type        = 2
0.00.053.162 I llm_load_print_meta: rope scaling     = linear
0.00.053.162 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.162 I llm_load_print_meta: freq_scale_train = 1
0.00.053.162 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.163 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.163 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.165 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.165 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.165 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.165 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.165 I llm_load_print_meta: model type       = 1.4B
0.00.053.178 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.053.179 I llm_load_print_meta: model params     = 1.41 B
0.00.053.179 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.053.179 I llm_load_print_meta: general.name     = 1.4B
0.00.053.180 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.180 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.180 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.181 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.182 I llm_load_print_meta: LF token         = 128 ''
0.00.053.182 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.182 I llm_load_print_meta: max token length = 1024
0.00.055.139 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.140 I llm_load_tensors: offloading output layer to GPU
0.00.055.140 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.151 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.055.153 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.055.479 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.480 I llama_new_context_with_model: n_ctx         = 128
0.00.055.480 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.480 I llama_new_context_with_model: n_batch       = 128
0.00.055.480 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.480 I llama_new_context_with_model: flash_attn    = 0
0.00.055.481 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.481 I llama_new_context_with_model: freq_scale    = 1
0.00.055.482 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.482 I ggml_metal_init: allocating
0.00.055.487 I ggml_metal_init: found device: Apple M4
0.00.055.490 I ggml_metal_init: picking default device: Apple M4
0.00.056.078 I ggml_metal_init: using embedded metal library
0.00.058.417 I ggml_metal_init: GPU name:   Apple M4
0.00.058.419 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.419 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.420 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.420 I ggml_metal_init: simdgroup reduction   = true
0.00.058.420 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.420 I ggml_metal_init: has bfloat            = true
0.00.058.420 I ggml_metal_init: use bfloat            = true
0.00.058.421 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.422 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.724 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.070.103 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.070.107 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.070.122 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.981 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.983 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.983 I llama_new_context_with_model: graph nodes  = 967
0.00.070.983 I llama_new_context_with_model: graph splits = 2
0.00.070.984 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.985 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.414.128 I 
0.00.414.163 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.414.176 I perplexity: tokenizing the input ..
0.00.422.132 I perplexity: tokenization took 7.955 ms
0.00.422.136 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.554.362 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.555.511 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.555.534 I llama_perf_context_print:        load time =     405.39 ms
0.00.555.535 I llama_perf_context_print: prompt eval time =     131.99 ms /   128 tokens (    1.03 ms per token,   969.79 tokens per second)
0.00.555.536 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.555.536 I llama_perf_context_print:       total time =     141.41 ms /   129 tokens
0.00.555.982 I ggml_metal_free: deallocating

real	0m0.570s
user	0m0.082s
sys	0m0.071s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4459 (ff3fcabc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.009.486 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.236 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.240 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.242 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.244 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.244 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.244 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.244 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.248 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.248 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.248 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.249 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.249 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.249 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.254 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.258 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.258 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.259 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.122 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.108 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.908 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.909 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.909 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.910 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.910 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.910 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.911 I llama_model_loader: - type  f32:  194 tensors
0.00.025.911 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.911 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.911 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.912 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.945 I llm_load_vocab: special tokens cache size = 25
0.00.051.761 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.764 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.764 I llm_load_print_meta: arch             = gptneox
0.00.051.764 I llm_load_print_meta: vocab type       = BPE
0.00.051.765 I llm_load_print_meta: n_vocab          = 50304
0.00.051.765 I llm_load_print_meta: n_merges         = 50009
0.00.051.765 I llm_load_print_meta: vocab_only       = 0
0.00.051.765 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.765 I llm_load_print_meta: n_embd           = 2048
0.00.051.765 I llm_load_print_meta: n_layer          = 24
0.00.051.768 I llm_load_print_meta: n_head           = 16
0.00.051.769 I llm_load_print_meta: n_head_kv        = 16
0.00.051.769 I llm_load_print_meta: n_rot            = 32
0.00.051.769 I llm_load_print_meta: n_swa            = 0
0.00.051.770 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.770 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.771 I llm_load_print_meta: n_gqa            = 1
0.00.051.771 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.772 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.772 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.774 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.774 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.774 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.775 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.775 I llm_load_print_meta: n_ff             = 8192
0.00.051.777 I llm_load_print_meta: n_expert         = 0
0.00.051.778 I llm_load_print_meta: n_expert_used    = 0
0.00.051.778 I llm_load_print_meta: causal attn      = 1
0.00.051.778 I llm_load_print_meta: pooling type     = 0
0.00.051.779 I llm_load_print_meta: rope type        = 2
0.00.051.779 I llm_load_print_meta: rope scaling     = linear
0.00.051.779 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.780 I llm_load_print_meta: freq_scale_train = 1
0.00.051.780 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.780 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.780 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.780 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.780 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.780 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.781 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.781 I llm_load_print_meta: model type       = 1.4B
0.00.051.793 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.793 I llm_load_print_meta: model params     = 1.41 B
0.00.051.793 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.794 I llm_load_print_meta: general.name     = 1.4B
0.00.051.794 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.794 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.794 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.794 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.794 I llm_load_print_meta: LF token         = 128 ''
0.00.051.795 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.795 I llm_load_print_meta: max token length = 1024
0.00.053.707 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.707 I llm_load_tensors: offloading output layer to GPU
0.00.053.707 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.718 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.719 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.057 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.058 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.058 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.058 I llama_new_context_with_model: n_batch       = 2048
0.00.054.059 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.059 I llama_new_context_with_model: flash_attn    = 0
0.00.054.059 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.059 I llama_new_context_with_model: freq_scale    = 1
0.00.054.060 I ggml_metal_init: allocating
0.00.054.066 I ggml_metal_init: found device: Apple M4
0.00.054.068 I ggml_metal_init: picking default device: Apple M4
0.00.054.691 I ggml_metal_init: using embedded metal library
0.00.057.023 I ggml_metal_init: GPU name:   Apple M4
0.00.057.027 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.027 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.027 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.028 I ggml_metal_init: simdgroup reduction   = true
0.00.057.028 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.028 I ggml_metal_init: has bfloat            = true
0.00.057.028 I ggml_metal_init: use bfloat            = true
0.00.057.028 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.030 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.661 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.635 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.644 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.668 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.656 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.658 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.658 I llama_new_context_with_model: graph nodes  = 967
0.00.087.658 I llama_new_context_with_model: graph splits = 2
0.00.087.661 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.801 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.802 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.531.826 I main: llama threadpool init, n_threads = 4
0.00.531.869 I 
0.00.531.893 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.531.893 I 
0.00.532.127 I sampler seed: 1234
0.00.532.132 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.532.142 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.532.143 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.532.143 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.281.979 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57817.59 tokens per second)
0.01.281.979 I llama_perf_context_print:        load time =     522.33 ms
0.01.281.980 I llama_perf_context_print: prompt eval time =      44.39 ms /     7 tokens (    6.34 ms per token,   157.69 tokens per second)
0.01.281.981 I llama_perf_context_print:        eval time =     702.34 ms /    63 runs   (   11.15 ms per token,    89.70 tokens per second)
0.01.281.981 I llama_perf_context_print:       total time =     750.16 ms /    70 tokens
0.01.282.177 I ggml_metal_free: deallocating

real	0m1.299s
user	0m0.110s
sys	0m0.123s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4459 (ff3fcabc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.667 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.878 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.884 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.885 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.886 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.886 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.886 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.887 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.888 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.888 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.888 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.889 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.889 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.889 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.890 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.891 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.892 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.892 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.675 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.671 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.473 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.474 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.474 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.475 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.475 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.475 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.476 I llama_model_loader: - type  f32:  194 tensors
0.00.024.476 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.476 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.476 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.476 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.543 I llm_load_vocab: special tokens cache size = 25
0.00.050.674 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.676 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.677 I llm_load_print_meta: arch             = gptneox
0.00.050.677 I llm_load_print_meta: vocab type       = BPE
0.00.050.677 I llm_load_print_meta: n_vocab          = 50304
0.00.050.678 I llm_load_print_meta: n_merges         = 50009
0.00.050.678 I llm_load_print_meta: vocab_only       = 0
0.00.050.678 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.678 I llm_load_print_meta: n_embd           = 2048
0.00.050.678 I llm_load_print_meta: n_layer          = 24
0.00.050.681 I llm_load_print_meta: n_head           = 16
0.00.050.683 I llm_load_print_meta: n_head_kv        = 16
0.00.050.683 I llm_load_print_meta: n_rot            = 32
0.00.050.683 I llm_load_print_meta: n_swa            = 0
0.00.050.683 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.684 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.684 I llm_load_print_meta: n_gqa            = 1
0.00.050.685 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.686 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.686 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.687 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.688 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.688 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.688 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.689 I llm_load_print_meta: n_ff             = 8192
0.00.050.689 I llm_load_print_meta: n_expert         = 0
0.00.050.689 I llm_load_print_meta: n_expert_used    = 0
0.00.050.690 I llm_load_print_meta: causal attn      = 1
0.00.050.690 I llm_load_print_meta: pooling type     = 0
0.00.050.690 I llm_load_print_meta: rope type        = 2
0.00.050.690 I llm_load_print_meta: rope scaling     = linear
0.00.050.690 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.691 I llm_load_print_meta: freq_scale_train = 1
0.00.050.691 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.691 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.691 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.691 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.693 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.693 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.694 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.694 I llm_load_print_meta: model type       = 1.4B
0.00.050.705 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.707 I llm_load_print_meta: model params     = 1.41 B
0.00.050.707 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.707 I llm_load_print_meta: general.name     = 1.4B
0.00.050.708 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.708 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.708 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.708 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.708 I llm_load_print_meta: LF token         = 128 ''
0.00.050.708 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.710 I llm_load_print_meta: max token length = 1024
0.00.052.587 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.587 I llm_load_tensors: offloading output layer to GPU
0.00.052.587 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.598 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.599 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.917 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.918 I llama_new_context_with_model: n_ctx         = 128
0.00.052.918 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.918 I llama_new_context_with_model: n_batch       = 128
0.00.052.918 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.918 I llama_new_context_with_model: flash_attn    = 0
0.00.052.919 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.919 I llama_new_context_with_model: freq_scale    = 1
0.00.052.919 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.920 I ggml_metal_init: allocating
0.00.052.922 I ggml_metal_init: found device: Apple M4
0.00.052.924 I ggml_metal_init: picking default device: Apple M4
0.00.053.476 I ggml_metal_init: using embedded metal library
0.00.055.810 I ggml_metal_init: GPU name:   Apple M4
0.00.055.812 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.812 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.812 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.813 I ggml_metal_init: simdgroup reduction   = true
0.00.055.813 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.813 I ggml_metal_init: has bfloat            = true
0.00.055.813 I ggml_metal_init: use bfloat            = true
0.00.055.814 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.814 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.375 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.142 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.159 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.184 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.055 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.056 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.056 I llama_new_context_with_model: graph nodes  = 967
0.00.068.057 I llama_new_context_with_model: graph splits = 2
0.00.068.058 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.058 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.476.228 I 
0.00.476.263 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.476.273 I perplexity: tokenizing the input ..
0.00.484.359 I perplexity: tokenization took 8.084 ms
0.00.484.362 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.615.604 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.616.796 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.616.821 I llama_perf_context_print:        load time =     467.55 ms
0.00.616.823 I llama_perf_context_print: prompt eval time =     131.02 ms /   128 tokens (    1.02 ms per token,   976.99 tokens per second)
0.00.616.823 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.616.824 I llama_perf_context_print:       total time =     140.60 ms /   129 tokens
0.00.617.281 I ggml_metal_free: deallocating

real	0m0.631s
user	0m0.079s
sys	0m0.085s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4459 (ff3fcabc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.010.723 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.247 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.018.257 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.259 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.261 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.261 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.261 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.262 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.263 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.263 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.263 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.264 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.264 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.265 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.265 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.267 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.267 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.267 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.048 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.085 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.816 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.817 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.818 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.818 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.818 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.819 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.819 I llama_model_loader: - type  f32:  194 tensors
0.00.026.819 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.820 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.820 I llama_model_loader: - type q6_K:   13 tensors
0.00.046.945 I llm_load_vocab: special tokens cache size = 25
0.00.053.004 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.007 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.008 I llm_load_print_meta: arch             = gptneox
0.00.053.008 I llm_load_print_meta: vocab type       = BPE
0.00.053.008 I llm_load_print_meta: n_vocab          = 50304
0.00.053.008 I llm_load_print_meta: n_merges         = 50009
0.00.053.009 I llm_load_print_meta: vocab_only       = 0
0.00.053.009 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.009 I llm_load_print_meta: n_embd           = 2048
0.00.053.009 I llm_load_print_meta: n_layer          = 24
0.00.053.012 I llm_load_print_meta: n_head           = 16
0.00.053.013 I llm_load_print_meta: n_head_kv        = 16
0.00.053.013 I llm_load_print_meta: n_rot            = 32
0.00.053.013 I llm_load_print_meta: n_swa            = 0
0.00.053.015 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.016 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.016 I llm_load_print_meta: n_gqa            = 1
0.00.053.017 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.018 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.018 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.020 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.020 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.020 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.021 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.021 I llm_load_print_meta: n_ff             = 8192
0.00.053.021 I llm_load_print_meta: n_expert         = 0
0.00.053.022 I llm_load_print_meta: n_expert_used    = 0
0.00.053.022 I llm_load_print_meta: causal attn      = 1
0.00.053.022 I llm_load_print_meta: pooling type     = 0
0.00.053.022 I llm_load_print_meta: rope type        = 2
0.00.053.022 I llm_load_print_meta: rope scaling     = linear
0.00.053.024 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.024 I llm_load_print_meta: freq_scale_train = 1
0.00.053.024 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.025 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.025 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.025 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.025 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.025 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.025 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.026 I llm_load_print_meta: model type       = 1.4B
0.00.053.040 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.053.041 I llm_load_print_meta: model params     = 1.41 B
0.00.053.041 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.053.041 I llm_load_print_meta: general.name     = 1.4B
0.00.053.042 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.042 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.042 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.042 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.042 I llm_load_print_meta: LF token         = 128 ''
0.00.053.043 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.043 I llm_load_print_meta: max token length = 1024
0.00.054.984 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.984 I llm_load_tensors: offloading output layer to GPU
0.00.054.985 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.995 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.054.996 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.055.341 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.342 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.342 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.342 I llama_new_context_with_model: n_batch       = 2048
0.00.055.342 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.342 I llama_new_context_with_model: flash_attn    = 0
0.00.055.343 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.343 I llama_new_context_with_model: freq_scale    = 1
0.00.055.343 I ggml_metal_init: allocating
0.00.055.346 I ggml_metal_init: found device: Apple M4
0.00.055.348 I ggml_metal_init: picking default device: Apple M4
0.00.055.920 I ggml_metal_init: using embedded metal library
0.00.058.226 I ggml_metal_init: GPU name:   Apple M4
0.00.058.228 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.228 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.228 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.229 I ggml_metal_init: simdgroup reduction   = true
0.00.058.229 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.229 I ggml_metal_init: has bfloat            = true
0.00.058.229 I ggml_metal_init: use bfloat            = true
0.00.058.229 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.230 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.892 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.782 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.788 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.812 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.908 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.909 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.909 I llama_new_context_with_model: graph nodes  = 967
0.00.087.910 I llama_new_context_with_model: graph splits = 2
0.00.087.918 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.052 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.053 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.629.244 I main: llama threadpool init, n_threads = 4
0.00.629.289 I 
0.00.629.313 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.629.313 I 
0.00.629.549 I sampler seed: 1234
0.00.629.554 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.629.565 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.629.565 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.629.565 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.383.274 I llama_perf_sampler_print:    sampling time =       1.51 ms /    71 runs   (    0.02 ms per token, 47144.75 tokens per second)
0.01.383.276 I llama_perf_context_print:        load time =     618.51 ms
0.01.383.277 I llama_perf_context_print: prompt eval time =      47.07 ms /     7 tokens (    6.72 ms per token,   148.72 tokens per second)
0.01.383.278 I llama_perf_context_print:        eval time =     703.91 ms /    63 runs   (   11.17 ms per token,    89.50 tokens per second)
0.01.383.278 I llama_perf_context_print:       total time =     754.03 ms /    70 tokens
0.01.383.562 I ggml_metal_free: deallocating

real	0m1.402s
user	0m0.110s
sys	0m0.153s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4459 (ff3fcabc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.381 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.564 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.569 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.571 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.572 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.572 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.572 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.573 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.573 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.574 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.574 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.575 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.575 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.575 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.578 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.581 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.581 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.581 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.412 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.382 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.139 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.140 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.140 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.141 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.141 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.141 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.142 I llama_model_loader: - type  f32:  194 tensors
0.00.026.142 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.142 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.142 I llama_model_loader: - type q6_K:   13 tensors
0.00.046.782 I llm_load_vocab: special tokens cache size = 25
0.00.052.879 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.882 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.882 I llm_load_print_meta: arch             = gptneox
0.00.052.882 I llm_load_print_meta: vocab type       = BPE
0.00.052.883 I llm_load_print_meta: n_vocab          = 50304
0.00.052.883 I llm_load_print_meta: n_merges         = 50009
0.00.052.883 I llm_load_print_meta: vocab_only       = 0
0.00.052.883 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.883 I llm_load_print_meta: n_embd           = 2048
0.00.052.884 I llm_load_print_meta: n_layer          = 24
0.00.052.886 I llm_load_print_meta: n_head           = 16
0.00.052.887 I llm_load_print_meta: n_head_kv        = 16
0.00.052.887 I llm_load_print_meta: n_rot            = 32
0.00.052.887 I llm_load_print_meta: n_swa            = 0
0.00.052.888 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.888 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.889 I llm_load_print_meta: n_gqa            = 1
0.00.052.889 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.890 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.891 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.891 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.891 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.891 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.891 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.892 I llm_load_print_meta: n_ff             = 8192
0.00.052.892 I llm_load_print_meta: n_expert         = 0
0.00.052.893 I llm_load_print_meta: n_expert_used    = 0
0.00.052.893 I llm_load_print_meta: causal attn      = 1
0.00.052.893 I llm_load_print_meta: pooling type     = 0
0.00.052.893 I llm_load_print_meta: rope type        = 2
0.00.052.893 I llm_load_print_meta: rope scaling     = linear
0.00.052.894 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.894 I llm_load_print_meta: freq_scale_train = 1
0.00.052.894 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.894 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.894 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.895 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.895 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.895 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.895 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.895 I llm_load_print_meta: model type       = 1.4B
0.00.052.907 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.052.908 I llm_load_print_meta: model params     = 1.41 B
0.00.052.908 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.052.908 I llm_load_print_meta: general.name     = 1.4B
0.00.052.909 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.909 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.909 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.909 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.909 I llm_load_print_meta: LF token         = 128 ''
0.00.052.909 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.909 I llm_load_print_meta: max token length = 1024
0.00.054.926 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.926 I llm_load_tensors: offloading output layer to GPU
0.00.054.926 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.936 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.054.938 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.055.270 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.270 I llama_new_context_with_model: n_ctx         = 128
0.00.055.270 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.271 I llama_new_context_with_model: n_batch       = 128
0.00.055.271 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.271 I llama_new_context_with_model: flash_attn    = 0
0.00.055.271 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.271 I llama_new_context_with_model: freq_scale    = 1
0.00.055.272 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.272 I ggml_metal_init: allocating
0.00.055.276 I ggml_metal_init: found device: Apple M4
0.00.055.277 I ggml_metal_init: picking default device: Apple M4
0.00.055.845 I ggml_metal_init: using embedded metal library
0.00.058.287 I ggml_metal_init: GPU name:   Apple M4
0.00.058.288 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.289 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.289 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.289 I ggml_metal_init: simdgroup reduction   = true
0.00.058.289 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.290 I ggml_metal_init: has bfloat            = true
0.00.058.290 I ggml_metal_init: use bfloat            = true
0.00.058.290 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.291 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.965 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.069.625 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.627 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.641 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.489 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.490 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.490 I llama_new_context_with_model: graph nodes  = 967
0.00.070.491 I llama_new_context_with_model: graph splits = 2
0.00.070.492 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.492 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.572.610 I 
0.00.572.642 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.572.654 I perplexity: tokenizing the input ..
0.00.580.488 I perplexity: tokenization took 7.832 ms
0.00.580.491 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.714.960 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.716.124 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.716.158 I llama_perf_context_print:        load time =     562.22 ms
0.00.716.159 I llama_perf_context_print: prompt eval time =     134.24 ms /   128 tokens (    1.05 ms per token,   953.50 tokens per second)
0.00.716.160 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.716.160 I llama_perf_context_print:       total time =     143.55 ms /   129 tokens
0.00.716.666 I ggml_metal_free: deallocating

real	0m0.732s
user	0m0.079s
sys	0m0.116s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4459 (ff3fcabc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.011.380 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.718 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.723 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.724 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.730 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.731 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.731 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.731 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.732 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.732 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.733 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.734 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.734 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.735 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.735 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.737 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.737 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.737 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.514 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.556 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.316 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.317 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.317 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.317 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.318 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.318 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.027.318 I llama_model_loader: - type  f32:  194 tensors
0.00.027.319 I llama_model_loader: - type q5_K:   61 tensors
0.00.027.319 I llama_model_loader: - type q6_K:   37 tensors
0.00.047.437 I llm_load_vocab: special tokens cache size = 25
0.00.053.513 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.516 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.517 I llm_load_print_meta: arch             = gptneox
0.00.053.517 I llm_load_print_meta: vocab type       = BPE
0.00.053.517 I llm_load_print_meta: n_vocab          = 50304
0.00.053.517 I llm_load_print_meta: n_merges         = 50009
0.00.053.517 I llm_load_print_meta: vocab_only       = 0
0.00.053.518 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.518 I llm_load_print_meta: n_embd           = 2048
0.00.053.518 I llm_load_print_meta: n_layer          = 24
0.00.053.521 I llm_load_print_meta: n_head           = 16
0.00.053.522 I llm_load_print_meta: n_head_kv        = 16
0.00.053.522 I llm_load_print_meta: n_rot            = 32
0.00.053.525 I llm_load_print_meta: n_swa            = 0
0.00.053.525 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.525 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.526 I llm_load_print_meta: n_gqa            = 1
0.00.053.526 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.527 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.528 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.528 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.528 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.528 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.529 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.529 I llm_load_print_meta: n_ff             = 8192
0.00.053.529 I llm_load_print_meta: n_expert         = 0
0.00.053.530 I llm_load_print_meta: n_expert_used    = 0
0.00.053.530 I llm_load_print_meta: causal attn      = 1
0.00.053.530 I llm_load_print_meta: pooling type     = 0
0.00.053.530 I llm_load_print_meta: rope type        = 2
0.00.053.531 I llm_load_print_meta: rope scaling     = linear
0.00.053.533 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.533 I llm_load_print_meta: freq_scale_train = 1
0.00.053.533 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.534 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.534 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.534 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.534 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.534 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.534 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.535 I llm_load_print_meta: model type       = 1.4B
0.00.053.546 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.053.547 I llm_load_print_meta: model params     = 1.41 B
0.00.053.547 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.053.547 I llm_load_print_meta: general.name     = 1.4B
0.00.053.548 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.548 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.548 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.548 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.548 I llm_load_print_meta: LF token         = 128 ''
0.00.053.549 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.549 I llm_load_print_meta: max token length = 1024
0.00.055.550 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.550 I llm_load_tensors: offloading output layer to GPU
0.00.055.551 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.561 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.055.562 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.055.882 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.882 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.882 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.883 I llama_new_context_with_model: n_batch       = 2048
0.00.055.883 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.883 I llama_new_context_with_model: flash_attn    = 0
0.00.055.883 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.884 I llama_new_context_with_model: freq_scale    = 1
0.00.055.884 I ggml_metal_init: allocating
0.00.055.887 I ggml_metal_init: found device: Apple M4
0.00.055.889 I ggml_metal_init: picking default device: Apple M4
0.00.056.455 I ggml_metal_init: using embedded metal library
0.00.058.785 I ggml_metal_init: GPU name:   Apple M4
0.00.058.787 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.787 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.787 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.787 I ggml_metal_init: simdgroup reduction   = true
0.00.058.788 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.788 I ggml_metal_init: has bfloat            = true
0.00.058.788 I ggml_metal_init: use bfloat            = true
0.00.058.788 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.789 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.385 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.577 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.586 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.607 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.600 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.602 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.602 I llama_new_context_with_model: graph nodes  = 967
0.00.088.602 I llama_new_context_with_model: graph splits = 2
0.00.088.605 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.746 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.746 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.719.959 I main: llama threadpool init, n_threads = 4
0.00.719.998 I 
0.00.720.024 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.720.024 I 
0.00.720.243 I sampler seed: 1234
0.00.720.247 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.720.290 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.720.291 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.720.291 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.564.604 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57165.86 tokens per second)
0.01.564.605 I llama_perf_context_print:        load time =     708.57 ms
0.01.564.606 I llama_perf_context_print: prompt eval time =      51.63 ms /     7 tokens (    7.38 ms per token,   135.58 tokens per second)
0.01.564.606 I llama_perf_context_print:        eval time =     789.61 ms /    63 runs   (   12.53 ms per token,    79.79 tokens per second)
0.01.564.606 I llama_perf_context_print:       total time =     844.65 ms /    70 tokens
0.01.564.813 I ggml_metal_free: deallocating

real	0m1.584s
user	0m0.110s
sys	0m0.176s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4459 (ff3fcabc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.843 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.733 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.738 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.740 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.740 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.741 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.741 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.741 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.742 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.742 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.743 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.744 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.746 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.747 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.747 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.749 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.749 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.749 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.481 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.466 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.221 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.222 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.223 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.223 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.223 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.224 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.224 I llama_model_loader: - type  f32:  194 tensors
0.00.024.224 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.225 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.162 I llm_load_vocab: special tokens cache size = 25
0.00.050.161 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.164 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.164 I llm_load_print_meta: arch             = gptneox
0.00.050.165 I llm_load_print_meta: vocab type       = BPE
0.00.050.165 I llm_load_print_meta: n_vocab          = 50304
0.00.050.165 I llm_load_print_meta: n_merges         = 50009
0.00.050.165 I llm_load_print_meta: vocab_only       = 0
0.00.050.165 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.165 I llm_load_print_meta: n_embd           = 2048
0.00.050.166 I llm_load_print_meta: n_layer          = 24
0.00.050.169 I llm_load_print_meta: n_head           = 16
0.00.050.170 I llm_load_print_meta: n_head_kv        = 16
0.00.050.170 I llm_load_print_meta: n_rot            = 32
0.00.050.171 I llm_load_print_meta: n_swa            = 0
0.00.050.171 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.172 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.173 I llm_load_print_meta: n_gqa            = 1
0.00.050.174 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.174 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.175 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.175 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.175 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.176 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.176 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.176 I llm_load_print_meta: n_ff             = 8192
0.00.050.178 I llm_load_print_meta: n_expert         = 0
0.00.050.178 I llm_load_print_meta: n_expert_used    = 0
0.00.050.178 I llm_load_print_meta: causal attn      = 1
0.00.050.178 I llm_load_print_meta: pooling type     = 0
0.00.050.178 I llm_load_print_meta: rope type        = 2
0.00.050.179 I llm_load_print_meta: rope scaling     = linear
0.00.050.179 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.179 I llm_load_print_meta: freq_scale_train = 1
0.00.050.179 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.180 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.180 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.180 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.180 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.180 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.181 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.181 I llm_load_print_meta: model type       = 1.4B
0.00.050.194 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.194 I llm_load_print_meta: model params     = 1.41 B
0.00.050.194 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.194 I llm_load_print_meta: general.name     = 1.4B
0.00.050.195 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.195 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.195 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.195 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.195 I llm_load_print_meta: LF token         = 128 ''
0.00.050.196 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.196 I llm_load_print_meta: max token length = 1024
0.00.051.756 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.756 I llm_load_tensors: offloading output layer to GPU
0.00.051.756 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.766 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.767 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.103 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.104 I llama_new_context_with_model: n_ctx         = 128
0.00.052.104 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.104 I llama_new_context_with_model: n_batch       = 128
0.00.052.104 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.105 I llama_new_context_with_model: flash_attn    = 0
0.00.052.105 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.105 I llama_new_context_with_model: freq_scale    = 1
0.00.052.105 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.106 I ggml_metal_init: allocating
0.00.052.109 I ggml_metal_init: found device: Apple M4
0.00.052.111 I ggml_metal_init: picking default device: Apple M4
0.00.052.643 I ggml_metal_init: using embedded metal library
0.00.054.938 I ggml_metal_init: GPU name:   Apple M4
0.00.054.940 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.940 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.941 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.941 I ggml_metal_init: simdgroup reduction   = true
0.00.054.941 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.941 I ggml_metal_init: has bfloat            = true
0.00.054.941 I ggml_metal_init: use bfloat            = true
0.00.054.942 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.942 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.510 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.829 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.831 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.846 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.693 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.694 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.694 I llama_new_context_with_model: graph nodes  = 967
0.00.066.694 I llama_new_context_with_model: graph splits = 2
0.00.066.696 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.696 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.679.022 I 
0.00.679.052 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.679.065 I perplexity: tokenizing the input ..
0.00.687.446 I perplexity: tokenization took 8.379 ms
0.00.687.455 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.828.200 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.829.364 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.829.391 I llama_perf_context_print:        load time =     670.17 ms
0.00.829.392 I llama_perf_context_print: prompt eval time =     140.52 ms /   128 tokens (    1.10 ms per token,   910.90 tokens per second)
0.00.829.393 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.829.394 I llama_perf_context_print:       total time =     150.37 ms /   129 tokens
0.00.829.840 I ggml_metal_free: deallocating

real	0m0.843s
user	0m0.079s
sys	0m0.143s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4459 (ff3fcabc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.009.687 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.227 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.018.231 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.233 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.233 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.234 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.235 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.236 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.239 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.240 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.240 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.241 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.241 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.245 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.247 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.250 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.250 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.250 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.032 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.014 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.832 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.833 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.833 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.833 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.834 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.834 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.835 I llama_model_loader: - type  f32:  194 tensors
0.00.026.835 I llama_model_loader: - type q6_K:   98 tensors
0.00.047.868 I llm_load_vocab: special tokens cache size = 25
0.00.053.887 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.891 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.891 I llm_load_print_meta: arch             = gptneox
0.00.053.892 I llm_load_print_meta: vocab type       = BPE
0.00.053.892 I llm_load_print_meta: n_vocab          = 50304
0.00.053.892 I llm_load_print_meta: n_merges         = 50009
0.00.053.892 I llm_load_print_meta: vocab_only       = 0
0.00.053.892 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.893 I llm_load_print_meta: n_embd           = 2048
0.00.053.893 I llm_load_print_meta: n_layer          = 24
0.00.053.896 I llm_load_print_meta: n_head           = 16
0.00.053.896 I llm_load_print_meta: n_head_kv        = 16
0.00.053.897 I llm_load_print_meta: n_rot            = 32
0.00.053.897 I llm_load_print_meta: n_swa            = 0
0.00.053.897 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.897 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.898 I llm_load_print_meta: n_gqa            = 1
0.00.053.899 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.899 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.900 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.900 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.900 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.900 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.901 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.901 I llm_load_print_meta: n_ff             = 8192
0.00.053.902 I llm_load_print_meta: n_expert         = 0
0.00.053.902 I llm_load_print_meta: n_expert_used    = 0
0.00.053.902 I llm_load_print_meta: causal attn      = 1
0.00.053.904 I llm_load_print_meta: pooling type     = 0
0.00.053.905 I llm_load_print_meta: rope type        = 2
0.00.053.905 I llm_load_print_meta: rope scaling     = linear
0.00.053.906 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.906 I llm_load_print_meta: freq_scale_train = 1
0.00.053.906 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.907 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.907 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.907 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.907 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.907 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.907 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.907 I llm_load_print_meta: model type       = 1.4B
0.00.053.919 I llm_load_print_meta: model ftype      = Q6_K
0.00.053.919 I llm_load_print_meta: model params     = 1.41 B
0.00.053.920 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.053.920 I llm_load_print_meta: general.name     = 1.4B
0.00.053.920 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.922 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.922 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.922 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.922 I llm_load_print_meta: LF token         = 128 ''
0.00.053.922 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.922 I llm_load_print_meta: max token length = 1024
0.00.056.029 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.029 I llm_load_tensors: offloading output layer to GPU
0.00.056.029 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.040 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.056.041 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.056.396 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.397 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.397 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.397 I llama_new_context_with_model: n_batch       = 2048
0.00.056.398 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.398 I llama_new_context_with_model: flash_attn    = 0
0.00.056.398 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.398 I llama_new_context_with_model: freq_scale    = 1
0.00.056.399 I ggml_metal_init: allocating
0.00.056.402 I ggml_metal_init: found device: Apple M4
0.00.056.404 I ggml_metal_init: picking default device: Apple M4
0.00.057.009 I ggml_metal_init: using embedded metal library
0.00.059.399 I ggml_metal_init: GPU name:   Apple M4
0.00.059.400 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.400 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.401 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.401 I ggml_metal_init: simdgroup reduction   = true
0.00.059.403 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.403 I ggml_metal_init: has bfloat            = true
0.00.059.403 I ggml_metal_init: use bfloat            = true
0.00.059.403 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.408 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.356 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.089.486 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.491 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.508 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.587 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.588 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.589 I llama_new_context_with_model: graph nodes  = 967
0.00.090.589 I llama_new_context_with_model: graph splits = 2
0.00.090.592 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.738 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.738 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.758.338 I main: llama threadpool init, n_threads = 4
0.00.758.374 I 
0.00.758.418 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.758.419 I 
0.00.758.651 I sampler seed: 1234
0.00.758.656 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.758.667 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.758.668 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.758.668 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.639.971 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59166.67 tokens per second)
0.01.639.971 I llama_perf_context_print:        load time =     748.65 ms
0.01.639.972 I llama_perf_context_print: prompt eval time =      54.40 ms /     7 tokens (    7.77 ms per token,   128.68 tokens per second)
0.01.639.973 I llama_perf_context_print:        eval time =     823.93 ms /    63 runs   (   13.08 ms per token,    76.46 tokens per second)
0.01.639.974 I llama_perf_context_print:       total time =     881.64 ms /    70 tokens
0.01.640.193 I ggml_metal_free: deallocating

real	0m1.659s
user	0m0.111s
sys	0m0.159s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4459 (ff3fcabc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.848 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.881 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.885 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.891 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.891 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.893 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.893 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.894 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.895 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.895 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.895 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.899 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.899 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.900 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.900 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.902 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.903 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.904 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.702 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.709 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.505 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.506 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.507 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.507 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.507 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.508 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.508 I llama_model_loader: - type  f32:  194 tensors
0.00.024.508 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.295 I llm_load_vocab: special tokens cache size = 25
0.00.051.242 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.246 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.246 I llm_load_print_meta: arch             = gptneox
0.00.051.246 I llm_load_print_meta: vocab type       = BPE
0.00.051.247 I llm_load_print_meta: n_vocab          = 50304
0.00.051.247 I llm_load_print_meta: n_merges         = 50009
0.00.051.247 I llm_load_print_meta: vocab_only       = 0
0.00.051.247 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.247 I llm_load_print_meta: n_embd           = 2048
0.00.051.248 I llm_load_print_meta: n_layer          = 24
0.00.051.251 I llm_load_print_meta: n_head           = 16
0.00.051.252 I llm_load_print_meta: n_head_kv        = 16
0.00.051.252 I llm_load_print_meta: n_rot            = 32
0.00.051.252 I llm_load_print_meta: n_swa            = 0
0.00.051.252 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.252 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.253 I llm_load_print_meta: n_gqa            = 1
0.00.051.254 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.255 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.255 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.255 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.256 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.258 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.258 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.259 I llm_load_print_meta: n_ff             = 8192
0.00.051.259 I llm_load_print_meta: n_expert         = 0
0.00.051.259 I llm_load_print_meta: n_expert_used    = 0
0.00.051.259 I llm_load_print_meta: causal attn      = 1
0.00.051.259 I llm_load_print_meta: pooling type     = 0
0.00.051.259 I llm_load_print_meta: rope type        = 2
0.00.051.259 I llm_load_print_meta: rope scaling     = linear
0.00.051.260 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.260 I llm_load_print_meta: freq_scale_train = 1
0.00.051.260 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.261 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.261 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.261 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.261 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.261 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.261 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.261 I llm_load_print_meta: model type       = 1.4B
0.00.051.273 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.274 I llm_load_print_meta: model params     = 1.41 B
0.00.051.275 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.275 I llm_load_print_meta: general.name     = 1.4B
0.00.051.275 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.276 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.276 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.276 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.276 I llm_load_print_meta: LF token         = 128 ''
0.00.051.277 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.278 I llm_load_print_meta: max token length = 1024
0.00.053.359 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.359 I llm_load_tensors: offloading output layer to GPU
0.00.053.359 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.370 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.371 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.775 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.775 I llama_new_context_with_model: n_ctx         = 128
0.00.053.776 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.776 I llama_new_context_with_model: n_batch       = 128
0.00.053.776 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.776 I llama_new_context_with_model: flash_attn    = 0
0.00.053.777 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.777 I llama_new_context_with_model: freq_scale    = 1
0.00.053.777 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.778 I ggml_metal_init: allocating
0.00.053.783 I ggml_metal_init: found device: Apple M4
0.00.053.785 I ggml_metal_init: picking default device: Apple M4
0.00.054.366 I ggml_metal_init: using embedded metal library
0.00.056.669 I ggml_metal_init: GPU name:   Apple M4
0.00.056.671 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.671 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.671 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.672 I ggml_metal_init: simdgroup reduction   = true
0.00.056.672 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.672 I ggml_metal_init: has bfloat            = true
0.00.056.672 I ggml_metal_init: use bfloat            = true
0.00.056.673 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.673 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.947 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.349 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.351 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.365 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.294 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.295 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.295 I llama_new_context_with_model: graph nodes  = 967
0.00.068.295 I llama_new_context_with_model: graph splits = 2
0.00.068.297 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.297 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.630.566 I 
0.00.630.595 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.630.605 I perplexity: tokenizing the input ..
0.00.638.810 I perplexity: tokenization took 8.204 ms
0.00.638.814 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.779.221 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.780.502 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.780.528 I llama_perf_context_print:        load time =     621.71 ms
0.00.780.529 I llama_perf_context_print: prompt eval time =     140.18 ms /   128 tokens (    1.10 ms per token,   913.09 tokens per second)
0.00.780.531 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.780.532 I llama_perf_context_print:       total time =     149.97 ms /   129 tokens
0.00.780.976 I ggml_metal_free: deallocating

real	0m0.795s
user	0m0.079s
sys	0m0.111s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4459 (ff3fcabc)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x120b0a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x120b0ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x120b0b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x120b0b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x120b0be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x120b0c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x120b0c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x120b0cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x120b0d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x120b0d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x120b0dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x120b0e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x120b0ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x120b0f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x120b0fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x120b105e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x120b10d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x120b11420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x120b11b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x120b12310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x120b12a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x120b13150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x120b13870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x120b14110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x120b14830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x120b14af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x120b15100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x120b15d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x120b162b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x120b16570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x120b16a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x120b16cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x120b17560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x120b17aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x120b17d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x120b18200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x120b186a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x120b18b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x120b18fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x120b19480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x120b19920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x120b19dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x120b1a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x120b1a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x120b1a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x120b1afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x120b1b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x120b1bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x120b1c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x120b1cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x120b1d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x120b1d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x120b1dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x120b1e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x120b1eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x120b1eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x120b1f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x120b1f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x120b1fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x120b20550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x120b20810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x120b20cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x120b21150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x120b215f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x120b21a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x120b21f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x120b223d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x120b22870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x120b22d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x120b231b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x120b23650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x120b23af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x120b23f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x120b244e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x120b24a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x120b24f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x120b254d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x120b25a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x120b25f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x120b264c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x120b26a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x120b26f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x120b274b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x120b27a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x120b27f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x120b284a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x120b289f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x120b28f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x120b29490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x120b299e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x120b29f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x120b2a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x120b2a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x120b2af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x120b2b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x120b2b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x120b2bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x120b1bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x120b2c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x120b2cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x120b2d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x120b2d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x120b2db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x120b2e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x120b2e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x120b2eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x120b2f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x120b2f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x120b2fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x120b30050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x120b305a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x120b30af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x120b31040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x120b314e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x120b31980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x120b31e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x120b322c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x120b32760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x120b32c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x120b330a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x120b33540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x120b339e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x120b33e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x120b34320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x120b347c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x120b34c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x120b35100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x120b355a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x120b35a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x120b35ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x120b36380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x120b36820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x120b36cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x120b37160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x120b37600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x120b37aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x120b37f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x120b383e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x120b38880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x120b38d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x120b391c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x120b39660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x120b39b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x120b39fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x120b3a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x120b3a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x120b3ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x120b3b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x120b3b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x120b3bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x120b3c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x120b3c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x120b3c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x120b3cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x120b3d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x120b3d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x120b3dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x120b3e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x120b3e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x120b3e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x120b3ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x120b3f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x120b3f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x120b3fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x120b400c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x120b40560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x120b40a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x120b40ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x120b41340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x120b417e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x120b41c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x120b42120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x120b425c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x120b42a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x120b42f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x120b433a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x120b43840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x120b43ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x120b44180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x120b44620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x120b44ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x120b44f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x120b45400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x120b458a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x120b45d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x120b461e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x120b46680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x120b46b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x120b46fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x120b47460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x120b47900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x120b47da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x120b48240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x120b48790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x120b48ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x120b49230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x120b49780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x120b49a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x120b4a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x120b4a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x120b4ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x120b4b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x120b4b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x120b4bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x120b4c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x120b4c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x120b4cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x120b4d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x120b4d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x120b4ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x120b4e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x120b4eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x120b4f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x120b4f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x120b4faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x120b4fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x120b50540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x120b50a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x120b50fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x120b51530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x120b51a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x120b51fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x120b52520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x120b52a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x120b52fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x120b53510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x120b53a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x120b53fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x120b54500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x120b54a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x120b54fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x120b554f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x120b55a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x120b55f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x120b564e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x120b56a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x120b56f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x120b574d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x120b57a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x120b57f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x120b584c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x120b58a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x120b58f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x120b594b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x120b59a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x120b59f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x120b5a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x120b5a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x120b5af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x120b5b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x120b5b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x120b5bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x120b5c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x120b5c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x120b5cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x120b5d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x120b5d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x120b5df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x120b5e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x120b5e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x120b5ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x120b5f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x120b5f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x120b5fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x120b60440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x120b60990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x120b60ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x120b61380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x120b61820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x120b61cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x120b62160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x120b62600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x120b62aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x120b62f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x120b633e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x120b63880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x120b63d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x120b641c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x120b64660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x120b64b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x120b64fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x120b65440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x120b65990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x120b660b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x120b667d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x120b66ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x120b67610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x120b678d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x120b680c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x120b68380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x120b68990 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.154.296 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.154.299 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10c504fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10c505440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10c5058b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10c505d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10c506190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10c506600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10c506a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10c506ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10c507350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10c5077c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10c507c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10c508320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10c508e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10c5095f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10c509e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10c50a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10c50ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10c50b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10c50ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10c50c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10c50c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10c50cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10c50d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10c50de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10c50e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10c50e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10c50ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10c50ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10c50f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10c50f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10c50fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10c5101c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10c510630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10c5108f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10c510d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10c5111d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10c511640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10c511ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10c511f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10c512390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10c512800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10c512c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10c5130e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10c513550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10c5139c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10c513e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10c5142a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10c514710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10c514b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10c514ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10c515460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10c5158d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10c515d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10c5161b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10c516620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10c516a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10c517000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10c517500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10c517970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10c517de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10c518250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10c5186c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10c518b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10c518fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10c519410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10c519880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10c519cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10c51a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10c51a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10c51aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10c51aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10c51b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10c51b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10c51bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10c51c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10c51c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10c51c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10c51cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10c51d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10c51d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10c51db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10c51df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10c51e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10c51e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10c51ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10c51f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10c51f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10c51fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10c51fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10c520300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10c520770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10c520be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10c521050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10c5214c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10c521930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10c521da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10c522210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10c522680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10c522af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10c522f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10c5233d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10c523840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10c523cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10c524120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10c524590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10c524a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10c524e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10c5252e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10c525750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10c525bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10c526030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10c5264a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10c526910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10c526d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10c5271f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10c527660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10c527ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10c527f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10c5283b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10c528820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10c528c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10c529100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10c529570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10c5299e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10c529e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10c52a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10c52a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10c52aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10c52b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10c52b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10c52b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10c52bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10c52c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10c52c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10c52cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10c52cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10c52d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10c52d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10c52dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10c52e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10c52e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10c52e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10c52ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10c52f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10c52f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10c52fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10c52fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10c530460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10c5308d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10c530d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10c5311b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10c531620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10c531a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10c531f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10c532370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10c5327e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10c532c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10c5330c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10c533530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10c5339a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10c533e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10c534280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10c5346f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10c534b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10c534fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10c535440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10c536070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10c536330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10c5365f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10c536a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10c536ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10c537340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10c5377b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10c537c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10c538090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10c538500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10c538970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10c538de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10c539250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10c5396c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10c539b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10c539fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10c53a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10c53a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10c53acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10c53b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10c53b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10c53ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10c53beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10c53c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10c53c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10c53cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10c53d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10c53d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10c53d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10c53ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10c53e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10c53e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10c53eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10c53ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10c53f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10c53f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10c53fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10c5402d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10c540740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10c540bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10c541020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10c541490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10c5419b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10c541ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10c542a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10c542cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10c5432b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10c543870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10c543e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10c5443f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10c5449b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10c544f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10c545530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10c545af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10c5460b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10c546670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10c546c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10c5471f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10c5477b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10c547d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10c548330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10c5488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10c548eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10c549470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10c549a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10c549ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10c54a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10c54ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10c54b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10c54b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10c54bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10c54c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10c54c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10c54cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10c54d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10c54d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10c54df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10c54e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10c54eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10c54f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10c54f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10c54fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10c5501b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10c550770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10c550d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10c5512f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10c5518b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10c551e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10c552430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10c5529f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10c552fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10c553570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10c553b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10c5540f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10c5546b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10c554c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10c555230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10c5557f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10c555db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10c556370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10c556930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10c556ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10c5573f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10c5578f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10c557df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10c5582f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10c5587f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10c558cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10c5591f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10c5596f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10c559bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10c55a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10c55a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10c55aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10c55aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10c55b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10c55b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10c55c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10c55cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10c55d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10c55d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10c55dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10c55e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10c55e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10c55ece0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10cf044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10cf04950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10cf04dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10cf05230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10cf056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10cf05b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10cf05f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10cf063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10cf06860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10cf06dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10cf07240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10cf078c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10cf083e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10cf08b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10cf093a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10cf09ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10cf0a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10cf0a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10cf0b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10cf0b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10cf0bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10cf0c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10cf0cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10cf0d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10cf0db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10cf0de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10cf0e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10cf0e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10cf0e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10cf0ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10cf0f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10cf0f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10cf0fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10cf0ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10cf103a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10cf10810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10cf10c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10cf110f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10cf11560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10cf119d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10cf11e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10cf122b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10cf12720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10cf12b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10cf13000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10cf13470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10cf138e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10cf13d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10cf141c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10cf14630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10cf14aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10cf14f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10cf15380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10cf157f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10cf15c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10cf160d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10cf16640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10cf16b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10cf16fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10cf17420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10cf17890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10cf17d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10cf18170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10cf185e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10cf18a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10cf18ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10cf19330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10cf197a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10cf19c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10cf1a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10cf1a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10cf1a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10cf1add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10cf1b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10cf1b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10cf1bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10cf1bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10cf1c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10cf1c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10cf1cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10cf1d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10cf1d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10cf1da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10cf1dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10cf1e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10cf1e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10cf1ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10cf1f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10cf1f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10cf1f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10cf1fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10cf20220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10cf20690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10cf20b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10cf20f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10cf213e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10cf21850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10cf21cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10cf22130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10cf225a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10cf22a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10cf22e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10cf232f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10cf23b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10cf23e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10cf242b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10cf24720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10cf24b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10cf25000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10cf25470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10cf258e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10cf25d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10cf261c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10cf26630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10cf26aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10cf26f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10cf27380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10cf277f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10cf27c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10cf280d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10cf28540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10cf289b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10cf28e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10cf29290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10cf29700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10cf29b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10cf29fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10cf2a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10cf2a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10cf2ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10cf2b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10cf2b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10cf2ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10cf2bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10cf2c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10cf2c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10cf2cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10cf2d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10cf2d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10cf2d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10cf2de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10cf2e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10cf2e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10cf2eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10cf2efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10cf2f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10cf2f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10cf2fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10cf30180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10cf305f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10cf30a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10cf30ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10cf31340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10cf317b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10cf31c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10cf32090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10cf32500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10cf32970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10cf32de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10cf33250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10cf336c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10cf33b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10cf33fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10cf34410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10cf34880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10cf34cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10cf35160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10cf355d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10cf35a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10cf35eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10cf36320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10cf36790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10cf36c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10cf37070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10cf374e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10cf37950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10cf37dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10cf38230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10cf386a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10cf38b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10cf38f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10cf393f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10cf39860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10cf39cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10cf3a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10cf3a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10cf3aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10cf3ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10cf3b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10cf3b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10cf3bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10cf3c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10cf3c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10cf3c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10cf3cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10cf3d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10cf3d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10cf3daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10cf3df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10cf3e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10cf3e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10cf3ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10cf3f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10cf3f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10cf3fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10cf3fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10cf402e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10cf40750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10cf40bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10cf41030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10cf41bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10cf41e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10cf42130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10cf425a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10cf42a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10cf42e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10cf432f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10cf43760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10cf43bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10cf44040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10cf444b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10cf44920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10cf44d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10cf45200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10cf45670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10cf45ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10cf45f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10cf463c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10cf46830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10cf46ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10cf47110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10cf47580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10cf479f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10cf47e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10cf482d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10cf48740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10cf48bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10cf49020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10cf49490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10cf49900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10cf49d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10cf4a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10cf4a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10cf4aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10cf4af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10cf4b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10cf4b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10cf4bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10cf4c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10cf4c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10cf4c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10cf4ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10cf4d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10cf4d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10cf4db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10cf4e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10cf4e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10cf4e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10cf4ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10cf4f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10cf4f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10cf4faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10cf4ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10cf50380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10cf507f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10cf50c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10cf510d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10cf51540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10cf519b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10cf51e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10cf52290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10cf52700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10cf52b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10cf52fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10cf53450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10cf538c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10cf53d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10cf541a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10cf54610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10cf54a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10cf54ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10cf55360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10cf557d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10cf56240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10cf56960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10cf57080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10cf577a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10cf57a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10cf57ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10cf584d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10cf58ae0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.767s
user	0m0.297s
sys	0m0.294s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4459 (ff3fcabc)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12b807640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12b807d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12b808300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12b8088b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12b808e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12b809410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12b8099c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12b809f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12b80a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12b80aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12b80af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12b80b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12b80bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12b80c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12b80cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12b80d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12b80dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12b80e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12b80eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12b80f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12b80fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12b810190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12b8108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12b811150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12b811870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12b811b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12b812140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12b812db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12b8132f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12b8135b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12b813a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12b813d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12b8145a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12b814ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12b814da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12b815240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12b8156e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12b815b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12b816020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12b8164c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12b816960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12b816e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12b8172a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12b817740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12b817a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12b818010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12b818620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12b818f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12b819550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12b819b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12b81a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12b81a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12b81ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12b81b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12b81bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12b81c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12b81c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12b81c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12b81cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12b81d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12b81d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12b81dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12b81e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12b81e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12b81ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12b81ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12b81f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12b81f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12b81fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12b8201f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12b820690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12b820b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12b820fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12b821520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12b821a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12b821fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12b822510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12b822a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12b822fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12b823500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12b823a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12b823fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12b8244f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12b824a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12b824f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12b8254e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12b825a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12b825f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12b8264d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12b826a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12b826f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12b8274c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12b827a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12b827f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12b8284b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12b828a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12b828f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12b818c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12b8293c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12b829b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12b82a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12b82a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12b82ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12b82b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12b82b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12b82bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12b82c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12b82c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12b82cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12b82d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12b82d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12b82db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12b82e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12b82e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12b82e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12b82ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12b82f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12b82f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12b82fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12b8300e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12b830580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12b830a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12b830ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12b831360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12b831800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12b831ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12b832140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12b8325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12b832a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12b832f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12b8333c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12b833860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12b833d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12b8341a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12b834640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12b834ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12b834f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12b835420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12b8358c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12b835d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12b836200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12b8366a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12b836b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12b836fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12b837480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12b837920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12b837dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12b838260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12b838700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12b838ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12b839040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12b8394e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12b839980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12b839e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12b83a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12b83a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12b83ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12b83b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12b83b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12b83b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12b83be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12b83c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12b83c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12b83cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12b83d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12b83d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12b83da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12b83dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12b83e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12b83e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12b83ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12b83f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12b83f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12b83faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12b83ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12b8403e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12b840880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12b840d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12b8411c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12b841660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12b841b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12b841fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12b842440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12b8428e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12b842d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12b843220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12b8436c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12b843b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12b844000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12b8444a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12b844940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12b844de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12b845280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12b8457d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12b845d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12b846270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12b8467c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12b846a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12b847090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12b8476a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12b847cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12b8484a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12b848940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12b848c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12b849210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12b849820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12b84a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12b84a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12b84a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12b84adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12b84b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12b84baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12b84c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12b84c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12b84cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12b84d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12b84d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12b84dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12b84e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12b84e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12b84eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12b84f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12b84f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12b84fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12b850000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12b850550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12b850aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12b850ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12b851540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12b851a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12b851fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12b852530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12b852a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12b852fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12b853520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12b853a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12b853fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12b854510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12b854a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12b854fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12b855500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12b855a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12b855fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12b8564f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12b856a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12b856f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12b8574e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12b857a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12b857f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12b8584d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12b858a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12b858f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12b8594c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12b859a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12b859f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12b85a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12b85aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12b85af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12b85b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12b85b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12b85bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12b85c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12b85c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12b85cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12b85d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12b85d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12b85df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12b85e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12b85e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12b85ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12b85f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12b85f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12b85fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12b85ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12b860420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12b8608c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12b860d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12b861200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12b8616a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12b861b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12b861fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12b862480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12b8629d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12b8630f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12b863810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12b863f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12b864650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12b864910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12b865100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12b8653c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12b8659d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.091.675 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.091.679 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12a608950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12a608dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12a609230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12a60b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12a60bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12a60c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12a60ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12a60cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12a60d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12a60d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12a60dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12a60e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12a60ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12a60f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12a60f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12a610100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12a610820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12a610f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12a611660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12a612010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12a612730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12a612e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12a613570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12a613c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12a6143b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12a614670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12a614c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12a615290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12a6158a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12a616090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12a616530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12a6167f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12a617080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12a6175c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12a617880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12a617d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12a6181c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12a618660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12a618b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12a618fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12a619440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12a6198e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12a619d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12a61a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12a61a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12a61aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12a61b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12a61b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12a61bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12a61c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12a61c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12a61cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12a61d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12a61db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12a61e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12a61e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12a61eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12a61ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12a61f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12a61fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12a620200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12a6206a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12a620b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12a620fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12a621480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12a621920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12a621dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12a622260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12a622700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12a622ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12a623040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12a6234e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12a623980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12a623ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12a624420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12a624970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12a624ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12a625410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12a625960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12a625eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12a626400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12a626950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12a626ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12a6273f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12a627940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12a627e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12a6283e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12a628930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12a628e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12a6293d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12a629920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12a629e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12a62a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12a62a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12a62ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12a62b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12a62b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12a62be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12a62c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12a62c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12a62ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12a62d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12a62d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12a62de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12a62e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12a62e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12a62ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12a62f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12a62f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12a62fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12a630360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12a6308b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12a630e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12a6312a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12a631740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12a631be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12a632080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12a632520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12a6329c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12a632e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12a633300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12a6337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12a633c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12a6340e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12a634580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12a634a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12a634ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12a635360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12a635800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12a635ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12a636140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12a6365e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12a636a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12a636f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12a6373c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12a637860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12a637d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12a6381a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12a638640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12a638ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12a638f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12a639420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12a6398c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12a639d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12a63a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12a63a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12a63ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12a63afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12a63b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12a63b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12a63bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12a63c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12a63c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12a63cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12a63d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12a63d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12a63d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12a63de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12a63e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12a63e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12a63ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12a63f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12a63f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12a63f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12a63fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12a640320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12a6407c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12a640c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12a641100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12a6415a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12a641a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12a641ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12a642380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12a642820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12a642cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12a643160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12a643600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12a643aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12a643f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12a6443e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12a644880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12a644d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12a6451c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12a645660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12a645b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12a645fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12a646440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12a6468e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12a646d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12a647220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12a6476c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12a647b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12a648000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12a648550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12a648aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12a648ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12a649540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12a649800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12a649e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12a64a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12a64aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12a64b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12a64b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12a64b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12a64bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12a64c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12a64cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12a64d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12a64d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12a64db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12a64e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12a64e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12a64edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12a64f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12a64f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12a64fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12a650300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12a650850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12a650da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12a6512f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12a651840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12a651d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12a6522e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12a652830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12a652d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12a6532d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12a653820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12a653d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12a6542c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12a654810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12a654d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12a6552b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12a655800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12a655d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12a6562a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12a6567f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12a656d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12a657290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12a6577e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12a657d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12a658280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12a6587d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12a658d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12a659270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12a6597c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12a659d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12a65a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12a65a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12a65ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12a65b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12a65b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12a65bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12a65c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12a65c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12a65cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12a65d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12a65d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12a65dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12a65e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12a65e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12a65ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12a65f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12a65f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12a65fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12a660200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12a660750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12a660ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12a661140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12a6615e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12a661a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12a661f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12a6623c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12a662860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12a662d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12a6631a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12a663640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12a663ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12a663f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12a664420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12a6648c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12a664d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12a665200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12a665750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12a665e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12a666590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12a666cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12a6673d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12a667690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12a667e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12a668140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12a668750 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11df044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11df04950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11df04dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11df05230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11df056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11df05b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11df05f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11df063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11df06860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11df06dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11df07240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11df078c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11df083e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11df08b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11df093a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11df09ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11df0a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11df0a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11df0b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11df0b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11df0bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11df0c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11df0cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11df0d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11df0db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11df0de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11df0e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11df0e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11df0e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11df0ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11df0f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11df0f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11df0fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11df0ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11df103a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11df10810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11df10c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11df110f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11df11560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11df119d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11df11e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11df122b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11df12720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11df12b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11df13000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11df13470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11df138e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11df13d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11df141c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11df14630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11df14aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11df14f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11df15380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11df157f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11df15c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11df160d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11df16640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11df16b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11df16fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11df17420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11df17890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11df17d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11df18170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11df185e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11df18a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11df18ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11df19330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11df197a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11df19c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11df1a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11df1a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11df1a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11df1add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11df1b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11df1b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11df1bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11df1bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11df1c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11df1c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11df1cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11df1d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11df1d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11df1da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11df1dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11df1e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11df1e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11df1ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11df1f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11df1f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11df1f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11df1fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11df20220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11df20690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11df20b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11df20f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11df213e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11df21850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11df21cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11df22130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11df225a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11df22a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11df22e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11df232f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11df23b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11df23e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11df242b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11df24720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11df24b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11df25000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11df25470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11df258e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11df25d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11df261c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11df26630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11df26aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11df26f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11df27380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11df277f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11df27c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11df280d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11df28540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11df289b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11df28e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11df29290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11df29700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11df29b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11df29fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11df2a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11df2a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11df2ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11df2b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11df2b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11df2ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11df2bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11df2c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11df2c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11df2cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11df2d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11df2d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11df2d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11df2de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11df2e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11df2e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11df2eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11df2efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11df2f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11df2f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11df2fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11df30180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11df305f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11df30a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11df30ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11df31340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11df317b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11df31c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11df32090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11df32500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11df32970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11df32de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11df33250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11df336c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11df33b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11df33fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11df34410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11df34880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11df34cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11df35160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11df355d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11df35a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11df35eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11df36320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11df36790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11df36c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11df37070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11df374e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11df37950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11df37dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11df38230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11df386a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11df38b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11df38f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11df393f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11df39860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11df39cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11df3a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11df3a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11df3aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11df3ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11df3b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11df3b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11df3bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11df3c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11df3c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11df3c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11df3cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11df3d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11df3d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11df3daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11df3df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11df3e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11df3e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11df3ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11df3f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11df3f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11df3fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11df3fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11df402e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11df40750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11df40bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11df41030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11df41bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11df41e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11df42130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11df425a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11df42a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11df42e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11df432f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11df43760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11df43bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11df44040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11df444b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11df44920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11df44d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11df45200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11df45670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11df45ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11df45f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11df463c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11df46830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11df46ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11df47110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11df47580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11df479f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11df47e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11df482d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11df48740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11df48bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11df49020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11df49490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11df49900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11df49d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11df4a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11df4a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11df4aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11df4af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11df4b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11df4b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11df4bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11df4c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11df4c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11df4c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11df4ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11df4d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11df4d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11df4db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11df4e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11df4e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11df4e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11df4ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11df4f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11df4f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11df4faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11df4ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11df50380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11df507f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11df50c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11df510d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11df51540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11df519b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11df51e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11df52290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11df52700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11df52b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11df52fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11df53450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11df538c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11df53d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11df541a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11df54610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11df54a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11df54ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11df55360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11df557d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11df56240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11df56960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11df57080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11df577a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11df57a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11df57ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11df584d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11df58ae0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.927s
user	0m0.244s
sys	0m0.142s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.55 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.60 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.15 sec*proc (2 tests)

Total Test time (real) =   1.15 sec
        1.18 real         0.75 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.25 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.27 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.52 sec*proc (2 tests)

Total Test time (real) =   0.53 sec
        0.53 real         0.15 user         0.04 sys
```
