Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:298 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD - Success
-- ARM feature DOTPROD enabled
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8 - Success
-- ARM feature MATMUL_INT8 enabled
-- Adding CPU backend variant ggml-cpu: -march=armv8.2a+dotprod+i8mm __ARM_FEATURE_DOTPROD;__ARM_FEATURE_MATMUL_INT8
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (1.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m1.702s
user	0m0.719s
sys	0m1.020s
++ nproc
+ make -j10
[  1%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  7%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  7%] Built target build_info
[  7%] Built target sha1
[  7%] Built target sha256
[  7%] Built target xxhash
[  7%] Linking CXX shared library libggml-base.dylib
[  7%] Built target ggml-base
[  8%] Generate assembly for embedded Metal library
Embedding Metal library
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 11%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[ 10%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 12%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 14%] Linking CXX shared library libggml-blas.dylib
[ 14%] Linking CXX shared library libggml-cpu.dylib
[ 14%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 15%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 15%] Built target ggml-blas
[ 15%] Built target ggml-cpu
[ 16%] Linking C shared library libggml-metal.dylib
[ 16%] Built target ggml-metal
[ 17%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 17%] Linking CXX shared library libggml.dylib
[ 17%] Built target ggml
[ 17%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 20%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Linking CXX shared library libllama.dylib
[ 22%] Built target llama-gguf-hash
[ 22%] Built target llama-gguf
[ 22%] Built target llama
[ 22%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 23%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 23%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 23%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 24%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 25%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 25%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 27%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 27%] Linking C executable ../bin/test-c
[ 28%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 29%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 29%] Linking CXX executable ../../bin/llama-simple
[ 30%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-quantize-stats
[ 31%] Linking CXX executable ../../bin/llama-simple-chat
[ 31%] Linking CXX static library libcommon.a
[ 31%] Built target llava
[ 31%] Built target test-c
[ 31%] Built target llama-simple
[ 31%] Built target llama-quantize-stats
[ 31%] Linking CXX shared library libllava_shared.dylib
[ 32%] Linking CXX static library libllava_static.a
[ 32%] Built target llama-simple-chat
[ 32%] Built target common
[ 32%] Built target llava_static
[ 33%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 34%] Built target llava_shared
[ 34%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 37%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 40%] Linking CXX executable ../bin/test-tokenizer-0
[ 41%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 42%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 43%] Linking CXX executable ../bin/test-grammar-integration
[ 43%] Linking CXX executable ../bin/test-grammar-parser
[ 43%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-llama-grammar
[ 44%] Linking CXX executable ../bin/test-sampling
[ 45%] Linking CXX executable ../bin/test-log
[ 46%] Built target test-tokenizer-0
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 46%] Built target test-tokenizer-1-bpe
[ 46%] Built target test-tokenizer-1-spm
[ 46%] Built target test-json-schema-to-grammar
[ 46%] Built target test-grammar-integration
[ 46%] Built target test-sampling
[ 46%] Built target test-grammar-parser
[ 47%] Linking CXX executable ../bin/test-arg-parser
[ 47%] Built target test-llama-grammar
[ 48%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 51%] Built target test-log
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 53%] Linking CXX executable ../bin/test-chat-template
[ 54%] Linking CXX executable ../bin/test-backend-ops
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 55%] Linking CXX executable ../bin/test-model-load-cancel
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-autorelease
[ 56%] Built target test-arg-parser
[ 56%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 57%] Linking CXX executable ../bin/test-quantize-fns
[ 58%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 58%] Linking CXX executable ../bin/test-barrier
[ 59%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 59%] Built target test-backend-ops
[ 59%] Built target test-chat-template
[ 60%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 60%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 60%] Built target test-model-load-cancel
[ 60%] Built target test-autorelease
[ 60%] Linking CXX executable ../../bin/llama-batched-bench
[ 60%] Built target test-quantize-fns
[ 60%] Linking CXX executable ../../bin/llama-batched
[ 60%] Linking CXX executable ../bin/test-rope
[ 61%] Linking CXX executable ../bin/test-quantize-perf
[ 61%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 61%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 61%] Built target test-barrier
[ 62%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 62%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 63%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 64%] Linking CXX executable ../../bin/llama-embedding
[ 65%] Linking CXX executable ../../bin/llama-eval-callback
[ 66%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 67%] Built target test-quantize-perf
[ 67%] Linking CXX executable ../../bin/llama-gguf-split
[ 67%] Built target test-rope
[ 67%] Built target llama-batched-bench
[ 67%] Linking CXX executable ../../bin/llama-gritlm
[ 67%] Built target llama-batched
[ 67%] Linking CXX executable ../../bin/llama-imatrix
[ 69%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 69%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 70%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 70%] Built target llama-eval-callback
[ 70%] Built target llama-embedding
[ 71%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 71%] Built target llama-gguf-split
[ 71%] Built target llama-gbnf-validator
[ 71%] Linking CXX executable ../../bin/llama-bench
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Built target llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-lookahead
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 72%] Built target llama-imatrix
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 73%] Linking CXX executable ../../bin/llama-lookup
[ 74%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 74%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 74%] Linking CXX executable ../../bin/llama-lookup-create
[ 75%] Linking CXX executable ../../bin/llama-lookup-merge
[ 75%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 75%] Linking CXX executable ../../bin/llama-cli
[ 76%] Linking CXX executable ../../bin/llama-lookup-stats
[ 76%] Built target llama-infill
[ 76%] Built target llama-lookahead
[ 77%] Built target llama-bench
[ 77%] Linking CXX executable ../../bin/llama-parallel
[ 78%] Linking CXX executable ../../bin/llama-passkey
[ 78%] Built target llama-lookup
[ 78%] Generating loading.html.hpp
[ 78%] Built target llama-lookup-create
[ 79%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 79%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 79%] Built target llama-cli
[ 79%] Built target llama-lookup-merge
[ 79%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 79%] Built target llama-lookup-stats
[ 79%] Built target llama-parallel
[ 80%] Generating index.html.gz.hpp
[ 80%] Linking CXX executable ../../bin/llama-retrieval
[ 81%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Linking CXX executable ../../bin/llama-perplexity
[ 84%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 84%] Built target llama-passkey
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 85%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 85%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-save-load-state
[ 85%] Linking CXX executable ../../bin/llama-run
[ 86%] Linking CXX executable ../../bin/llama-speculative
[ 87%] Linking CXX executable ../../bin/llama-speculative-simple
[ 87%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-tokenize
[ 88%] Built target llama-perplexity
[ 88%] Built target llama-retrieval
[ 88%] Built target llama-quantize
[ 89%] Linking CXX executable ../../bin/llama-tts
[ 89%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 89%] Built target llama-save-load-state
[ 89%] Built target llama-run
[ 89%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 90%] Built target llama-speculative
[ 90%] Built target llama-tokenize
[ 90%] Built target llama-speculative-simple
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 92%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 95%] Built target llama-tts
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 97%] Linking CXX executable ../../bin/llama-export-lora
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Built target llama-convert-llama2c-to-ggml
[ 98%] Built target llama-cvector-generator
[ 98%] Built target llama-gen-docs
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.868s
user	0m5.798s
sys	0m8.765s

main: quantize time =  3013.42 ms
main:    total time =  3013.42 ms

main: quantize time =  1880.55 ms
main:    total time =  1880.55 ms

main: quantize time =  1594.54 ms
main:    total time =  1594.54 ms

main: quantize time =  1474.00 ms
main:    total time =  1474.00 ms

main: quantize time =  2598.66 ms
main:    total time =  2598.66 ms

main: quantize time =  5045.69 ms
main:    total time =  5045.69 ms

main: quantize time =  5815.85 ms
main:    total time =  5815.85 ms

main: quantize time =  6842.44 ms
main:    total time =  6842.44 ms

main: quantize time =  5950.81 ms
main:    total time =  5950.81 ms

main: quantize time =  4652.63 ms
main:    total time =  4652.63 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.106 I build: 4365 (609f17d7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.215 I main: llama backend init
0.00.000.221 I main: load the model and apply lora adapter, if any
0.00.057.345 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.068.660 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.068.674 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.068.678 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.068.679 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.068.680 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.068.680 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.068.681 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.068.683 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.068.683 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.068.684 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.068.685 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.068.686 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.068.686 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.068.687 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.068.692 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.068.692 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.068.693 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.075.881 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.078.210 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.085.461 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.085.470 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.085.471 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.085.472 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.085.473 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.085.475 I llama_model_loader: - type  f32:  194 tensors
0.00.085.475 I llama_model_loader: - type  f16:   98 tensors
0.00.124.850 I llm_load_vocab: special tokens cache size = 25
0.00.132.276 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.132.280 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.132.281 I llm_load_print_meta: arch             = gptneox
0.00.132.281 I llm_load_print_meta: vocab type       = BPE
0.00.132.281 I llm_load_print_meta: n_vocab          = 50304
0.00.132.281 I llm_load_print_meta: n_merges         = 50009
0.00.132.282 I llm_load_print_meta: vocab_only       = 0
0.00.132.282 I llm_load_print_meta: n_ctx_train      = 2048
0.00.132.282 I llm_load_print_meta: n_embd           = 2048
0.00.132.284 I llm_load_print_meta: n_layer          = 24
0.00.132.287 I llm_load_print_meta: n_head           = 16
0.00.132.288 I llm_load_print_meta: n_head_kv        = 16
0.00.132.288 I llm_load_print_meta: n_rot            = 32
0.00.132.289 I llm_load_print_meta: n_swa            = 0
0.00.132.289 I llm_load_print_meta: n_embd_head_k    = 128
0.00.132.289 I llm_load_print_meta: n_embd_head_v    = 128
0.00.132.290 I llm_load_print_meta: n_gqa            = 1
0.00.132.292 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.132.293 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.132.294 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.132.294 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.132.294 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.132.296 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.132.296 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.132.297 I llm_load_print_meta: n_ff             = 8192
0.00.132.297 I llm_load_print_meta: n_expert         = 0
0.00.132.297 I llm_load_print_meta: n_expert_used    = 0
0.00.132.297 I llm_load_print_meta: causal attn      = 1
0.00.132.297 I llm_load_print_meta: pooling type     = 0
0.00.132.297 I llm_load_print_meta: rope type        = 2
0.00.132.298 I llm_load_print_meta: rope scaling     = linear
0.00.132.298 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.132.298 I llm_load_print_meta: freq_scale_train = 1
0.00.132.298 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.132.299 I llm_load_print_meta: rope_finetuned   = unknown
0.00.132.299 I llm_load_print_meta: ssm_d_conv       = 0
0.00.132.299 I llm_load_print_meta: ssm_d_inner      = 0
0.00.132.299 I llm_load_print_meta: ssm_d_state      = 0
0.00.132.299 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.132.299 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.132.299 I llm_load_print_meta: model type       = 1.4B
0.00.132.300 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.132.300 I llm_load_print_meta: model params     = 1.41 B
0.00.132.301 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.132.301 I llm_load_print_meta: general.name     = 1.4B
0.00.132.301 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.132.301 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.132.301 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.132.302 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.132.302 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.132.302 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.132.302 I llm_load_print_meta: max token length = 1024
0.00.135.011 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.135.011 I llm_load_tensors: offloading output layer to GPU
0.00.135.011 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.135.030 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.135.031 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.136.060 I llama_new_context_with_model: n_seq_max     = 1
0.00.136.061 I llama_new_context_with_model: n_ctx         = 2048
0.00.136.061 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.136.062 I llama_new_context_with_model: n_batch       = 2048
0.00.136.062 I llama_new_context_with_model: n_ubatch      = 512
0.00.136.062 I llama_new_context_with_model: flash_attn    = 0
0.00.136.063 I llama_new_context_with_model: freq_base     = 10000.0
0.00.136.063 I llama_new_context_with_model: freq_scale    = 1
0.00.136.063 I ggml_metal_init: allocating
0.00.136.072 I ggml_metal_init: found device: Apple M4
0.00.136.077 I ggml_metal_init: picking default device: Apple M4
0.00.136.788 I ggml_metal_init: using embedded metal library
0.00.171.957 I ggml_metal_init: GPU name:   Apple M4
0.00.171.959 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.171.960 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.171.960 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.171.961 I ggml_metal_init: simdgroup reduction   = true
0.00.171.961 I ggml_metal_init: simdgroup matrix mul. = true
0.00.171.961 I ggml_metal_init: has bfloat            = true
0.00.171.961 I ggml_metal_init: use bfloat            = true
0.00.171.962 I ggml_metal_init: hasUnifiedMemory      = true
0.00.171.962 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.232.805 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.254.703 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.254.711 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.254.732 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.255.715 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.255.717 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.255.718 I llama_new_context_with_model: graph nodes  = 967
0.00.255.718 I llama_new_context_with_model: graph splits = 2
0.00.255.744 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.255.884 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.255.885 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.333.996 I main: llama threadpool init, n_threads = 4
0.00.334.029 I 
0.00.334.063 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.334.064 I 
0.00.334.133 I sampler seed: 1234
0.00.334.138 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.334.172 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.334.174 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.334.174 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.175.637 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56845.48 tokens per second)
0.02.175.637 I llama_perf_context_print:        load time =     276.64 ms
0.02.175.638 I llama_perf_context_print: prompt eval time =      43.73 ms /     7 tokens (    6.25 ms per token,   160.07 tokens per second)
0.02.175.639 I llama_perf_context_print:        eval time =    1794.82 ms /    63 runs   (   28.49 ms per token,    35.10 tokens per second)
0.02.175.640 I llama_perf_context_print:       total time =    1841.64 ms /    70 tokens
0.02.175.801 I ggml_metal_free: deallocating

real	0m2.518s
user	0m0.154s
sys	0m0.104s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4365 (609f17d7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.009.745 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.838 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.024.842 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.844 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.844 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.844 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.845 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.845 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.845 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.846 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.846 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.846 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.847 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.847 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.847 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.849 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.849 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.850 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.830 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.892 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.902 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.903 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.903 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.904 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.904 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.905 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.905 I llama_model_loader: - type  f32:  194 tensors
0.00.033.906 I llama_model_loader: - type q8_0:   98 tensors
0.00.057.197 I llm_load_vocab: special tokens cache size = 25
0.00.063.232 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.063.235 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.063.236 I llm_load_print_meta: arch             = gptneox
0.00.063.236 I llm_load_print_meta: vocab type       = BPE
0.00.063.236 I llm_load_print_meta: n_vocab          = 50304
0.00.063.237 I llm_load_print_meta: n_merges         = 50009
0.00.063.237 I llm_load_print_meta: vocab_only       = 0
0.00.063.237 I llm_load_print_meta: n_ctx_train      = 2048
0.00.063.239 I llm_load_print_meta: n_embd           = 2048
0.00.063.239 I llm_load_print_meta: n_layer          = 24
0.00.063.243 I llm_load_print_meta: n_head           = 16
0.00.063.244 I llm_load_print_meta: n_head_kv        = 16
0.00.063.244 I llm_load_print_meta: n_rot            = 32
0.00.063.244 I llm_load_print_meta: n_swa            = 0
0.00.063.244 I llm_load_print_meta: n_embd_head_k    = 128
0.00.063.244 I llm_load_print_meta: n_embd_head_v    = 128
0.00.063.245 I llm_load_print_meta: n_gqa            = 1
0.00.063.246 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.063.247 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.063.247 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.063.248 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.063.248 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.063.248 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.063.248 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.063.249 I llm_load_print_meta: n_ff             = 8192
0.00.063.249 I llm_load_print_meta: n_expert         = 0
0.00.063.249 I llm_load_print_meta: n_expert_used    = 0
0.00.063.250 I llm_load_print_meta: causal attn      = 1
0.00.063.252 I llm_load_print_meta: pooling type     = 0
0.00.063.252 I llm_load_print_meta: rope type        = 2
0.00.063.252 I llm_load_print_meta: rope scaling     = linear
0.00.063.253 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.063.253 I llm_load_print_meta: freq_scale_train = 1
0.00.063.253 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.063.253 I llm_load_print_meta: rope_finetuned   = unknown
0.00.063.254 I llm_load_print_meta: ssm_d_conv       = 0
0.00.063.254 I llm_load_print_meta: ssm_d_inner      = 0
0.00.063.254 I llm_load_print_meta: ssm_d_state      = 0
0.00.063.254 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.063.254 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.063.254 I llm_load_print_meta: model type       = 1.4B
0.00.063.255 I llm_load_print_meta: model ftype      = Q8_0
0.00.063.255 I llm_load_print_meta: model params     = 1.41 B
0.00.063.256 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.063.256 I llm_load_print_meta: general.name     = 1.4B
0.00.063.256 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.063.256 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.063.257 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.063.257 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.063.257 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.063.258 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.063.258 I llm_load_print_meta: max token length = 1024
0.00.065.399 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.065.399 I llm_load_tensors: offloading output layer to GPU
0.00.065.399 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.065.405 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.065.406 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.066.372 I llama_new_context_with_model: n_seq_max     = 1
0.00.066.373 I llama_new_context_with_model: n_ctx         = 2048
0.00.066.373 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.066.373 I llama_new_context_with_model: n_batch       = 2048
0.00.066.373 I llama_new_context_with_model: n_ubatch      = 512
0.00.066.373 I llama_new_context_with_model: flash_attn    = 0
0.00.066.374 I llama_new_context_with_model: freq_base     = 10000.0
0.00.066.374 I llama_new_context_with_model: freq_scale    = 1
0.00.066.375 I ggml_metal_init: allocating
0.00.066.381 I ggml_metal_init: found device: Apple M4
0.00.066.384 I ggml_metal_init: picking default device: Apple M4
0.00.067.106 I ggml_metal_init: using embedded metal library
0.00.069.708 I ggml_metal_init: GPU name:   Apple M4
0.00.069.710 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.069.711 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.069.711 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.069.711 I ggml_metal_init: simdgroup reduction   = true
0.00.069.711 I ggml_metal_init: simdgroup matrix mul. = true
0.00.069.712 I ggml_metal_init: has bfloat            = true
0.00.069.712 I ggml_metal_init: use bfloat            = true
0.00.069.712 I ggml_metal_init: hasUnifiedMemory      = true
0.00.069.713 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.080.259 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.106.630 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.106.639 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.106.664 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.107.789 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.107.791 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.107.791 I llama_new_context_with_model: graph nodes  = 967
0.00.107.791 I llama_new_context_with_model: graph splits = 2
0.00.107.808 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.107.948 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.107.949 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.512.355 I main: llama threadpool init, n_threads = 4
0.01.512.395 I 
0.01.512.423 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.512.425 I 
0.01.512.775 I sampler seed: 1234
0.01.512.785 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.512.804 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.512.805 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.512.805 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.614.563 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56709.27 tokens per second)
0.02.614.564 I llama_perf_context_print:        load time =    1502.60 ms
0.02.614.565 I llama_perf_context_print: prompt eval time =      39.59 ms /     7 tokens (    5.66 ms per token,   176.82 tokens per second)
0.02.614.567 I llama_perf_context_print:        eval time =    1059.43 ms /    63 runs   (   16.82 ms per token,    59.47 tokens per second)
0.02.614.567 I llama_perf_context_print:       total time =    1102.21 ms /    70 tokens
0.02.614.784 I ggml_metal_free: deallocating

real	0m2.631s
user	0m0.115s
sys	0m0.213s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4365 (609f17d7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.018.510 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.486 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.037.492 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.494 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.494 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.499 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.500 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.500 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.502 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.502 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.502 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.503 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.503 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.504 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.504 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.507 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.508 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.508 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.930 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.245 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.047.982 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.047.984 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.047.984 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.047.984 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.047.985 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.047.985 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.047.986 I llama_model_loader: - type  f32:  194 tensors
0.00.047.986 I llama_model_loader: - type q4_0:   97 tensors
0.00.047.986 I llama_model_loader: - type q6_K:    1 tensors
0.00.076.914 I llm_load_vocab: special tokens cache size = 25
0.00.087.504 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.087.509 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.087.509 I llm_load_print_meta: arch             = gptneox
0.00.087.510 I llm_load_print_meta: vocab type       = BPE
0.00.087.510 I llm_load_print_meta: n_vocab          = 50304
0.00.087.510 I llm_load_print_meta: n_merges         = 50009
0.00.087.511 I llm_load_print_meta: vocab_only       = 0
0.00.087.517 I llm_load_print_meta: n_ctx_train      = 2048
0.00.087.518 I llm_load_print_meta: n_embd           = 2048
0.00.087.518 I llm_load_print_meta: n_layer          = 24
0.00.087.524 I llm_load_print_meta: n_head           = 16
0.00.087.525 I llm_load_print_meta: n_head_kv        = 16
0.00.087.525 I llm_load_print_meta: n_rot            = 32
0.00.087.526 I llm_load_print_meta: n_swa            = 0
0.00.087.526 I llm_load_print_meta: n_embd_head_k    = 128
0.00.087.526 I llm_load_print_meta: n_embd_head_v    = 128
0.00.087.527 I llm_load_print_meta: n_gqa            = 1
0.00.087.528 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.087.529 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.087.530 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.087.531 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.087.531 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.087.531 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.087.531 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.087.532 I llm_load_print_meta: n_ff             = 8192
0.00.087.533 I llm_load_print_meta: n_expert         = 0
0.00.087.533 I llm_load_print_meta: n_expert_used    = 0
0.00.087.533 I llm_load_print_meta: causal attn      = 1
0.00.087.533 I llm_load_print_meta: pooling type     = 0
0.00.087.536 I llm_load_print_meta: rope type        = 2
0.00.087.536 I llm_load_print_meta: rope scaling     = linear
0.00.087.537 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.087.537 I llm_load_print_meta: freq_scale_train = 1
0.00.087.537 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.087.538 I llm_load_print_meta: rope_finetuned   = unknown
0.00.087.538 I llm_load_print_meta: ssm_d_conv       = 0
0.00.087.540 I llm_load_print_meta: ssm_d_inner      = 0
0.00.087.540 I llm_load_print_meta: ssm_d_state      = 0
0.00.087.540 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.087.541 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.087.541 I llm_load_print_meta: model type       = 1.4B
0.00.087.543 I llm_load_print_meta: model ftype      = Q4_0
0.00.087.544 I llm_load_print_meta: model params     = 1.41 B
0.00.087.545 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.087.545 I llm_load_print_meta: general.name     = 1.4B
0.00.087.545 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.087.545 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.087.546 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.087.546 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.087.548 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.087.548 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.087.548 I llm_load_print_meta: max token length = 1024
0.00.090.208 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.090.208 I llm_load_tensors: offloading output layer to GPU
0.00.090.208 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.090.220 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.090.222 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.091.663 I llama_new_context_with_model: n_seq_max     = 1
0.00.091.665 I llama_new_context_with_model: n_ctx         = 2048
0.00.091.665 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.091.665 I llama_new_context_with_model: n_batch       = 2048
0.00.091.666 I llama_new_context_with_model: n_ubatch      = 512
0.00.091.666 I llama_new_context_with_model: flash_attn    = 0
0.00.091.667 I llama_new_context_with_model: freq_base     = 10000.0
0.00.091.667 I llama_new_context_with_model: freq_scale    = 1
0.00.091.668 I ggml_metal_init: allocating
0.00.091.673 I ggml_metal_init: found device: Apple M4
0.00.091.675 I ggml_metal_init: picking default device: Apple M4
0.00.092.660 I ggml_metal_init: using embedded metal library
0.00.096.472 I ggml_metal_init: GPU name:   Apple M4
0.00.096.475 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.096.475 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.096.476 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.096.476 I ggml_metal_init: simdgroup reduction   = true
0.00.096.476 I ggml_metal_init: simdgroup matrix mul. = true
0.00.096.477 I ggml_metal_init: has bfloat            = true
0.00.096.477 I ggml_metal_init: use bfloat            = true
0.00.096.477 I ggml_metal_init: hasUnifiedMemory      = true
0.00.096.478 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.110.374 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.135.851 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.135.861 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.135.890 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.137.062 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.137.064 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.137.064 I llama_new_context_with_model: graph nodes  = 967
0.00.137.064 I llama_new_context_with_model: graph splits = 2
0.00.137.083 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.137.198 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.137.198 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.900.741 I main: llama threadpool init, n_threads = 4
0.00.900.817 I 
0.00.900.862 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.900.865 I 
0.00.901.108 I sampler seed: 1234
0.00.901.114 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.901.270 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.901.276 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.901.276 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.621.438 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55642.63 tokens per second)
0.01.621.439 I llama_perf_context_print:        load time =     882.22 ms
0.01.621.440 I llama_perf_context_print: prompt eval time =      50.38 ms /     7 tokens (    7.20 ms per token,   138.95 tokens per second)
0.01.621.441 I llama_perf_context_print:        eval time =     666.65 ms /    63 runs   (   10.58 ms per token,    94.50 tokens per second)
0.01.621.442 I llama_perf_context_print:       total time =     720.70 ms /    70 tokens
0.01.621.657 I ggml_metal_free: deallocating

real	0m1.642s
user	0m0.142s
sys	0m0.171s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4365 (609f17d7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.969 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.580 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.584 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.586 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.586 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.587 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.587 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.587 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.590 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.590 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.591 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.592 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.592 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.593 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.593 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.596 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.596 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.596 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.508 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.529 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.375 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.376 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.376 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.377 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.377 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.377 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.378 I llama_model_loader: - type  f32:  194 tensors
0.00.026.378 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.379 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.815 I llm_load_vocab: special tokens cache size = 25
0.00.052.889 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.892 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.892 I llm_load_print_meta: arch             = gptneox
0.00.052.893 I llm_load_print_meta: vocab type       = BPE
0.00.052.893 I llm_load_print_meta: n_vocab          = 50304
0.00.052.893 I llm_load_print_meta: n_merges         = 50009
0.00.052.893 I llm_load_print_meta: vocab_only       = 0
0.00.052.893 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.893 I llm_load_print_meta: n_embd           = 2048
0.00.052.894 I llm_load_print_meta: n_layer          = 24
0.00.052.896 I llm_load_print_meta: n_head           = 16
0.00.052.897 I llm_load_print_meta: n_head_kv        = 16
0.00.052.897 I llm_load_print_meta: n_rot            = 32
0.00.052.900 I llm_load_print_meta: n_swa            = 0
0.00.052.900 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.900 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.901 I llm_load_print_meta: n_gqa            = 1
0.00.052.902 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.902 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.903 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.903 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.904 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.904 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.906 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.906 I llm_load_print_meta: n_ff             = 8192
0.00.052.906 I llm_load_print_meta: n_expert         = 0
0.00.052.906 I llm_load_print_meta: n_expert_used    = 0
0.00.052.908 I llm_load_print_meta: causal attn      = 1
0.00.052.908 I llm_load_print_meta: pooling type     = 0
0.00.052.908 I llm_load_print_meta: rope type        = 2
0.00.052.908 I llm_load_print_meta: rope scaling     = linear
0.00.052.908 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.909 I llm_load_print_meta: freq_scale_train = 1
0.00.052.909 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.909 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.909 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.909 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.910 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.910 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.910 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.914 I llm_load_print_meta: model type       = 1.4B
0.00.052.914 I llm_load_print_meta: model ftype      = Q4_1
0.00.052.915 I llm_load_print_meta: model params     = 1.41 B
0.00.052.915 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.052.915 I llm_load_print_meta: general.name     = 1.4B
0.00.052.916 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.916 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.916 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.916 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.917 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.917 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.917 I llm_load_print_meta: max token length = 1024
0.00.054.571 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.571 I llm_load_tensors: offloading output layer to GPU
0.00.054.571 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.581 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.054.582 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.055.436 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.437 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.437 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.437 I llama_new_context_with_model: n_batch       = 2048
0.00.055.438 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.438 I llama_new_context_with_model: flash_attn    = 0
0.00.055.438 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.438 I llama_new_context_with_model: freq_scale    = 1
0.00.055.439 I ggml_metal_init: allocating
0.00.055.442 I ggml_metal_init: found device: Apple M4
0.00.055.444 I ggml_metal_init: picking default device: Apple M4
0.00.056.042 I ggml_metal_init: using embedded metal library
0.00.058.366 I ggml_metal_init: GPU name:   Apple M4
0.00.058.367 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.368 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.368 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.368 I ggml_metal_init: simdgroup reduction   = true
0.00.058.368 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.368 I ggml_metal_init: has bfloat            = true
0.00.058.369 I ggml_metal_init: use bfloat            = true
0.00.058.370 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.370 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.232 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.089.033 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.042 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.066 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.256 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.257 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.258 I llama_new_context_with_model: graph nodes  = 967
0.00.090.258 I llama_new_context_with_model: graph splits = 2
0.00.090.274 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.423 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.423 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.734.553 I main: llama threadpool init, n_threads = 4
0.00.734.594 I 
0.00.734.626 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.734.626 I 
0.00.734.833 I sampler seed: 1234
0.00.734.837 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.734.865 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.734.866 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.734.866 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.491.326 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62555.07 tokens per second)
0.01.491.327 I llama_perf_context_print:        load time =     724.58 ms
0.01.491.328 I llama_perf_context_print: prompt eval time =      39.62 ms /     7 tokens (    5.66 ms per token,   176.67 tokens per second)
0.01.491.330 I llama_perf_context_print:        eval time =     713.88 ms /    63 runs   (   11.33 ms per token,    88.25 tokens per second)
0.01.491.330 I llama_perf_context_print:       total time =     756.78 ms /    70 tokens
0.01.491.551 I ggml_metal_free: deallocating

real	0m1.509s
user	0m0.111s
sys	0m0.148s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4365 (609f17d7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.010.341 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.117 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.121 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.126 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.127 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.127 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.128 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.130 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.130 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.131 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.131 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.132 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.132 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.132 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.133 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.135 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.135 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.135 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.069 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.105 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.051 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.053 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.053 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.053 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.053 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.054 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.054 I llama_model_loader: - type  f32:  194 tensors
0.00.027.054 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.055 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.039 I llm_load_vocab: special tokens cache size = 25
0.00.053.898 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.901 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.901 I llm_load_print_meta: arch             = gptneox
0.00.053.902 I llm_load_print_meta: vocab type       = BPE
0.00.053.902 I llm_load_print_meta: n_vocab          = 50304
0.00.053.902 I llm_load_print_meta: n_merges         = 50009
0.00.053.902 I llm_load_print_meta: vocab_only       = 0
0.00.053.902 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.903 I llm_load_print_meta: n_embd           = 2048
0.00.053.903 I llm_load_print_meta: n_layer          = 24
0.00.053.905 I llm_load_print_meta: n_head           = 16
0.00.053.906 I llm_load_print_meta: n_head_kv        = 16
0.00.053.906 I llm_load_print_meta: n_rot            = 32
0.00.053.906 I llm_load_print_meta: n_swa            = 0
0.00.053.907 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.907 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.908 I llm_load_print_meta: n_gqa            = 1
0.00.053.908 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.909 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.910 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.910 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.910 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.911 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.911 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.911 I llm_load_print_meta: n_ff             = 8192
0.00.053.911 I llm_load_print_meta: n_expert         = 0
0.00.053.912 I llm_load_print_meta: n_expert_used    = 0
0.00.053.912 I llm_load_print_meta: causal attn      = 1
0.00.053.912 I llm_load_print_meta: pooling type     = 0
0.00.053.912 I llm_load_print_meta: rope type        = 2
0.00.053.912 I llm_load_print_meta: rope scaling     = linear
0.00.053.913 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.913 I llm_load_print_meta: freq_scale_train = 1
0.00.053.913 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.913 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.913 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.914 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.914 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.914 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.914 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.914 I llm_load_print_meta: model type       = 1.4B
0.00.053.914 I llm_load_print_meta: model ftype      = Q5_0
0.00.053.915 I llm_load_print_meta: model params     = 1.41 B
0.00.053.916 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.053.917 I llm_load_print_meta: general.name     = 1.4B
0.00.053.917 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.917 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.917 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.917 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.918 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.918 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.918 I llm_load_print_meta: max token length = 1024
0.00.055.682 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.683 I llm_load_tensors: offloading output layer to GPU
0.00.055.683 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.693 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.055.694 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.056.565 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.565 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.566 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.566 I llama_new_context_with_model: n_batch       = 2048
0.00.056.566 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.566 I llama_new_context_with_model: flash_attn    = 0
0.00.056.567 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.567 I llama_new_context_with_model: freq_scale    = 1
0.00.056.568 I ggml_metal_init: allocating
0.00.056.574 I ggml_metal_init: found device: Apple M4
0.00.056.576 I ggml_metal_init: picking default device: Apple M4
0.00.057.145 I ggml_metal_init: using embedded metal library
0.00.059.502 I ggml_metal_init: GPU name:   Apple M4
0.00.059.504 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.504 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.504 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.504 I ggml_metal_init: simdgroup reduction   = true
0.00.059.505 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.505 I ggml_metal_init: has bfloat            = true
0.00.059.505 I ggml_metal_init: use bfloat            = true
0.00.059.505 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.506 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.122 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.089.308 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.314 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.335 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.319 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.321 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.321 I llama_new_context_with_model: graph nodes  = 967
0.00.090.321 I llama_new_context_with_model: graph splits = 2
0.00.090.336 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.463 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.463 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.780.462 I main: llama threadpool init, n_threads = 4
0.00.780.508 I 
0.00.780.541 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.780.541 I 
0.00.780.707 I sampler seed: 1234
0.00.780.714 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.780.745 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.780.747 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.780.747 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.607.902 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56438.79 tokens per second)
0.01.607.904 I llama_perf_context_print:        load time =     770.12 ms
0.01.607.905 I llama_perf_context_print: prompt eval time =      43.28 ms /     7 tokens (    6.18 ms per token,   161.75 tokens per second)
0.01.607.906 I llama_perf_context_print:        eval time =     780.83 ms /    63 runs   (   12.39 ms per token,    80.68 tokens per second)
0.01.607.906 I llama_perf_context_print:       total time =     827.44 ms /    70 tokens
0.01.608.111 I ggml_metal_free: deallocating

real	0m1.624s
user	0m0.112s
sys	0m0.158s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4365 (609f17d7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.009.612 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.077 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.080 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.081 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.085 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.085 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.086 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.086 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.087 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.087 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.088 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.088 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.089 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.090 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.090 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.092 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.093 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.094 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.972 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.009 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.867 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.869 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.869 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.869 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.869 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.870 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.870 I llama_model_loader: - type  f32:  194 tensors
0.00.025.870 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.871 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.289 I llm_load_vocab: special tokens cache size = 25
0.00.052.313 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.316 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.316 I llm_load_print_meta: arch             = gptneox
0.00.052.317 I llm_load_print_meta: vocab type       = BPE
0.00.052.317 I llm_load_print_meta: n_vocab          = 50304
0.00.052.317 I llm_load_print_meta: n_merges         = 50009
0.00.052.317 I llm_load_print_meta: vocab_only       = 0
0.00.052.318 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.318 I llm_load_print_meta: n_embd           = 2048
0.00.052.318 I llm_load_print_meta: n_layer          = 24
0.00.052.321 I llm_load_print_meta: n_head           = 16
0.00.052.322 I llm_load_print_meta: n_head_kv        = 16
0.00.052.322 I llm_load_print_meta: n_rot            = 32
0.00.052.325 I llm_load_print_meta: n_swa            = 0
0.00.052.325 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.325 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.326 I llm_load_print_meta: n_gqa            = 1
0.00.052.327 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.327 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.328 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.328 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.328 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.329 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.329 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.330 I llm_load_print_meta: n_ff             = 8192
0.00.052.330 I llm_load_print_meta: n_expert         = 0
0.00.052.330 I llm_load_print_meta: n_expert_used    = 0
0.00.052.330 I llm_load_print_meta: causal attn      = 1
0.00.052.330 I llm_load_print_meta: pooling type     = 0
0.00.052.330 I llm_load_print_meta: rope type        = 2
0.00.052.331 I llm_load_print_meta: rope scaling     = linear
0.00.052.331 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.333 I llm_load_print_meta: freq_scale_train = 1
0.00.052.333 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.333 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.334 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.334 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.334 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.334 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.334 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.334 I llm_load_print_meta: model type       = 1.4B
0.00.052.335 I llm_load_print_meta: model ftype      = Q5_1
0.00.052.335 I llm_load_print_meta: model params     = 1.41 B
0.00.052.336 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.052.336 I llm_load_print_meta: general.name     = 1.4B
0.00.052.340 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.340 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.341 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.341 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.341 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.342 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.342 I llm_load_print_meta: max token length = 1024
0.00.053.990 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.990 I llm_load_tensors: offloading output layer to GPU
0.00.053.990 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.001 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.054.002 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.876 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.877 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.877 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.877 I llama_new_context_with_model: n_batch       = 2048
0.00.054.877 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.877 I llama_new_context_with_model: flash_attn    = 0
0.00.054.878 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.878 I llama_new_context_with_model: freq_scale    = 1
0.00.054.879 I ggml_metal_init: allocating
0.00.054.884 I ggml_metal_init: found device: Apple M4
0.00.054.886 I ggml_metal_init: picking default device: Apple M4
0.00.055.497 I ggml_metal_init: using embedded metal library
0.00.057.846 I ggml_metal_init: GPU name:   Apple M4
0.00.057.848 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.848 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.849 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.849 I ggml_metal_init: simdgroup reduction   = true
0.00.057.849 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.849 I ggml_metal_init: has bfloat            = true
0.00.057.849 I ggml_metal_init: use bfloat            = true
0.00.057.850 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.850 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.778 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.088.125 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.133 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.153 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.201 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.203 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.203 I llama_new_context_with_model: graph nodes  = 967
0.00.089.204 I llama_new_context_with_model: graph splits = 2
0.00.089.219 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.365 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.366 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.741.556 I main: llama threadpool init, n_threads = 4
0.00.741.594 I 
0.00.741.626 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.741.627 I 
0.00.741.850 I sampler seed: 1234
0.00.741.854 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.741.895 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.741.899 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.741.900 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.585.995 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61631.94 tokens per second)
0.01.585.995 I llama_perf_context_print:        load time =     731.94 ms
0.01.586.001 I llama_perf_context_print: prompt eval time =      46.16 ms /     7 tokens (    6.59 ms per token,   151.63 tokens per second)
0.01.586.003 I llama_perf_context_print:        eval time =     795.09 ms /    63 runs   (   12.62 ms per token,    79.24 tokens per second)
0.01.586.004 I llama_perf_context_print:       total time =     844.44 ms /    70 tokens
0.01.586.173 I ggml_metal_free: deallocating

real	0m1.604s
user	0m0.109s
sys	0m0.184s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4365 (609f17d7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.009.585 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.123 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.128 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.130 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.130 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.131 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.131 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.133 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.134 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.135 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.135 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.135 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.136 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.136 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.136 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.138 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.138 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.138 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.936 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.949 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.837 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.838 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.838 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.839 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.839 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.839 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.840 I llama_model_loader: - type  f32:  194 tensors
0.00.023.840 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.840 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.840 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.119 I llm_load_vocab: special tokens cache size = 25
0.00.050.959 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.962 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.962 I llm_load_print_meta: arch             = gptneox
0.00.050.963 I llm_load_print_meta: vocab type       = BPE
0.00.050.963 I llm_load_print_meta: n_vocab          = 50304
0.00.050.963 I llm_load_print_meta: n_merges         = 50009
0.00.050.963 I llm_load_print_meta: vocab_only       = 0
0.00.050.963 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.964 I llm_load_print_meta: n_embd           = 2048
0.00.050.964 I llm_load_print_meta: n_layer          = 24
0.00.050.967 I llm_load_print_meta: n_head           = 16
0.00.050.968 I llm_load_print_meta: n_head_kv        = 16
0.00.050.968 I llm_load_print_meta: n_rot            = 32
0.00.050.970 I llm_load_print_meta: n_swa            = 0
0.00.050.970 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.970 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.971 I llm_load_print_meta: n_gqa            = 1
0.00.050.972 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.973 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.974 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.974 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.974 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.974 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.975 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.975 I llm_load_print_meta: n_ff             = 8192
0.00.050.975 I llm_load_print_meta: n_expert         = 0
0.00.050.975 I llm_load_print_meta: n_expert_used    = 0
0.00.050.976 I llm_load_print_meta: causal attn      = 1
0.00.050.982 I llm_load_print_meta: pooling type     = 0
0.00.050.982 I llm_load_print_meta: rope type        = 2
0.00.050.983 I llm_load_print_meta: rope scaling     = linear
0.00.050.983 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.984 I llm_load_print_meta: freq_scale_train = 1
0.00.050.984 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.984 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.984 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.985 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.986 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.986 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.986 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.986 I llm_load_print_meta: model type       = 1.4B
0.00.050.986 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.987 I llm_load_print_meta: model params     = 1.41 B
0.00.050.987 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.987 I llm_load_print_meta: general.name     = 1.4B
0.00.050.987 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.988 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.988 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.988 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.988 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.988 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.989 I llm_load_print_meta: max token length = 1024
0.00.052.857 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.857 I llm_load_tensors: offloading output layer to GPU
0.00.052.857 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.868 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.869 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.785 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.786 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.786 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.786 I llama_new_context_with_model: n_batch       = 2048
0.00.053.786 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.786 I llama_new_context_with_model: flash_attn    = 0
0.00.053.787 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.787 I llama_new_context_with_model: freq_scale    = 1
0.00.053.788 I ggml_metal_init: allocating
0.00.053.792 I ggml_metal_init: found device: Apple M4
0.00.053.795 I ggml_metal_init: picking default device: Apple M4
0.00.054.388 I ggml_metal_init: using embedded metal library
0.00.057.001 I ggml_metal_init: GPU name:   Apple M4
0.00.057.002 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.002 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.003 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.003 I ggml_metal_init: simdgroup reduction   = true
0.00.057.003 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.003 I ggml_metal_init: has bfloat            = true
0.00.057.004 I ggml_metal_init: use bfloat            = true
0.00.057.004 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.005 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.630 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.085.777 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.785 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.810 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.892 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.894 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.894 I llama_new_context_with_model: graph nodes  = 967
0.00.086.894 I llama_new_context_with_model: graph splits = 2
0.00.086.910 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.059 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.059 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.434.396 I main: llama threadpool init, n_threads = 4
0.00.434.432 I 
0.00.434.462 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.434.462 I 
0.00.434.622 I sampler seed: 1234
0.00.434.627 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.434.670 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.434.671 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.434.671 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.114.551 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61739.13 tokens per second)
0.01.114.551 I llama_perf_context_print:        load time =     424.81 ms
0.01.114.552 I llama_perf_context_print: prompt eval time =      35.73 ms /     7 tokens (    5.10 ms per token,   195.91 tokens per second)
0.01.114.552 I llama_perf_context_print:        eval time =     641.18 ms /    63 runs   (   10.18 ms per token,    98.26 tokens per second)
0.01.114.553 I llama_perf_context_print:       total time =     680.16 ms /    70 tokens
0.01.114.718 I ggml_metal_free: deallocating

real	0m1.133s
user	0m0.110s
sys	0m0.103s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4365 (609f17d7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.008.775 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.282 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.287 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.290 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.291 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.291 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.291 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.292 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.294 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.294 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.295 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.295 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.295 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.296 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.300 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.302 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.302 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.302 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.230 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.256 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.140 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.141 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.141 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.142 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.142 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.142 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.143 I llama_model_loader: - type  f32:  194 tensors
0.00.024.143 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.143 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.144 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.144 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.405 I llm_load_vocab: special tokens cache size = 25
0.00.051.349 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.352 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.352 I llm_load_print_meta: arch             = gptneox
0.00.051.353 I llm_load_print_meta: vocab type       = BPE
0.00.051.353 I llm_load_print_meta: n_vocab          = 50304
0.00.051.353 I llm_load_print_meta: n_merges         = 50009
0.00.051.353 I llm_load_print_meta: vocab_only       = 0
0.00.051.353 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.354 I llm_load_print_meta: n_embd           = 2048
0.00.051.354 I llm_load_print_meta: n_layer          = 24
0.00.051.357 I llm_load_print_meta: n_head           = 16
0.00.051.358 I llm_load_print_meta: n_head_kv        = 16
0.00.051.358 I llm_load_print_meta: n_rot            = 32
0.00.051.358 I llm_load_print_meta: n_swa            = 0
0.00.051.358 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.358 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.359 I llm_load_print_meta: n_gqa            = 1
0.00.051.360 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.361 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.363 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.364 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.364 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.364 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.364 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.365 I llm_load_print_meta: n_ff             = 8192
0.00.051.366 I llm_load_print_meta: n_expert         = 0
0.00.051.368 I llm_load_print_meta: n_expert_used    = 0
0.00.051.368 I llm_load_print_meta: causal attn      = 1
0.00.051.368 I llm_load_print_meta: pooling type     = 0
0.00.051.368 I llm_load_print_meta: rope type        = 2
0.00.051.368 I llm_load_print_meta: rope scaling     = linear
0.00.051.369 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.369 I llm_load_print_meta: freq_scale_train = 1
0.00.051.369 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.370 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.370 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.370 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.370 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.370 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.370 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.371 I llm_load_print_meta: model type       = 1.4B
0.00.051.371 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.371 I llm_load_print_meta: model params     = 1.41 B
0.00.051.372 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.372 I llm_load_print_meta: general.name     = 1.4B
0.00.051.372 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.372 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.373 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.373 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.377 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.377 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.377 I llm_load_print_meta: max token length = 1024
0.00.053.005 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.005 I llm_load_tensors: offloading output layer to GPU
0.00.053.005 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.015 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.016 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.846 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.847 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.847 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.848 I llama_new_context_with_model: n_batch       = 2048
0.00.053.848 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.848 I llama_new_context_with_model: flash_attn    = 0
0.00.053.849 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.849 I llama_new_context_with_model: freq_scale    = 1
0.00.053.849 I ggml_metal_init: allocating
0.00.053.855 I ggml_metal_init: found device: Apple M4
0.00.053.858 I ggml_metal_init: picking default device: Apple M4
0.00.054.436 I ggml_metal_init: using embedded metal library
0.00.056.791 I ggml_metal_init: GPU name:   Apple M4
0.00.056.793 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.793 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.794 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.794 I ggml_metal_init: simdgroup reduction   = true
0.00.056.794 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.795 I ggml_metal_init: has bfloat            = true
0.00.056.795 I ggml_metal_init: use bfloat            = true
0.00.056.795 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.796 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.317 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.085.094 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.098 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.118 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.131 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.133 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.133 I llama_new_context_with_model: graph nodes  = 967
0.00.086.133 I llama_new_context_with_model: graph splits = 2
0.00.086.149 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.275 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.275 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.540.997 I main: llama threadpool init, n_threads = 4
0.00.541.035 I 
0.00.541.065 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.541.066 I 
0.00.541.321 I sampler seed: 1234
0.00.541.324 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.541.368 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.541.372 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.541.372 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.290.733 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61792.86 tokens per second)
0.01.290.734 I llama_perf_context_print:        load time =     532.22 ms
0.01.290.735 I llama_perf_context_print: prompt eval time =      44.40 ms /     7 tokens (    6.34 ms per token,   157.67 tokens per second)
0.01.290.735 I llama_perf_context_print:        eval time =     702.06 ms /    63 runs   (   11.14 ms per token,    89.74 tokens per second)
0.01.290.739 I llama_perf_context_print:       total time =     749.74 ms /    70 tokens
0.01.290.951 I ggml_metal_free: deallocating

real	0m1.306s
user	0m0.111s
sys	0m0.126s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4365 (609f17d7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.012.044 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.625 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.018.630 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.636 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.637 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.637 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.638 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.638 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.639 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.639 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.639 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.640 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.640 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.640 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.641 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.642 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.642 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.643 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.590 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.686 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.618 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.619 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.619 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.619 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.620 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.620 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.027.621 I llama_model_loader: - type  f32:  194 tensors
0.00.027.621 I llama_model_loader: - type q4_K:   61 tensors
0.00.027.621 I llama_model_loader: - type q5_K:   24 tensors
0.00.027.621 I llama_model_loader: - type q6_K:   13 tensors
0.00.048.964 I llm_load_vocab: special tokens cache size = 25
0.00.055.216 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.055.220 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.055.221 I llm_load_print_meta: arch             = gptneox
0.00.055.221 I llm_load_print_meta: vocab type       = BPE
0.00.055.221 I llm_load_print_meta: n_vocab          = 50304
0.00.055.222 I llm_load_print_meta: n_merges         = 50009
0.00.055.222 I llm_load_print_meta: vocab_only       = 0
0.00.055.222 I llm_load_print_meta: n_ctx_train      = 2048
0.00.055.222 I llm_load_print_meta: n_embd           = 2048
0.00.055.222 I llm_load_print_meta: n_layer          = 24
0.00.055.226 I llm_load_print_meta: n_head           = 16
0.00.055.227 I llm_load_print_meta: n_head_kv        = 16
0.00.055.227 I llm_load_print_meta: n_rot            = 32
0.00.055.230 I llm_load_print_meta: n_swa            = 0
0.00.055.230 I llm_load_print_meta: n_embd_head_k    = 128
0.00.055.230 I llm_load_print_meta: n_embd_head_v    = 128
0.00.055.231 I llm_load_print_meta: n_gqa            = 1
0.00.055.231 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.055.232 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.055.232 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.055.233 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.055.233 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.055.233 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.055.233 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.055.234 I llm_load_print_meta: n_ff             = 8192
0.00.055.234 I llm_load_print_meta: n_expert         = 0
0.00.055.234 I llm_load_print_meta: n_expert_used    = 0
0.00.055.234 I llm_load_print_meta: causal attn      = 1
0.00.055.234 I llm_load_print_meta: pooling type     = 0
0.00.055.234 I llm_load_print_meta: rope type        = 2
0.00.055.234 I llm_load_print_meta: rope scaling     = linear
0.00.055.236 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.055.236 I llm_load_print_meta: freq_scale_train = 1
0.00.055.236 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.055.236 I llm_load_print_meta: rope_finetuned   = unknown
0.00.055.236 I llm_load_print_meta: ssm_d_conv       = 0
0.00.055.254 I llm_load_print_meta: ssm_d_inner      = 0
0.00.055.256 I llm_load_print_meta: ssm_d_state      = 0
0.00.055.256 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.055.257 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.055.257 I llm_load_print_meta: model type       = 1.4B
0.00.055.257 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.055.258 I llm_load_print_meta: model params     = 1.41 B
0.00.055.258 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.055.262 I llm_load_print_meta: general.name     = 1.4B
0.00.055.262 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.055.262 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.055.263 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.055.264 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.055.264 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.055.264 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.055.264 I llm_load_print_meta: max token length = 1024
0.00.057.229 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.057.229 I llm_load_tensors: offloading output layer to GPU
0.00.057.230 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.057.240 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.057.241 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.058.192 I llama_new_context_with_model: n_seq_max     = 1
0.00.058.193 I llama_new_context_with_model: n_ctx         = 2048
0.00.058.194 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.058.194 I llama_new_context_with_model: n_batch       = 2048
0.00.058.194 I llama_new_context_with_model: n_ubatch      = 512
0.00.058.194 I llama_new_context_with_model: flash_attn    = 0
0.00.058.195 I llama_new_context_with_model: freq_base     = 10000.0
0.00.058.195 I llama_new_context_with_model: freq_scale    = 1
0.00.058.195 I ggml_metal_init: allocating
0.00.058.200 I ggml_metal_init: found device: Apple M4
0.00.058.202 I ggml_metal_init: picking default device: Apple M4
0.00.058.813 I ggml_metal_init: using embedded metal library
0.00.061.482 I ggml_metal_init: GPU name:   Apple M4
0.00.061.485 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.485 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.486 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.486 I ggml_metal_init: simdgroup reduction   = true
0.00.061.486 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.486 I ggml_metal_init: has bfloat            = true
0.00.061.486 I ggml_metal_init: use bfloat            = true
0.00.061.487 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.488 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.628 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.090.674 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.090.684 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.090.704 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.091.753 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.091.755 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.091.755 I llama_new_context_with_model: graph nodes  = 967
0.00.091.755 I llama_new_context_with_model: graph splits = 2
0.00.091.770 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.091.918 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.091.919 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.588.411 I main: llama threadpool init, n_threads = 4
0.00.588.452 I 
0.00.588.483 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.588.484 I 
0.00.588.708 I sampler seed: 1234
0.00.588.713 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.588.755 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.588.757 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.588.758 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.344.968 I llama_perf_sampler_print:    sampling time =       1.51 ms /    71 runs   (    0.02 ms per token, 46926.64 tokens per second)
0.01.344.969 I llama_perf_context_print:        load time =     576.36 ms
0.01.344.970 I llama_perf_context_print: prompt eval time =      47.29 ms /     7 tokens (    6.76 ms per token,   148.03 tokens per second)
0.01.344.971 I llama_perf_context_print:        eval time =     706.28 ms /    63 runs   (   11.21 ms per token,    89.20 tokens per second)
0.01.344.971 I llama_perf_context_print:       total time =     756.56 ms /    70 tokens
0.01.345.224 I ggml_metal_free: deallocating

real	0m1.365s
user	0m0.110s
sys	0m0.118s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4365 (609f17d7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.009.644 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.567 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.572 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.578 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.579 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.579 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.580 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.582 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.583 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.583 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.583 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.583 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.584 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.584 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.584 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.586 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.586 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.586 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.525 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.577 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.462 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.463 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.463 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.464 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.464 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.464 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.465 I llama_model_loader: - type  f32:  194 tensors
0.00.025.465 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.465 I llama_model_loader: - type q6_K:   37 tensors
0.00.046.014 I llm_load_vocab: special tokens cache size = 25
0.00.051.926 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.929 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.929 I llm_load_print_meta: arch             = gptneox
0.00.051.929 I llm_load_print_meta: vocab type       = BPE
0.00.051.930 I llm_load_print_meta: n_vocab          = 50304
0.00.051.930 I llm_load_print_meta: n_merges         = 50009
0.00.051.930 I llm_load_print_meta: vocab_only       = 0
0.00.051.930 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.930 I llm_load_print_meta: n_embd           = 2048
0.00.051.930 I llm_load_print_meta: n_layer          = 24
0.00.051.933 I llm_load_print_meta: n_head           = 16
0.00.051.934 I llm_load_print_meta: n_head_kv        = 16
0.00.051.934 I llm_load_print_meta: n_rot            = 32
0.00.051.934 I llm_load_print_meta: n_swa            = 0
0.00.051.935 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.935 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.935 I llm_load_print_meta: n_gqa            = 1
0.00.051.936 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.937 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.938 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.938 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.938 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.938 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.938 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.939 I llm_load_print_meta: n_ff             = 8192
0.00.051.939 I llm_load_print_meta: n_expert         = 0
0.00.051.939 I llm_load_print_meta: n_expert_used    = 0
0.00.051.940 I llm_load_print_meta: causal attn      = 1
0.00.051.940 I llm_load_print_meta: pooling type     = 0
0.00.051.940 I llm_load_print_meta: rope type        = 2
0.00.051.942 I llm_load_print_meta: rope scaling     = linear
0.00.051.942 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.943 I llm_load_print_meta: freq_scale_train = 1
0.00.051.943 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.943 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.943 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.943 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.944 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.944 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.944 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.944 I llm_load_print_meta: model type       = 1.4B
0.00.051.944 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.945 I llm_load_print_meta: model params     = 1.41 B
0.00.051.946 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.946 I llm_load_print_meta: general.name     = 1.4B
0.00.051.946 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.946 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.946 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.947 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.947 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.947 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.947 I llm_load_print_meta: max token length = 1024
0.00.053.989 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.989 I llm_load_tensors: offloading output layer to GPU
0.00.053.989 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.999 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.054.001 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.973 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.974 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.975 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.975 I llama_new_context_with_model: n_batch       = 2048
0.00.054.975 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.975 I llama_new_context_with_model: flash_attn    = 0
0.00.054.976 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.976 I llama_new_context_with_model: freq_scale    = 1
0.00.054.976 I ggml_metal_init: allocating
0.00.054.979 I ggml_metal_init: found device: Apple M4
0.00.054.981 I ggml_metal_init: picking default device: Apple M4
0.00.055.568 I ggml_metal_init: using embedded metal library
0.00.057.916 I ggml_metal_init: GPU name:   Apple M4
0.00.057.917 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.918 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.918 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.918 I ggml_metal_init: simdgroup reduction   = true
0.00.057.918 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.919 I ggml_metal_init: has bfloat            = true
0.00.057.919 I ggml_metal_init: use bfloat            = true
0.00.057.919 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.920 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.007 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.088.952 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.957 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.976 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.003 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.005 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.005 I llama_new_context_with_model: graph nodes  = 967
0.00.090.005 I llama_new_context_with_model: graph splits = 2
0.00.090.021 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.137 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.138 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.689.550 I main: llama threadpool init, n_threads = 4
0.00.689.588 I 
0.00.689.629 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.689.630 I 
0.00.689.858 I sampler seed: 1234
0.00.689.862 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.689.877 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.689.878 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.689.878 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.539.964 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61792.86 tokens per second)
0.01.539.965 I llama_perf_context_print:        load time =     679.90 ms
0.01.539.968 I llama_perf_context_print: prompt eval time =      51.59 ms /     7 tokens (    7.37 ms per token,   135.67 tokens per second)
0.01.539.969 I llama_perf_context_print:        eval time =     795.60 ms /    63 runs   (   12.63 ms per token,    79.19 tokens per second)
0.01.539.969 I llama_perf_context_print:       total time =     850.42 ms /    70 tokens
0.01.540.158 I ggml_metal_free: deallocating

real	0m1.559s
user	0m0.110s
sys	0m0.153s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4365 (609f17d7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.009.658 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.681 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.685 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.687 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.687 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.688 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.688 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.688 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.689 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.689 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.690 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.690 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.691 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.691 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.691 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.694 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.694 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.695 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.663 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.714 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.602 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.603 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.603 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.604 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.604 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.604 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.605 I llama_model_loader: - type  f32:  194 tensors
0.00.025.605 I llama_model_loader: - type q6_K:   98 tensors
0.00.046.952 I llm_load_vocab: special tokens cache size = 25
0.00.052.954 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.957 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.957 I llm_load_print_meta: arch             = gptneox
0.00.052.957 I llm_load_print_meta: vocab type       = BPE
0.00.052.958 I llm_load_print_meta: n_vocab          = 50304
0.00.052.958 I llm_load_print_meta: n_merges         = 50009
0.00.052.958 I llm_load_print_meta: vocab_only       = 0
0.00.052.958 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.958 I llm_load_print_meta: n_embd           = 2048
0.00.052.959 I llm_load_print_meta: n_layer          = 24
0.00.052.962 I llm_load_print_meta: n_head           = 16
0.00.052.962 I llm_load_print_meta: n_head_kv        = 16
0.00.052.962 I llm_load_print_meta: n_rot            = 32
0.00.052.963 I llm_load_print_meta: n_swa            = 0
0.00.052.965 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.965 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.966 I llm_load_print_meta: n_gqa            = 1
0.00.052.967 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.967 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.968 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.968 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.968 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.969 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.969 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.970 I llm_load_print_meta: n_ff             = 8192
0.00.052.970 I llm_load_print_meta: n_expert         = 0
0.00.052.970 I llm_load_print_meta: n_expert_used    = 0
0.00.052.970 I llm_load_print_meta: causal attn      = 1
0.00.052.971 I llm_load_print_meta: pooling type     = 0
0.00.052.972 I llm_load_print_meta: rope type        = 2
0.00.052.972 I llm_load_print_meta: rope scaling     = linear
0.00.052.973 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.973 I llm_load_print_meta: freq_scale_train = 1
0.00.052.973 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.974 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.974 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.974 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.974 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.974 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.974 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.974 I llm_load_print_meta: model type       = 1.4B
0.00.052.975 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.979 I llm_load_print_meta: model params     = 1.41 B
0.00.052.980 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.980 I llm_load_print_meta: general.name     = 1.4B
0.00.052.981 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.981 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.981 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.982 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.982 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.982 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.982 I llm_load_print_meta: max token length = 1024
0.00.054.565 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.565 I llm_load_tensors: offloading output layer to GPU
0.00.054.565 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.575 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.576 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.055.436 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.437 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.437 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.437 I llama_new_context_with_model: n_batch       = 2048
0.00.055.437 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.437 I llama_new_context_with_model: flash_attn    = 0
0.00.055.438 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.438 I llama_new_context_with_model: freq_scale    = 1
0.00.055.439 I ggml_metal_init: allocating
0.00.055.445 I ggml_metal_init: found device: Apple M4
0.00.055.447 I ggml_metal_init: picking default device: Apple M4
0.00.056.032 I ggml_metal_init: using embedded metal library
0.00.058.339 I ggml_metal_init: GPU name:   Apple M4
0.00.058.340 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.341 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.341 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.341 I ggml_metal_init: simdgroup reduction   = true
0.00.058.342 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.342 I ggml_metal_init: has bfloat            = true
0.00.058.342 I ggml_metal_init: use bfloat            = true
0.00.058.343 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.343 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.948 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.087.434 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.439 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.458 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.428 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.429 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.429 I llama_new_context_with_model: graph nodes  = 967
0.00.088.429 I llama_new_context_with_model: graph splits = 2
0.00.088.444 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.588 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.589 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.760.070 I main: llama threadpool init, n_threads = 4
0.00.760.105 I 
0.00.760.151 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.760.152 I 
0.00.760.378 I sampler seed: 1234
0.00.760.384 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.760.400 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.760.400 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.760.400 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.644.044 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61206.90 tokens per second)
0.01.644.045 I llama_perf_context_print:        load time =     750.41 ms
0.01.644.045 I llama_perf_context_print: prompt eval time =      54.40 ms /     7 tokens (    7.77 ms per token,   128.67 tokens per second)
0.01.644.046 I llama_perf_context_print:        eval time =     826.36 ms /    63 runs   (   13.12 ms per token,    76.24 tokens per second)
0.01.644.046 I llama_perf_context_print:       total time =     883.98 ms /    70 tokens
0.01.644.238 I ggml_metal_free: deallocating

real	0m1.663s
user	0m0.111s
sys	0m0.165s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.735 I build: 4365 (609f17d7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.160 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.973 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.980 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.982 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.983 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.984 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.985 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.985 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.986 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.987 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.988 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.988 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.989 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.989 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.990 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.993 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.993 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.994 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.527 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.044.498 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.112 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.051.114 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.114 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.051.115 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.051.115 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.051.116 I llama_model_loader: - type  f32:  194 tensors
0.00.051.116 I llama_model_loader: - type  f16:   98 tensors
0.00.079.702 I llm_load_vocab: special tokens cache size = 25
0.00.086.148 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.086.151 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.086.151 I llm_load_print_meta: arch             = gptneox
0.00.086.152 I llm_load_print_meta: vocab type       = BPE
0.00.086.152 I llm_load_print_meta: n_vocab          = 50304
0.00.086.152 I llm_load_print_meta: n_merges         = 50009
0.00.086.152 I llm_load_print_meta: vocab_only       = 0
0.00.086.152 I llm_load_print_meta: n_ctx_train      = 2048
0.00.086.152 I llm_load_print_meta: n_embd           = 2048
0.00.086.152 I llm_load_print_meta: n_layer          = 24
0.00.086.155 I llm_load_print_meta: n_head           = 16
0.00.086.156 I llm_load_print_meta: n_head_kv        = 16
0.00.086.156 I llm_load_print_meta: n_rot            = 32
0.00.086.156 I llm_load_print_meta: n_swa            = 0
0.00.086.159 I llm_load_print_meta: n_embd_head_k    = 128
0.00.086.159 I llm_load_print_meta: n_embd_head_v    = 128
0.00.086.160 I llm_load_print_meta: n_gqa            = 1
0.00.086.160 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.086.161 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.086.162 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.086.162 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.086.162 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.086.162 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.086.162 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.086.163 I llm_load_print_meta: n_ff             = 8192
0.00.086.163 I llm_load_print_meta: n_expert         = 0
0.00.086.163 I llm_load_print_meta: n_expert_used    = 0
0.00.086.163 I llm_load_print_meta: causal attn      = 1
0.00.086.164 I llm_load_print_meta: pooling type     = 0
0.00.086.164 I llm_load_print_meta: rope type        = 2
0.00.086.164 I llm_load_print_meta: rope scaling     = linear
0.00.086.168 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.086.168 I llm_load_print_meta: freq_scale_train = 1
0.00.086.169 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.086.169 I llm_load_print_meta: rope_finetuned   = unknown
0.00.086.169 I llm_load_print_meta: ssm_d_conv       = 0
0.00.086.169 I llm_load_print_meta: ssm_d_inner      = 0
0.00.086.169 I llm_load_print_meta: ssm_d_state      = 0
0.00.086.169 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.086.169 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.086.170 I llm_load_print_meta: model type       = 1.4B
0.00.086.170 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.086.171 I llm_load_print_meta: model params     = 1.41 B
0.00.086.171 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.086.171 I llm_load_print_meta: general.name     = 1.4B
0.00.086.172 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.086.172 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.086.172 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.086.172 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.086.172 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.086.173 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.086.173 I llm_load_print_meta: max token length = 1024
0.00.088.833 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.088.833 I llm_load_tensors: offloading output layer to GPU
0.00.088.833 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.088.844 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.088.845 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.089.805 I llama_new_context_with_model: n_seq_max     = 1
0.00.089.806 I llama_new_context_with_model: n_ctx         = 128
0.00.089.806 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.089.806 I llama_new_context_with_model: n_batch       = 128
0.00.089.807 I llama_new_context_with_model: n_ubatch      = 128
0.00.089.807 I llama_new_context_with_model: flash_attn    = 0
0.00.089.807 I llama_new_context_with_model: freq_base     = 10000.0
0.00.089.807 I llama_new_context_with_model: freq_scale    = 1
0.00.089.808 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.089.809 I ggml_metal_init: allocating
0.00.089.816 I ggml_metal_init: found device: Apple M4
0.00.089.818 I ggml_metal_init: picking default device: Apple M4
0.00.090.429 I ggml_metal_init: using embedded metal library
0.00.093.075 I ggml_metal_init: GPU name:   Apple M4
0.00.093.077 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.093.077 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.093.078 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.093.078 I ggml_metal_init: simdgroup reduction   = true
0.00.093.078 I ggml_metal_init: simdgroup matrix mul. = true
0.00.093.078 I ggml_metal_init: has bfloat            = true
0.00.093.078 I ggml_metal_init: use bfloat            = true
0.00.093.079 I ggml_metal_init: hasUnifiedMemory      = true
0.00.093.080 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.102.679 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.104.030 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.104.032 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.104.048 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.104.889 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.104.890 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.104.891 I llama_new_context_with_model: graph nodes  = 967
0.00.104.891 I llama_new_context_with_model: graph splits = 2
0.00.104.903 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.104.903 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.884.443 I 
0.00.884.492 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.884.536 I perplexity: tokenizing the input ..
0.00.896.596 I perplexity: tokenization took 12.059 ms
0.00.896.602 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.017.366 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.019.181 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.019.204 I llama_perf_context_print:        load time =     863.27 ms
0.01.019.206 I llama_perf_context_print: prompt eval time =     120.38 ms /   128 tokens (    0.94 ms per token,  1063.33 tokens per second)
0.01.019.207 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.019.214 I llama_perf_context_print:       total time =     134.76 ms /   129 tokens
0.01.019.829 I ggml_metal_free: deallocating

real	0m1.211s
user	0m0.122s
sys	0m0.195s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.145 I build: 4365 (609f17d7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.236 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.879 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.021.886 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.888 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.889 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.889 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.890 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.890 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.891 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.892 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.892 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.893 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.893 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.894 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.894 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.896 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.897 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.899 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.512 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.193 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.616 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.618 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.618 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.619 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.619 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.619 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.620 I llama_model_loader: - type  f32:  194 tensors
0.00.035.620 I llama_model_loader: - type q8_0:   98 tensors
0.00.062.500 I llm_load_vocab: special tokens cache size = 25
0.00.068.747 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.068.750 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.068.750 I llm_load_print_meta: arch             = gptneox
0.00.068.751 I llm_load_print_meta: vocab type       = BPE
0.00.068.751 I llm_load_print_meta: n_vocab          = 50304
0.00.068.751 I llm_load_print_meta: n_merges         = 50009
0.00.068.752 I llm_load_print_meta: vocab_only       = 0
0.00.068.752 I llm_load_print_meta: n_ctx_train      = 2048
0.00.068.752 I llm_load_print_meta: n_embd           = 2048
0.00.068.752 I llm_load_print_meta: n_layer          = 24
0.00.068.756 I llm_load_print_meta: n_head           = 16
0.00.068.757 I llm_load_print_meta: n_head_kv        = 16
0.00.068.757 I llm_load_print_meta: n_rot            = 32
0.00.068.757 I llm_load_print_meta: n_swa            = 0
0.00.068.757 I llm_load_print_meta: n_embd_head_k    = 128
0.00.068.757 I llm_load_print_meta: n_embd_head_v    = 128
0.00.068.758 I llm_load_print_meta: n_gqa            = 1
0.00.068.759 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.068.759 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.068.760 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.068.760 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.068.760 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.068.760 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.068.761 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.068.764 I llm_load_print_meta: n_ff             = 8192
0.00.068.764 I llm_load_print_meta: n_expert         = 0
0.00.068.764 I llm_load_print_meta: n_expert_used    = 0
0.00.068.764 I llm_load_print_meta: causal attn      = 1
0.00.068.764 I llm_load_print_meta: pooling type     = 0
0.00.068.765 I llm_load_print_meta: rope type        = 2
0.00.068.765 I llm_load_print_meta: rope scaling     = linear
0.00.068.765 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.068.766 I llm_load_print_meta: freq_scale_train = 1
0.00.068.766 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.068.766 I llm_load_print_meta: rope_finetuned   = unknown
0.00.068.766 I llm_load_print_meta: ssm_d_conv       = 0
0.00.068.766 I llm_load_print_meta: ssm_d_inner      = 0
0.00.068.766 I llm_load_print_meta: ssm_d_state      = 0
0.00.068.767 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.068.767 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.068.767 I llm_load_print_meta: model type       = 1.4B
0.00.068.768 I llm_load_print_meta: model ftype      = Q8_0
0.00.068.768 I llm_load_print_meta: model params     = 1.41 B
0.00.068.769 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.068.769 I llm_load_print_meta: general.name     = 1.4B
0.00.068.769 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.068.769 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.068.769 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.068.769 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.068.770 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.068.770 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.068.770 I llm_load_print_meta: max token length = 1024
0.00.071.257 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.071.257 I llm_load_tensors: offloading output layer to GPU
0.00.071.258 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.071.269 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.071.270 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.072.334 I llama_new_context_with_model: n_seq_max     = 1
0.00.072.335 I llama_new_context_with_model: n_ctx         = 128
0.00.072.336 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.072.336 I llama_new_context_with_model: n_batch       = 128
0.00.072.336 I llama_new_context_with_model: n_ubatch      = 128
0.00.072.336 I llama_new_context_with_model: flash_attn    = 0
0.00.072.337 I llama_new_context_with_model: freq_base     = 10000.0
0.00.072.337 I llama_new_context_with_model: freq_scale    = 1
0.00.072.337 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.072.338 I ggml_metal_init: allocating
0.00.072.345 I ggml_metal_init: found device: Apple M4
0.00.072.350 I ggml_metal_init: picking default device: Apple M4
0.00.073.050 I ggml_metal_init: using embedded metal library
0.00.075.834 I ggml_metal_init: GPU name:   Apple M4
0.00.075.835 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.075.836 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.075.836 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.075.837 I ggml_metal_init: simdgroup reduction   = true
0.00.075.837 I ggml_metal_init: simdgroup matrix mul. = true
0.00.075.837 I ggml_metal_init: has bfloat            = true
0.00.075.837 I ggml_metal_init: use bfloat            = true
0.00.075.838 I ggml_metal_init: hasUnifiedMemory      = true
0.00.075.838 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.126 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.087.546 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.087.549 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.087.567 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.490 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.088.491 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.088.492 I llama_new_context_with_model: graph nodes  = 967
0.00.088.492 I llama_new_context_with_model: graph splits = 2
0.00.088.506 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.088.507 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.939.605 I 
0.00.939.664 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.939.684 I perplexity: tokenizing the input ..
0.00.948.258 I perplexity: tokenization took 8.572 ms
0.00.948.262 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.071.832 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.073.223 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.073.238 I llama_perf_context_print:        load time =     927.36 ms
0.01.073.239 I llama_perf_context_print: prompt eval time =     123.33 ms /   128 tokens (    0.96 ms per token,  1037.83 tokens per second)
0.01.073.239 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.073.240 I llama_perf_context_print:       total time =     133.64 ms /   129 tokens
0.01.073.622 I ggml_metal_free: deallocating

real	0m1.092s
user	0m0.098s
sys	0m0.165s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4365 (609f17d7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.339 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.615 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.620 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.622 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.622 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.622 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.623 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.623 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.625 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.625 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.625 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.626 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.626 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.626 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.627 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.628 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.629 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.629 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.611 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.716 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.678 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.680 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.680 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.680 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.681 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.681 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.682 I llama_model_loader: - type  f32:  194 tensors
0.00.024.682 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.682 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.938 I llm_load_vocab: special tokens cache size = 25
0.00.052.045 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.049 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.050 I llm_load_print_meta: arch             = gptneox
0.00.052.050 I llm_load_print_meta: vocab type       = BPE
0.00.052.050 I llm_load_print_meta: n_vocab          = 50304
0.00.052.050 I llm_load_print_meta: n_merges         = 50009
0.00.052.050 I llm_load_print_meta: vocab_only       = 0
0.00.052.051 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.051 I llm_load_print_meta: n_embd           = 2048
0.00.052.051 I llm_load_print_meta: n_layer          = 24
0.00.052.054 I llm_load_print_meta: n_head           = 16
0.00.052.055 I llm_load_print_meta: n_head_kv        = 16
0.00.052.055 I llm_load_print_meta: n_rot            = 32
0.00.052.055 I llm_load_print_meta: n_swa            = 0
0.00.052.056 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.056 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.056 I llm_load_print_meta: n_gqa            = 1
0.00.052.057 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.058 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.058 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.059 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.059 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.059 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.059 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.060 I llm_load_print_meta: n_ff             = 8192
0.00.052.060 I llm_load_print_meta: n_expert         = 0
0.00.052.063 I llm_load_print_meta: n_expert_used    = 0
0.00.052.063 I llm_load_print_meta: causal attn      = 1
0.00.052.063 I llm_load_print_meta: pooling type     = 0
0.00.052.063 I llm_load_print_meta: rope type        = 2
0.00.052.063 I llm_load_print_meta: rope scaling     = linear
0.00.052.063 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.064 I llm_load_print_meta: freq_scale_train = 1
0.00.052.064 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.064 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.064 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.064 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.064 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.065 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.065 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.065 I llm_load_print_meta: model type       = 1.4B
0.00.052.065 I llm_load_print_meta: model ftype      = Q4_0
0.00.052.066 I llm_load_print_meta: model params     = 1.41 B
0.00.052.066 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.052.066 I llm_load_print_meta: general.name     = 1.4B
0.00.052.066 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.067 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.067 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.067 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.067 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.067 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.068 I llm_load_print_meta: max token length = 1024
0.00.054.064 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.064 I llm_load_tensors: offloading output layer to GPU
0.00.054.064 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.076 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.054.077 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.054.987 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.987 I llama_new_context_with_model: n_ctx         = 128
0.00.054.988 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.988 I llama_new_context_with_model: n_batch       = 128
0.00.054.988 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.988 I llama_new_context_with_model: flash_attn    = 0
0.00.054.989 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.989 I llama_new_context_with_model: freq_scale    = 1
0.00.054.989 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.990 I ggml_metal_init: allocating
0.00.054.994 I ggml_metal_init: found device: Apple M4
0.00.054.996 I ggml_metal_init: picking default device: Apple M4
0.00.055.623 I ggml_metal_init: using embedded metal library
0.00.058.081 I ggml_metal_init: GPU name:   Apple M4
0.00.058.083 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.083 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.083 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.084 I ggml_metal_init: simdgroup reduction   = true
0.00.058.084 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.084 I ggml_metal_init: has bfloat            = true
0.00.058.084 I ggml_metal_init: use bfloat            = true
0.00.058.085 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.086 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.478 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.750 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.752 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.778 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.698 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.700 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.700 I llama_new_context_with_model: graph nodes  = 967
0.00.069.700 I llama_new_context_with_model: graph splits = 2
0.00.069.713 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.714 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.614.130 I 
0.00.614.166 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.614.179 I perplexity: tokenizing the input ..
0.00.622.340 I perplexity: tokenization took 8.16 ms
0.00.622.344 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.745.224 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.746.493 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.746.539 I llama_perf_context_print:        load time =     604.78 ms
0.00.746.540 I llama_perf_context_print: prompt eval time =     122.65 ms /   128 tokens (    0.96 ms per token,  1043.65 tokens per second)
0.00.746.541 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.746.542 I llama_perf_context_print:       total time =     132.41 ms /   129 tokens
0.00.747.086 I ggml_metal_free: deallocating

real	0m0.763s
user	0m0.080s
sys	0m0.100s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4365 (609f17d7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.864 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.813 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.817 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.819 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.819 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.820 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.820 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.820 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.821 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.821 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.822 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.822 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.822 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.823 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.823 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.826 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.826 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.827 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.697 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.728 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.462 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.464 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.464 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.464 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.464 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.465 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.465 I llama_model_loader: - type  f32:  194 tensors
0.00.023.466 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.466 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.618 I llm_load_vocab: special tokens cache size = 25
0.00.050.520 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.523 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.523 I llm_load_print_meta: arch             = gptneox
0.00.050.524 I llm_load_print_meta: vocab type       = BPE
0.00.050.524 I llm_load_print_meta: n_vocab          = 50304
0.00.050.524 I llm_load_print_meta: n_merges         = 50009
0.00.050.524 I llm_load_print_meta: vocab_only       = 0
0.00.050.524 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.524 I llm_load_print_meta: n_embd           = 2048
0.00.050.525 I llm_load_print_meta: n_layer          = 24
0.00.050.527 I llm_load_print_meta: n_head           = 16
0.00.050.528 I llm_load_print_meta: n_head_kv        = 16
0.00.050.528 I llm_load_print_meta: n_rot            = 32
0.00.050.528 I llm_load_print_meta: n_swa            = 0
0.00.050.529 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.529 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.531 I llm_load_print_meta: n_gqa            = 1
0.00.050.532 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.532 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.533 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.533 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.533 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.533 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.533 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.534 I llm_load_print_meta: n_ff             = 8192
0.00.050.534 I llm_load_print_meta: n_expert         = 0
0.00.050.534 I llm_load_print_meta: n_expert_used    = 0
0.00.050.535 I llm_load_print_meta: causal attn      = 1
0.00.050.535 I llm_load_print_meta: pooling type     = 0
0.00.050.535 I llm_load_print_meta: rope type        = 2
0.00.050.537 I llm_load_print_meta: rope scaling     = linear
0.00.050.537 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.538 I llm_load_print_meta: freq_scale_train = 1
0.00.050.538 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.538 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.538 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.538 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.538 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.538 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.539 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.539 I llm_load_print_meta: model type       = 1.4B
0.00.050.539 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.540 I llm_load_print_meta: model params     = 1.41 B
0.00.050.540 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.540 I llm_load_print_meta: general.name     = 1.4B
0.00.050.545 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.545 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.546 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.546 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.546 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.546 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.546 I llm_load_print_meta: max token length = 1024
0.00.052.613 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.613 I llm_load_tensors: offloading output layer to GPU
0.00.052.613 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.624 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.625 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.600 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.601 I llama_new_context_with_model: n_ctx         = 128
0.00.053.601 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.601 I llama_new_context_with_model: n_batch       = 128
0.00.053.601 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.602 I llama_new_context_with_model: flash_attn    = 0
0.00.053.602 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.602 I llama_new_context_with_model: freq_scale    = 1
0.00.053.603 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.603 I ggml_metal_init: allocating
0.00.053.606 I ggml_metal_init: found device: Apple M4
0.00.053.608 I ggml_metal_init: picking default device: Apple M4
0.00.054.190 I ggml_metal_init: using embedded metal library
0.00.056.551 I ggml_metal_init: GPU name:   Apple M4
0.00.056.552 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.553 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.553 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.553 I ggml_metal_init: simdgroup reduction   = true
0.00.056.554 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.554 I ggml_metal_init: has bfloat            = true
0.00.056.554 I ggml_metal_init: use bfloat            = true
0.00.056.554 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.555 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.371 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.641 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.644 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.658 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.558 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.560 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.560 I llama_new_context_with_model: graph nodes  = 967
0.00.068.560 I llama_new_context_with_model: graph splits = 2
0.00.068.572 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.573 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.663.739 I 
0.00.663.773 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.663.784 I perplexity: tokenizing the input ..
0.00.672.219 I perplexity: tokenization took 8.432 ms
0.00.672.223 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.795.151 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.796.331 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.796.347 I llama_perf_context_print:        load time =     654.87 ms
0.00.796.349 I llama_perf_context_print: prompt eval time =     122.70 ms /   128 tokens (    0.96 ms per token,  1043.18 tokens per second)
0.00.796.349 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.796.350 I llama_perf_context_print:       total time =     132.61 ms /   129 tokens
0.00.796.740 I ggml_metal_free: deallocating

real	0m0.810s
user	0m0.079s
sys	0m0.101s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4365 (609f17d7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.610 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.367 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.371 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.372 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.373 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.373 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.374 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.374 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.375 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.375 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.376 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.376 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.376 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.377 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.377 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.379 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.379 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.379 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.264 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.289 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.142 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.143 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.143 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.144 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.144 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.144 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.145 I llama_model_loader: - type  f32:  194 tensors
0.00.024.145 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.145 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.208 I llm_load_vocab: special tokens cache size = 25
0.00.050.987 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.990 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.990 I llm_load_print_meta: arch             = gptneox
0.00.050.991 I llm_load_print_meta: vocab type       = BPE
0.00.050.991 I llm_load_print_meta: n_vocab          = 50304
0.00.050.991 I llm_load_print_meta: n_merges         = 50009
0.00.050.991 I llm_load_print_meta: vocab_only       = 0
0.00.050.991 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.992 I llm_load_print_meta: n_embd           = 2048
0.00.050.992 I llm_load_print_meta: n_layer          = 24
0.00.050.994 I llm_load_print_meta: n_head           = 16
0.00.050.995 I llm_load_print_meta: n_head_kv        = 16
0.00.050.995 I llm_load_print_meta: n_rot            = 32
0.00.050.996 I llm_load_print_meta: n_swa            = 0
0.00.050.996 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.996 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.997 I llm_load_print_meta: n_gqa            = 1
0.00.050.998 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.998 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.999 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.001 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.001 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.001 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.001 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.003 I llm_load_print_meta: n_ff             = 8192
0.00.051.003 I llm_load_print_meta: n_expert         = 0
0.00.051.003 I llm_load_print_meta: n_expert_used    = 0
0.00.051.003 I llm_load_print_meta: causal attn      = 1
0.00.051.003 I llm_load_print_meta: pooling type     = 0
0.00.051.004 I llm_load_print_meta: rope type        = 2
0.00.051.004 I llm_load_print_meta: rope scaling     = linear
0.00.051.004 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.004 I llm_load_print_meta: freq_scale_train = 1
0.00.051.005 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.005 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.005 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.005 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.005 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.005 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.007 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.007 I llm_load_print_meta: model type       = 1.4B
0.00.051.008 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.008 I llm_load_print_meta: model params     = 1.41 B
0.00.051.009 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.009 I llm_load_print_meta: general.name     = 1.4B
0.00.051.009 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.009 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.009 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.010 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.010 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.010 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.010 I llm_load_print_meta: max token length = 1024
0.00.053.074 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.074 I llm_load_tensors: offloading output layer to GPU
0.00.053.075 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.085 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.086 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.992 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.993 I llama_new_context_with_model: n_ctx         = 128
0.00.053.993 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.993 I llama_new_context_with_model: n_batch       = 128
0.00.053.993 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.994 I llama_new_context_with_model: flash_attn    = 0
0.00.053.994 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.994 I llama_new_context_with_model: freq_scale    = 1
0.00.053.995 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.995 I ggml_metal_init: allocating
0.00.053.998 I ggml_metal_init: found device: Apple M4
0.00.054.000 I ggml_metal_init: picking default device: Apple M4
0.00.054.565 I ggml_metal_init: using embedded metal library
0.00.056.944 I ggml_metal_init: GPU name:   Apple M4
0.00.056.946 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.946 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.946 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.947 I ggml_metal_init: simdgroup reduction   = true
0.00.056.947 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.947 I ggml_metal_init: has bfloat            = true
0.00.056.947 I ggml_metal_init: use bfloat            = true
0.00.056.948 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.948 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.753 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.995 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.999 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.015 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.894 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.895 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.896 I llama_new_context_with_model: graph nodes  = 967
0.00.068.896 I llama_new_context_with_model: graph splits = 2
0.00.068.908 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.909 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.642.098 I 
0.00.642.141 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.642.152 I perplexity: tokenizing the input ..
0.00.649.853 I perplexity: tokenization took 7.699 ms
0.00.649.857 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.784.771 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.785.932 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.785.949 I llama_perf_context_print:        load time =     632.48 ms
0.00.785.949 I llama_perf_context_print: prompt eval time =     134.69 ms /   128 tokens (    1.05 ms per token,   950.34 tokens per second)
0.00.785.950 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.785.951 I llama_perf_context_print:       total time =     143.85 ms /   129 tokens
0.00.786.426 I ggml_metal_free: deallocating

real	0m0.802s
user	0m0.079s
sys	0m0.111s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4365 (609f17d7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.902 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.802 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.806 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.812 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.813 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.813 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.814 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.814 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.815 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.815 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.816 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.816 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.816 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.817 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.817 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.819 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.819 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.819 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.680 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.685 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.527 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.528 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.528 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.529 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.529 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.529 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.530 I llama_model_loader: - type  f32:  194 tensors
0.00.023.530 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.530 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.836 I llm_load_vocab: special tokens cache size = 25
0.00.049.847 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.850 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.850 I llm_load_print_meta: arch             = gptneox
0.00.049.851 I llm_load_print_meta: vocab type       = BPE
0.00.049.851 I llm_load_print_meta: n_vocab          = 50304
0.00.049.851 I llm_load_print_meta: n_merges         = 50009
0.00.049.851 I llm_load_print_meta: vocab_only       = 0
0.00.049.851 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.852 I llm_load_print_meta: n_embd           = 2048
0.00.049.852 I llm_load_print_meta: n_layer          = 24
0.00.049.855 I llm_load_print_meta: n_head           = 16
0.00.049.855 I llm_load_print_meta: n_head_kv        = 16
0.00.049.856 I llm_load_print_meta: n_rot            = 32
0.00.049.856 I llm_load_print_meta: n_swa            = 0
0.00.049.856 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.856 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.857 I llm_load_print_meta: n_gqa            = 1
0.00.049.859 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.859 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.860 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.860 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.861 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.861 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.861 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.862 I llm_load_print_meta: n_ff             = 8192
0.00.049.862 I llm_load_print_meta: n_expert         = 0
0.00.049.862 I llm_load_print_meta: n_expert_used    = 0
0.00.049.862 I llm_load_print_meta: causal attn      = 1
0.00.049.862 I llm_load_print_meta: pooling type     = 0
0.00.049.862 I llm_load_print_meta: rope type        = 2
0.00.049.863 I llm_load_print_meta: rope scaling     = linear
0.00.049.863 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.863 I llm_load_print_meta: freq_scale_train = 1
0.00.049.863 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.864 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.864 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.864 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.864 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.864 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.864 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.864 I llm_load_print_meta: model type       = 1.4B
0.00.049.865 I llm_load_print_meta: model ftype      = Q5_1
0.00.049.865 I llm_load_print_meta: model params     = 1.41 B
0.00.049.866 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.049.866 I llm_load_print_meta: general.name     = 1.4B
0.00.049.866 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.866 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.866 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.867 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.867 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.867 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.867 I llm_load_print_meta: max token length = 1024
0.00.051.892 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.892 I llm_load_tensors: offloading output layer to GPU
0.00.051.892 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.903 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.904 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.817 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.818 I llama_new_context_with_model: n_ctx         = 128
0.00.052.818 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.819 I llama_new_context_with_model: n_batch       = 128
0.00.052.819 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.819 I llama_new_context_with_model: flash_attn    = 0
0.00.052.819 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.820 I llama_new_context_with_model: freq_scale    = 1
0.00.052.820 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.820 I ggml_metal_init: allocating
0.00.052.824 I ggml_metal_init: found device: Apple M4
0.00.052.826 I ggml_metal_init: picking default device: Apple M4
0.00.053.395 I ggml_metal_init: using embedded metal library
0.00.055.688 I ggml_metal_init: GPU name:   Apple M4
0.00.055.690 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.690 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.691 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.691 I ggml_metal_init: simdgroup reduction   = true
0.00.055.691 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.691 I ggml_metal_init: has bfloat            = true
0.00.055.691 I ggml_metal_init: use bfloat            = true
0.00.055.692 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.692 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.312 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.602 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.607 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.623 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.514 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.515 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.516 I llama_new_context_with_model: graph nodes  = 967
0.00.067.516 I llama_new_context_with_model: graph splits = 2
0.00.067.528 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.529 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.670.396 I 
0.00.670.435 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.670.446 I perplexity: tokenizing the input ..
0.00.678.313 I perplexity: tokenization took 7.865 ms
0.00.678.316 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.813.538 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.814.704 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.814.719 I llama_perf_context_print:        load time =     661.49 ms
0.00.814.720 I llama_perf_context_print: prompt eval time =     135.00 ms /   128 tokens (    1.05 ms per token,   948.18 tokens per second)
0.00.814.721 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.814.722 I llama_perf_context_print:       total time =     144.32 ms /   129 tokens
0.00.815.216 I ggml_metal_free: deallocating

real	0m0.828s
user	0m0.078s
sys	0m0.130s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4365 (609f17d7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.667 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.263 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.268 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.270 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.270 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.271 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.271 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.271 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.272 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.272 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.273 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.273 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.273 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.274 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.274 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.275 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.276 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.276 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.065 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.105 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.859 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.860 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.860 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.861 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.861 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.861 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.862 I llama_model_loader: - type  f32:  194 tensors
0.00.023.862 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.862 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.863 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.172 I llm_load_vocab: special tokens cache size = 25
0.00.050.215 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.218 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.218 I llm_load_print_meta: arch             = gptneox
0.00.050.218 I llm_load_print_meta: vocab type       = BPE
0.00.050.219 I llm_load_print_meta: n_vocab          = 50304
0.00.050.219 I llm_load_print_meta: n_merges         = 50009
0.00.050.219 I llm_load_print_meta: vocab_only       = 0
0.00.050.219 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.219 I llm_load_print_meta: n_embd           = 2048
0.00.050.219 I llm_load_print_meta: n_layer          = 24
0.00.050.222 I llm_load_print_meta: n_head           = 16
0.00.050.223 I llm_load_print_meta: n_head_kv        = 16
0.00.050.223 I llm_load_print_meta: n_rot            = 32
0.00.050.224 I llm_load_print_meta: n_swa            = 0
0.00.050.224 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.224 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.225 I llm_load_print_meta: n_gqa            = 1
0.00.050.226 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.226 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.227 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.227 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.227 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.228 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.228 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.228 I llm_load_print_meta: n_ff             = 8192
0.00.050.229 I llm_load_print_meta: n_expert         = 0
0.00.050.229 I llm_load_print_meta: n_expert_used    = 0
0.00.050.229 I llm_load_print_meta: causal attn      = 1
0.00.050.229 I llm_load_print_meta: pooling type     = 0
0.00.050.229 I llm_load_print_meta: rope type        = 2
0.00.050.232 I llm_load_print_meta: rope scaling     = linear
0.00.050.232 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.232 I llm_load_print_meta: freq_scale_train = 1
0.00.050.232 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.233 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.233 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.233 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.233 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.233 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.233 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.234 I llm_load_print_meta: model type       = 1.4B
0.00.050.234 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.234 I llm_load_print_meta: model params     = 1.41 B
0.00.050.235 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.235 I llm_load_print_meta: general.name     = 1.4B
0.00.050.235 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.236 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.236 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.236 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.236 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.237 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.237 I llm_load_print_meta: max token length = 1024
0.00.052.112 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.112 I llm_load_tensors: offloading output layer to GPU
0.00.052.113 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.123 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.124 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.006 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.007 I llama_new_context_with_model: n_ctx         = 128
0.00.053.007 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.007 I llama_new_context_with_model: n_batch       = 128
0.00.053.007 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.008 I llama_new_context_with_model: flash_attn    = 0
0.00.053.008 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.008 I llama_new_context_with_model: freq_scale    = 1
0.00.053.009 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.009 I ggml_metal_init: allocating
0.00.053.012 I ggml_metal_init: found device: Apple M4
0.00.053.014 I ggml_metal_init: picking default device: Apple M4
0.00.053.571 I ggml_metal_init: using embedded metal library
0.00.055.892 I ggml_metal_init: GPU name:   Apple M4
0.00.055.894 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.894 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.895 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.895 I ggml_metal_init: simdgroup reduction   = true
0.00.055.895 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.895 I ggml_metal_init: has bfloat            = true
0.00.055.895 I ggml_metal_init: use bfloat            = true
0.00.055.896 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.896 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.469 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.692 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.694 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.718 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.688 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.689 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.689 I llama_new_context_with_model: graph nodes  = 967
0.00.067.689 I llama_new_context_with_model: graph splits = 2
0.00.067.701 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.703 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.390.871 I 
0.00.390.911 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.390.924 I perplexity: tokenizing the input ..
0.00.399.123 I perplexity: tokenization took 8.197 ms
0.00.399.130 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.531.674 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.532.932 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.532.952 I llama_perf_context_print:        load time =     381.20 ms
0.00.532.953 I llama_perf_context_print: prompt eval time =     132.32 ms /   128 tokens (    1.03 ms per token,   967.36 tokens per second)
0.00.532.954 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.532.955 I llama_perf_context_print:       total time =     142.09 ms /   129 tokens
0.00.533.418 I ggml_metal_free: deallocating

real	0m0.550s
user	0m0.078s
sys	0m0.074s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4365 (609f17d7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.888 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.806 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.812 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.813 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.814 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.814 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.814 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.815 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.816 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.816 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.816 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.817 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.817 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.818 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.818 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.820 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.820 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.820 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.731 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.761 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.688 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.689 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.690 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.690 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.690 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.691 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.691 I llama_model_loader: - type  f32:  194 tensors
0.00.023.692 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.692 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.692 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.692 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.737 I llm_load_vocab: special tokens cache size = 25
0.00.050.792 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.795 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.795 I llm_load_print_meta: arch             = gptneox
0.00.050.796 I llm_load_print_meta: vocab type       = BPE
0.00.050.796 I llm_load_print_meta: n_vocab          = 50304
0.00.050.796 I llm_load_print_meta: n_merges         = 50009
0.00.050.796 I llm_load_print_meta: vocab_only       = 0
0.00.050.796 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.796 I llm_load_print_meta: n_embd           = 2048
0.00.050.797 I llm_load_print_meta: n_layer          = 24
0.00.050.800 I llm_load_print_meta: n_head           = 16
0.00.050.801 I llm_load_print_meta: n_head_kv        = 16
0.00.050.801 I llm_load_print_meta: n_rot            = 32
0.00.050.801 I llm_load_print_meta: n_swa            = 0
0.00.050.801 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.801 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.802 I llm_load_print_meta: n_gqa            = 1
0.00.050.803 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.803 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.804 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.804 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.805 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.805 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.805 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.806 I llm_load_print_meta: n_ff             = 8192
0.00.050.806 I llm_load_print_meta: n_expert         = 0
0.00.050.806 I llm_load_print_meta: n_expert_used    = 0
0.00.050.806 I llm_load_print_meta: causal attn      = 1
0.00.050.806 I llm_load_print_meta: pooling type     = 0
0.00.050.808 I llm_load_print_meta: rope type        = 2
0.00.050.809 I llm_load_print_meta: rope scaling     = linear
0.00.050.810 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.810 I llm_load_print_meta: freq_scale_train = 1
0.00.050.810 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.811 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.811 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.811 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.811 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.811 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.811 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.811 I llm_load_print_meta: model type       = 1.4B
0.00.050.812 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.816 I llm_load_print_meta: model params     = 1.41 B
0.00.050.817 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.817 I llm_load_print_meta: general.name     = 1.4B
0.00.050.817 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.817 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.817 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.818 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.818 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.818 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.818 I llm_load_print_meta: max token length = 1024
0.00.052.799 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.799 I llm_load_tensors: offloading output layer to GPU
0.00.052.799 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.810 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.811 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.819 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.820 I llama_new_context_with_model: n_ctx         = 128
0.00.053.820 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.820 I llama_new_context_with_model: n_batch       = 128
0.00.053.820 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.821 I llama_new_context_with_model: flash_attn    = 0
0.00.053.821 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.821 I llama_new_context_with_model: freq_scale    = 1
0.00.053.822 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.822 I ggml_metal_init: allocating
0.00.053.828 I ggml_metal_init: found device: Apple M4
0.00.053.831 I ggml_metal_init: picking default device: Apple M4
0.00.054.419 I ggml_metal_init: using embedded metal library
0.00.056.761 I ggml_metal_init: GPU name:   Apple M4
0.00.056.762 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.763 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.763 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.765 I ggml_metal_init: simdgroup reduction   = true
0.00.056.765 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.765 I ggml_metal_init: has bfloat            = true
0.00.056.765 I ggml_metal_init: use bfloat            = true
0.00.056.766 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.766 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.499 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.825 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.829 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.843 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.699 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.700 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.701 I llama_new_context_with_model: graph nodes  = 967
0.00.068.701 I llama_new_context_with_model: graph splits = 2
0.00.068.714 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.715 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.486.228 I 
0.00.486.270 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.486.283 I perplexity: tokenizing the input ..
0.00.494.699 I perplexity: tokenization took 8.414 ms
0.00.494.704 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.626.888 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.628.099 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.628.111 I llama_perf_context_print:        load time =     477.33 ms
0.00.628.111 I llama_perf_context_print: prompt eval time =     131.96 ms /   128 tokens (    1.03 ms per token,   969.99 tokens per second)
0.00.628.112 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.628.113 I llama_perf_context_print:       total time =     141.89 ms /   129 tokens
0.00.628.472 I ggml_metal_free: deallocating

real	0m0.641s
user	0m0.079s
sys	0m0.085s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4365 (609f17d7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.191 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.925 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.930 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.932 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.932 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.932 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.933 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.933 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.934 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.935 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.936 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.936 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.936 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.937 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.937 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.938 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.939 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.939 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.733 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.750 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.533 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.535 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.535 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.535 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.536 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.536 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.536 I llama_model_loader: - type  f32:  194 tensors
0.00.023.537 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.537 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.537 I llama_model_loader: - type q6_K:   13 tensors
0.00.043.835 I llm_load_vocab: special tokens cache size = 25
0.00.049.800 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.803 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.803 I llm_load_print_meta: arch             = gptneox
0.00.049.804 I llm_load_print_meta: vocab type       = BPE
0.00.049.804 I llm_load_print_meta: n_vocab          = 50304
0.00.049.804 I llm_load_print_meta: n_merges         = 50009
0.00.049.804 I llm_load_print_meta: vocab_only       = 0
0.00.049.805 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.805 I llm_load_print_meta: n_embd           = 2048
0.00.049.805 I llm_load_print_meta: n_layer          = 24
0.00.049.807 I llm_load_print_meta: n_head           = 16
0.00.049.808 I llm_load_print_meta: n_head_kv        = 16
0.00.049.808 I llm_load_print_meta: n_rot            = 32
0.00.049.808 I llm_load_print_meta: n_swa            = 0
0.00.049.809 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.810 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.810 I llm_load_print_meta: n_gqa            = 1
0.00.049.811 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.812 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.813 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.815 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.815 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.815 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.815 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.816 I llm_load_print_meta: n_ff             = 8192
0.00.049.817 I llm_load_print_meta: n_expert         = 0
0.00.049.817 I llm_load_print_meta: n_expert_used    = 0
0.00.049.817 I llm_load_print_meta: causal attn      = 1
0.00.049.818 I llm_load_print_meta: pooling type     = 0
0.00.049.818 I llm_load_print_meta: rope type        = 2
0.00.049.818 I llm_load_print_meta: rope scaling     = linear
0.00.049.818 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.819 I llm_load_print_meta: freq_scale_train = 1
0.00.049.819 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.819 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.819 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.819 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.819 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.820 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.820 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.824 I llm_load_print_meta: model type       = 1.4B
0.00.049.824 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.049.829 I llm_load_print_meta: model params     = 1.41 B
0.00.049.833 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.049.833 I llm_load_print_meta: general.name     = 1.4B
0.00.049.835 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.835 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.835 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.835 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.836 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.838 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.838 I llm_load_print_meta: max token length = 1024
0.00.051.804 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.805 I llm_load_tensors: offloading output layer to GPU
0.00.051.805 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.815 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.816 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.713 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.714 I llama_new_context_with_model: n_ctx         = 128
0.00.052.714 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.715 I llama_new_context_with_model: n_batch       = 128
0.00.052.715 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.715 I llama_new_context_with_model: flash_attn    = 0
0.00.052.716 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.716 I llama_new_context_with_model: freq_scale    = 1
0.00.052.716 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.717 I ggml_metal_init: allocating
0.00.052.722 I ggml_metal_init: found device: Apple M4
0.00.052.724 I ggml_metal_init: picking default device: Apple M4
0.00.053.287 I ggml_metal_init: using embedded metal library
0.00.055.618 I ggml_metal_init: GPU name:   Apple M4
0.00.055.620 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.620 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.620 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.621 I ggml_metal_init: simdgroup reduction   = true
0.00.055.621 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.621 I ggml_metal_init: has bfloat            = true
0.00.055.621 I ggml_metal_init: use bfloat            = true
0.00.055.622 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.624 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.335 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.568 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.572 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.588 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.509 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.510 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.511 I llama_new_context_with_model: graph nodes  = 967
0.00.067.511 I llama_new_context_with_model: graph splits = 2
0.00.067.523 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.524 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.581.211 I 
0.00.581.248 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.581.259 I perplexity: tokenizing the input ..
0.00.589.404 I perplexity: tokenization took 8.144 ms
0.00.589.407 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.722.990 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.724.198 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.724.213 I llama_perf_context_print:        load time =     572.01 ms
0.00.724.214 I llama_perf_context_print: prompt eval time =     133.36 ms /   128 tokens (    1.04 ms per token,   959.84 tokens per second)
0.00.724.215 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.724.217 I llama_perf_context_print:       total time =     143.01 ms /   129 tokens
0.00.724.762 I ggml_metal_free: deallocating

real	0m0.741s
user	0m0.078s
sys	0m0.113s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4365 (609f17d7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.724 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.576 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.581 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.583 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.584 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.584 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.584 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.586 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.586 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.587 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.588 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.588 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.588 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.589 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.589 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.590 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.591 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.591 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.446 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.459 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.305 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.306 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.307 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.307 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.307 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.308 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.308 I llama_model_loader: - type  f32:  194 tensors
0.00.023.309 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.309 I llama_model_loader: - type q6_K:   37 tensors
0.00.043.494 I llm_load_vocab: special tokens cache size = 25
0.00.049.421 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.424 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.424 I llm_load_print_meta: arch             = gptneox
0.00.049.425 I llm_load_print_meta: vocab type       = BPE
0.00.049.425 I llm_load_print_meta: n_vocab          = 50304
0.00.049.425 I llm_load_print_meta: n_merges         = 50009
0.00.049.425 I llm_load_print_meta: vocab_only       = 0
0.00.049.426 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.426 I llm_load_print_meta: n_embd           = 2048
0.00.049.426 I llm_load_print_meta: n_layer          = 24
0.00.049.428 I llm_load_print_meta: n_head           = 16
0.00.049.429 I llm_load_print_meta: n_head_kv        = 16
0.00.049.429 I llm_load_print_meta: n_rot            = 32
0.00.049.429 I llm_load_print_meta: n_swa            = 0
0.00.049.430 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.430 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.430 I llm_load_print_meta: n_gqa            = 1
0.00.049.431 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.432 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.433 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.433 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.433 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.433 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.433 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.434 I llm_load_print_meta: n_ff             = 8192
0.00.049.434 I llm_load_print_meta: n_expert         = 0
0.00.049.434 I llm_load_print_meta: n_expert_used    = 0
0.00.049.435 I llm_load_print_meta: causal attn      = 1
0.00.049.435 I llm_load_print_meta: pooling type     = 0
0.00.049.437 I llm_load_print_meta: rope type        = 2
0.00.049.437 I llm_load_print_meta: rope scaling     = linear
0.00.049.437 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.438 I llm_load_print_meta: freq_scale_train = 1
0.00.049.438 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.438 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.439 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.439 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.440 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.440 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.440 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.440 I llm_load_print_meta: model type       = 1.4B
0.00.049.440 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.441 I llm_load_print_meta: model params     = 1.41 B
0.00.049.441 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.442 I llm_load_print_meta: general.name     = 1.4B
0.00.049.442 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.442 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.442 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.446 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.446 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.447 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.447 I llm_load_print_meta: max token length = 1024
0.00.051.423 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.423 I llm_load_tensors: offloading output layer to GPU
0.00.051.423 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.434 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.435 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.330 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.331 I llama_new_context_with_model: n_ctx         = 128
0.00.052.331 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.332 I llama_new_context_with_model: n_batch       = 128
0.00.052.332 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.332 I llama_new_context_with_model: flash_attn    = 0
0.00.052.332 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.333 I llama_new_context_with_model: freq_scale    = 1
0.00.052.333 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.334 I ggml_metal_init: allocating
0.00.052.339 I ggml_metal_init: found device: Apple M4
0.00.052.341 I ggml_metal_init: picking default device: Apple M4
0.00.052.884 I ggml_metal_init: using embedded metal library
0.00.055.210 I ggml_metal_init: GPU name:   Apple M4
0.00.055.211 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.212 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.212 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.212 I ggml_metal_init: simdgroup reduction   = true
0.00.055.212 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.213 I ggml_metal_init: has bfloat            = true
0.00.055.213 I ggml_metal_init: use bfloat            = true
0.00.055.213 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.214 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.858 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.108 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.111 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.124 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.909 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.910 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.910 I llama_new_context_with_model: graph nodes  = 967
0.00.066.910 I llama_new_context_with_model: graph splits = 2
0.00.066.923 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.923 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.629.797 I 
0.00.629.829 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.629.840 I perplexity: tokenizing the input ..
0.00.637.160 I perplexity: tokenization took 7.319 ms
0.00.637.164 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.778.212 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.779.503 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.779.520 I llama_perf_context_print:        load time =     621.07 ms
0.00.779.524 I llama_perf_context_print: prompt eval time =     140.83 ms /   128 tokens (    1.10 ms per token,   908.93 tokens per second)
0.00.779.524 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.779.525 I llama_perf_context_print:       total time =     149.72 ms /   129 tokens
0.00.779.962 I ggml_metal_free: deallocating

real	0m0.793s
user	0m0.077s
sys	0m0.108s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4365 (609f17d7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.647 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.417 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.421 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.423 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.425 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.426 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.426 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.426 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.428 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.429 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.430 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.431 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.431 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.434 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.435 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.436 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.436 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.436 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.185 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.253 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.046 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.047 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.047 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.048 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.048 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.049 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.049 I llama_model_loader: - type  f32:  194 tensors
0.00.024.049 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.183 I llm_load_vocab: special tokens cache size = 25
0.00.051.092 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.095 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.095 I llm_load_print_meta: arch             = gptneox
0.00.051.095 I llm_load_print_meta: vocab type       = BPE
0.00.051.096 I llm_load_print_meta: n_vocab          = 50304
0.00.051.096 I llm_load_print_meta: n_merges         = 50009
0.00.051.096 I llm_load_print_meta: vocab_only       = 0
0.00.051.096 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.096 I llm_load_print_meta: n_embd           = 2048
0.00.051.097 I llm_load_print_meta: n_layer          = 24
0.00.051.099 I llm_load_print_meta: n_head           = 16
0.00.051.100 I llm_load_print_meta: n_head_kv        = 16
0.00.051.101 I llm_load_print_meta: n_rot            = 32
0.00.051.101 I llm_load_print_meta: n_swa            = 0
0.00.051.101 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.102 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.103 I llm_load_print_meta: n_gqa            = 1
0.00.051.104 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.105 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.105 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.105 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.105 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.106 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.106 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.106 I llm_load_print_meta: n_ff             = 8192
0.00.051.107 I llm_load_print_meta: n_expert         = 0
0.00.051.107 I llm_load_print_meta: n_expert_used    = 0
0.00.051.107 I llm_load_print_meta: causal attn      = 1
0.00.051.107 I llm_load_print_meta: pooling type     = 0
0.00.051.107 I llm_load_print_meta: rope type        = 2
0.00.051.107 I llm_load_print_meta: rope scaling     = linear
0.00.051.108 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.108 I llm_load_print_meta: freq_scale_train = 1
0.00.051.108 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.109 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.109 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.109 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.109 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.109 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.109 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.109 I llm_load_print_meta: model type       = 1.4B
0.00.051.110 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.110 I llm_load_print_meta: model params     = 1.41 B
0.00.051.111 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.111 I llm_load_print_meta: general.name     = 1.4B
0.00.051.111 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.111 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.111 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.112 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.112 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.112 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.112 I llm_load_print_meta: max token length = 1024
0.00.053.178 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.178 I llm_load_tensors: offloading output layer to GPU
0.00.053.178 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.189 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.190 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.145 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.146 I llama_new_context_with_model: n_ctx         = 128
0.00.054.146 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.147 I llama_new_context_with_model: n_batch       = 128
0.00.054.147 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.147 I llama_new_context_with_model: flash_attn    = 0
0.00.054.147 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.148 I llama_new_context_with_model: freq_scale    = 1
0.00.054.148 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.148 I ggml_metal_init: allocating
0.00.054.155 I ggml_metal_init: found device: Apple M4
0.00.054.157 I ggml_metal_init: picking default device: Apple M4
0.00.054.698 I ggml_metal_init: using embedded metal library
0.00.057.001 I ggml_metal_init: GPU name:   Apple M4
0.00.057.002 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.002 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.003 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.003 I ggml_metal_init: simdgroup reduction   = true
0.00.057.003 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.003 I ggml_metal_init: has bfloat            = true
0.00.057.003 I ggml_metal_init: use bfloat            = true
0.00.057.004 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.004 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.677 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.975 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.977 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.000 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.912 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.914 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.914 I llama_new_context_with_model: graph nodes  = 967
0.00.068.914 I llama_new_context_with_model: graph splits = 2
0.00.068.927 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.928 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.129.463 I 
0.00.129.498 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.129.512 I perplexity: tokenizing the input ..
0.00.137.290 I perplexity: tokenization took 7.776 ms
0.00.137.294 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.275.916 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.277.255 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.277.265 I llama_perf_context_print:        load time =     119.81 ms
0.00.277.267 I llama_perf_context_print: prompt eval time =     138.37 ms /   128 tokens (    1.08 ms per token,   925.09 tokens per second)
0.00.277.267 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.277.269 I llama_perf_context_print:       total time =     147.80 ms /   129 tokens
0.00.277.619 I ggml_metal_free: deallocating

real	0m0.293s
user	0m0.079s
sys	0m0.038s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.182 I build: 4365 (609f17d7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.016.180 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.885 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.027.892 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.897 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.898 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.898 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.898 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.899 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.900 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.900 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.900 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.901 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.901 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.901 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.902 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.904 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.904 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.904 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.671 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.761 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.932 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.039.934 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.934 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.934 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.934 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.935 I llama_model_loader: - type  f32:  194 tensors
0.00.039.935 I llama_model_loader: - type  f16:   98 tensors
0.00.060.611 I llm_load_vocab: special tokens cache size = 25
0.00.066.612 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.066.615 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.066.616 I llm_load_print_meta: arch             = gptneox
0.00.066.616 I llm_load_print_meta: vocab type       = BPE
0.00.066.616 I llm_load_print_meta: n_vocab          = 50304
0.00.066.616 I llm_load_print_meta: n_merges         = 50009
0.00.066.617 I llm_load_print_meta: vocab_only       = 0
0.00.066.617 I llm_load_print_meta: n_ctx_train      = 2048
0.00.066.617 I llm_load_print_meta: n_embd           = 2048
0.00.066.617 I llm_load_print_meta: n_layer          = 24
0.00.066.621 I llm_load_print_meta: n_head           = 16
0.00.066.624 I llm_load_print_meta: n_head_kv        = 16
0.00.066.625 I llm_load_print_meta: n_rot            = 32
0.00.066.625 I llm_load_print_meta: n_swa            = 0
0.00.066.625 I llm_load_print_meta: n_embd_head_k    = 128
0.00.066.625 I llm_load_print_meta: n_embd_head_v    = 128
0.00.066.626 I llm_load_print_meta: n_gqa            = 1
0.00.066.627 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.066.627 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.066.628 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.066.628 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.066.628 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.066.628 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.066.629 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.066.629 I llm_load_print_meta: n_ff             = 8192
0.00.066.629 I llm_load_print_meta: n_expert         = 0
0.00.066.629 I llm_load_print_meta: n_expert_used    = 0
0.00.066.629 I llm_load_print_meta: causal attn      = 1
0.00.066.631 I llm_load_print_meta: pooling type     = 0
0.00.066.631 I llm_load_print_meta: rope type        = 2
0.00.066.631 I llm_load_print_meta: rope scaling     = linear
0.00.066.632 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.066.633 I llm_load_print_meta: freq_scale_train = 1
0.00.066.633 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.066.633 I llm_load_print_meta: rope_finetuned   = unknown
0.00.066.634 I llm_load_print_meta: ssm_d_conv       = 0
0.00.066.634 I llm_load_print_meta: ssm_d_inner      = 0
0.00.066.634 I llm_load_print_meta: ssm_d_state      = 0
0.00.066.634 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.066.634 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.066.635 I llm_load_print_meta: model type       = 1.4B
0.00.066.635 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.066.636 I llm_load_print_meta: model params     = 1.41 B
0.00.066.636 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.066.636 I llm_load_print_meta: general.name     = 1.4B
0.00.066.637 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.066.637 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.066.637 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.066.637 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.066.637 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.066.638 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.066.638 I llm_load_print_meta: max token length = 1024
0.00.068.317 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.068.318 I llm_load_tensors: offloading output layer to GPU
0.00.068.318 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.068.328 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.068.329 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.069.153 I llama_new_context_with_model: n_seq_max     = 1
0.00.069.154 I llama_new_context_with_model: n_ctx         = 128
0.00.069.154 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.069.154 I llama_new_context_with_model: n_batch       = 128
0.00.069.155 I llama_new_context_with_model: n_ubatch      = 128
0.00.069.155 I llama_new_context_with_model: flash_attn    = 0
0.00.069.155 I llama_new_context_with_model: freq_base     = 10000.0
0.00.069.156 I llama_new_context_with_model: freq_scale    = 1
0.00.069.156 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.069.157 I ggml_metal_init: allocating
0.00.069.163 I ggml_metal_init: found device: Apple M4
0.00.069.165 I ggml_metal_init: picking default device: Apple M4
0.00.069.763 I ggml_metal_init: using embedded metal library
0.00.072.113 I ggml_metal_init: GPU name:   Apple M4
0.00.072.115 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.072.115 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.072.115 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.072.116 I ggml_metal_init: simdgroup reduction   = true
0.00.072.116 I ggml_metal_init: simdgroup matrix mul. = true
0.00.072.116 I ggml_metal_init: has bfloat            = true
0.00.072.116 I ggml_metal_init: use bfloat            = true
0.00.072.117 I ggml_metal_init: hasUnifiedMemory      = true
0.00.072.117 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.255 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.083.489 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.083.492 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.083.507 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.363 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.084.364 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.084.365 I llama_new_context_with_model: graph nodes  = 967
0.00.084.365 I llama_new_context_with_model: graph splits = 2
0.00.084.377 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.084.378 I 
0.00.084.407 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.084.408 I compute_imatrix: tokenizing the input ..
0.00.091.472 I compute_imatrix: tokenization took 7.063 ms
0.00.091.474 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.606.489 I compute_imatrix: 1.51 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.608.952 I llama_perf_context_print:        load time =    1590.31 ms
0.01.608.953 I llama_perf_context_print: prompt eval time =    1514.55 ms /   128 tokens (   11.83 ms per token,    84.51 tokens per second)
0.01.608.954 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.608.954 I llama_perf_context_print:       total time =    1592.77 ms /   129 tokens
0.01.609.479 I ggml_metal_free: deallocating

real	0m1.815s
user	0m0.149s
sys	0m0.243s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4365 (609f17d7)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13030a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13030a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13030ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13030b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13030b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13030bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13030c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13030cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13030d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13030d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13030dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13030dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13030ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13030f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13030fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1303101b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1303108d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x130310ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x130311710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x130311ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x130312600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x130312d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x130313440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x130313ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x130314400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1303146c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x130314cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x130315940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x130315e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x130316140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1303165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1303168a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x130317130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x130317670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x130317930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x130317dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x130318270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x130318710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x130318bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x130319050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1303194f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x130319990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x130319e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13031a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13031a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13031aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13031b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13031bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13031c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13031c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13031cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13031d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13031d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13031df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13031e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13031ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13031f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13031f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13031f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x130320120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1303203e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x130320880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x130320d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1303211c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x130321660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x130321b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x130321fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x130322440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1303228e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x130322d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x130323220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1303236c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x130323b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1303240b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x130324600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x130324b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1303250a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1303255f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x130325b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x130326090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1303265e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x130326b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x130327080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1303275d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x130327b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x130328070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1303285c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x130328b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x130329060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1303295b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x130329b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13032a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13032a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13032aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13032b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13032b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13032bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13031b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13032bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13032c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13032cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13032d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13032d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13032dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13032e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13032e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13032ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13032f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13032f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13032fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x130330170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1303306c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x130330c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1303310b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x130331550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1303319f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x130331e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x130332330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1303327d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x130332c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x130333110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1303335b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x130333a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x130333ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x130334390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x130334830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x130334cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x130335170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x130335610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x130335ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x130335f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1303363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x130336890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x130336d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1303371d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x130337670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x130337b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x130337fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x130338450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1303388f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x130338d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x130339230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1303396d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x130339b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13033a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13033a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13033a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13033adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13033b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13033b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13033bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13033c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13033c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13033c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13033ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13033d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13033d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13033dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13033e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13033e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13033ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13033eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13033f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13033f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13033fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x130340130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1303405d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x130340a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x130340f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1303413b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x130341850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x130341cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x130342190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x130342630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x130342ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x130342f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x130343410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1303438b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x130343d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1303441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x130344690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x130344b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x130344fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x130345470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x130345910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x130345db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x130346250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1303466f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x130346b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x130347030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1303474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x130347970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x130347e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x130348360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1303488b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x130348e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x130349350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x130349610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x130349c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13034a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13034a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13034b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13034b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13034b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13034bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13034c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13034cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13034d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13034d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13034d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13034e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13034e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13034ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13034f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13034f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13034fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x130350110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x130350660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x130350bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x130351100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x130351650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x130351ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1303520f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x130352640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x130352b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1303530e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x130353630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x130353b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1303540d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x130354620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x130354b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1303550c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x130355610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x130355b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1303560b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x130356600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x130356b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1303570a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1303575f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x130357b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x130358090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1303585e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x130358b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x130359080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1303595d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x130359b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13035a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13035a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13035ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13035b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13035b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13035bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13035c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13035c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13035caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13035d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13035d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13035dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13035e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13035e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13035ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13035f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13035f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13035fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x130360010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x130360560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x130360ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x130360f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1303613f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x130361890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x130361d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1303621d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x130362670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x130362b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x130362fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x130363450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1303638f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x130363d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x130364230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1303646d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x130364b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x130365010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x130365560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x130365c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1303663a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x130366ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1303671e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1303674a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x130367c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x130367f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x130368560 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.138.185 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.138.188 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1304063d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x130406840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x130406cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x130407120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x130407590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x130407a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x130407e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1304082e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x130408750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x130408ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x130409110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x130409790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13040a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13040aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13040b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13040b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13040c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13040c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13040cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13040d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13040dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13040e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13040ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13040f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13040fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13040fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13040ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x130410450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1304108c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x130410d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x130411230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x130411740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x130411bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x130411e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1304122e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x130412750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x130412cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1304131b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1304136b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x130413bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1304140b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1304145b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x130414ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x130414fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1304154b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x130415920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x130415d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x130416200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x130416670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x130416ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x130416f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1304173c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x130417830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x130417ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x130418110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1304188e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x130418d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x130419040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x130419650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x130419e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13041a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13041a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13041ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13041b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13041b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13041ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13041bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13041c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13041c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13041cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13041d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13041d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13041da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13041dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13041e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13041ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13041efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13041f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13041fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13041ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1304204e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x130420a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x130420f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1304214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x130421a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x130421f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1304224c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x130422a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x130422f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1304234b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x130423a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x130423f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1304244a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1304249f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x130424f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x130425490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1304259e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x130425f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x130426480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1304269d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x130426f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x130427470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1304279c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x130427f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x130428460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1304289b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x130428f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x130429450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1304299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x130429ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13042a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13042a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13042aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13042b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13042b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13042bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13042c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13042c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13042caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13042cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13042d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13042d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13042dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13042e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13042e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13042eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13042efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13042f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13042f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13042fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x130430220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1304306c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x130430b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x130431000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1304314a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x130431940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x130431de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x130432280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x130432720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x130432bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x130433060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x130433500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1304339a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x130433e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1304342e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x130434780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x130434c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1304350c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x130435560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x130435a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x130435ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x130436340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1304367e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x130436c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x130437120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1304375c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x130437a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x130437f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1304383a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x130438840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x130438ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x130439180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x130439620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x130439ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x130439f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13043a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13043a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13043ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13043b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13043b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13043bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13043bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13043c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13043c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13043cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13043d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13043d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13043db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13043e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13043e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13043e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13043ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13043f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13043f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13043fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x130440080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x130440520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1304409c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x130440e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x130441300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1304417a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x130441c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1304420e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x130442630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x130442b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1304430d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x130443620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1304438e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x130443ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x130444500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x130444b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x130445300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1304457a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x130445a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x130446070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x130446680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x130446e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x130447310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1304477b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x130447c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x130448400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x130448950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x130448ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1304493f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x130449940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x130449e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13044a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13044a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13044ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13044b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13044b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13044be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13044c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13044c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13044ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13044d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13044d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13044de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13044e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13044e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13044ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13044f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13044f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13044fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x130450380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1304508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x130450e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x130451370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1304518c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x130451e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x130452360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1304528b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x130452e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x130453350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1304538a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x130453df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x130454340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x130454890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x130454de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x130455330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x130455880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x130455dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x130456320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x130456870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x130456dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x130457310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x130457860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x130457db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x130458300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x130458850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x130458da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1304592f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x130459840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x130459d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13045a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13045a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13045ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13045b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13045b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13045bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13045c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13045c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13045c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13045cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13045d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13045d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13045dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13045e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13045e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13045e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13045ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13045f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13045f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13045ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x130460670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x130460d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1304614b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x130461770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x130461f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x130462220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x130462830 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1305052d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x130505740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x130505bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x130506020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x130506490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x130506900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x130506d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1305071e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x130507650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x130507b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x130507fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x130508650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x130509170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x130509920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13050a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13050a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13050af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13050b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13050bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13050c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13050cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13050d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13050dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13050e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13050e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13050ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13050eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13050f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13050f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13050fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x130510060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x130510590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x130510a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x130510cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x130511130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1305115a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x130511a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x130511e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1305122f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x130512760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x130512bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x130513040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1305134b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x130513920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x130513d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x130514200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x130514670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x130514ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x130514f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1305153c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x130515830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x130515ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x130516110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x130516580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1305169f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x130516e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1305173d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1305178d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x130517d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1305181b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x130518620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x130518a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x130518f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x130519370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1305197e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x130519c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13051a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13051a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13051a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13051ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13051b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13051b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13051bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13051bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13051c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13051c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13051cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13051d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13051d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13051da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13051dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13051e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13051e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13051ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13051f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13051f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13051f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13051fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x130520260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1305206d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x130520b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x130520fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x130521420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x130521890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x130521d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x130522170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1305225e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x130522a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x130522ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x130523330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1305237a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x130523c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x130524080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1305244f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x130524960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x130524dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x130525240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1305256b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x130525b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x130525f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x130526400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x130526870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x130526ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x130527150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1305275c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x130527a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x130527ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x130528310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x130528780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x130528bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x130529060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1305294d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x130529940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x130529db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13052a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13052a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13052ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13052af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13052b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13052b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13052bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13052c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13052c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13052ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13052ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13052d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13052d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13052dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13052e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13052e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13052e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13052ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13052f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13052f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13052fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13052ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1305303c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x130530830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x130530ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x130531110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x130531580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1305319f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x130531e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1305322d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x130532740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x130532bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x130533020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x130533490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x130533900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x130533d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1305341e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x130534650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x130534ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x130534f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1305353a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x130535810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x130535c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1305360f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x130536560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1305369d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x130536e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1305372b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x130537720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x130537b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x130538000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x130538470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1305388e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x130538d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1305391c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x130539630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x130539aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x130539f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13053a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13053a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13053ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13053b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13053b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13053b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13053be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13053c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13053c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13053cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13053cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13053d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13053d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13053dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13053e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13053e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13053ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13053eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13053f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13053f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13053fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1305400b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x130540520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x130540990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x130540e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x130541390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x130541800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x130541c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1305427c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x130542a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x130542d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1305431b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x130543620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x130543a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x130543f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x130544370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1305447e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x130544c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1305450c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x130545530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1305459a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x130545e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x130546280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1305466f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x130546b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x130546fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x130547440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1305478b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x130547d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x130548190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x130548600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x130548a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x130548ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x130549350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1305497c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x130549c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13054a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13054a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13054a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13054adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13054b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13054b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13054c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13054c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13054c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13054cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13054d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13054d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13054d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13054dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13054e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13054e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13054eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13054ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13054f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13054f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13054fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x130550120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x130550590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x130550a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x130550e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1305512e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x130551750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x130551bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x130552030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1305524a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x130552910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x130552d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1305531f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x130553660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x130553ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x130553f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1305543b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x130554820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x130554c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x130555100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x130555570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1305559e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x130555e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1305562c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x130556730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1305571a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1305578c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x130557fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x130558700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1305589c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x130558e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x130559430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x130559a40 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.835s
user	0m0.289s
sys	0m0.313s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4365 (609f17d7)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13760b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13760bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13760c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13760c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13760ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13760d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13760d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13760dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13760e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13760e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13760ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13760f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13760fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x137610560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x137610d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x137611490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x137611bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1376122d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1376129f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1376131c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1376138e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x137614000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x137614720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x137614fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1376156e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1376159a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x137615fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x137616c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x137617160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x137617420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1376178c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x137617b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x137618410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x137618950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x137618c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1376190b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x137619550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1376199f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x137619e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13761a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13761a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13761ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13761b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13761b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13761b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13761be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13761c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13761cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13761d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13761d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13761dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13761e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13761ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13761f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13761fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13761fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x137620340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x137620600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x137620c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x137621400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1376216c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x137621b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x137622000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1376224a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x137622940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x137622de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x137623280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x137623720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x137623bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x137624060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x137624500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1376249a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x137624e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x137625390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1376258e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x137625e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x137626380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1376268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x137626e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x137627370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1376278c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x137627e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x137628360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1376288b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x137628e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x137629350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1376298a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x137629df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13762a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13762a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13762ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13762b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13762b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13762bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13762c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13762c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13762cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13761caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13762d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13762d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13762df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13762e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13762e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13762ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13762f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13762f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13762ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x137630460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1376309b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x137630f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x137631450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1376319a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x137631ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x137632390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x137632830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x137632cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x137633170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x137633610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x137633ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x137633f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1376343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x137634890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x137634d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1376351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x137635670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x137635b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x137635fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x137636450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1376368f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x137636d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x137637230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1376376d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x137637b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x137638010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1376384b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x137638950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x137638df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x137639290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x137639730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x137639bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13763a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13763a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13763a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13763ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13763b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13763b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13763bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13763c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13763c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13763ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13763ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13763d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13763d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13763dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13763e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13763e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13763ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13763ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13763f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13763f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13763fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x137640190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x137640630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x137640ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x137640f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x137641410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1376418b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x137641d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1376421f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x137642690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x137642b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x137642fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x137643470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x137643910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x137643db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x137644250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1376446f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x137644b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x137645030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1376454d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x137645970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x137645e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1376462b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x137646750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x137646bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x137647090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x137647530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1376479d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x137647e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x137648310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1376487b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x137648c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1376490f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x137649640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x137649b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13764a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13764a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13764a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13764af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13764b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13764bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13764c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13764c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13764ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13764d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13764d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13764de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13764e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13764e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13764ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13764f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13764f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13764feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x137650400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x137650950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x137650ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1376513f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x137651940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x137651e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1376523e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x137652930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x137652e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1376533d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x137653920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x137653e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1376543c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x137654910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x137654e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1376553b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x137655900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x137655e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1376563a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1376568f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x137656e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x137657390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1376578e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x137657e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x137658380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1376588d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x137658e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x137659370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1376598c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x137659e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13765a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13765a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13765ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13765b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13765b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13765bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13765c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13765c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13765cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13765d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13765d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13765ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13765e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13765e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13765edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13765f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13765f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13765fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x137660300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x137660850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x137660da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1376612f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x137661840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x137661d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x137662230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1376626d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x137662b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x137663010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1376634b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x137663950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x137663df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x137664290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x137664730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x137664bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x137665070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x137665510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1376659b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x137665e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1376662f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x137666840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x137666f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x137667680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x137667da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1376684c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x137668780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x137668f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x137669230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x137669840 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.089.027 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.030 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13770ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13770b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13770b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13770bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13770c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13770c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13770cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13770d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13770d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13770dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13770e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13770e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13770eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13770f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13770fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x137710480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x137710ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1377112c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1377119e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x137712390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x137712ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1377131d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1377138f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x137714010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x137714730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1377149f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x137715000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x137715610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x137715c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x137716410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1377168b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x137716b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x137717400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x137717940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x137717c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1377180a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x137718540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1377189e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x137718e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x137719320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1377197c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x137719c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13771a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13771a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13771a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13771ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x138804230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1388046a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x138804b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x138804f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1388053f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x138805860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x138805cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x138806140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1388065b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x138806a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x138806f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x138807470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1388078e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x138807d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1388081c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x138808630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x138808aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x138808f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x138809380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1388097f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x138809c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13880a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13880a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13880a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13880ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13880b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13880b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13880bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13880bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13880c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13880c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13880cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13880d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13880d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13880da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13880def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13880e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13880e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13880ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13880f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13880f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13880f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13880fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x138810270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1388106e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x138810b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x138810fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x138811430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1388118a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x138811d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x138812180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1388125f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x138812a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x138812ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x138813340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1388137b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x138813c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x138814090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x138814500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x138814970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x138814de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x138815250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1388156c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x138815b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x138815fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x138816410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x138816880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x138816cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x138817160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1388175d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x138817a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x138817eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x138818320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x138818790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x138818c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x138819070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1388194e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x138819950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x138819dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13881a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13881a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13881ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13881af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13881b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13881b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13881bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13881c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13881c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13881ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13881ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13881d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13881d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13881dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13881e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13881e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13881e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13881eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13881f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13881f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13881faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13881ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1388203d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x138820840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x138820cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x138821120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x138821590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x138821a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x138821e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1388222e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x138822750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x138822bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x138823030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1388234a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x138823910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x138823d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1388241f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x138824660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x138824ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x138824f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1388253b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x138825820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x138825c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x138826100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x138826570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1388269e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x138826e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1388272c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x138827730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x138827ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x138828010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x138828480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1388288f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x138828d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1388291d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x138829640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x138829ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x138829f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13882a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13882a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13882ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13882b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13882b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13882b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13882be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13882c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13882c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13882cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13882cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13882d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13882d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13882dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13882e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13882e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13882ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13882ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13882f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13882f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13882fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1388300c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x138830530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1388309a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x138830f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1388313a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x138831810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x138832360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x138832620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1388328e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x138832d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1388331c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x138833630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x138833aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x138833f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x138834380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1388347f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x138834c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1388350d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x138835540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1388359b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x138835e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x138836290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x138836700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x138836b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x138836fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x138837450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1388378c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x138837d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1388381a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x138838610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x138838a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x138838ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x138839360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1388397d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x138839c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13883a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13883a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13883a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13883ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13883b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13883b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13883bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13883bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13883c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13883c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13883cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13883d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13883d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13883da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13883ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13883e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13883e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13883ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13883f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13883f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13883f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13883fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x138840250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1388406c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x138840b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x138840fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x138841410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x138841880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x138841cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x138842160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1388425d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x138842a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x138842eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x138843320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x138843790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x138843c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x138844070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1388444e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x138844950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x138844dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x138845230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1388456a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x138845b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x138845f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1388469f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x138847110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x138847830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x138847f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x138848210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x138848680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x138848c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x138849290 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1388044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x138804950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x138804dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x138805230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1388056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x138805b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x138805f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1388063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x138806860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x138806cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x138807140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x138807720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x138808010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x138808790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x138808f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x138809660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x138809d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13880a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13880ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13880b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13880bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13880c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13880c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13880d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13880d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13880dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13880e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13880e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13880e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13880ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13880f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13880f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13880fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13880fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x138810210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x138810680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x138810af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x138810f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1388113d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x138811840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x138811cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x138812120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x138812590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x138812a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x138812e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1388132e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x138813750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x138813bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x138814030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1388144a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x138814910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x138814d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1388151f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x138815660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x138815ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x138815f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1388163b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x138816820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11ac04230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11ac046a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11ac04b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11ac04f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11ac053f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11ac05860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11ac05cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11ac06140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11ac065b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11ac06a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11ac06e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11ac07300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11ac07770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11ac07be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11ac08050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11ac084c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11ac08930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11ac08da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11ac09210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11ac09680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11ac09af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11ac09f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11ac0a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11ac0a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11ac0acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11ac0b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11ac0b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11ac0ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11ac0be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11ac0c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11ac0c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11ac0cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11ac0d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11ac0d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11ac0d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11ac0dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11ac0e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11ac0e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11ac0ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11ac0ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11ac0f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11ac0f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11ac0fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11ac10100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11ac10570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11ac109e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11ac10e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11ac112c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11ac11730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11ac11ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11ac12010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11ac12480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11ac128f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11ac12d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11ac131d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11ac13640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11ac13ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11ac13f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11ac14390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11ac14800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11ac14c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11ac150e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11ac15550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11ac159c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11ac15e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11ac162a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11ac16710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11ac16b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11ac16ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11ac17460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11ac178d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11ac17d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11ac181b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11ac18620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11ac18a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11ac18f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11ac19370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11ac197e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11ac19c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11ac1a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11ac1a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11ac1a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11ac1ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11ac1b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11ac1b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11ac1bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11ac1bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11ac1c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11ac1c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11ac1cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11ac1d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11ac1d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11ac1da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11ac1dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11ac1e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11ac1e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11ac1ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11ac1f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11ac1f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11ac1f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11ac1fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11ac20260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11ac206d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11ac20b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11ac20fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11ac21420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11ac21890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11ac21d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11ac22170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11ac225e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11ac22a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11ac22ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11ac23330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11ac237a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11ac23c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11ac24080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11ac244f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11ac24960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11ac24dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11ac25240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11ac256b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11ac25b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11ac25f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11ac26400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11ac26870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11ac26ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11ac27150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11ac275c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11ac27a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11ac27ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11ac28310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11ac28780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11ac28bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11ac29060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11ac294d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11ac29940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11ac29db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11ac2a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11ac2a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11ac2ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11ac2af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11ac2b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11ac2b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11ac2bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11ac2c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11ac2c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11ac2ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11ac2ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11ac2d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11ac2d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11ac2dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11ac2e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11ac2ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11ac2ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11ac2f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11ac2f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11ac2fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11ac2ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11ac303f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11ac30860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11ac30cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11ac31140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11ac315b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11ac31a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11ac31e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11ac32300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11ac32770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11ac32be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11ac33050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11ac334c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11ac33930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11ac33da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11ac34210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11ac34680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11ac34af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11ac34f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11ac353d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11ac35840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11ac35cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11ac36120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11ac36590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11ac36a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11ac36e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11ac372e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11ac37750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11ac37bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11ac38530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11ac387f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11ac38c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11ac390d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11ac39540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11ac399b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11ac39e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11ac3a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11ac3a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11ac3ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11ac3afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11ac3b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11ac3b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11ac3bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11ac3c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11ac3c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11ac3ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11ac3cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11ac3d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11ac3d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11ac3dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11ac3e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11ac3e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11ac3e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11ac3ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11ac3f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11ac3f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11ac3fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11ac3ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11ac40430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11ac408a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11ac40d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11ac41180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11ac415f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11ac41a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11ac41ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11ac42340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11ac427b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11ac42c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11ac43690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11ac43db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11ac444d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11ac44bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11ac44eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11ac45320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11ac45920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11ac45f30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.938s
user	0m0.244s
sys	0m0.149s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
