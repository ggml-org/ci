### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.41 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.10 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.16 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.44 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.28 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.23 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.67 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.61 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.22 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.21 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.19 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.27 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.20 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   18.05 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.31 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.12 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.23 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.35 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    3.04 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.96 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  104.99 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.85 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   26.14 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.34 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 167.22 sec*proc (29 tests)

Total Test time (real) = 167.23 sec

real	2m47.224s
user	4m40.186s
sys	0m5.838s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.24 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.23 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.12 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.92 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.19 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.78 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.23 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.18 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.24 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.48 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.44 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   24.38 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.27 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.08 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.21 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  48.41 sec*proc (29 tests)

Total Test time (real) =  48.42 sec

real	0m48.431s
user	0m54.485s
sys	0m5.150s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.138 I build: 4864 (6ef79a67) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.019.029 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.490 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.025.498 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.501 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.025.502 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.503 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.025.504 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.025.505 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.025.506 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.025.507 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.025.508 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.025.512 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.025.513 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.025.516 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.025.517 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.025.518 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.025.518 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.025.519 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.025.520 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.025.520 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.030.927 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.032.232 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.234 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.032.235 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.032.235 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.032.236 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.032.236 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.032.237 I llama_model_loader: - type  f32:  124 tensors
0.00.032.238 I llama_model_loader: - type  f16:   73 tensors
0.00.032.238 I print_info: file format = GGUF V3 (latest)
0.00.032.239 I print_info: file type   = F16
0.00.032.241 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.037.251 I load: special tokens cache size = 5
0.00.039.806 I load: token to piece cache size = 0.2032 MB
0.00.039.835 I print_info: arch             = bert
0.00.039.837 I print_info: vocab_only       = 0
0.00.039.837 I print_info: n_ctx_train      = 512
0.00.039.837 I print_info: n_embd           = 384
0.00.039.838 I print_info: n_layer          = 12
0.00.039.841 I print_info: n_head           = 12
0.00.039.842 I print_info: n_head_kv        = 12
0.00.039.842 I print_info: n_rot            = 32
0.00.039.842 I print_info: n_swa            = 0
0.00.039.842 I print_info: n_embd_head_k    = 32
0.00.039.842 I print_info: n_embd_head_v    = 32
0.00.039.843 I print_info: n_gqa            = 1
0.00.039.844 I print_info: n_embd_k_gqa     = 384
0.00.039.845 I print_info: n_embd_v_gqa     = 384
0.00.039.846 I print_info: f_norm_eps       = 1.0e-12
0.00.039.847 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.847 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.848 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.848 I print_info: f_logit_scale    = 0.0e+00
0.00.039.851 I print_info: n_ff             = 1536
0.00.039.851 I print_info: n_expert         = 0
0.00.039.851 I print_info: n_expert_used    = 0
0.00.039.852 I print_info: causal attn      = 0
0.00.039.852 I print_info: pooling type     = 2
0.00.039.853 I print_info: rope type        = 2
0.00.039.853 I print_info: rope scaling     = linear
0.00.039.854 I print_info: freq_base_train  = 10000.0
0.00.039.854 I print_info: freq_scale_train = 1
0.00.039.854 I print_info: n_ctx_orig_yarn  = 512
0.00.039.856 I print_info: rope_finetuned   = unknown
0.00.039.857 I print_info: ssm_d_conv       = 0
0.00.039.857 I print_info: ssm_d_inner      = 0
0.00.039.857 I print_info: ssm_d_state      = 0
0.00.039.857 I print_info: ssm_dt_rank      = 0
0.00.039.857 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.858 I print_info: model type       = 33M
0.00.039.858 I print_info: model params     = 33.21 M
0.00.039.859 I print_info: general.name     = Bge Small
0.00.039.860 I print_info: vocab type       = WPM
0.00.039.860 I print_info: n_vocab          = 30522
0.00.039.860 I print_info: n_merges         = 0
0.00.039.860 I print_info: BOS token        = 101 '[CLS]'
0.00.039.861 I print_info: UNK token        = 100 '[UNK]'
0.00.039.862 I print_info: SEP token        = 102 '[SEP]'
0.00.039.871 I print_info: PAD token        = 0 '[PAD]'
0.00.039.873 I print_info: MASK token       = 103 '[MASK]'
0.00.039.873 I print_info: LF token         = 0 '[PAD]'
0.00.039.874 I print_info: max token length = 21
0.00.039.874 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.043.167 I load_tensors: offloading 12 repeating layers to GPU
0.00.043.168 I load_tensors: offloading output layer to GPU
0.00.043.169 I load_tensors: offloaded 13/13 layers to GPU
0.00.043.194 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.043.195 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.043.499 I llama_init_from_model: n_seq_max     = 1
0.00.043.500 I llama_init_from_model: n_ctx         = 512
0.00.043.501 I llama_init_from_model: n_ctx_per_seq = 512
0.00.043.501 I llama_init_from_model: n_batch       = 2048
0.00.043.501 I llama_init_from_model: n_ubatch      = 2048
0.00.043.502 I llama_init_from_model: flash_attn    = 0
0.00.043.502 I llama_init_from_model: freq_base     = 10000.0
0.00.043.503 I llama_init_from_model: freq_scale    = 1
0.00.043.503 I ggml_metal_init: allocating
0.00.043.511 I ggml_metal_init: found device: Apple M4
0.00.043.516 I ggml_metal_init: picking default device: Apple M4
0.00.044.214 I ggml_metal_init: using embedded metal library
0.00.048.729 I ggml_metal_init: GPU name:   Apple M4
0.00.048.732 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.048.733 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.048.733 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.048.734 I ggml_metal_init: simdgroup reduction   = true
0.00.048.734 I ggml_metal_init: simdgroup matrix mul. = true
0.00.048.734 I ggml_metal_init: has residency sets    = true
0.00.048.734 I ggml_metal_init: has bfloat            = true
0.00.048.734 I ggml_metal_init: use bfloat            = true
0.00.048.735 I ggml_metal_init: hasUnifiedMemory      = true
0.00.048.736 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.041 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.062.795 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.062.798 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.062.800 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.064.118 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.064.119 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.064.120 I llama_init_from_model: graph nodes  = 429
0.00.064.120 I llama_init_from_model: graph splits = 2
0.00.064.121 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.064.121 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.070.001 I 
0.00.070.029 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.070.705 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.075.876 I llama_perf_context_print:        load time =      50.97 ms
0.00.075.879 I llama_perf_context_print: prompt eval time =       5.00 ms /     9 tokens (    0.56 ms per token,  1798.20 tokens per second)
0.00.075.880 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.075.880 I llama_perf_context_print:       total time =       5.88 ms /    10 tokens
0.00.076.045 I ggml_metal_free: deallocating

real	0m0.284s
user	0m0.055s
sys	0m0.035s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.050 I build: 4864 (6ef79a67) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.126 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.012.934 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.938 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.940 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.940 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.941 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.941 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.941 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.942 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.942 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.943 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.943 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.943 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.945 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.946 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.012.946 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.012.947 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.947 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.012.947 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.015.511 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.016.184 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.016.185 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.016.185 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.016.186 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.016.186 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.016.186 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.016.187 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.016.187 I llama_model_loader: - type  f32:  124 tensors
0.00.016.187 I llama_model_loader: - type q8_0:   73 tensors
0.00.016.188 I print_info: file format = GGUF V3 (latest)
0.00.016.188 I print_info: file type   = Q8_0
0.00.016.191 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.018.752 I load: special tokens cache size = 5
0.00.020.148 I load: token to piece cache size = 0.2032 MB
0.00.020.157 I print_info: arch             = bert
0.00.020.158 I print_info: vocab_only       = 0
0.00.020.158 I print_info: n_ctx_train      = 512
0.00.020.158 I print_info: n_embd           = 384
0.00.020.159 I print_info: n_layer          = 12
0.00.020.162 I print_info: n_head           = 12
0.00.020.164 I print_info: n_head_kv        = 12
0.00.020.165 I print_info: n_rot            = 32
0.00.020.165 I print_info: n_swa            = 0
0.00.020.165 I print_info: n_embd_head_k    = 32
0.00.020.165 I print_info: n_embd_head_v    = 32
0.00.020.166 I print_info: n_gqa            = 1
0.00.020.166 I print_info: n_embd_k_gqa     = 384
0.00.020.167 I print_info: n_embd_v_gqa     = 384
0.00.020.167 I print_info: f_norm_eps       = 1.0e-12
0.00.020.169 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.020.169 I print_info: f_clamp_kqv      = 0.0e+00
0.00.020.169 I print_info: f_max_alibi_bias = 0.0e+00
0.00.020.170 I print_info: f_logit_scale    = 0.0e+00
0.00.020.170 I print_info: n_ff             = 1536
0.00.020.170 I print_info: n_expert         = 0
0.00.020.171 I print_info: n_expert_used    = 0
0.00.020.171 I print_info: causal attn      = 0
0.00.020.171 I print_info: pooling type     = 2
0.00.020.171 I print_info: rope type        = 2
0.00.020.171 I print_info: rope scaling     = linear
0.00.020.175 I print_info: freq_base_train  = 10000.0
0.00.020.176 I print_info: freq_scale_train = 1
0.00.020.176 I print_info: n_ctx_orig_yarn  = 512
0.00.020.176 I print_info: rope_finetuned   = unknown
0.00.020.176 I print_info: ssm_d_conv       = 0
0.00.020.176 I print_info: ssm_d_inner      = 0
0.00.020.176 I print_info: ssm_d_state      = 0
0.00.020.177 I print_info: ssm_dt_rank      = 0
0.00.020.177 I print_info: ssm_dt_b_c_rms   = 0
0.00.020.177 I print_info: model type       = 33M
0.00.020.177 I print_info: model params     = 33.21 M
0.00.020.178 I print_info: general.name     = Bge Small
0.00.020.178 I print_info: vocab type       = WPM
0.00.020.179 I print_info: n_vocab          = 30522
0.00.020.179 I print_info: n_merges         = 0
0.00.020.179 I print_info: BOS token        = 101 '[CLS]'
0.00.020.184 I print_info: UNK token        = 100 '[UNK]'
0.00.020.184 I print_info: SEP token        = 102 '[SEP]'
0.00.020.184 I print_info: PAD token        = 0 '[PAD]'
0.00.020.184 I print_info: MASK token       = 103 '[MASK]'
0.00.020.184 I print_info: LF token         = 0 '[PAD]'
0.00.020.185 I print_info: max token length = 21
0.00.020.185 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.022.211 I load_tensors: offloading 12 repeating layers to GPU
0.00.022.212 I load_tensors: offloading output layer to GPU
0.00.022.213 I load_tensors: offloaded 13/13 layers to GPU
0.00.022.221 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.022.221 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.022.504 I llama_init_from_model: n_seq_max     = 1
0.00.022.505 I llama_init_from_model: n_ctx         = 512
0.00.022.505 I llama_init_from_model: n_ctx_per_seq = 512
0.00.022.505 I llama_init_from_model: n_batch       = 2048
0.00.022.505 I llama_init_from_model: n_ubatch      = 2048
0.00.022.505 I llama_init_from_model: flash_attn    = 0
0.00.022.506 I llama_init_from_model: freq_base     = 10000.0
0.00.022.506 I llama_init_from_model: freq_scale    = 1
0.00.022.507 I ggml_metal_init: allocating
0.00.022.533 I ggml_metal_init: found device: Apple M4
0.00.022.538 I ggml_metal_init: picking default device: Apple M4
0.00.023.081 I ggml_metal_init: using embedded metal library
0.00.025.756 I ggml_metal_init: GPU name:   Apple M4
0.00.025.758 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.025.759 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.025.759 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.025.760 I ggml_metal_init: simdgroup reduction   = true
0.00.025.760 I ggml_metal_init: simdgroup matrix mul. = true
0.00.025.760 I ggml_metal_init: has residency sets    = true
0.00.025.760 I ggml_metal_init: has bfloat            = true
0.00.025.760 I ggml_metal_init: use bfloat            = true
0.00.025.761 I ggml_metal_init: hasUnifiedMemory      = true
0.00.025.763 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.036.512 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.037.145 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.037.147 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.037.149 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.038.209 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.038.210 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.038.210 I llama_init_from_model: graph nodes  = 429
0.00.038.211 I llama_init_from_model: graph splits = 2
0.00.038.212 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.038.212 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.042.503 I 
0.00.042.529 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.043.075 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.047.540 I llama_perf_context_print:        load time =      32.37 ms
0.00.047.541 I llama_perf_context_print: prompt eval time =       4.33 ms /     9 tokens (    0.48 ms per token,  2076.12 tokens per second)
0.00.047.542 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.047.542 I llama_perf_context_print:       total time =       5.04 ms /    10 tokens
0.00.047.751 I ggml_metal_free: deallocating

real	0m0.060s
user	0m0.032s
sys	0m0.018s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.221 I build: 4864 (6ef79a67) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.813 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.143 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.148 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.150 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.036.151 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.154 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.036.155 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.036.156 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.036.157 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.036.158 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.036.158 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.036.159 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.036.160 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.036.163 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.036.164 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.036.164 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.036.165 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.165 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.043.865 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.045.883 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.349 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.050.351 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.351 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.050.352 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.050.352 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.050.352 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.050.353 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.050.353 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.050.353 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.050.354 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.050.354 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.050.355 I llama_model_loader: - type  f32:   40 tensors
0.00.050.355 I llama_model_loader: - type  f16:   30 tensors
0.00.050.356 I print_info: file format = GGUF V3 (latest)
0.00.050.357 I print_info: file type   = F16
0.00.050.359 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.054.549 W load: empty token at index 5
0.00.059.751 W load: model vocab missing newline token, using special_pad_id instead
0.00.061.301 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.061.338 I load: special tokens cache size = 5
0.00.326.590 I load: token to piece cache size = 1.5060 MB
0.00.326.620 I print_info: arch             = jina-bert-v2
0.00.326.621 I print_info: vocab_only       = 0
0.00.326.621 I print_info: n_ctx_train      = 8192
0.00.326.622 I print_info: n_embd           = 384
0.00.326.622 I print_info: n_layer          = 4
0.00.326.630 I print_info: n_head           = 12
0.00.326.633 I print_info: n_head_kv        = 12
0.00.326.633 I print_info: n_rot            = 32
0.00.326.633 I print_info: n_swa            = 0
0.00.326.634 I print_info: n_embd_head_k    = 32
0.00.326.634 I print_info: n_embd_head_v    = 32
0.00.326.634 I print_info: n_gqa            = 1
0.00.326.635 I print_info: n_embd_k_gqa     = 384
0.00.326.635 I print_info: n_embd_v_gqa     = 384
0.00.326.636 I print_info: f_norm_eps       = 1.0e-12
0.00.326.637 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.326.637 I print_info: f_clamp_kqv      = 0.0e+00
0.00.326.637 I print_info: f_max_alibi_bias = 8.0e+00
0.00.326.637 I print_info: f_logit_scale    = 0.0e+00
0.00.326.638 I print_info: n_ff             = 1536
0.00.326.638 I print_info: n_expert         = 0
0.00.326.638 I print_info: n_expert_used    = 0
0.00.326.638 I print_info: causal attn      = 0
0.00.326.638 I print_info: pooling type     = -1
0.00.326.639 I print_info: rope type        = -1
0.00.326.639 I print_info: rope scaling     = linear
0.00.326.639 I print_info: freq_base_train  = 10000.0
0.00.326.640 I print_info: freq_scale_train = 1
0.00.326.640 I print_info: n_ctx_orig_yarn  = 8192
0.00.326.640 I print_info: rope_finetuned   = unknown
0.00.326.640 I print_info: ssm_d_conv       = 0
0.00.326.640 I print_info: ssm_d_inner      = 0
0.00.326.641 I print_info: ssm_d_state      = 0
0.00.326.641 I print_info: ssm_dt_rank      = 0
0.00.326.641 I print_info: ssm_dt_b_c_rms   = 0
0.00.326.641 I print_info: model type       = 33M
0.00.326.642 I print_info: model params     = 32.90 M
0.00.326.642 I print_info: general.name     = Jina Bert Implementation
0.00.326.643 I print_info: vocab type       = BPE
0.00.326.644 I print_info: n_vocab          = 61056
0.00.326.644 I print_info: n_merges         = 39382
0.00.326.644 I print_info: BOS token        = 0 '<s>'
0.00.326.644 I print_info: EOS token        = 2 '</s>'
0.00.326.644 I print_info: UNK token        = 3 '<unk>'
0.00.326.645 I print_info: SEP token        = 2 '</s>'
0.00.326.645 I print_info: PAD token        = 1 '<pad>'
0.00.326.645 I print_info: MASK token       = 4 '<mask>'
0.00.326.645 I print_info: EOG token        = 2 '</s>'
0.00.326.645 I print_info: max token length = 45
0.00.326.646 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.328.791 I load_tensors: offloading 4 repeating layers to GPU
0.00.328.792 I load_tensors: offloading output layer to GPU
0.00.328.792 I load_tensors: offloaded 5/5 layers to GPU
0.00.328.814 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.328.815 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.329.084 I llama_init_from_model: n_seq_max     = 1
0.00.329.084 I llama_init_from_model: n_ctx         = 8192
0.00.329.084 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.329.085 I llama_init_from_model: n_batch       = 2048
0.00.329.085 I llama_init_from_model: n_ubatch      = 2048
0.00.329.085 I llama_init_from_model: flash_attn    = 0
0.00.329.086 I llama_init_from_model: freq_base     = 10000.0
0.00.329.086 I llama_init_from_model: freq_scale    = 1
0.00.329.086 I ggml_metal_init: allocating
0.00.329.091 I ggml_metal_init: found device: Apple M4
0.00.329.094 I ggml_metal_init: picking default device: Apple M4
0.00.329.689 I ggml_metal_init: using embedded metal library
0.00.332.702 I ggml_metal_init: GPU name:   Apple M4
0.00.332.704 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.332.704 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.332.705 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.332.705 I ggml_metal_init: simdgroup reduction   = true
0.00.332.705 I ggml_metal_init: simdgroup matrix mul. = true
0.00.332.705 I ggml_metal_init: has residency sets    = true
0.00.332.705 I ggml_metal_init: has bfloat            = true
0.00.332.705 I ggml_metal_init: use bfloat            = true
0.00.332.706 I ggml_metal_init: hasUnifiedMemory      = true
0.00.332.706 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.342.644 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.345.695 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.345.697 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.345.701 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.352.222 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.352.224 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.352.224 I llama_init_from_model: graph nodes  = 154
0.00.352.224 I llama_init_from_model: graph splits = 2
0.00.352.226 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.352.226 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.359.688 I 
0.00.359.718 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.359.973 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.359.974 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.359.984 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.359.984 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.359.990 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.359.990 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.360.516 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.364.180 I llama_perf_context_print:        load time =     336.87 ms
0.00.364.181 I llama_perf_context_print: prompt eval time =       3.66 ms /    62 tokens (    0.06 ms per token, 16958.42 tokens per second)
0.00.364.182 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.364.183 I llama_perf_context_print:       total time =       4.49 ms /    63 tokens
0.00.364.354 I ggml_metal_free: deallocating

real	0m1.155s
user	0m0.334s
sys	0m0.050s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.209 I build: 4864 (6ef79a67) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.407 I main: llama backend init
0.00.000.419 I main: load the model and apply lora adapter, if any
0.00.050.055 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.063.084 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.063.116 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.063.120 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.063.121 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.063.122 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.063.123 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.063.124 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.063.127 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.063.128 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.063.132 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.063.133 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.063.134 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.063.146 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.063.151 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.063.158 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.063.159 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.063.160 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.072.018 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.074.410 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.082.254 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.082.257 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.082.257 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.082.258 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.082.258 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.082.259 I llama_model_loader: - type  f32:  194 tensors
0.00.082.260 I llama_model_loader: - type  f16:   98 tensors
0.00.082.261 I print_info: file format = GGUF V3 (latest)
0.00.082.265 I print_info: file type   = all F32 (guessed)
0.00.082.267 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.096.388 I load: special tokens cache size = 25
0.00.105.718 I load: token to piece cache size = 0.2984 MB
0.00.105.739 I print_info: arch             = gptneox
0.00.105.740 I print_info: vocab_only       = 0
0.00.105.740 I print_info: n_ctx_train      = 2048
0.00.105.741 I print_info: n_embd           = 2048
0.00.105.741 I print_info: n_layer          = 24
0.00.105.744 I print_info: n_head           = 16
0.00.105.745 I print_info: n_head_kv        = 16
0.00.105.746 I print_info: n_rot            = 32
0.00.105.747 I print_info: n_swa            = 0
0.00.105.747 I print_info: n_embd_head_k    = 128
0.00.105.747 I print_info: n_embd_head_v    = 128
0.00.105.748 I print_info: n_gqa            = 1
0.00.105.751 I print_info: n_embd_k_gqa     = 2048
0.00.105.751 I print_info: n_embd_v_gqa     = 2048
0.00.105.752 I print_info: f_norm_eps       = 1.0e-05
0.00.105.752 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.105.753 I print_info: f_clamp_kqv      = 0.0e+00
0.00.105.753 I print_info: f_max_alibi_bias = 0.0e+00
0.00.105.753 I print_info: f_logit_scale    = 0.0e+00
0.00.105.754 I print_info: n_ff             = 8192
0.00.105.754 I print_info: n_expert         = 0
0.00.105.754 I print_info: n_expert_used    = 0
0.00.105.754 I print_info: causal attn      = 1
0.00.105.754 I print_info: pooling type     = 0
0.00.105.756 I print_info: rope type        = 2
0.00.105.756 I print_info: rope scaling     = linear
0.00.105.757 I print_info: freq_base_train  = 10000.0
0.00.105.757 I print_info: freq_scale_train = 1
0.00.105.759 I print_info: n_ctx_orig_yarn  = 2048
0.00.105.759 I print_info: rope_finetuned   = unknown
0.00.105.759 I print_info: ssm_d_conv       = 0
0.00.105.759 I print_info: ssm_d_inner      = 0
0.00.105.759 I print_info: ssm_d_state      = 0
0.00.105.760 I print_info: ssm_dt_rank      = 0
0.00.105.760 I print_info: ssm_dt_b_c_rms   = 0
0.00.105.760 I print_info: model type       = 1.4B
0.00.105.760 I print_info: model params     = 1.41 B
0.00.105.760 I print_info: general.name     = 1.4B
0.00.105.761 I print_info: vocab type       = BPE
0.00.105.761 I print_info: n_vocab          = 50304
0.00.105.761 I print_info: n_merges         = 50009
0.00.105.762 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.105.767 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.105.767 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.105.767 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.105.767 I print_info: LF token         = 187 ''
0.00.105.768 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.105.774 I print_info: max token length = 1024
0.00.105.775 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.169.758 I load_tensors: offloading 24 repeating layers to GPU
0.00.169.762 I load_tensors: offloading output layer to GPU
0.00.169.762 I load_tensors: offloaded 25/25 layers to GPU
0.00.169.790 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.169.792 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.170.488 I llama_init_from_model: n_seq_max     = 1
0.00.170.489 I llama_init_from_model: n_ctx         = 2048
0.00.170.489 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.170.490 I llama_init_from_model: n_batch       = 2048
0.00.170.490 I llama_init_from_model: n_ubatch      = 512
0.00.170.490 I llama_init_from_model: flash_attn    = 0
0.00.170.490 I llama_init_from_model: freq_base     = 10000.0
0.00.170.491 I llama_init_from_model: freq_scale    = 1
0.00.170.493 I ggml_metal_init: allocating
0.00.170.546 I ggml_metal_init: found device: Apple M4
0.00.170.552 I ggml_metal_init: picking default device: Apple M4
0.00.171.140 I ggml_metal_init: using embedded metal library
0.00.537.397 I ggml_metal_init: GPU name:   Apple M4
0.00.537.413 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.537.414 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.537.414 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.537.419 I ggml_metal_init: simdgroup reduction   = true
0.00.537.420 I ggml_metal_init: simdgroup matrix mul. = true
0.00.537.425 I ggml_metal_init: has residency sets    = true
0.00.537.426 I ggml_metal_init: has bfloat            = true
0.00.537.426 I ggml_metal_init: use bfloat            = true
0.00.537.428 I ggml_metal_init: hasUnifiedMemory      = true
0.00.537.433 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.576.201 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.614.615 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.614.630 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.614.667 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.618.366 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.618.369 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.618.369 I llama_init_from_model: graph nodes  = 967
0.00.618.369 I llama_init_from_model: graph splits = 2
0.00.618.377 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.618.505 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.618.505 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.684.749 I main: llama threadpool init, n_threads = 4
0.00.684.793 I 
0.00.684.823 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.684.824 I 
0.00.685.016 I sampler seed: 1234
0.00.685.021 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.685.055 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.685.057 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.685.057 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.524.400 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58532.56 tokens per second)
0.02.524.400 I llama_perf_context_print:        load time =     633.77 ms
0.02.524.401 I llama_perf_context_print: prompt eval time =      43.71 ms /     7 tokens (    6.24 ms per token,   160.14 tokens per second)
0.02.524.403 I llama_perf_context_print:        eval time =    1792.78 ms /    63 runs   (   28.46 ms per token,    35.14 tokens per second)
0.02.524.406 I llama_perf_context_print:       total time =    1840.57 ms /    70 tokens
0.02.524.673 I ggml_metal_free: deallocating

real	0m2.899s
user	0m0.145s
sys	0m0.171s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.719 I build: 4864 (6ef79a67) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.477 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.797 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.803 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.805 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.805 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.809 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.810 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.810 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.811 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.811 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.812 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.812 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.812 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.813 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.813 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.815 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.816 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.816 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.899 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.425 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.103 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.052.105 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.106 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.052.106 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.052.106 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.052.107 I llama_model_loader: - type  f32:  194 tensors
0.00.052.107 I llama_model_loader: - type  f16:   98 tensors
0.00.052.107 I print_info: file format = GGUF V3 (latest)
0.00.052.108 I print_info: file type   = all F32 (guessed)
0.00.052.110 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.064.193 I load: special tokens cache size = 25
0.00.072.680 I load: token to piece cache size = 0.2984 MB
0.00.072.695 I print_info: arch             = gptneox
0.00.072.696 I print_info: vocab_only       = 0
0.00.072.696 I print_info: n_ctx_train      = 2048
0.00.072.697 I print_info: n_embd           = 2048
0.00.072.697 I print_info: n_layer          = 24
0.00.072.700 I print_info: n_head           = 16
0.00.072.701 I print_info: n_head_kv        = 16
0.00.072.701 I print_info: n_rot            = 32
0.00.072.701 I print_info: n_swa            = 0
0.00.072.701 I print_info: n_embd_head_k    = 128
0.00.072.701 I print_info: n_embd_head_v    = 128
0.00.072.702 I print_info: n_gqa            = 1
0.00.072.703 I print_info: n_embd_k_gqa     = 2048
0.00.072.704 I print_info: n_embd_v_gqa     = 2048
0.00.072.704 I print_info: f_norm_eps       = 1.0e-05
0.00.072.708 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.072.708 I print_info: f_clamp_kqv      = 0.0e+00
0.00.072.708 I print_info: f_max_alibi_bias = 0.0e+00
0.00.072.708 I print_info: f_logit_scale    = 0.0e+00
0.00.072.709 I print_info: n_ff             = 8192
0.00.072.709 I print_info: n_expert         = 0
0.00.072.709 I print_info: n_expert_used    = 0
0.00.072.709 I print_info: causal attn      = 1
0.00.072.710 I print_info: pooling type     = 0
0.00.072.710 I print_info: rope type        = 2
0.00.072.714 I print_info: rope scaling     = linear
0.00.072.715 I print_info: freq_base_train  = 10000.0
0.00.072.715 I print_info: freq_scale_train = 1
0.00.072.715 I print_info: n_ctx_orig_yarn  = 2048
0.00.072.716 I print_info: rope_finetuned   = unknown
0.00.072.716 I print_info: ssm_d_conv       = 0
0.00.072.716 I print_info: ssm_d_inner      = 0
0.00.072.716 I print_info: ssm_d_state      = 0
0.00.072.716 I print_info: ssm_dt_rank      = 0
0.00.072.718 I print_info: ssm_dt_b_c_rms   = 0
0.00.072.718 I print_info: model type       = 1.4B
0.00.072.719 I print_info: model params     = 1.41 B
0.00.072.719 I print_info: general.name     = 1.4B
0.00.072.719 I print_info: vocab type       = BPE
0.00.072.720 I print_info: n_vocab          = 50304
0.00.072.720 I print_info: n_merges         = 50009
0.00.072.720 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.072.721 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.072.721 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.072.721 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.072.721 I print_info: LF token         = 187 ''
0.00.072.722 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.072.727 I print_info: max token length = 1024
0.00.072.728 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.433.989 I load_tensors: offloading 24 repeating layers to GPU
0.01.433.994 I load_tensors: offloading output layer to GPU
0.01.433.994 I load_tensors: offloaded 25/25 layers to GPU
0.01.434.021 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.434.023 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.434.960 I llama_init_from_model: n_seq_max     = 1
0.01.434.961 I llama_init_from_model: n_ctx         = 128
0.01.434.961 I llama_init_from_model: n_ctx_per_seq = 128
0.01.434.962 I llama_init_from_model: n_batch       = 128
0.01.434.962 I llama_init_from_model: n_ubatch      = 128
0.01.434.962 I llama_init_from_model: flash_attn    = 0
0.01.434.963 I llama_init_from_model: freq_base     = 10000.0
0.01.434.963 I llama_init_from_model: freq_scale    = 1
0.01.434.964 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.434.965 I ggml_metal_init: allocating
0.01.435.043 I ggml_metal_init: found device: Apple M4
0.01.435.061 I ggml_metal_init: picking default device: Apple M4
0.01.436.068 I ggml_metal_init: using embedded metal library
0.01.440.243 I ggml_metal_init: GPU name:   Apple M4
0.01.440.246 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.440.247 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.440.247 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.440.248 I ggml_metal_init: simdgroup reduction   = true
0.01.440.248 I ggml_metal_init: simdgroup matrix mul. = true
0.01.440.248 I ggml_metal_init: has residency sets    = true
0.01.440.248 I ggml_metal_init: has bfloat            = true
0.01.440.249 I ggml_metal_init: use bfloat            = true
0.01.440.249 I ggml_metal_init: hasUnifiedMemory      = true
0.01.440.251 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.452.252 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.454.031 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.454.034 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.454.048 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.455.765 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.455.766 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.455.767 I llama_init_from_model: graph nodes  = 967
0.01.455.767 I llama_init_from_model: graph splits = 2
0.01.455.768 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.455.768 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.492.001 I 
0.01.492.051 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.492.055 I perplexity: tokenizing the input ..
0.01.497.604 I perplexity: tokenization took 5.548 ms
0.01.497.612 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.629.450 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.630.797 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.630.835 I llama_perf_context_print:        load time =    1467.51 ms
0.01.630.836 I llama_perf_context_print: prompt eval time =     131.53 ms /   128 tokens (    1.03 ms per token,   973.17 tokens per second)
0.01.630.837 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.630.837 I llama_perf_context_print:       total time =     138.83 ms /   129 tokens
0.01.631.255 I ggml_metal_free: deallocating

real	0m1.900s
user	0m0.094s
sys	0m0.264s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.064 I build: 4864 (6ef79a67) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.105 I main: llama backend init
0.00.000.107 I main: load the model and apply lora adapter, if any
0.00.009.583 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.029.454 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.029.460 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.462 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.462 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.462 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.463 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.463 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.464 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.464 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.465 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.465 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.465 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.466 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.467 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.469 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.469 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.469 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.460 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.557 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.389 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.038.390 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.390 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.391 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.391 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.391 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.038.392 I llama_model_loader: - type  f32:  194 tensors
0.00.038.392 I llama_model_loader: - type q8_0:   98 tensors
0.00.038.393 I print_info: file format = GGUF V3 (latest)
0.00.038.393 I print_info: file type   = Q8_0
0.00.038.394 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.046.931 I load: special tokens cache size = 25
0.00.054.093 I load: token to piece cache size = 0.2984 MB
0.00.054.108 I print_info: arch             = gptneox
0.00.054.109 I print_info: vocab_only       = 0
0.00.054.109 I print_info: n_ctx_train      = 2048
0.00.054.110 I print_info: n_embd           = 2048
0.00.054.110 I print_info: n_layer          = 24
0.00.054.113 I print_info: n_head           = 16
0.00.054.114 I print_info: n_head_kv        = 16
0.00.054.117 I print_info: n_rot            = 32
0.00.054.117 I print_info: n_swa            = 0
0.00.054.117 I print_info: n_embd_head_k    = 128
0.00.054.118 I print_info: n_embd_head_v    = 128
0.00.054.118 I print_info: n_gqa            = 1
0.00.054.119 I print_info: n_embd_k_gqa     = 2048
0.00.054.120 I print_info: n_embd_v_gqa     = 2048
0.00.054.121 I print_info: f_norm_eps       = 1.0e-05
0.00.054.122 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.122 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.122 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.122 I print_info: f_logit_scale    = 0.0e+00
0.00.054.123 I print_info: n_ff             = 8192
0.00.054.123 I print_info: n_expert         = 0
0.00.054.123 I print_info: n_expert_used    = 0
0.00.054.123 I print_info: causal attn      = 1
0.00.054.123 I print_info: pooling type     = 0
0.00.054.123 I print_info: rope type        = 2
0.00.054.124 I print_info: rope scaling     = linear
0.00.054.125 I print_info: freq_base_train  = 10000.0
0.00.054.125 I print_info: freq_scale_train = 1
0.00.054.125 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.125 I print_info: rope_finetuned   = unknown
0.00.054.127 I print_info: ssm_d_conv       = 0
0.00.054.127 I print_info: ssm_d_inner      = 0
0.00.054.127 I print_info: ssm_d_state      = 0
0.00.054.127 I print_info: ssm_dt_rank      = 0
0.00.054.127 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.127 I print_info: model type       = 1.4B
0.00.054.128 I print_info: model params     = 1.41 B
0.00.054.128 I print_info: general.name     = 1.4B
0.00.054.128 I print_info: vocab type       = BPE
0.00.054.129 I print_info: n_vocab          = 50304
0.00.054.129 I print_info: n_merges         = 50009
0.00.054.129 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.129 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.130 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.130 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.130 I print_info: LF token         = 187 ''
0.00.054.130 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.131 I print_info: max token length = 1024
0.00.054.131 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.076.920 I load_tensors: offloading 24 repeating layers to GPU
0.01.076.923 I load_tensors: offloading output layer to GPU
0.01.076.923 I load_tensors: offloaded 25/25 layers to GPU
0.01.076.947 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.076.950 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.078.323 I llama_init_from_model: n_seq_max     = 1
0.01.078.325 I llama_init_from_model: n_ctx         = 2048
0.01.078.325 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.078.326 I llama_init_from_model: n_batch       = 2048
0.01.078.326 I llama_init_from_model: n_ubatch      = 512
0.01.078.326 I llama_init_from_model: flash_attn    = 0
0.01.078.327 I llama_init_from_model: freq_base     = 10000.0
0.01.078.328 I llama_init_from_model: freq_scale    = 1
0.01.078.329 I ggml_metal_init: allocating
0.01.078.356 I ggml_metal_init: found device: Apple M4
0.01.078.364 I ggml_metal_init: picking default device: Apple M4
0.01.079.402 I ggml_metal_init: using embedded metal library
0.01.084.700 I ggml_metal_init: GPU name:   Apple M4
0.01.084.703 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.084.704 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.084.704 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.084.705 I ggml_metal_init: simdgroup reduction   = true
0.01.084.705 I ggml_metal_init: simdgroup matrix mul. = true
0.01.084.705 I ggml_metal_init: has residency sets    = true
0.01.084.706 I ggml_metal_init: has bfloat            = true
0.01.084.706 I ggml_metal_init: use bfloat            = true
0.01.084.706 I ggml_metal_init: hasUnifiedMemory      = true
0.01.084.708 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.101.820 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.155.412 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.155.422 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.155.445 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.159.834 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.159.836 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.159.836 I llama_init_from_model: graph nodes  = 967
0.01.159.836 I llama_init_from_model: graph splits = 2
0.01.159.842 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.159.966 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.159.967 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.215.632 I main: llama threadpool init, n_threads = 4
0.01.215.677 I 
0.01.215.696 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.215.697 I 
0.01.215.869 I sampler seed: 1234
0.01.215.874 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.215.912 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.215.916 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.215.916 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.307.763 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55773.76 tokens per second)
0.02.307.764 I llama_perf_context_print:        load time =    1205.32 ms
0.02.307.766 I llama_perf_context_print: prompt eval time =      44.41 ms /     7 tokens (    6.34 ms per token,   157.62 tokens per second)
0.02.307.767 I llama_perf_context_print:        eval time =    1044.56 ms /    63 runs   (   16.58 ms per token,    60.31 tokens per second)
0.02.307.767 I llama_perf_context_print:       total time =    1092.86 ms /    70 tokens
0.02.307.998 I ggml_metal_free: deallocating

real	0m2.336s
user	0m0.111s
sys	0m0.275s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.240 I build: 4864 (6ef79a67) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.259 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.538 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.544 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.548 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.549 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.549 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.549 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.550 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.551 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.551 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.552 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.552 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.552 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.553 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.553 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.555 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.555 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.555 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.351 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.336 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.057 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.059 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.059 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.059 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.060 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.060 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.061 I llama_model_loader: - type  f32:  194 tensors
0.00.026.061 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.062 I print_info: file format = GGUF V3 (latest)
0.00.026.062 I print_info: file type   = Q8_0
0.00.026.064 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.034.932 I load: special tokens cache size = 25
0.00.041.273 I load: token to piece cache size = 0.2984 MB
0.00.041.290 I print_info: arch             = gptneox
0.00.041.291 I print_info: vocab_only       = 0
0.00.041.291 I print_info: n_ctx_train      = 2048
0.00.041.292 I print_info: n_embd           = 2048
0.00.041.292 I print_info: n_layer          = 24
0.00.041.296 I print_info: n_head           = 16
0.00.041.297 I print_info: n_head_kv        = 16
0.00.041.297 I print_info: n_rot            = 32
0.00.041.297 I print_info: n_swa            = 0
0.00.041.297 I print_info: n_embd_head_k    = 128
0.00.041.297 I print_info: n_embd_head_v    = 128
0.00.041.298 I print_info: n_gqa            = 1
0.00.041.298 I print_info: n_embd_k_gqa     = 2048
0.00.041.299 I print_info: n_embd_v_gqa     = 2048
0.00.041.300 I print_info: f_norm_eps       = 1.0e-05
0.00.041.300 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.300 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.300 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.302 I print_info: f_logit_scale    = 0.0e+00
0.00.041.302 I print_info: n_ff             = 8192
0.00.041.303 I print_info: n_expert         = 0
0.00.041.303 I print_info: n_expert_used    = 0
0.00.041.303 I print_info: causal attn      = 1
0.00.041.303 I print_info: pooling type     = 0
0.00.041.303 I print_info: rope type        = 2
0.00.041.303 I print_info: rope scaling     = linear
0.00.041.304 I print_info: freq_base_train  = 10000.0
0.00.041.304 I print_info: freq_scale_train = 1
0.00.041.306 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.306 I print_info: rope_finetuned   = unknown
0.00.041.307 I print_info: ssm_d_conv       = 0
0.00.041.307 I print_info: ssm_d_inner      = 0
0.00.041.307 I print_info: ssm_d_state      = 0
0.00.041.307 I print_info: ssm_dt_rank      = 0
0.00.041.307 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.307 I print_info: model type       = 1.4B
0.00.041.307 I print_info: model params     = 1.41 B
0.00.041.307 I print_info: general.name     = 1.4B
0.00.041.308 I print_info: vocab type       = BPE
0.00.041.309 I print_info: n_vocab          = 50304
0.00.041.310 I print_info: n_merges         = 50009
0.00.041.310 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.310 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.310 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.310 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.311 I print_info: LF token         = 187 ''
0.00.041.311 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.311 I print_info: max token length = 1024
0.00.041.311 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.896.224 I load_tensors: offloading 24 repeating layers to GPU
0.00.896.230 I load_tensors: offloading output layer to GPU
0.00.896.230 I load_tensors: offloaded 25/25 layers to GPU
0.00.896.261 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.896.263 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.897.598 I llama_init_from_model: n_seq_max     = 1
0.00.897.600 I llama_init_from_model: n_ctx         = 128
0.00.897.601 I llama_init_from_model: n_ctx_per_seq = 128
0.00.897.601 I llama_init_from_model: n_batch       = 128
0.00.897.602 I llama_init_from_model: n_ubatch      = 128
0.00.897.602 I llama_init_from_model: flash_attn    = 0
0.00.897.603 I llama_init_from_model: freq_base     = 10000.0
0.00.897.604 I llama_init_from_model: freq_scale    = 1
0.00.897.604 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.897.605 I ggml_metal_init: allocating
0.00.897.668 I ggml_metal_init: found device: Apple M4
0.00.897.677 I ggml_metal_init: picking default device: Apple M4
0.00.898.823 I ggml_metal_init: using embedded metal library
0.00.904.211 I ggml_metal_init: GPU name:   Apple M4
0.00.904.214 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.904.215 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.904.216 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.904.216 I ggml_metal_init: simdgroup reduction   = true
0.00.904.216 I ggml_metal_init: simdgroup matrix mul. = true
0.00.904.217 I ggml_metal_init: has residency sets    = true
0.00.904.217 I ggml_metal_init: has bfloat            = true
0.00.904.217 I ggml_metal_init: use bfloat            = true
0.00.904.218 I ggml_metal_init: hasUnifiedMemory      = true
0.00.904.225 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.919.191 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.922.542 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.922.546 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.922.572 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.925.996 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.925.998 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.925.998 I llama_init_from_model: graph nodes  = 967
0.00.925.999 I llama_init_from_model: graph splits = 2
0.00.926.001 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.926.002 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.954.356 I 
0.00.954.443 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.954.452 I perplexity: tokenizing the input ..
0.00.961.831 I perplexity: tokenization took 7.376 ms
0.00.961.840 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.098.233 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.099.570 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.099.591 I llama_perf_context_print:        load time =     944.09 ms
0.01.099.592 I llama_perf_context_print: prompt eval time =     135.50 ms /   128 tokens (    1.06 ms per token,   944.66 tokens per second)
0.01.099.593 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.099.593 I llama_perf_context_print:       total time =     145.24 ms /   129 tokens
0.01.099.964 I ggml_metal_free: deallocating

real	0m1.116s
user	0m0.077s
sys	0m0.174s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4864 (6ef79a67) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.091 I main: llama backend init
0.00.000.093 I main: load the model and apply lora adapter, if any
0.00.011.627 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.423 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.019.428 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.431 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.431 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.431 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.432 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.432 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.433 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.435 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.436 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.436 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.436 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.437 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.437 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.439 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.440 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.440 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.377 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.422 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.211 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.028.212 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.213 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.213 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.213 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.214 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.028.214 I llama_model_loader: - type  f32:  194 tensors
0.00.028.215 I llama_model_loader: - type q4_0:   97 tensors
0.00.028.215 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.216 I print_info: file format = GGUF V3 (latest)
0.00.028.216 I print_info: file type   = Q4_0
0.00.028.217 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.036.625 I load: special tokens cache size = 25
0.00.042.874 I load: token to piece cache size = 0.2984 MB
0.00.042.889 I print_info: arch             = gptneox
0.00.042.890 I print_info: vocab_only       = 0
0.00.042.890 I print_info: n_ctx_train      = 2048
0.00.042.891 I print_info: n_embd           = 2048
0.00.042.891 I print_info: n_layer          = 24
0.00.042.895 I print_info: n_head           = 16
0.00.042.896 I print_info: n_head_kv        = 16
0.00.042.896 I print_info: n_rot            = 32
0.00.042.898 I print_info: n_swa            = 0
0.00.042.899 I print_info: n_embd_head_k    = 128
0.00.042.899 I print_info: n_embd_head_v    = 128
0.00.042.900 I print_info: n_gqa            = 1
0.00.042.900 I print_info: n_embd_k_gqa     = 2048
0.00.042.901 I print_info: n_embd_v_gqa     = 2048
0.00.042.901 I print_info: f_norm_eps       = 1.0e-05
0.00.042.902 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.902 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.902 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.902 I print_info: f_logit_scale    = 0.0e+00
0.00.042.903 I print_info: n_ff             = 8192
0.00.042.903 I print_info: n_expert         = 0
0.00.042.903 I print_info: n_expert_used    = 0
0.00.042.903 I print_info: causal attn      = 1
0.00.042.903 I print_info: pooling type     = 0
0.00.042.904 I print_info: rope type        = 2
0.00.042.904 I print_info: rope scaling     = linear
0.00.042.904 I print_info: freq_base_train  = 10000.0
0.00.042.904 I print_info: freq_scale_train = 1
0.00.042.905 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.905 I print_info: rope_finetuned   = unknown
0.00.042.905 I print_info: ssm_d_conv       = 0
0.00.042.905 I print_info: ssm_d_inner      = 0
0.00.042.905 I print_info: ssm_d_state      = 0
0.00.042.905 I print_info: ssm_dt_rank      = 0
0.00.042.905 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.906 I print_info: model type       = 1.4B
0.00.042.906 I print_info: model params     = 1.41 B
0.00.042.906 I print_info: general.name     = 1.4B
0.00.042.907 I print_info: vocab type       = BPE
0.00.042.907 I print_info: n_vocab          = 50304
0.00.042.907 I print_info: n_merges         = 50009
0.00.042.907 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.908 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.908 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.908 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.909 I print_info: LF token         = 187 ''
0.00.042.909 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.909 I print_info: max token length = 1024
0.00.042.910 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.588.286 I load_tensors: offloading 24 repeating layers to GPU
0.00.588.305 I load_tensors: offloading output layer to GPU
0.00.588.306 I load_tensors: offloaded 25/25 layers to GPU
0.00.588.340 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.588.341 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.590.127 I llama_init_from_model: n_seq_max     = 1
0.00.590.130 I llama_init_from_model: n_ctx         = 2048
0.00.590.131 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.590.131 I llama_init_from_model: n_batch       = 2048
0.00.590.132 I llama_init_from_model: n_ubatch      = 512
0.00.590.132 I llama_init_from_model: flash_attn    = 0
0.00.590.134 I llama_init_from_model: freq_base     = 10000.0
0.00.590.135 I llama_init_from_model: freq_scale    = 1
0.00.590.145 I ggml_metal_init: allocating
0.00.590.215 I ggml_metal_init: found device: Apple M4
0.00.590.228 I ggml_metal_init: picking default device: Apple M4
0.00.591.787 I ggml_metal_init: using embedded metal library
0.00.598.496 I ggml_metal_init: GPU name:   Apple M4
0.00.598.500 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.598.501 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.598.502 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.598.502 I ggml_metal_init: simdgroup reduction   = true
0.00.598.503 I ggml_metal_init: simdgroup matrix mul. = true
0.00.598.503 I ggml_metal_init: has residency sets    = true
0.00.598.503 I ggml_metal_init: has bfloat            = true
0.00.598.504 I ggml_metal_init: use bfloat            = true
0.00.598.504 I ggml_metal_init: hasUnifiedMemory      = true
0.00.598.508 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.617.356 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.673.489 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.673.496 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.673.518 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.678.006 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.678.008 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.678.008 I llama_init_from_model: graph nodes  = 967
0.00.678.008 I llama_init_from_model: graph splits = 2
0.00.678.013 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.678.144 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.678.144 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.733.241 I main: llama threadpool init, n_threads = 4
0.00.733.293 I 
0.00.733.315 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.733.319 I 
0.00.733.471 I sampler seed: 1234
0.00.733.476 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.733.490 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.733.491 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.733.491 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.420.935 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50318.92 tokens per second)
0.01.420.935 I llama_perf_context_print:        load time =     720.88 ms
0.01.420.936 I llama_perf_context_print: prompt eval time =      49.02 ms /     7 tokens (    7.00 ms per token,   142.79 tokens per second)
0.01.420.937 I llama_perf_context_print:        eval time =     635.56 ms /    63 runs   (   10.09 ms per token,    99.12 tokens per second)
0.01.420.937 I llama_perf_context_print:       total time =     688.42 ms /    70 tokens
0.01.421.194 I ggml_metal_free: deallocating

real	0m1.441s
user	0m0.113s
sys	0m0.200s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.241 I build: 4864 (6ef79a67) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.035 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.530 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.537 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.543 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.544 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.544 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.545 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.545 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.546 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.546 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.547 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.547 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.547 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.547 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.548 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.550 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.550 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.550 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.371 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.380 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.129 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.130 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.130 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.131 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.131 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.132 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.132 I llama_model_loader: - type  f32:  194 tensors
0.00.026.133 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.133 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.134 I print_info: file format = GGUF V3 (latest)
0.00.026.134 I print_info: file type   = Q4_0
0.00.026.135 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.781 I load: special tokens cache size = 25
0.00.041.323 I load: token to piece cache size = 0.2984 MB
0.00.041.341 I print_info: arch             = gptneox
0.00.041.342 I print_info: vocab_only       = 0
0.00.041.342 I print_info: n_ctx_train      = 2048
0.00.041.342 I print_info: n_embd           = 2048
0.00.041.342 I print_info: n_layer          = 24
0.00.041.346 I print_info: n_head           = 16
0.00.041.347 I print_info: n_head_kv        = 16
0.00.041.347 I print_info: n_rot            = 32
0.00.041.347 I print_info: n_swa            = 0
0.00.041.347 I print_info: n_embd_head_k    = 128
0.00.041.347 I print_info: n_embd_head_v    = 128
0.00.041.348 I print_info: n_gqa            = 1
0.00.041.348 I print_info: n_embd_k_gqa     = 2048
0.00.041.349 I print_info: n_embd_v_gqa     = 2048
0.00.041.349 I print_info: f_norm_eps       = 1.0e-05
0.00.041.350 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.350 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.350 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.350 I print_info: f_logit_scale    = 0.0e+00
0.00.041.351 I print_info: n_ff             = 8192
0.00.041.351 I print_info: n_expert         = 0
0.00.041.351 I print_info: n_expert_used    = 0
0.00.041.351 I print_info: causal attn      = 1
0.00.041.352 I print_info: pooling type     = 0
0.00.041.352 I print_info: rope type        = 2
0.00.041.352 I print_info: rope scaling     = linear
0.00.041.352 I print_info: freq_base_train  = 10000.0
0.00.041.353 I print_info: freq_scale_train = 1
0.00.041.353 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.353 I print_info: rope_finetuned   = unknown
0.00.041.353 I print_info: ssm_d_conv       = 0
0.00.041.353 I print_info: ssm_d_inner      = 0
0.00.041.353 I print_info: ssm_d_state      = 0
0.00.041.353 I print_info: ssm_dt_rank      = 0
0.00.041.354 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.354 I print_info: model type       = 1.4B
0.00.041.354 I print_info: model params     = 1.41 B
0.00.041.354 I print_info: general.name     = 1.4B
0.00.041.355 I print_info: vocab type       = BPE
0.00.041.355 I print_info: n_vocab          = 50304
0.00.041.355 I print_info: n_merges         = 50009
0.00.041.355 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.355 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.355 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.356 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.356 I print_info: LF token         = 187 ''
0.00.041.356 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.356 I print_info: max token length = 1024
0.00.041.357 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.580.159 I load_tensors: offloading 24 repeating layers to GPU
0.00.580.174 I load_tensors: offloading output layer to GPU
0.00.580.174 I load_tensors: offloaded 25/25 layers to GPU
0.00.580.208 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.580.210 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.581.849 I llama_init_from_model: n_seq_max     = 1
0.00.581.851 I llama_init_from_model: n_ctx         = 128
0.00.581.852 I llama_init_from_model: n_ctx_per_seq = 128
0.00.581.853 I llama_init_from_model: n_batch       = 128
0.00.581.853 I llama_init_from_model: n_ubatch      = 128
0.00.581.853 I llama_init_from_model: flash_attn    = 0
0.00.581.855 I llama_init_from_model: freq_base     = 10000.0
0.00.581.856 I llama_init_from_model: freq_scale    = 1
0.00.581.856 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.581.859 I ggml_metal_init: allocating
0.00.581.955 I ggml_metal_init: found device: Apple M4
0.00.581.969 I ggml_metal_init: picking default device: Apple M4
0.00.583.552 I ggml_metal_init: using embedded metal library
0.00.590.420 I ggml_metal_init: GPU name:   Apple M4
0.00.590.428 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.590.429 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.590.430 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.590.430 I ggml_metal_init: simdgroup reduction   = true
0.00.590.431 I ggml_metal_init: simdgroup matrix mul. = true
0.00.590.431 I ggml_metal_init: has residency sets    = true
0.00.590.431 I ggml_metal_init: has bfloat            = true
0.00.590.432 I ggml_metal_init: use bfloat            = true
0.00.590.433 I ggml_metal_init: hasUnifiedMemory      = true
0.00.590.437 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.608.615 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.612.319 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.612.325 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.612.355 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.615.436 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.615.438 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.615.438 I llama_init_from_model: graph nodes  = 967
0.00.615.439 I llama_init_from_model: graph splits = 2
0.00.615.442 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.615.442 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.641.393 I 
0.00.641.487 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.641.494 I perplexity: tokenizing the input ..
0.00.648.870 I perplexity: tokenization took 7.374 ms
0.00.648.878 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.784.470 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.785.884 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.785.908 I llama_perf_context_print:        load time =     631.35 ms
0.00.785.908 I llama_perf_context_print: prompt eval time =     134.64 ms /   128 tokens (    1.05 ms per token,   950.66 tokens per second)
0.00.785.909 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.785.909 I llama_perf_context_print:       total time =     144.52 ms /   129 tokens
0.00.786.303 I ggml_metal_free: deallocating

real	0m0.802s
user	0m0.081s
sys	0m0.123s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4864 (6ef79a67) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.067 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.302 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.307 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.309 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.314 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.314 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.316 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.316 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.317 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.318 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.318 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.318 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.322 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.323 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.323 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.325 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.325 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.325 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.161 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.145 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.901 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.902 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.902 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.903 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.903 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.903 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.904 I llama_model_loader: - type  f32:  194 tensors
0.00.025.904 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.904 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.905 I print_info: file format = GGUF V3 (latest)
0.00.025.905 I print_info: file type   = Q4_1
0.00.025.906 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.034.083 I load: special tokens cache size = 25
0.00.040.298 I load: token to piece cache size = 0.2984 MB
0.00.040.312 I print_info: arch             = gptneox
0.00.040.313 I print_info: vocab_only       = 0
0.00.040.313 I print_info: n_ctx_train      = 2048
0.00.040.314 I print_info: n_embd           = 2048
0.00.040.314 I print_info: n_layer          = 24
0.00.040.316 I print_info: n_head           = 16
0.00.040.317 I print_info: n_head_kv        = 16
0.00.040.318 I print_info: n_rot            = 32
0.00.040.318 I print_info: n_swa            = 0
0.00.040.318 I print_info: n_embd_head_k    = 128
0.00.040.320 I print_info: n_embd_head_v    = 128
0.00.040.321 I print_info: n_gqa            = 1
0.00.040.322 I print_info: n_embd_k_gqa     = 2048
0.00.040.322 I print_info: n_embd_v_gqa     = 2048
0.00.040.323 I print_info: f_norm_eps       = 1.0e-05
0.00.040.323 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.323 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.324 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.324 I print_info: f_logit_scale    = 0.0e+00
0.00.040.326 I print_info: n_ff             = 8192
0.00.040.326 I print_info: n_expert         = 0
0.00.040.326 I print_info: n_expert_used    = 0
0.00.040.326 I print_info: causal attn      = 1
0.00.040.326 I print_info: pooling type     = 0
0.00.040.328 I print_info: rope type        = 2
0.00.040.329 I print_info: rope scaling     = linear
0.00.040.329 I print_info: freq_base_train  = 10000.0
0.00.040.330 I print_info: freq_scale_train = 1
0.00.040.330 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.330 I print_info: rope_finetuned   = unknown
0.00.040.330 I print_info: ssm_d_conv       = 0
0.00.040.330 I print_info: ssm_d_inner      = 0
0.00.040.330 I print_info: ssm_d_state      = 0
0.00.040.330 I print_info: ssm_dt_rank      = 0
0.00.040.331 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.331 I print_info: model type       = 1.4B
0.00.040.331 I print_info: model params     = 1.41 B
0.00.040.331 I print_info: general.name     = 1.4B
0.00.040.333 I print_info: vocab type       = BPE
0.00.040.333 I print_info: n_vocab          = 50304
0.00.040.333 I print_info: n_merges         = 50009
0.00.040.333 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.333 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.333 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.333 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.334 I print_info: LF token         = 187 ''
0.00.040.334 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.334 I print_info: max token length = 1024
0.00.040.334 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.628.834 I load_tensors: offloading 24 repeating layers to GPU
0.00.628.851 I load_tensors: offloading output layer to GPU
0.00.628.852 I load_tensors: offloaded 25/25 layers to GPU
0.00.628.885 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.628.886 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.630.393 I llama_init_from_model: n_seq_max     = 1
0.00.630.396 I llama_init_from_model: n_ctx         = 2048
0.00.630.396 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.630.397 I llama_init_from_model: n_batch       = 2048
0.00.630.398 I llama_init_from_model: n_ubatch      = 512
0.00.630.398 I llama_init_from_model: flash_attn    = 0
0.00.630.400 I llama_init_from_model: freq_base     = 10000.0
0.00.630.401 I llama_init_from_model: freq_scale    = 1
0.00.630.406 I ggml_metal_init: allocating
0.00.630.484 I ggml_metal_init: found device: Apple M4
0.00.630.497 I ggml_metal_init: picking default device: Apple M4
0.00.632.043 I ggml_metal_init: using embedded metal library
0.00.637.586 I ggml_metal_init: GPU name:   Apple M4
0.00.637.590 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.637.591 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.637.592 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.637.593 I ggml_metal_init: simdgroup reduction   = true
0.00.637.593 I ggml_metal_init: simdgroup matrix mul. = true
0.00.637.593 I ggml_metal_init: has residency sets    = true
0.00.637.594 I ggml_metal_init: has bfloat            = true
0.00.637.594 I ggml_metal_init: use bfloat            = true
0.00.637.595 I ggml_metal_init: hasUnifiedMemory      = true
0.00.637.596 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.657.614 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.715.788 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.715.795 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.715.819 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.719.925 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.719.927 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.719.928 I llama_init_from_model: graph nodes  = 967
0.00.719.928 I llama_init_from_model: graph splits = 2
0.00.719.934 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.720.055 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.720.056 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.776.796 I main: llama threadpool init, n_threads = 4
0.00.776.852 I 
0.00.776.873 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.776.873 I 
0.00.777.025 I sampler seed: 1234
0.00.777.032 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.777.047 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.777.048 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.777.048 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.502.190 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53383.46 tokens per second)
0.01.502.191 I llama_perf_context_print:        load time =     767.00 ms
0.01.502.192 I llama_perf_context_print: prompt eval time =      48.85 ms /     7 tokens (    6.98 ms per token,   143.30 tokens per second)
0.01.502.192 I llama_perf_context_print:        eval time =     673.42 ms /    63 runs   (   10.69 ms per token,    93.55 tokens per second)
0.01.502.193 I llama_perf_context_print:       total time =     726.12 ms /    70 tokens
0.01.502.406 I ggml_metal_free: deallocating

real	0m1.519s
user	0m0.110s
sys	0m0.199s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4864 (6ef79a67) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.036 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.885 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.890 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.892 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.898 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.899 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.899 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.899 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.900 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.901 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.901 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.901 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.902 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.902 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.902 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.904 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.905 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.905 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.728 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.751 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.582 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.584 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.584 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.584 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.584 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.585 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.585 I llama_model_loader: - type  f32:  194 tensors
0.00.024.586 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.586 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.587 I print_info: file format = GGUF V3 (latest)
0.00.024.587 I print_info: file type   = Q4_1
0.00.024.588 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.697 I load: special tokens cache size = 25
0.00.039.079 I load: token to piece cache size = 0.2984 MB
0.00.039.096 I print_info: arch             = gptneox
0.00.039.097 I print_info: vocab_only       = 0
0.00.039.098 I print_info: n_ctx_train      = 2048
0.00.039.098 I print_info: n_embd           = 2048
0.00.039.098 I print_info: n_layer          = 24
0.00.039.102 I print_info: n_head           = 16
0.00.039.103 I print_info: n_head_kv        = 16
0.00.039.103 I print_info: n_rot            = 32
0.00.039.103 I print_info: n_swa            = 0
0.00.039.103 I print_info: n_embd_head_k    = 128
0.00.039.103 I print_info: n_embd_head_v    = 128
0.00.039.104 I print_info: n_gqa            = 1
0.00.039.105 I print_info: n_embd_k_gqa     = 2048
0.00.039.105 I print_info: n_embd_v_gqa     = 2048
0.00.039.106 I print_info: f_norm_eps       = 1.0e-05
0.00.039.106 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.106 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.106 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.108 I print_info: f_logit_scale    = 0.0e+00
0.00.039.108 I print_info: n_ff             = 8192
0.00.039.109 I print_info: n_expert         = 0
0.00.039.109 I print_info: n_expert_used    = 0
0.00.039.109 I print_info: causal attn      = 1
0.00.039.109 I print_info: pooling type     = 0
0.00.039.109 I print_info: rope type        = 2
0.00.039.109 I print_info: rope scaling     = linear
0.00.039.110 I print_info: freq_base_train  = 10000.0
0.00.039.110 I print_info: freq_scale_train = 1
0.00.039.110 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.110 I print_info: rope_finetuned   = unknown
0.00.039.110 I print_info: ssm_d_conv       = 0
0.00.039.111 I print_info: ssm_d_inner      = 0
0.00.039.111 I print_info: ssm_d_state      = 0
0.00.039.111 I print_info: ssm_dt_rank      = 0
0.00.039.111 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.111 I print_info: model type       = 1.4B
0.00.039.111 I print_info: model params     = 1.41 B
0.00.039.112 I print_info: general.name     = 1.4B
0.00.039.112 I print_info: vocab type       = BPE
0.00.039.112 I print_info: n_vocab          = 50304
0.00.039.112 I print_info: n_merges         = 50009
0.00.039.113 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.113 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.113 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.113 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.113 I print_info: LF token         = 187 ''
0.00.039.113 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.114 I print_info: max token length = 1024
0.00.039.114 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.634.275 I load_tensors: offloading 24 repeating layers to GPU
0.00.634.289 I load_tensors: offloading output layer to GPU
0.00.634.290 I load_tensors: offloaded 25/25 layers to GPU
0.00.634.323 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.634.324 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.635.981 I llama_init_from_model: n_seq_max     = 1
0.00.635.984 I llama_init_from_model: n_ctx         = 128
0.00.635.984 I llama_init_from_model: n_ctx_per_seq = 128
0.00.635.985 I llama_init_from_model: n_batch       = 128
0.00.635.985 I llama_init_from_model: n_ubatch      = 128
0.00.635.985 I llama_init_from_model: flash_attn    = 0
0.00.635.988 I llama_init_from_model: freq_base     = 10000.0
0.00.635.988 I llama_init_from_model: freq_scale    = 1
0.00.635.989 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.635.991 I ggml_metal_init: allocating
0.00.636.050 I ggml_metal_init: found device: Apple M4
0.00.636.063 I ggml_metal_init: picking default device: Apple M4
0.00.637.560 I ggml_metal_init: using embedded metal library
0.00.643.900 I ggml_metal_init: GPU name:   Apple M4
0.00.643.908 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.643.910 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.643.910 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.643.911 I ggml_metal_init: simdgroup reduction   = true
0.00.643.911 I ggml_metal_init: simdgroup matrix mul. = true
0.00.643.912 I ggml_metal_init: has residency sets    = true
0.00.643.912 I ggml_metal_init: has bfloat            = true
0.00.643.912 I ggml_metal_init: use bfloat            = true
0.00.643.914 I ggml_metal_init: hasUnifiedMemory      = true
0.00.643.926 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.662.850 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.666.269 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.666.276 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.666.309 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.669.458 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.669.460 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.669.460 I llama_init_from_model: graph nodes  = 967
0.00.669.460 I llama_init_from_model: graph splits = 2
0.00.669.464 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.669.464 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.699.586 I 
0.00.699.681 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.699.691 I perplexity: tokenizing the input ..
0.00.706.856 I perplexity: tokenization took 7.165 ms
0.00.706.863 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.841.062 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.842.390 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.842.416 I llama_perf_context_print:        load time =     690.54 ms
0.00.842.417 I llama_perf_context_print: prompt eval time =     133.97 ms /   128 tokens (    1.05 ms per token,   955.44 tokens per second)
0.00.842.417 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.842.418 I llama_perf_context_print:       total time =     142.83 ms /   129 tokens
0.00.842.806 I ggml_metal_free: deallocating

real	0m0.857s
user	0m0.079s
sys	0m0.129s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4864 (6ef79a67) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.010.096 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.611 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.615 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.617 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.618 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.618 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.620 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.621 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.622 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.623 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.623 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.624 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.624 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.624 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.625 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.628 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.629 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.630 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.374 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.375 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.090 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.091 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.092 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.092 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.092 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.092 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.093 I llama_model_loader: - type  f32:  194 tensors
0.00.026.093 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.094 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.094 I print_info: file format = GGUF V3 (latest)
0.00.026.095 I print_info: file type   = Q5_0
0.00.026.096 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.972 I load: special tokens cache size = 25
0.00.040.388 I load: token to piece cache size = 0.2984 MB
0.00.040.402 I print_info: arch             = gptneox
0.00.040.403 I print_info: vocab_only       = 0
0.00.040.404 I print_info: n_ctx_train      = 2048
0.00.040.404 I print_info: n_embd           = 2048
0.00.040.404 I print_info: n_layer          = 24
0.00.040.407 I print_info: n_head           = 16
0.00.040.408 I print_info: n_head_kv        = 16
0.00.040.408 I print_info: n_rot            = 32
0.00.040.408 I print_info: n_swa            = 0
0.00.040.408 I print_info: n_embd_head_k    = 128
0.00.040.409 I print_info: n_embd_head_v    = 128
0.00.040.409 I print_info: n_gqa            = 1
0.00.040.410 I print_info: n_embd_k_gqa     = 2048
0.00.040.411 I print_info: n_embd_v_gqa     = 2048
0.00.040.411 I print_info: f_norm_eps       = 1.0e-05
0.00.040.412 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.412 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.412 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.412 I print_info: f_logit_scale    = 0.0e+00
0.00.040.413 I print_info: n_ff             = 8192
0.00.040.415 I print_info: n_expert         = 0
0.00.040.415 I print_info: n_expert_used    = 0
0.00.040.415 I print_info: causal attn      = 1
0.00.040.415 I print_info: pooling type     = 0
0.00.040.416 I print_info: rope type        = 2
0.00.040.417 I print_info: rope scaling     = linear
0.00.040.417 I print_info: freq_base_train  = 10000.0
0.00.040.420 I print_info: freq_scale_train = 1
0.00.040.421 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.421 I print_info: rope_finetuned   = unknown
0.00.040.421 I print_info: ssm_d_conv       = 0
0.00.040.422 I print_info: ssm_d_inner      = 0
0.00.040.423 I print_info: ssm_d_state      = 0
0.00.040.423 I print_info: ssm_dt_rank      = 0
0.00.040.423 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.423 I print_info: model type       = 1.4B
0.00.040.423 I print_info: model params     = 1.41 B
0.00.040.424 I print_info: general.name     = 1.4B
0.00.040.424 I print_info: vocab type       = BPE
0.00.040.424 I print_info: n_vocab          = 50304
0.00.040.425 I print_info: n_merges         = 50009
0.00.040.425 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.425 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.425 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.425 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.425 I print_info: LF token         = 187 ''
0.00.040.426 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.426 I print_info: max token length = 1024
0.00.040.426 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.695.646 I load_tensors: offloading 24 repeating layers to GPU
0.00.695.662 I load_tensors: offloading output layer to GPU
0.00.695.663 I load_tensors: offloaded 25/25 layers to GPU
0.00.695.698 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.695.699 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.697.412 I llama_init_from_model: n_seq_max     = 1
0.00.697.414 I llama_init_from_model: n_ctx         = 2048
0.00.697.415 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.697.415 I llama_init_from_model: n_batch       = 2048
0.00.697.416 I llama_init_from_model: n_ubatch      = 512
0.00.697.416 I llama_init_from_model: flash_attn    = 0
0.00.697.419 I llama_init_from_model: freq_base     = 10000.0
0.00.697.419 I llama_init_from_model: freq_scale    = 1
0.00.697.422 I ggml_metal_init: allocating
0.00.697.509 I ggml_metal_init: found device: Apple M4
0.00.697.522 I ggml_metal_init: picking default device: Apple M4
0.00.699.145 I ggml_metal_init: using embedded metal library
0.00.705.935 I ggml_metal_init: GPU name:   Apple M4
0.00.705.940 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.705.940 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.705.942 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.705.942 I ggml_metal_init: simdgroup reduction   = true
0.00.705.942 I ggml_metal_init: simdgroup matrix mul. = true
0.00.705.943 I ggml_metal_init: has residency sets    = true
0.00.705.943 I ggml_metal_init: has bfloat            = true
0.00.705.943 I ggml_metal_init: use bfloat            = true
0.00.705.944 I ggml_metal_init: hasUnifiedMemory      = true
0.00.705.945 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.724.134 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.778.600 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.778.607 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.778.630 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.783.129 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.783.131 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.783.132 I llama_init_from_model: graph nodes  = 967
0.00.783.132 I llama_init_from_model: graph splits = 2
0.00.783.138 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.783.266 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.783.267 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.841.467 I main: llama threadpool init, n_threads = 4
0.00.841.517 I 
0.00.841.538 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.841.540 I 
0.00.841.702 I sampler seed: 1234
0.00.841.707 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.841.722 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.841.724 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.841.724 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.630.201 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52359.88 tokens per second)
0.01.630.202 I llama_perf_context_print:        load time =     830.64 ms
0.01.630.202 I llama_perf_context_print: prompt eval time =      52.89 ms /     7 tokens (    7.56 ms per token,   132.35 tokens per second)
0.01.630.203 I llama_perf_context_print:        eval time =     732.69 ms /    63 runs   (   11.63 ms per token,    85.98 tokens per second)
0.01.630.203 I llama_perf_context_print:       total time =     789.46 ms /    70 tokens
0.01.630.445 I ggml_metal_free: deallocating

real	0m1.650s
user	0m0.109s
sys	0m0.224s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4864 (6ef79a67) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.365 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.763 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.768 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.770 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.774 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.775 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.775 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.775 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.776 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.778 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.778 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.779 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.779 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.779 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.780 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.782 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.782 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.782 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.489 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.480 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.190 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.192 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.192 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.192 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.193 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.193 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.193 I llama_model_loader: - type  f32:  194 tensors
0.00.026.194 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.194 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.195 I print_info: file format = GGUF V3 (latest)
0.00.026.195 I print_info: file type   = Q5_0
0.00.026.197 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.583 I load: special tokens cache size = 25
0.00.041.034 I load: token to piece cache size = 0.2984 MB
0.00.041.052 I print_info: arch             = gptneox
0.00.041.053 I print_info: vocab_only       = 0
0.00.041.053 I print_info: n_ctx_train      = 2048
0.00.041.053 I print_info: n_embd           = 2048
0.00.041.053 I print_info: n_layer          = 24
0.00.041.058 I print_info: n_head           = 16
0.00.041.059 I print_info: n_head_kv        = 16
0.00.041.059 I print_info: n_rot            = 32
0.00.041.059 I print_info: n_swa            = 0
0.00.041.059 I print_info: n_embd_head_k    = 128
0.00.041.059 I print_info: n_embd_head_v    = 128
0.00.041.060 I print_info: n_gqa            = 1
0.00.041.060 I print_info: n_embd_k_gqa     = 2048
0.00.041.061 I print_info: n_embd_v_gqa     = 2048
0.00.041.062 I print_info: f_norm_eps       = 1.0e-05
0.00.041.062 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.062 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.062 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.062 I print_info: f_logit_scale    = 0.0e+00
0.00.041.063 I print_info: n_ff             = 8192
0.00.041.063 I print_info: n_expert         = 0
0.00.041.063 I print_info: n_expert_used    = 0
0.00.041.063 I print_info: causal attn      = 1
0.00.041.066 I print_info: pooling type     = 0
0.00.041.066 I print_info: rope type        = 2
0.00.041.067 I print_info: rope scaling     = linear
0.00.041.067 I print_info: freq_base_train  = 10000.0
0.00.041.067 I print_info: freq_scale_train = 1
0.00.041.067 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.068 I print_info: rope_finetuned   = unknown
0.00.041.068 I print_info: ssm_d_conv       = 0
0.00.041.068 I print_info: ssm_d_inner      = 0
0.00.041.068 I print_info: ssm_d_state      = 0
0.00.041.068 I print_info: ssm_dt_rank      = 0
0.00.041.068 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.068 I print_info: model type       = 1.4B
0.00.041.069 I print_info: model params     = 1.41 B
0.00.041.070 I print_info: general.name     = 1.4B
0.00.041.070 I print_info: vocab type       = BPE
0.00.041.071 I print_info: n_vocab          = 50304
0.00.041.071 I print_info: n_merges         = 50009
0.00.041.071 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.071 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.071 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.072 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.072 I print_info: LF token         = 187 ''
0.00.041.072 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.072 I print_info: max token length = 1024
0.00.041.073 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.662.793 I load_tensors: offloading 24 repeating layers to GPU
0.00.662.809 I load_tensors: offloading output layer to GPU
0.00.662.810 I load_tensors: offloaded 25/25 layers to GPU
0.00.662.846 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.662.848 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.664.545 I llama_init_from_model: n_seq_max     = 1
0.00.664.548 I llama_init_from_model: n_ctx         = 128
0.00.664.549 I llama_init_from_model: n_ctx_per_seq = 128
0.00.664.549 I llama_init_from_model: n_batch       = 128
0.00.664.549 I llama_init_from_model: n_ubatch      = 128
0.00.664.550 I llama_init_from_model: flash_attn    = 0
0.00.664.553 I llama_init_from_model: freq_base     = 10000.0
0.00.664.553 I llama_init_from_model: freq_scale    = 1
0.00.664.554 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.664.556 I ggml_metal_init: allocating
0.00.664.686 I ggml_metal_init: found device: Apple M4
0.00.664.701 I ggml_metal_init: picking default device: Apple M4
0.00.666.326 I ggml_metal_init: using embedded metal library
0.00.673.170 I ggml_metal_init: GPU name:   Apple M4
0.00.673.178 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.673.179 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.673.180 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.673.181 I ggml_metal_init: simdgroup reduction   = true
0.00.673.181 I ggml_metal_init: simdgroup matrix mul. = true
0.00.673.181 I ggml_metal_init: has residency sets    = true
0.00.673.181 I ggml_metal_init: has bfloat            = true
0.00.673.182 I ggml_metal_init: use bfloat            = true
0.00.673.183 I ggml_metal_init: hasUnifiedMemory      = true
0.00.673.186 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.692.033 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.695.616 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.695.620 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.695.678 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.698.701 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.698.702 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.698.703 I llama_init_from_model: graph nodes  = 967
0.00.698.703 I llama_init_from_model: graph splits = 2
0.00.698.706 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.698.707 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.729.472 I 
0.00.729.561 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.729.569 I perplexity: tokenizing the input ..
0.00.736.572 I perplexity: tokenization took 6.999 ms
0.00.736.579 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.881.394 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.882.708 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.882.733 I llama_perf_context_print:        load time =     719.10 ms
0.00.882.734 I llama_perf_context_print: prompt eval time =     143.92 ms /   128 tokens (    1.12 ms per token,   889.39 tokens per second)
0.00.882.735 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.882.736 I llama_perf_context_print:       total time =     153.27 ms /   129 tokens
0.00.883.161 I ggml_metal_free: deallocating

real	0m0.899s
user	0m0.081s
sys	0m0.130s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4864 (6ef79a67) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.018 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.402 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.407 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.408 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.409 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.409 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.413 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.413 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.416 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.416 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.417 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.417 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.417 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.421 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.421 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.425 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.426 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.427 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.182 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.197 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.866 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.867 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.868 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.868 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.868 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.868 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.869 I llama_model_loader: - type  f32:  194 tensors
0.00.024.869 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.870 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.870 I print_info: file format = GGUF V3 (latest)
0.00.024.871 I print_info: file type   = Q5_1
0.00.024.872 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.745 I load: special tokens cache size = 25
0.00.039.173 I load: token to piece cache size = 0.2984 MB
0.00.039.187 I print_info: arch             = gptneox
0.00.039.188 I print_info: vocab_only       = 0
0.00.039.188 I print_info: n_ctx_train      = 2048
0.00.039.189 I print_info: n_embd           = 2048
0.00.039.189 I print_info: n_layer          = 24
0.00.039.192 I print_info: n_head           = 16
0.00.039.192 I print_info: n_head_kv        = 16
0.00.039.193 I print_info: n_rot            = 32
0.00.039.193 I print_info: n_swa            = 0
0.00.039.193 I print_info: n_embd_head_k    = 128
0.00.039.193 I print_info: n_embd_head_v    = 128
0.00.039.194 I print_info: n_gqa            = 1
0.00.039.195 I print_info: n_embd_k_gqa     = 2048
0.00.039.195 I print_info: n_embd_v_gqa     = 2048
0.00.039.196 I print_info: f_norm_eps       = 1.0e-05
0.00.039.196 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.200 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.200 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.202 I print_info: f_logit_scale    = 0.0e+00
0.00.039.202 I print_info: n_ff             = 8192
0.00.039.202 I print_info: n_expert         = 0
0.00.039.203 I print_info: n_expert_used    = 0
0.00.039.203 I print_info: causal attn      = 1
0.00.039.203 I print_info: pooling type     = 0
0.00.039.205 I print_info: rope type        = 2
0.00.039.206 I print_info: rope scaling     = linear
0.00.039.207 I print_info: freq_base_train  = 10000.0
0.00.039.207 I print_info: freq_scale_train = 1
0.00.039.207 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.207 I print_info: rope_finetuned   = unknown
0.00.039.207 I print_info: ssm_d_conv       = 0
0.00.039.208 I print_info: ssm_d_inner      = 0
0.00.039.209 I print_info: ssm_d_state      = 0
0.00.039.209 I print_info: ssm_dt_rank      = 0
0.00.039.209 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.209 I print_info: model type       = 1.4B
0.00.039.210 I print_info: model params     = 1.41 B
0.00.039.210 I print_info: general.name     = 1.4B
0.00.039.210 I print_info: vocab type       = BPE
0.00.039.210 I print_info: n_vocab          = 50304
0.00.039.211 I print_info: n_merges         = 50009
0.00.039.212 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.213 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.213 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.213 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.213 I print_info: LF token         = 187 ''
0.00.039.213 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.213 I print_info: max token length = 1024
0.00.039.214 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.610.851 I load_tensors: offloading 24 repeating layers to GPU
0.00.610.855 I load_tensors: offloading output layer to GPU
0.00.610.856 I load_tensors: offloaded 25/25 layers to GPU
0.00.610.879 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.610.881 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.612.477 I llama_init_from_model: n_seq_max     = 1
0.00.612.479 I llama_init_from_model: n_ctx         = 2048
0.00.612.479 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.612.480 I llama_init_from_model: n_batch       = 2048
0.00.612.480 I llama_init_from_model: n_ubatch      = 512
0.00.612.481 I llama_init_from_model: flash_attn    = 0
0.00.612.482 I llama_init_from_model: freq_base     = 10000.0
0.00.612.482 I llama_init_from_model: freq_scale    = 1
0.00.612.483 I ggml_metal_init: allocating
0.00.612.503 I ggml_metal_init: found device: Apple M4
0.00.612.511 I ggml_metal_init: picking default device: Apple M4
0.00.613.783 I ggml_metal_init: using embedded metal library
0.00.620.054 I ggml_metal_init: GPU name:   Apple M4
0.00.620.057 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.620.057 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.620.058 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.620.059 I ggml_metal_init: simdgroup reduction   = true
0.00.620.059 I ggml_metal_init: simdgroup matrix mul. = true
0.00.620.059 I ggml_metal_init: has residency sets    = true
0.00.620.060 I ggml_metal_init: has bfloat            = true
0.00.620.060 I ggml_metal_init: use bfloat            = true
0.00.620.061 I ggml_metal_init: hasUnifiedMemory      = true
0.00.620.062 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.636.874 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.692.787 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.692.794 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.692.817 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.696.966 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.696.968 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.696.968 I llama_init_from_model: graph nodes  = 967
0.00.696.969 I llama_init_from_model: graph splits = 2
0.00.696.975 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.697.099 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.697.100 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.752.349 I main: llama threadpool init, n_threads = 4
0.00.752.392 I 
0.00.752.409 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.752.411 I 
0.00.752.535 I sampler seed: 1234
0.00.752.540 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.752.575 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.752.578 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.752.579 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.603.486 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52398.52 tokens per second)
0.01.603.487 I llama_perf_context_print:        load time =     742.63 ms
0.01.603.487 I llama_perf_context_print: prompt eval time =      52.54 ms /     7 tokens (    7.51 ms per token,   133.23 tokens per second)
0.01.603.489 I llama_perf_context_print:        eval time =     795.45 ms /    63 runs   (   12.63 ms per token,    79.20 tokens per second)
0.01.603.489 I llama_perf_context_print:       total time =     851.83 ms /    70 tokens
0.01.603.742 I ggml_metal_free: deallocating

real	0m1.622s
user	0m0.108s
sys	0m0.213s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4864 (6ef79a67) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.519 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.725 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.731 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.732 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.734 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.738 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.739 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.739 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.740 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.740 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.741 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.741 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.741 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.742 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.742 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.744 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.744 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.744 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.502 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.493 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.312 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.313 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.314 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.314 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.315 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.315 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.315 I llama_model_loader: - type  f32:  194 tensors
0.00.025.316 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.316 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.317 I print_info: file format = GGUF V3 (latest)
0.00.025.317 I print_info: file type   = Q5_1
0.00.025.318 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.405 I load: special tokens cache size = 25
0.00.040.716 I load: token to piece cache size = 0.2984 MB
0.00.040.733 I print_info: arch             = gptneox
0.00.040.734 I print_info: vocab_only       = 0
0.00.040.734 I print_info: n_ctx_train      = 2048
0.00.040.734 I print_info: n_embd           = 2048
0.00.040.734 I print_info: n_layer          = 24
0.00.040.740 I print_info: n_head           = 16
0.00.040.740 I print_info: n_head_kv        = 16
0.00.040.740 I print_info: n_rot            = 32
0.00.040.740 I print_info: n_swa            = 0
0.00.040.741 I print_info: n_embd_head_k    = 128
0.00.040.741 I print_info: n_embd_head_v    = 128
0.00.040.741 I print_info: n_gqa            = 1
0.00.040.742 I print_info: n_embd_k_gqa     = 2048
0.00.040.743 I print_info: n_embd_v_gqa     = 2048
0.00.040.743 I print_info: f_norm_eps       = 1.0e-05
0.00.040.743 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.743 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.744 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.744 I print_info: f_logit_scale    = 0.0e+00
0.00.040.744 I print_info: n_ff             = 8192
0.00.040.747 I print_info: n_expert         = 0
0.00.040.747 I print_info: n_expert_used    = 0
0.00.040.748 I print_info: causal attn      = 1
0.00.040.748 I print_info: pooling type     = 0
0.00.040.748 I print_info: rope type        = 2
0.00.040.748 I print_info: rope scaling     = linear
0.00.040.748 I print_info: freq_base_train  = 10000.0
0.00.040.748 I print_info: freq_scale_train = 1
0.00.040.749 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.749 I print_info: rope_finetuned   = unknown
0.00.040.749 I print_info: ssm_d_conv       = 0
0.00.040.749 I print_info: ssm_d_inner      = 0
0.00.040.749 I print_info: ssm_d_state      = 0
0.00.040.749 I print_info: ssm_dt_rank      = 0
0.00.040.749 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.750 I print_info: model type       = 1.4B
0.00.040.750 I print_info: model params     = 1.41 B
0.00.040.750 I print_info: general.name     = 1.4B
0.00.040.751 I print_info: vocab type       = BPE
0.00.040.751 I print_info: n_vocab          = 50304
0.00.040.751 I print_info: n_merges         = 50009
0.00.040.753 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.753 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.753 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.753 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.753 I print_info: LF token         = 187 ''
0.00.040.754 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.754 I print_info: max token length = 1024
0.00.040.754 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.609.499 I load_tensors: offloading 24 repeating layers to GPU
0.00.609.514 I load_tensors: offloading output layer to GPU
0.00.609.515 I load_tensors: offloaded 25/25 layers to GPU
0.00.609.548 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.609.550 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.611.276 I llama_init_from_model: n_seq_max     = 1
0.00.611.281 I llama_init_from_model: n_ctx         = 128
0.00.611.281 I llama_init_from_model: n_ctx_per_seq = 128
0.00.611.281 I llama_init_from_model: n_batch       = 128
0.00.611.282 I llama_init_from_model: n_ubatch      = 128
0.00.611.282 I llama_init_from_model: flash_attn    = 0
0.00.611.284 I llama_init_from_model: freq_base     = 10000.0
0.00.611.284 I llama_init_from_model: freq_scale    = 1
0.00.611.285 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.611.288 I ggml_metal_init: allocating
0.00.611.350 I ggml_metal_init: found device: Apple M4
0.00.611.364 I ggml_metal_init: picking default device: Apple M4
0.00.612.667 I ggml_metal_init: using embedded metal library
0.00.619.052 I ggml_metal_init: GPU name:   Apple M4
0.00.619.057 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.619.057 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.619.058 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.619.058 I ggml_metal_init: simdgroup reduction   = true
0.00.619.059 I ggml_metal_init: simdgroup matrix mul. = true
0.00.619.059 I ggml_metal_init: has residency sets    = true
0.00.619.059 I ggml_metal_init: has bfloat            = true
0.00.619.059 I ggml_metal_init: use bfloat            = true
0.00.619.061 I ggml_metal_init: hasUnifiedMemory      = true
0.00.619.063 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.636.622 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.640.092 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.640.102 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.640.140 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.643.381 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.643.383 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.643.384 I llama_init_from_model: graph nodes  = 967
0.00.643.384 I llama_init_from_model: graph splits = 2
0.00.643.387 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.643.387 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.670.636 I 
0.00.670.729 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.670.738 I perplexity: tokenizing the input ..
0.00.677.894 I perplexity: tokenization took 7.152 ms
0.00.677.909 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.813.618 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.815.029 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.815.052 I llama_perf_context_print:        load time =     661.11 ms
0.00.815.055 I llama_perf_context_print: prompt eval time =     134.84 ms /   128 tokens (    1.05 ms per token,   949.26 tokens per second)
0.00.815.055 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.815.056 I llama_perf_context_print:       total time =     144.42 ms /   129 tokens
0.00.815.374 I ggml_metal_free: deallocating

real	0m0.829s
user	0m0.080s
sys	0m0.141s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4864 (6ef79a67) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.010.353 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.041 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.045 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.050 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.050 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.051 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.051 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.051 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.052 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.053 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.053 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.054 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.054 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.054 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.056 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.057 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.059 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.059 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.783 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.756 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.494 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.495 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.496 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.496 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.496 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.497 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.497 I llama_model_loader: - type  f32:  194 tensors
0.00.025.497 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.498 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.498 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.498 I print_info: file format = GGUF V3 (latest)
0.00.025.499 I print_info: file type   = Q2_K - Medium
0.00.025.500 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.361 I load: special tokens cache size = 25
0.00.039.606 I load: token to piece cache size = 0.2984 MB
0.00.039.620 I print_info: arch             = gptneox
0.00.039.621 I print_info: vocab_only       = 0
0.00.039.622 I print_info: n_ctx_train      = 2048
0.00.039.622 I print_info: n_embd           = 2048
0.00.039.622 I print_info: n_layer          = 24
0.00.039.625 I print_info: n_head           = 16
0.00.039.626 I print_info: n_head_kv        = 16
0.00.039.626 I print_info: n_rot            = 32
0.00.039.626 I print_info: n_swa            = 0
0.00.039.626 I print_info: n_embd_head_k    = 128
0.00.039.626 I print_info: n_embd_head_v    = 128
0.00.039.627 I print_info: n_gqa            = 1
0.00.039.629 I print_info: n_embd_k_gqa     = 2048
0.00.039.629 I print_info: n_embd_v_gqa     = 2048
0.00.039.630 I print_info: f_norm_eps       = 1.0e-05
0.00.039.630 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.631 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.631 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.631 I print_info: f_logit_scale    = 0.0e+00
0.00.039.631 I print_info: n_ff             = 8192
0.00.039.632 I print_info: n_expert         = 0
0.00.039.632 I print_info: n_expert_used    = 0
0.00.039.632 I print_info: causal attn      = 1
0.00.039.632 I print_info: pooling type     = 0
0.00.039.632 I print_info: rope type        = 2
0.00.039.636 I print_info: rope scaling     = linear
0.00.039.636 I print_info: freq_base_train  = 10000.0
0.00.039.637 I print_info: freq_scale_train = 1
0.00.039.637 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.637 I print_info: rope_finetuned   = unknown
0.00.039.637 I print_info: ssm_d_conv       = 0
0.00.039.637 I print_info: ssm_d_inner      = 0
0.00.039.638 I print_info: ssm_d_state      = 0
0.00.039.638 I print_info: ssm_dt_rank      = 0
0.00.039.638 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.638 I print_info: model type       = 1.4B
0.00.039.638 I print_info: model params     = 1.41 B
0.00.039.639 I print_info: general.name     = 1.4B
0.00.039.639 I print_info: vocab type       = BPE
0.00.039.639 I print_info: n_vocab          = 50304
0.00.039.639 I print_info: n_merges         = 50009
0.00.039.639 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.640 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.640 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.640 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.640 I print_info: LF token         = 187 ''
0.00.039.640 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.641 I print_info: max token length = 1024
0.00.039.641 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.336.794 I load_tensors: offloading 24 repeating layers to GPU
0.00.336.809 I load_tensors: offloading output layer to GPU
0.00.336.810 I load_tensors: offloaded 25/25 layers to GPU
0.00.336.844 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.336.846 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.338.487 I llama_init_from_model: n_seq_max     = 1
0.00.338.490 I llama_init_from_model: n_ctx         = 2048
0.00.338.491 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.338.491 I llama_init_from_model: n_batch       = 2048
0.00.338.492 I llama_init_from_model: n_ubatch      = 512
0.00.338.492 I llama_init_from_model: flash_attn    = 0
0.00.338.494 I llama_init_from_model: freq_base     = 10000.0
0.00.338.495 I llama_init_from_model: freq_scale    = 1
0.00.338.498 I ggml_metal_init: allocating
0.00.338.572 I ggml_metal_init: found device: Apple M4
0.00.338.585 I ggml_metal_init: picking default device: Apple M4
0.00.340.116 I ggml_metal_init: using embedded metal library
0.00.347.498 I ggml_metal_init: GPU name:   Apple M4
0.00.347.514 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.347.515 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.347.516 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.347.516 I ggml_metal_init: simdgroup reduction   = true
0.00.347.517 I ggml_metal_init: simdgroup matrix mul. = true
0.00.347.517 I ggml_metal_init: has residency sets    = true
0.00.347.518 I ggml_metal_init: has bfloat            = true
0.00.347.518 I ggml_metal_init: use bfloat            = true
0.00.347.520 I ggml_metal_init: hasUnifiedMemory      = true
0.00.347.524 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.369.747 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.433.955 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.433.961 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.433.985 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.439.069 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.439.071 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.439.071 I llama_init_from_model: graph nodes  = 967
0.00.439.072 I llama_init_from_model: graph splits = 2
0.00.439.078 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.439.204 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.439.204 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.499.373 I main: llama threadpool init, n_threads = 4
0.00.499.423 I 
0.00.499.444 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.499.444 I 
0.00.499.626 I sampler seed: 1234
0.00.499.630 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.499.646 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.499.646 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.499.646 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.178.898 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55295.95 tokens per second)
0.01.178.899 I llama_perf_context_print:        load time =     488.28 ms
0.01.178.899 I llama_perf_context_print: prompt eval time =      42.51 ms /     7 tokens (    6.07 ms per token,   164.66 tokens per second)
0.01.178.900 I llama_perf_context_print:        eval time =     634.04 ms /    63 runs   (   10.06 ms per token,    99.36 tokens per second)
0.01.178.900 I llama_perf_context_print:       total time =     680.26 ms /    70 tokens
0.01.179.181 I ggml_metal_free: deallocating

real	0m1.199s
user	0m0.113s
sys	0m0.172s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4864 (6ef79a67) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.788 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.955 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.961 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.967 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.968 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.968 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.968 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.969 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.970 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.970 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.970 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.971 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.971 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.972 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.973 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.974 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.974 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.974 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.735 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.754 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.567 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.569 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.569 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.570 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.570 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.570 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.571 I llama_model_loader: - type  f32:  194 tensors
0.00.025.571 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.572 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.572 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.572 I print_info: file format = GGUF V3 (latest)
0.00.025.573 I print_info: file type   = Q2_K - Medium
0.00.025.574 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.678 I load: special tokens cache size = 25
0.00.040.358 I load: token to piece cache size = 0.2984 MB
0.00.040.375 I print_info: arch             = gptneox
0.00.040.376 I print_info: vocab_only       = 0
0.00.040.376 I print_info: n_ctx_train      = 2048
0.00.040.377 I print_info: n_embd           = 2048
0.00.040.377 I print_info: n_layer          = 24
0.00.040.380 I print_info: n_head           = 16
0.00.040.381 I print_info: n_head_kv        = 16
0.00.040.381 I print_info: n_rot            = 32
0.00.040.381 I print_info: n_swa            = 0
0.00.040.381 I print_info: n_embd_head_k    = 128
0.00.040.381 I print_info: n_embd_head_v    = 128
0.00.040.382 I print_info: n_gqa            = 1
0.00.040.382 I print_info: n_embd_k_gqa     = 2048
0.00.040.383 I print_info: n_embd_v_gqa     = 2048
0.00.040.383 I print_info: f_norm_eps       = 1.0e-05
0.00.040.384 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.384 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.384 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.388 I print_info: f_logit_scale    = 0.0e+00
0.00.040.388 I print_info: n_ff             = 8192
0.00.040.388 I print_info: n_expert         = 0
0.00.040.388 I print_info: n_expert_used    = 0
0.00.040.388 I print_info: causal attn      = 1
0.00.040.389 I print_info: pooling type     = 0
0.00.040.389 I print_info: rope type        = 2
0.00.040.389 I print_info: rope scaling     = linear
0.00.040.389 I print_info: freq_base_train  = 10000.0
0.00.040.390 I print_info: freq_scale_train = 1
0.00.040.390 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.390 I print_info: rope_finetuned   = unknown
0.00.040.390 I print_info: ssm_d_conv       = 0
0.00.040.390 I print_info: ssm_d_inner      = 0
0.00.040.390 I print_info: ssm_d_state      = 0
0.00.040.391 I print_info: ssm_dt_rank      = 0
0.00.040.391 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.391 I print_info: model type       = 1.4B
0.00.040.391 I print_info: model params     = 1.41 B
0.00.040.392 I print_info: general.name     = 1.4B
0.00.040.392 I print_info: vocab type       = BPE
0.00.040.392 I print_info: n_vocab          = 50304
0.00.040.392 I print_info: n_merges         = 50009
0.00.040.393 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.393 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.393 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.393 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.395 I print_info: LF token         = 187 ''
0.00.040.395 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.395 I print_info: max token length = 1024
0.00.040.395 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.344.572 I load_tensors: offloading 24 repeating layers to GPU
0.00.344.587 I load_tensors: offloading output layer to GPU
0.00.344.588 I load_tensors: offloaded 25/25 layers to GPU
0.00.344.622 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.344.624 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.346.264 I llama_init_from_model: n_seq_max     = 1
0.00.346.268 I llama_init_from_model: n_ctx         = 128
0.00.346.268 I llama_init_from_model: n_ctx_per_seq = 128
0.00.346.269 I llama_init_from_model: n_batch       = 128
0.00.346.269 I llama_init_from_model: n_ubatch      = 128
0.00.346.270 I llama_init_from_model: flash_attn    = 0
0.00.346.272 I llama_init_from_model: freq_base     = 10000.0
0.00.346.273 I llama_init_from_model: freq_scale    = 1
0.00.346.273 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.346.276 I ggml_metal_init: allocating
0.00.346.361 I ggml_metal_init: found device: Apple M4
0.00.346.374 I ggml_metal_init: picking default device: Apple M4
0.00.347.924 I ggml_metal_init: using embedded metal library
0.00.353.617 I ggml_metal_init: GPU name:   Apple M4
0.00.353.632 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.353.633 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.353.634 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.353.634 I ggml_metal_init: simdgroup reduction   = true
0.00.353.634 I ggml_metal_init: simdgroup matrix mul. = true
0.00.353.635 I ggml_metal_init: has residency sets    = true
0.00.353.635 I ggml_metal_init: has bfloat            = true
0.00.353.635 I ggml_metal_init: use bfloat            = true
0.00.353.637 I ggml_metal_init: hasUnifiedMemory      = true
0.00.353.641 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.375.177 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.378.913 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.378.919 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.378.975 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.382.232 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.382.233 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.382.234 I llama_init_from_model: graph nodes  = 967
0.00.382.234 I llama_init_from_model: graph splits = 2
0.00.382.238 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.382.238 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.416.434 I 
0.00.416.539 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.416.555 I perplexity: tokenizing the input ..
0.00.423.975 I perplexity: tokenization took 7.416 ms
0.00.423.983 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.568.483 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.569.817 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.569.836 I llama_perf_context_print:        load time =     406.64 ms
0.00.569.837 I llama_perf_context_print: prompt eval time =     144.05 ms /   128 tokens (    1.13 ms per token,   888.57 tokens per second)
0.00.569.841 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.569.843 I llama_perf_context_print:       total time =     153.41 ms /   129 tokens
0.00.570.231 I ggml_metal_free: deallocating

real	0m0.586s
user	0m0.081s
sys	0m0.096s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4864 (6ef79a67) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.010.452 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.032 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.018.037 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.039 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.039 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.039 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.042 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.042 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.043 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.045 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.045 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.045 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.047 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.047 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.048 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.052 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.052 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.053 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.869 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.868 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.637 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.638 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.639 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.639 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.639 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.639 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.640 I llama_model_loader: - type  f32:  194 tensors
0.00.026.640 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.641 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.641 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.641 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.642 I print_info: file format = GGUF V3 (latest)
0.00.026.646 I print_info: file type   = Q3_K - Medium
0.00.026.647 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.034.898 I load: special tokens cache size = 25
0.00.041.238 I load: token to piece cache size = 0.2984 MB
0.00.041.252 I print_info: arch             = gptneox
0.00.041.253 I print_info: vocab_only       = 0
0.00.041.253 I print_info: n_ctx_train      = 2048
0.00.041.254 I print_info: n_embd           = 2048
0.00.041.254 I print_info: n_layer          = 24
0.00.041.256 I print_info: n_head           = 16
0.00.041.257 I print_info: n_head_kv        = 16
0.00.041.257 I print_info: n_rot            = 32
0.00.041.257 I print_info: n_swa            = 0
0.00.041.258 I print_info: n_embd_head_k    = 128
0.00.041.258 I print_info: n_embd_head_v    = 128
0.00.041.259 I print_info: n_gqa            = 1
0.00.041.259 I print_info: n_embd_k_gqa     = 2048
0.00.041.260 I print_info: n_embd_v_gqa     = 2048
0.00.041.263 I print_info: f_norm_eps       = 1.0e-05
0.00.041.263 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.263 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.263 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.264 I print_info: f_logit_scale    = 0.0e+00
0.00.041.264 I print_info: n_ff             = 8192
0.00.041.264 I print_info: n_expert         = 0
0.00.041.265 I print_info: n_expert_used    = 0
0.00.041.266 I print_info: causal attn      = 1
0.00.041.267 I print_info: pooling type     = 0
0.00.041.267 I print_info: rope type        = 2
0.00.041.268 I print_info: rope scaling     = linear
0.00.041.268 I print_info: freq_base_train  = 10000.0
0.00.041.268 I print_info: freq_scale_train = 1
0.00.041.269 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.270 I print_info: rope_finetuned   = unknown
0.00.041.270 I print_info: ssm_d_conv       = 0
0.00.041.270 I print_info: ssm_d_inner      = 0
0.00.041.270 I print_info: ssm_d_state      = 0
0.00.041.270 I print_info: ssm_dt_rank      = 0
0.00.041.270 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.270 I print_info: model type       = 1.4B
0.00.041.271 I print_info: model params     = 1.41 B
0.00.041.271 I print_info: general.name     = 1.4B
0.00.041.272 I print_info: vocab type       = BPE
0.00.041.272 I print_info: n_vocab          = 50304
0.00.041.272 I print_info: n_merges         = 50009
0.00.041.273 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.273 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.273 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.273 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.274 I print_info: LF token         = 187 ''
0.00.041.274 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.275 I print_info: max token length = 1024
0.00.041.275 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.451.143 I load_tensors: offloading 24 repeating layers to GPU
0.00.451.157 I load_tensors: offloading output layer to GPU
0.00.451.157 I load_tensors: offloaded 25/25 layers to GPU
0.00.451.192 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.451.194 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.452.906 I llama_init_from_model: n_seq_max     = 1
0.00.452.909 I llama_init_from_model: n_ctx         = 2048
0.00.452.910 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.452.910 I llama_init_from_model: n_batch       = 2048
0.00.452.910 I llama_init_from_model: n_ubatch      = 512
0.00.452.911 I llama_init_from_model: flash_attn    = 0
0.00.452.913 I llama_init_from_model: freq_base     = 10000.0
0.00.452.914 I llama_init_from_model: freq_scale    = 1
0.00.452.916 I ggml_metal_init: allocating
0.00.452.975 I ggml_metal_init: found device: Apple M4
0.00.452.989 I ggml_metal_init: picking default device: Apple M4
0.00.454.572 I ggml_metal_init: using embedded metal library
0.00.460.354 I ggml_metal_init: GPU name:   Apple M4
0.00.460.365 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.460.366 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.460.367 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.460.367 I ggml_metal_init: simdgroup reduction   = true
0.00.460.368 I ggml_metal_init: simdgroup matrix mul. = true
0.00.460.368 I ggml_metal_init: has residency sets    = true
0.00.460.368 I ggml_metal_init: has bfloat            = true
0.00.460.368 I ggml_metal_init: use bfloat            = true
0.00.460.370 I ggml_metal_init: hasUnifiedMemory      = true
0.00.460.375 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.481.425 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.541.016 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.541.030 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.541.057 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.545.390 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.545.392 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.545.392 I llama_init_from_model: graph nodes  = 967
0.00.545.392 I llama_init_from_model: graph splits = 2
0.00.545.397 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.545.513 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.545.513 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.602.388 I main: llama threadpool init, n_threads = 4
0.00.602.439 I 
0.00.602.460 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.602.462 I 
0.00.602.627 I sampler seed: 1234
0.00.602.633 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.602.648 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.602.650 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.602.650 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.347.821 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50859.60 tokens per second)
0.01.347.822 I llama_perf_context_print:        load time =     591.19 ms
0.01.347.823 I llama_perf_context_print: prompt eval time =      49.77 ms /     7 tokens (    7.11 ms per token,   140.65 tokens per second)
0.01.347.824 I llama_perf_context_print:        eval time =     692.55 ms /    63 runs   (   10.99 ms per token,    90.97 tokens per second)
0.01.347.824 I llama_perf_context_print:       total time =     746.17 ms /    70 tokens
0.01.348.056 I ggml_metal_free: deallocating

real	0m1.365s
user	0m0.111s
sys	0m0.194s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4864 (6ef79a67) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.777 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.078 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.085 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.086 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.087 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.087 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.087 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.088 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.089 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.089 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.089 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.090 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.090 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.093 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.093 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.095 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.095 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.095 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.883 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.867 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.618 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.619 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.620 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.620 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.620 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.621 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.621 I llama_model_loader: - type  f32:  194 tensors
0.00.024.622 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.622 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.622 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.622 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.623 I print_info: file format = GGUF V3 (latest)
0.00.024.624 I print_info: file type   = Q3_K - Medium
0.00.024.625 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.162 I load: special tokens cache size = 25
0.00.039.519 I load: token to piece cache size = 0.2984 MB
0.00.039.536 I print_info: arch             = gptneox
0.00.039.537 I print_info: vocab_only       = 0
0.00.039.537 I print_info: n_ctx_train      = 2048
0.00.039.537 I print_info: n_embd           = 2048
0.00.039.537 I print_info: n_layer          = 24
0.00.039.542 I print_info: n_head           = 16
0.00.039.542 I print_info: n_head_kv        = 16
0.00.039.542 I print_info: n_rot            = 32
0.00.039.542 I print_info: n_swa            = 0
0.00.039.543 I print_info: n_embd_head_k    = 128
0.00.039.543 I print_info: n_embd_head_v    = 128
0.00.039.543 I print_info: n_gqa            = 1
0.00.039.544 I print_info: n_embd_k_gqa     = 2048
0.00.039.544 I print_info: n_embd_v_gqa     = 2048
0.00.039.545 I print_info: f_norm_eps       = 1.0e-05
0.00.039.546 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.547 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.547 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.547 I print_info: f_logit_scale    = 0.0e+00
0.00.039.548 I print_info: n_ff             = 8192
0.00.039.548 I print_info: n_expert         = 0
0.00.039.548 I print_info: n_expert_used    = 0
0.00.039.548 I print_info: causal attn      = 1
0.00.039.548 I print_info: pooling type     = 0
0.00.039.548 I print_info: rope type        = 2
0.00.039.548 I print_info: rope scaling     = linear
0.00.039.549 I print_info: freq_base_train  = 10000.0
0.00.039.549 I print_info: freq_scale_train = 1
0.00.039.549 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.549 I print_info: rope_finetuned   = unknown
0.00.039.549 I print_info: ssm_d_conv       = 0
0.00.039.550 I print_info: ssm_d_inner      = 0
0.00.039.550 I print_info: ssm_d_state      = 0
0.00.039.550 I print_info: ssm_dt_rank      = 0
0.00.039.550 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.550 I print_info: model type       = 1.4B
0.00.039.550 I print_info: model params     = 1.41 B
0.00.039.551 I print_info: general.name     = 1.4B
0.00.039.551 I print_info: vocab type       = BPE
0.00.039.551 I print_info: n_vocab          = 50304
0.00.039.551 I print_info: n_merges         = 50009
0.00.039.552 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.552 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.552 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.552 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.552 I print_info: LF token         = 187 ''
0.00.039.552 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.553 I print_info: max token length = 1024
0.00.039.553 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.446.801 I load_tensors: offloading 24 repeating layers to GPU
0.00.446.822 I load_tensors: offloading output layer to GPU
0.00.446.823 I load_tensors: offloaded 25/25 layers to GPU
0.00.446.858 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.446.859 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.448.470 I llama_init_from_model: n_seq_max     = 1
0.00.448.473 I llama_init_from_model: n_ctx         = 128
0.00.448.474 I llama_init_from_model: n_ctx_per_seq = 128
0.00.448.474 I llama_init_from_model: n_batch       = 128
0.00.448.474 I llama_init_from_model: n_ubatch      = 128
0.00.448.475 I llama_init_from_model: flash_attn    = 0
0.00.448.477 I llama_init_from_model: freq_base     = 10000.0
0.00.448.478 I llama_init_from_model: freq_scale    = 1
0.00.448.478 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.448.483 I ggml_metal_init: allocating
0.00.448.572 I ggml_metal_init: found device: Apple M4
0.00.448.586 I ggml_metal_init: picking default device: Apple M4
0.00.450.146 I ggml_metal_init: using embedded metal library
0.00.455.683 I ggml_metal_init: GPU name:   Apple M4
0.00.455.698 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.455.699 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.455.700 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.455.701 I ggml_metal_init: simdgroup reduction   = true
0.00.455.701 I ggml_metal_init: simdgroup matrix mul. = true
0.00.455.701 I ggml_metal_init: has residency sets    = true
0.00.455.701 I ggml_metal_init: has bfloat            = true
0.00.455.702 I ggml_metal_init: use bfloat            = true
0.00.455.704 I ggml_metal_init: hasUnifiedMemory      = true
0.00.455.708 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.476.714 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.480.440 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.480.454 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.480.490 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.483.866 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.483.868 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.483.869 I llama_init_from_model: graph nodes  = 967
0.00.483.869 I llama_init_from_model: graph splits = 2
0.00.483.872 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.483.873 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.513.766 I 
0.00.513.856 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.513.865 I perplexity: tokenizing the input ..
0.00.521.019 I perplexity: tokenization took 7.151 ms
0.00.521.035 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.665.678 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.667.012 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.667.039 I llama_perf_context_print:        load time =     504.98 ms
0.00.667.040 I llama_perf_context_print: prompt eval time =     143.69 ms /   128 tokens (    1.12 ms per token,   890.84 tokens per second)
0.00.667.041 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.667.042 I llama_perf_context_print:       total time =     153.28 ms /   129 tokens
0.00.667.427 I ggml_metal_free: deallocating

real	0m0.681s
user	0m0.082s
sys	0m0.115s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4864 (6ef79a67) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.048 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.564 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.570 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.571 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.573 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.574 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.574 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.574 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.575 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.576 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.576 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.577 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.577 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.579 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.579 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.582 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.582 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.583 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.325 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.340 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.044 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.045 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.046 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.046 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.046 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.047 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.047 I llama_model_loader: - type  f32:  194 tensors
0.00.025.047 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.048 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.048 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.049 I print_info: file format = GGUF V3 (latest)
0.00.025.049 I print_info: file type   = Q4_K - Medium
0.00.025.050 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.892 I load: special tokens cache size = 25
0.00.039.152 I load: token to piece cache size = 0.2984 MB
0.00.039.166 I print_info: arch             = gptneox
0.00.039.167 I print_info: vocab_only       = 0
0.00.039.167 I print_info: n_ctx_train      = 2048
0.00.039.167 I print_info: n_embd           = 2048
0.00.039.167 I print_info: n_layer          = 24
0.00.039.170 I print_info: n_head           = 16
0.00.039.171 I print_info: n_head_kv        = 16
0.00.039.171 I print_info: n_rot            = 32
0.00.039.171 I print_info: n_swa            = 0
0.00.039.171 I print_info: n_embd_head_k    = 128
0.00.039.172 I print_info: n_embd_head_v    = 128
0.00.039.172 I print_info: n_gqa            = 1
0.00.039.173 I print_info: n_embd_k_gqa     = 2048
0.00.039.174 I print_info: n_embd_v_gqa     = 2048
0.00.039.174 I print_info: f_norm_eps       = 1.0e-05
0.00.039.176 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.176 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.177 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.177 I print_info: f_logit_scale    = 0.0e+00
0.00.039.177 I print_info: n_ff             = 8192
0.00.039.177 I print_info: n_expert         = 0
0.00.039.179 I print_info: n_expert_used    = 0
0.00.039.179 I print_info: causal attn      = 1
0.00.039.180 I print_info: pooling type     = 0
0.00.039.181 I print_info: rope type        = 2
0.00.039.181 I print_info: rope scaling     = linear
0.00.039.182 I print_info: freq_base_train  = 10000.0
0.00.039.182 I print_info: freq_scale_train = 1
0.00.039.182 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.182 I print_info: rope_finetuned   = unknown
0.00.039.183 I print_info: ssm_d_conv       = 0
0.00.039.183 I print_info: ssm_d_inner      = 0
0.00.039.183 I print_info: ssm_d_state      = 0
0.00.039.183 I print_info: ssm_dt_rank      = 0
0.00.039.183 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.183 I print_info: model type       = 1.4B
0.00.039.184 I print_info: model params     = 1.41 B
0.00.039.184 I print_info: general.name     = 1.4B
0.00.039.184 I print_info: vocab type       = BPE
0.00.039.184 I print_info: n_vocab          = 50304
0.00.039.184 I print_info: n_merges         = 50009
0.00.039.185 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.186 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.186 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.186 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.186 I print_info: LF token         = 187 ''
0.00.039.186 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.186 I print_info: max token length = 1024
0.00.039.188 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.541.601 I load_tensors: offloading 24 repeating layers to GPU
0.00.541.613 I load_tensors: offloading output layer to GPU
0.00.541.614 I load_tensors: offloaded 25/25 layers to GPU
0.00.541.638 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.541.639 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.543.167 I llama_init_from_model: n_seq_max     = 1
0.00.543.171 I llama_init_from_model: n_ctx         = 2048
0.00.543.172 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.543.172 I llama_init_from_model: n_batch       = 2048
0.00.543.173 I llama_init_from_model: n_ubatch      = 512
0.00.543.173 I llama_init_from_model: flash_attn    = 0
0.00.543.175 I llama_init_from_model: freq_base     = 10000.0
0.00.543.175 I llama_init_from_model: freq_scale    = 1
0.00.543.178 I ggml_metal_init: allocating
0.00.543.227 I ggml_metal_init: found device: Apple M4
0.00.543.238 I ggml_metal_init: picking default device: Apple M4
0.00.544.707 I ggml_metal_init: using embedded metal library
0.00.550.366 I ggml_metal_init: GPU name:   Apple M4
0.00.550.371 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.550.372 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.550.373 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.550.374 I ggml_metal_init: simdgroup reduction   = true
0.00.550.374 I ggml_metal_init: simdgroup matrix mul. = true
0.00.550.374 I ggml_metal_init: has residency sets    = true
0.00.550.374 I ggml_metal_init: has bfloat            = true
0.00.550.375 I ggml_metal_init: use bfloat            = true
0.00.550.376 I ggml_metal_init: hasUnifiedMemory      = true
0.00.550.377 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.569.922 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.626.364 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.626.374 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.626.396 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.631.053 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.631.056 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.631.057 I llama_init_from_model: graph nodes  = 967
0.00.631.057 I llama_init_from_model: graph splits = 2
0.00.631.063 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.631.191 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.631.191 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.688.923 I main: llama threadpool init, n_threads = 4
0.00.688.968 I 
0.00.688.988 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.688.988 I 
0.00.689.153 I sampler seed: 1234
0.00.689.158 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.689.173 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.689.174 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.689.174 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.451.735 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 49100.97 tokens per second)
0.01.451.736 I llama_perf_context_print:        load time =     679.09 ms
0.01.451.737 I llama_perf_context_print: prompt eval time =      58.02 ms /     7 tokens (    8.29 ms per token,   120.66 tokens per second)
0.01.451.739 I llama_perf_context_print:        eval time =     701.67 ms /    63 runs   (   11.14 ms per token,    89.79 tokens per second)
0.01.451.740 I llama_perf_context_print:       total time =     763.60 ms /    70 tokens
0.01.451.972 I ggml_metal_free: deallocating

real	0m1.468s
user	0m0.109s
sys	0m0.213s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4864 (6ef79a67) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.934 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.313 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.329 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.332 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.338 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.338 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.339 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.339 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.340 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.340 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.340 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.341 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.341 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.341 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.341 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.343 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.343 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.344 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.248 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.242 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.022 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.023 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.023 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.024 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.024 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.024 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.025 I llama_model_loader: - type  f32:  194 tensors
0.00.025.025 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.025 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.026 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.026 I print_info: file format = GGUF V3 (latest)
0.00.025.027 I print_info: file type   = Q4_K - Medium
0.00.025.028 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.480 I load: special tokens cache size = 25
0.00.039.997 I load: token to piece cache size = 0.2984 MB
0.00.040.013 I print_info: arch             = gptneox
0.00.040.014 I print_info: vocab_only       = 0
0.00.040.014 I print_info: n_ctx_train      = 2048
0.00.040.015 I print_info: n_embd           = 2048
0.00.040.015 I print_info: n_layer          = 24
0.00.040.019 I print_info: n_head           = 16
0.00.040.019 I print_info: n_head_kv        = 16
0.00.040.020 I print_info: n_rot            = 32
0.00.040.020 I print_info: n_swa            = 0
0.00.040.020 I print_info: n_embd_head_k    = 128
0.00.040.020 I print_info: n_embd_head_v    = 128
0.00.040.021 I print_info: n_gqa            = 1
0.00.040.021 I print_info: n_embd_k_gqa     = 2048
0.00.040.022 I print_info: n_embd_v_gqa     = 2048
0.00.040.023 I print_info: f_norm_eps       = 1.0e-05
0.00.040.023 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.023 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.023 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.023 I print_info: f_logit_scale    = 0.0e+00
0.00.040.024 I print_info: n_ff             = 8192
0.00.040.024 I print_info: n_expert         = 0
0.00.040.024 I print_info: n_expert_used    = 0
0.00.040.024 I print_info: causal attn      = 1
0.00.040.025 I print_info: pooling type     = 0
0.00.040.025 I print_info: rope type        = 2
0.00.040.025 I print_info: rope scaling     = linear
0.00.040.025 I print_info: freq_base_train  = 10000.0
0.00.040.026 I print_info: freq_scale_train = 1
0.00.040.026 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.026 I print_info: rope_finetuned   = unknown
0.00.040.026 I print_info: ssm_d_conv       = 0
0.00.040.026 I print_info: ssm_d_inner      = 0
0.00.040.026 I print_info: ssm_d_state      = 0
0.00.040.026 I print_info: ssm_dt_rank      = 0
0.00.040.027 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.027 I print_info: model type       = 1.4B
0.00.040.027 I print_info: model params     = 1.41 B
0.00.040.027 I print_info: general.name     = 1.4B
0.00.040.028 I print_info: vocab type       = BPE
0.00.040.028 I print_info: n_vocab          = 50304
0.00.040.028 I print_info: n_merges         = 50009
0.00.040.028 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.028 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.028 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.029 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.029 I print_info: LF token         = 187 ''
0.00.040.029 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.029 I print_info: max token length = 1024
0.00.040.030 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.509.134 I load_tensors: offloading 24 repeating layers to GPU
0.00.509.149 I load_tensors: offloading output layer to GPU
0.00.509.150 I load_tensors: offloaded 25/25 layers to GPU
0.00.509.189 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.509.190 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.510.949 I llama_init_from_model: n_seq_max     = 1
0.00.510.952 I llama_init_from_model: n_ctx         = 128
0.00.510.953 I llama_init_from_model: n_ctx_per_seq = 128
0.00.510.953 I llama_init_from_model: n_batch       = 128
0.00.510.953 I llama_init_from_model: n_ubatch      = 128
0.00.510.954 I llama_init_from_model: flash_attn    = 0
0.00.510.956 I llama_init_from_model: freq_base     = 10000.0
0.00.510.956 I llama_init_from_model: freq_scale    = 1
0.00.510.957 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.510.959 I ggml_metal_init: allocating
0.00.511.041 I ggml_metal_init: found device: Apple M4
0.00.511.054 I ggml_metal_init: picking default device: Apple M4
0.00.512.612 I ggml_metal_init: using embedded metal library
0.00.519.481 I ggml_metal_init: GPU name:   Apple M4
0.00.519.487 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.519.488 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.519.489 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.519.490 I ggml_metal_init: simdgroup reduction   = true
0.00.519.490 I ggml_metal_init: simdgroup matrix mul. = true
0.00.519.490 I ggml_metal_init: has residency sets    = true
0.00.519.490 I ggml_metal_init: has bfloat            = true
0.00.519.491 I ggml_metal_init: use bfloat            = true
0.00.519.492 I ggml_metal_init: hasUnifiedMemory      = true
0.00.519.496 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.538.225 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.541.798 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.541.805 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.541.844 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.545.067 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.545.068 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.545.069 I llama_init_from_model: graph nodes  = 967
0.00.545.069 I llama_init_from_model: graph splits = 2
0.00.545.072 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.545.073 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.571.594 I 
0.00.571.682 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.571.689 I perplexity: tokenizing the input ..
0.00.579.148 I perplexity: tokenization took 7.455 ms
0.00.579.154 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.712.974 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.714.328 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.714.355 I llama_perf_context_print:        load time =     562.65 ms
0.00.714.356 I llama_perf_context_print: prompt eval time =     132.92 ms /   128 tokens (    1.04 ms per token,   963.01 tokens per second)
0.00.714.357 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.714.357 I llama_perf_context_print:       total time =     142.77 ms /   129 tokens
0.00.714.777 I ggml_metal_free: deallocating

real	0m0.728s
user	0m0.082s
sys	0m0.120s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4864 (6ef79a67) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.010.287 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.770 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.775 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.777 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.780 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.780 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.780 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.781 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.781 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.783 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.784 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.784 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.785 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.785 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.785 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.788 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.788 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.789 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.591 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.618 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.410 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.411 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.412 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.412 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.412 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.413 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.413 I llama_model_loader: - type  f32:  194 tensors
0.00.026.413 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.414 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.414 I print_info: file format = GGUF V3 (latest)
0.00.026.415 I print_info: file type   = Q5_K - Medium
0.00.026.416 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.331 I load: special tokens cache size = 25
0.00.040.655 I load: token to piece cache size = 0.2984 MB
0.00.040.670 I print_info: arch             = gptneox
0.00.040.671 I print_info: vocab_only       = 0
0.00.040.671 I print_info: n_ctx_train      = 2048
0.00.040.671 I print_info: n_embd           = 2048
0.00.040.671 I print_info: n_layer          = 24
0.00.040.674 I print_info: n_head           = 16
0.00.040.675 I print_info: n_head_kv        = 16
0.00.040.675 I print_info: n_rot            = 32
0.00.040.675 I print_info: n_swa            = 0
0.00.040.675 I print_info: n_embd_head_k    = 128
0.00.040.676 I print_info: n_embd_head_v    = 128
0.00.040.678 I print_info: n_gqa            = 1
0.00.040.679 I print_info: n_embd_k_gqa     = 2048
0.00.040.679 I print_info: n_embd_v_gqa     = 2048
0.00.040.680 I print_info: f_norm_eps       = 1.0e-05
0.00.040.680 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.681 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.681 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.681 I print_info: f_logit_scale    = 0.0e+00
0.00.040.682 I print_info: n_ff             = 8192
0.00.040.682 I print_info: n_expert         = 0
0.00.040.682 I print_info: n_expert_used    = 0
0.00.040.682 I print_info: causal attn      = 1
0.00.040.682 I print_info: pooling type     = 0
0.00.040.683 I print_info: rope type        = 2
0.00.040.684 I print_info: rope scaling     = linear
0.00.040.685 I print_info: freq_base_train  = 10000.0
0.00.040.685 I print_info: freq_scale_train = 1
0.00.040.685 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.686 I print_info: rope_finetuned   = unknown
0.00.040.688 I print_info: ssm_d_conv       = 0
0.00.040.689 I print_info: ssm_d_inner      = 0
0.00.040.689 I print_info: ssm_d_state      = 0
0.00.040.689 I print_info: ssm_dt_rank      = 0
0.00.040.690 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.690 I print_info: model type       = 1.4B
0.00.040.690 I print_info: model params     = 1.41 B
0.00.040.690 I print_info: general.name     = 1.4B
0.00.040.691 I print_info: vocab type       = BPE
0.00.040.691 I print_info: n_vocab          = 50304
0.00.040.691 I print_info: n_merges         = 50009
0.00.040.691 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.692 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.692 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.692 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.693 I print_info: LF token         = 187 ''
0.00.040.693 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.694 I print_info: max token length = 1024
0.00.040.694 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.597.101 I load_tensors: offloading 24 repeating layers to GPU
0.00.597.105 I load_tensors: offloading output layer to GPU
0.00.597.106 I load_tensors: offloaded 25/25 layers to GPU
0.00.597.130 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.597.131 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.598.667 I llama_init_from_model: n_seq_max     = 1
0.00.598.669 I llama_init_from_model: n_ctx         = 2048
0.00.598.670 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.598.670 I llama_init_from_model: n_batch       = 2048
0.00.598.670 I llama_init_from_model: n_ubatch      = 512
0.00.598.671 I llama_init_from_model: flash_attn    = 0
0.00.598.672 I llama_init_from_model: freq_base     = 10000.0
0.00.598.673 I llama_init_from_model: freq_scale    = 1
0.00.598.674 I ggml_metal_init: allocating
0.00.598.689 I ggml_metal_init: found device: Apple M4
0.00.598.698 I ggml_metal_init: picking default device: Apple M4
0.00.599.948 I ggml_metal_init: using embedded metal library
0.00.606.196 I ggml_metal_init: GPU name:   Apple M4
0.00.606.199 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.606.200 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.606.201 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.606.201 I ggml_metal_init: simdgroup reduction   = true
0.00.606.202 I ggml_metal_init: simdgroup matrix mul. = true
0.00.606.202 I ggml_metal_init: has residency sets    = true
0.00.606.202 I ggml_metal_init: has bfloat            = true
0.00.606.202 I ggml_metal_init: use bfloat            = true
0.00.606.203 I ggml_metal_init: hasUnifiedMemory      = true
0.00.606.204 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.623.204 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.683.172 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.683.179 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.683.204 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.688.713 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.688.716 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.688.716 I llama_init_from_model: graph nodes  = 967
0.00.688.716 I llama_init_from_model: graph splits = 2
0.00.688.722 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.688.851 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.688.851 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.753.022 I main: llama threadpool init, n_threads = 4
0.00.753.073 I 
0.00.753.094 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.753.094 I 
0.00.753.255 I sampler seed: 1234
0.00.753.260 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.753.294 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.753.295 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.753.295 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.607.258 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50497.87 tokens per second)
0.01.607.259 I llama_perf_context_print:        load time =     741.96 ms
0.01.607.259 I llama_perf_context_print: prompt eval time =      52.63 ms /     7 tokens (    7.52 ms per token,   133.00 tokens per second)
0.01.607.262 I llama_perf_context_print:        eval time =     798.60 ms /    63 runs   (   12.68 ms per token,    78.89 tokens per second)
0.01.607.263 I llama_perf_context_print:       total time =     855.01 ms /    70 tokens
0.01.607.517 I ggml_metal_free: deallocating

real	0m1.626s
user	0m0.110s
sys	0m0.219s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4864 (6ef79a67) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.245 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.490 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.496 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.502 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.503 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.503 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.504 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.504 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.505 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.506 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.506 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.506 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.507 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.507 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.508 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.510 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.510 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.510 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.407 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.463 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.329 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.330 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.331 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.331 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.331 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.332 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.332 I llama_model_loader: - type  f32:  194 tensors
0.00.026.333 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.333 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.333 I print_info: file format = GGUF V3 (latest)
0.00.026.334 I print_info: file type   = Q5_K - Medium
0.00.026.339 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.928 I load: special tokens cache size = 25
0.00.041.496 I load: token to piece cache size = 0.2984 MB
0.00.041.512 I print_info: arch             = gptneox
0.00.041.513 I print_info: vocab_only       = 0
0.00.041.513 I print_info: n_ctx_train      = 2048
0.00.041.513 I print_info: n_embd           = 2048
0.00.041.514 I print_info: n_layer          = 24
0.00.041.517 I print_info: n_head           = 16
0.00.041.519 I print_info: n_head_kv        = 16
0.00.041.520 I print_info: n_rot            = 32
0.00.041.520 I print_info: n_swa            = 0
0.00.041.520 I print_info: n_embd_head_k    = 128
0.00.041.520 I print_info: n_embd_head_v    = 128
0.00.041.521 I print_info: n_gqa            = 1
0.00.041.521 I print_info: n_embd_k_gqa     = 2048
0.00.041.522 I print_info: n_embd_v_gqa     = 2048
0.00.041.522 I print_info: f_norm_eps       = 1.0e-05
0.00.041.523 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.523 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.523 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.523 I print_info: f_logit_scale    = 0.0e+00
0.00.041.524 I print_info: n_ff             = 8192
0.00.041.524 I print_info: n_expert         = 0
0.00.041.524 I print_info: n_expert_used    = 0
0.00.041.524 I print_info: causal attn      = 1
0.00.041.524 I print_info: pooling type     = 0
0.00.041.529 I print_info: rope type        = 2
0.00.041.529 I print_info: rope scaling     = linear
0.00.041.529 I print_info: freq_base_train  = 10000.0
0.00.041.530 I print_info: freq_scale_train = 1
0.00.041.530 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.532 I print_info: rope_finetuned   = unknown
0.00.041.532 I print_info: ssm_d_conv       = 0
0.00.041.532 I print_info: ssm_d_inner      = 0
0.00.041.532 I print_info: ssm_d_state      = 0
0.00.041.532 I print_info: ssm_dt_rank      = 0
0.00.041.532 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.532 I print_info: model type       = 1.4B
0.00.041.533 I print_info: model params     = 1.41 B
0.00.041.533 I print_info: general.name     = 1.4B
0.00.041.533 I print_info: vocab type       = BPE
0.00.041.534 I print_info: n_vocab          = 50304
0.00.041.534 I print_info: n_merges         = 50009
0.00.041.534 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.534 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.534 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.534 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.535 I print_info: LF token         = 187 ''
0.00.041.535 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.535 I print_info: max token length = 1024
0.00.041.535 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.627.401 I load_tensors: offloading 24 repeating layers to GPU
0.00.627.409 I load_tensors: offloading output layer to GPU
0.00.627.410 I load_tensors: offloaded 25/25 layers to GPU
0.00.627.439 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.627.443 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.629.035 I llama_init_from_model: n_seq_max     = 1
0.00.629.037 I llama_init_from_model: n_ctx         = 128
0.00.629.037 I llama_init_from_model: n_ctx_per_seq = 128
0.00.629.038 I llama_init_from_model: n_batch       = 128
0.00.629.038 I llama_init_from_model: n_ubatch      = 128
0.00.629.038 I llama_init_from_model: flash_attn    = 0
0.00.629.040 I llama_init_from_model: freq_base     = 10000.0
0.00.629.040 I llama_init_from_model: freq_scale    = 1
0.00.629.041 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.629.043 I ggml_metal_init: allocating
0.00.629.097 I ggml_metal_init: found device: Apple M4
0.00.629.114 I ggml_metal_init: picking default device: Apple M4
0.00.630.435 I ggml_metal_init: using embedded metal library
0.00.636.484 I ggml_metal_init: GPU name:   Apple M4
0.00.636.488 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.636.489 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.636.490 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.636.491 I ggml_metal_init: simdgroup reduction   = true
0.00.636.491 I ggml_metal_init: simdgroup matrix mul. = true
0.00.636.491 I ggml_metal_init: has residency sets    = true
0.00.636.491 I ggml_metal_init: has bfloat            = true
0.00.636.492 I ggml_metal_init: use bfloat            = true
0.00.636.493 I ggml_metal_init: hasUnifiedMemory      = true
0.00.636.497 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.654.052 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.657.378 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.657.382 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.657.417 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.660.485 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.660.487 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.660.487 I llama_init_from_model: graph nodes  = 967
0.00.660.487 I llama_init_from_model: graph splits = 2
0.00.660.491 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.660.491 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.693.199 I 
0.00.693.283 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.693.291 I perplexity: tokenizing the input ..
0.00.700.517 I perplexity: tokenization took 7.224 ms
0.00.700.521 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.836.641 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.837.958 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.837.980 I llama_perf_context_print:        load time =     682.94 ms
0.00.837.981 I llama_perf_context_print: prompt eval time =     135.89 ms /   128 tokens (    1.06 ms per token,   941.94 tokens per second)
0.00.837.982 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.837.982 I llama_perf_context_print:       total time =     144.79 ms /   129 tokens
0.00.838.385 I ggml_metal_free: deallocating

real	0m0.854s
user	0m0.079s
sys	0m0.158s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4864 (6ef79a67) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.093 I main: llama backend init
0.00.000.096 I main: load the model and apply lora adapter, if any
0.00.009.285 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.184 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.020.190 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.197 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.198 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.198 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.199 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.199 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.200 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.200 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.202 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.202 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.202 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.203 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.205 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.207 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.207 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.207 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.079 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.110 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.896 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.028.897 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.897 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.897 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.898 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.898 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.028.898 I llama_model_loader: - type  f32:  194 tensors
0.00.028.899 I llama_model_loader: - type q6_K:   98 tensors
0.00.028.900 I print_info: file format = GGUF V3 (latest)
0.00.028.900 I print_info: file type   = Q6_K
0.00.028.901 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.036.924 I load: special tokens cache size = 25
0.00.043.435 I load: token to piece cache size = 0.2984 MB
0.00.043.449 I print_info: arch             = gptneox
0.00.043.450 I print_info: vocab_only       = 0
0.00.043.450 I print_info: n_ctx_train      = 2048
0.00.043.451 I print_info: n_embd           = 2048
0.00.043.451 I print_info: n_layer          = 24
0.00.043.454 I print_info: n_head           = 16
0.00.043.455 I print_info: n_head_kv        = 16
0.00.043.455 I print_info: n_rot            = 32
0.00.043.455 I print_info: n_swa            = 0
0.00.043.455 I print_info: n_embd_head_k    = 128
0.00.043.455 I print_info: n_embd_head_v    = 128
0.00.043.456 I print_info: n_gqa            = 1
0.00.043.457 I print_info: n_embd_k_gqa     = 2048
0.00.043.457 I print_info: n_embd_v_gqa     = 2048
0.00.043.458 I print_info: f_norm_eps       = 1.0e-05
0.00.043.458 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.043.459 I print_info: f_clamp_kqv      = 0.0e+00
0.00.043.459 I print_info: f_max_alibi_bias = 0.0e+00
0.00.043.459 I print_info: f_logit_scale    = 0.0e+00
0.00.043.459 I print_info: n_ff             = 8192
0.00.043.460 I print_info: n_expert         = 0
0.00.043.460 I print_info: n_expert_used    = 0
0.00.043.460 I print_info: causal attn      = 1
0.00.043.460 I print_info: pooling type     = 0
0.00.043.462 I print_info: rope type        = 2
0.00.043.462 I print_info: rope scaling     = linear
0.00.043.462 I print_info: freq_base_train  = 10000.0
0.00.043.463 I print_info: freq_scale_train = 1
0.00.043.463 I print_info: n_ctx_orig_yarn  = 2048
0.00.043.463 I print_info: rope_finetuned   = unknown
0.00.043.463 I print_info: ssm_d_conv       = 0
0.00.043.464 I print_info: ssm_d_inner      = 0
0.00.043.464 I print_info: ssm_d_state      = 0
0.00.043.464 I print_info: ssm_dt_rank      = 0
0.00.043.465 I print_info: ssm_dt_b_c_rms   = 0
0.00.043.465 I print_info: model type       = 1.4B
0.00.043.465 I print_info: model params     = 1.41 B
0.00.043.465 I print_info: general.name     = 1.4B
0.00.043.466 I print_info: vocab type       = BPE
0.00.043.466 I print_info: n_vocab          = 50304
0.00.043.466 I print_info: n_merges         = 50009
0.00.043.466 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.043.466 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.043.466 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.043.466 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.043.468 I print_info: LF token         = 187 ''
0.00.043.468 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.043.468 I print_info: max token length = 1024
0.00.043.469 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.646.216 I load_tensors: offloading 24 repeating layers to GPU
0.00.646.223 I load_tensors: offloading output layer to GPU
0.00.646.224 I load_tensors: offloaded 25/25 layers to GPU
0.00.646.244 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.646.245 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.647.001 I llama_init_from_model: n_seq_max     = 1
0.00.647.005 I llama_init_from_model: n_ctx         = 2048
0.00.647.006 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.647.006 I llama_init_from_model: n_batch       = 2048
0.00.647.006 I llama_init_from_model: n_ubatch      = 512
0.00.647.007 I llama_init_from_model: flash_attn    = 0
0.00.647.008 I llama_init_from_model: freq_base     = 10000.0
0.00.647.008 I llama_init_from_model: freq_scale    = 1
0.00.647.010 I ggml_metal_init: allocating
0.00.647.058 I ggml_metal_init: found device: Apple M4
0.00.647.070 I ggml_metal_init: picking default device: Apple M4
0.00.648.036 I ggml_metal_init: using embedded metal library
0.00.652.348 I ggml_metal_init: GPU name:   Apple M4
0.00.652.355 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.652.356 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.652.356 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.652.357 I ggml_metal_init: simdgroup reduction   = true
0.00.652.357 I ggml_metal_init: simdgroup matrix mul. = true
0.00.652.358 I ggml_metal_init: has residency sets    = true
0.00.652.358 I ggml_metal_init: has bfloat            = true
0.00.652.358 I ggml_metal_init: use bfloat            = true
0.00.652.359 I ggml_metal_init: hasUnifiedMemory      = true
0.00.652.361 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.664.839 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.696.376 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.696.387 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.696.406 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.700.737 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.700.740 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.700.740 I llama_init_from_model: graph nodes  = 967
0.00.700.740 I llama_init_from_model: graph splits = 2
0.00.700.745 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.700.876 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.700.877 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.765.980 I main: llama threadpool init, n_threads = 4
0.00.766.026 I 
0.00.766.044 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.766.047 I 
0.00.766.226 I sampler seed: 1234
0.00.766.230 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.766.245 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.766.245 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.766.245 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.661.944 I llama_perf_sampler_print:    sampling time =       1.46 ms /    71 runs   (    0.02 ms per token, 48797.25 tokens per second)
0.01.661.946 I llama_perf_context_print:        load time =     755.91 ms
0.01.661.947 I llama_perf_context_print: prompt eval time =      57.68 ms /     7 tokens (    8.24 ms per token,   121.35 tokens per second)
0.01.661.948 I llama_perf_context_print:        eval time =     835.40 ms /    63 runs   (   13.26 ms per token,    75.41 tokens per second)
0.01.661.948 I llama_perf_context_print:       total time =     896.74 ms /    70 tokens
0.01.662.200 I ggml_metal_free: deallocating

real	0m1.680s
user	0m0.103s
sys	0m0.166s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4864 (6ef79a67) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.963 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.134 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.140 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.142 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.148 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.148 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.148 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.149 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.149 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.150 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.150 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.150 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.151 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.151 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.151 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.153 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.154 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.154 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.983 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.002 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.856 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.857 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.858 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.858 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.858 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.859 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.859 I llama_model_loader: - type  f32:  194 tensors
0.00.024.860 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.860 I print_info: file format = GGUF V3 (latest)
0.00.024.861 I print_info: file type   = Q6_K
0.00.024.862 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.473 I load: special tokens cache size = 25
0.00.040.006 I load: token to piece cache size = 0.2984 MB
0.00.040.024 I print_info: arch             = gptneox
0.00.040.025 I print_info: vocab_only       = 0
0.00.040.025 I print_info: n_ctx_train      = 2048
0.00.040.025 I print_info: n_embd           = 2048
0.00.040.026 I print_info: n_layer          = 24
0.00.040.029 I print_info: n_head           = 16
0.00.040.030 I print_info: n_head_kv        = 16
0.00.040.030 I print_info: n_rot            = 32
0.00.040.030 I print_info: n_swa            = 0
0.00.040.031 I print_info: n_embd_head_k    = 128
0.00.040.031 I print_info: n_embd_head_v    = 128
0.00.040.031 I print_info: n_gqa            = 1
0.00.040.032 I print_info: n_embd_k_gqa     = 2048
0.00.040.032 I print_info: n_embd_v_gqa     = 2048
0.00.040.033 I print_info: f_norm_eps       = 1.0e-05
0.00.040.033 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.034 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.034 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.034 I print_info: f_logit_scale    = 0.0e+00
0.00.040.034 I print_info: n_ff             = 8192
0.00.040.035 I print_info: n_expert         = 0
0.00.040.035 I print_info: n_expert_used    = 0
0.00.040.035 I print_info: causal attn      = 1
0.00.040.035 I print_info: pooling type     = 0
0.00.040.035 I print_info: rope type        = 2
0.00.040.036 I print_info: rope scaling     = linear
0.00.040.036 I print_info: freq_base_train  = 10000.0
0.00.040.036 I print_info: freq_scale_train = 1
0.00.040.036 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.036 I print_info: rope_finetuned   = unknown
0.00.040.037 I print_info: ssm_d_conv       = 0
0.00.040.037 I print_info: ssm_d_inner      = 0
0.00.040.040 I print_info: ssm_d_state      = 0
0.00.040.040 I print_info: ssm_dt_rank      = 0
0.00.040.040 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.040 I print_info: model type       = 1.4B
0.00.040.041 I print_info: model params     = 1.41 B
0.00.040.041 I print_info: general.name     = 1.4B
0.00.040.041 I print_info: vocab type       = BPE
0.00.040.041 I print_info: n_vocab          = 50304
0.00.040.041 I print_info: n_merges         = 50009
0.00.040.042 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.042 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.042 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.042 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.042 I print_info: LF token         = 187 ''
0.00.040.043 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.043 I print_info: max token length = 1024
0.00.040.043 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.589.206 I load_tensors: offloading 24 repeating layers to GPU
0.00.589.213 I load_tensors: offloading output layer to GPU
0.00.589.214 I load_tensors: offloaded 25/25 layers to GPU
0.00.589.244 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.589.247 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.590.837 I llama_init_from_model: n_seq_max     = 1
0.00.590.839 I llama_init_from_model: n_ctx         = 128
0.00.590.840 I llama_init_from_model: n_ctx_per_seq = 128
0.00.590.840 I llama_init_from_model: n_batch       = 128
0.00.590.840 I llama_init_from_model: n_ubatch      = 128
0.00.590.841 I llama_init_from_model: flash_attn    = 0
0.00.590.842 I llama_init_from_model: freq_base     = 10000.0
0.00.590.842 I llama_init_from_model: freq_scale    = 1
0.00.590.843 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.590.845 I ggml_metal_init: allocating
0.00.590.900 I ggml_metal_init: found device: Apple M4
0.00.590.913 I ggml_metal_init: picking default device: Apple M4
0.00.592.141 I ggml_metal_init: using embedded metal library
0.00.598.127 I ggml_metal_init: GPU name:   Apple M4
0.00.598.130 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.598.131 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.598.132 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.598.132 I ggml_metal_init: simdgroup reduction   = true
0.00.598.133 I ggml_metal_init: simdgroup matrix mul. = true
0.00.598.133 I ggml_metal_init: has residency sets    = true
0.00.598.133 I ggml_metal_init: has bfloat            = true
0.00.598.133 I ggml_metal_init: use bfloat            = true
0.00.598.135 I ggml_metal_init: hasUnifiedMemory      = true
0.00.598.136 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.615.179 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.618.556 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.618.559 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.618.591 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.621.718 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.621.720 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.621.720 I llama_init_from_model: graph nodes  = 967
0.00.621.721 I llama_init_from_model: graph splits = 2
0.00.621.724 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.621.724 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.655.903 I 
0.00.655.998 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.656.006 I perplexity: tokenizing the input ..
0.00.663.068 I perplexity: tokenization took 7.058 ms
0.00.663.076 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.795.593 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.796.917 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.796.940 I llama_perf_context_print:        load time =     646.93 ms
0.00.796.943 I llama_perf_context_print: prompt eval time =     131.60 ms /   128 tokens (    1.03 ms per token,   972.61 tokens per second)
0.00.796.944 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.796.944 I llama_perf_context_print:       total time =     141.04 ms /   129 tokens
0.00.797.346 I ggml_metal_free: deallocating

real	0m0.811s
user	0m0.079s
sys	0m0.130s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4864 (6ef79a67)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x105205460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x105205b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x105206110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1052066c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x105206c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x105207220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1052077d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x105207d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x105208330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x105208830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x105208d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x105209230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x105209d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10520a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10520ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10520b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10520bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10520c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10520c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10520d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10520d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10520dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10520e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10520ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10520f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10520fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10520ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x105210660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x105210b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x105210fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x105211440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1052118e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x105211ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x105212040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1052124e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x105212980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x105212e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1052132c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x105213760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x105213c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1052140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x105214540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1052149e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x105214e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x105215320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1052157c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x105215c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x105216410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1052168b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x105216d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1052171f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x105217690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x105217b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x105217fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x105218470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x105218910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x105218db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x105219250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1052197a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x105219c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x105219f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10521a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10521a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10521ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10521b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10521b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10521bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10521bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10521c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10521c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10521cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10521d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10521d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10521dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10521e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10521e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10521ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10521f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10521f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10521fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x105220100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x105220650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x105220ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1052210f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x105221640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x105221b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1052220e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x105222630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x105222b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1052230d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x105223620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x105223b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1052240c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x105224610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x105224b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1052250b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x105225600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x105215f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x105225a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x105226220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x105226770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x105226cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x105227210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x105227760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x105227cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x105228200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x105228750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x105228ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1052291f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x105229740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x105229c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10522a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10522a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10522abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10522b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10522b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10522b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10522be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10522c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10522c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10522cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10522d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10522d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10522da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10522deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10522e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10522e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10522ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10522f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10522f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10522fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10522ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1052303b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x105230850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x105230cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x105231190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x105231630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x105231ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x105231f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x105232410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1052328b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x105232d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1052331f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x105233690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x105233b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x105233fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x105234470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x105234910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x105234db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x105235250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1052356f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x105235b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x105236030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1052364d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x105236970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x105236e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1052372b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x105237750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x105237bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x105238090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x105238530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1052389d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x105238e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x105239310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1052397b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x105239c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10523a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10523a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10523aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10523aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10523b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10523b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10523bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10523c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10523c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10523ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10523cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10523d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10523d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10523dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10523e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10523e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10523eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10523ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10523f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10523f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10523fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x105240210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1052406b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x105240b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x105240ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x105241490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x105241930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x105241e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1052423d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x105242920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x105242e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x105243310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1052437b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x105243c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1052440f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x105244590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x105244a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x105244f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x105245420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1052458c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x105245d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x105104db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x105105220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1051078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x105108420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x105108970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x105108d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x105109250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x105109800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x105109db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10510a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10510a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10510aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10510b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10510ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10510bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10510c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10510cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10510d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10510d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10510dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10510e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10510e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10510ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10510f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10510f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10510fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x105110410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1051109c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x105110f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x105111520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x105111ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x105112080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x105112630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x105112be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x105113190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x105113740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x105113cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1051142a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x105114850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x105114e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1051153b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x105115960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x105115f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1051164c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x105116a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x105117020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1051175d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x105117b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x105118130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1051186e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x105118c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x105119240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1051197f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x105119da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10511a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10511a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10511aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10511b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10511ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10511bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10511c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10511ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10511cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10511d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10511d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10511de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10511e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10511e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10511ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10511f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10511f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10511fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x105120170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x105120670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x105120b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x105121070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x105121570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x105121a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x105121f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x105122470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x105122970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x105122e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x105123370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x105123870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x105123d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x105124270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x105124c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1051253a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x105125ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1051261e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1051264a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x105126c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1051270d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x105127570 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.725.474 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.725.478 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10511abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x105113fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x105109ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10510fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x105111d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10510c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x105114b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10510f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x105115c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x105112ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10510e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10510a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x105109510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10511b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x105124530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1051117e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1051172e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x105117890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10510abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1051189a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x105116780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x105110120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x105128de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x105129500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x105129c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10512a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10512a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10512aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10512aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10512b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10512b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10512bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10510a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10512c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10512c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10512cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10512d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10512d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10512d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10512de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10512e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10512e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10512ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10512f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10512f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10512fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10512fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x105130390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x105130830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x105130cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x105131170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x105131610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x105131ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x105131f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1051323f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x105132890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x105132d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x105132ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1051334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x105133aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x105133fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1051344a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1051349a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x105134ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1051353a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1051358a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x105135da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1051362a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1051367a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x105136ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1051371a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1051376a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x105137ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1051380a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x105138650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x105138c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1051391b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x105139760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x105139d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10513a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10513a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10513ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10513b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10513b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10513bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10513c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10513ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10513d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10513d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10513dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10513e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10513e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10513ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10513f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10513f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10513fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x105140370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x105140920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x105140ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x105141480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x105141a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x105141fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x105142590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x105142b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1051430f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1051436a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x105143c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x105144200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1051447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x105144d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x105145310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1051458c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x105145e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x105146420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x105146920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x105146e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x105147320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x105147820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x105147d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x105148220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x105148720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x105148c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x105149120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x105149620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x105149b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10514a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10514a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10514aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10514af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10514b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10514b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10514be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10514c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10514c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10514cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10514d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10514d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10514dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10514e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10514e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10514eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10514f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10514f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10514fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10514ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x105150420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x105150920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x105150e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x105151320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x105151820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x105151d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x105152220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x105152720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x105152c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x105153120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x105153620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x105153b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x105154020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x105154520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x105154a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x105154f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x105155420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x105155920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x105155e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x105156320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x105156820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x105156d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x105157220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x105157720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x105157c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x105158120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x105158620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x105158b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x105159020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x105159520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x105159a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x105159f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10515a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10515a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10515ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10515b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10515b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10515bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10515c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10515c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10515cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10515d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10515d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10515db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10515e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10515e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10515ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10515ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10515f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10515f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10515ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x105160530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x105160ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x105160fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1051614e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1051619e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1051620c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x105162560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x105162820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x105162dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1051632d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1051639b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x105163e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1051642f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x105164790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x105164fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1051652a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x105165850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x105165e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1051663b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x105166960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x105166f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1051674c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x105167a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x105168020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1051685d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x105168b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x105169130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1051696e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x105169c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10516a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10516a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10516ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10516b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10516b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10516beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10516c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10516ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10516cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10516d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10516db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10516e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10516e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10516ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10516f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10516f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10516fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1051702f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1051708a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x105170e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x105171400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1051719b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x105171f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x105172510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x105172ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x105173070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x105173620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x105173bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x105174180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x105174730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x105174ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x105175290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x105175840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x105175df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1051763a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x105176950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x105176f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1051774b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x105177a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x105178010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1051785c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x105178b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x105179120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x105179620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x105179b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10517a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10517a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10517aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10517af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10517b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10517b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10517be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10517c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10517c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10517cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10517d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10517d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x10517dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x10517e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x10517e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x10517eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x10517f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x10517f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x10517fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x10517ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x105180420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x105180920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x105180e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x105181830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x105181f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x105182670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x105182d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x105183050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1051837e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x105183c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x105184120 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10a7044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10a704950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10a704dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10a705230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10a7056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10a705b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10a705f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10a7063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10a706860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10a706db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10a707220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10a7078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10a7083c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10a708b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10a709380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10a709aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10a70a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10a70a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10a70b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10a70b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10a70bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10a70c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10a70cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10a70d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10a70db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10a70de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10a70e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10a70e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10a70e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10a70ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10a70f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10a70f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10a70fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10a710490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10a710930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10a710dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10a711270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10a711710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10a711bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10a712050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10a7124f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10a712990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10a712e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10a7132d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10a713770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10a713c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10a7140b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10a714550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10a7149f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10a714e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10a715330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10a7157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10a715c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10a716110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10a7165b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10a716a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10a716ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10a7171b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10a717470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10a7178e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10a717d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10a7181c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10a718630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10a718aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10a718f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10a719380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10a7197f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10a719c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10a71a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10a71a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10a71a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10a71ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10a71b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10a71b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10a71bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10a71bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10a71c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10a71c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10a71cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10a71d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10a71d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10a71da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10a71def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10a71e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10a71e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10a71ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10a71f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10a71f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10a71f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10a71fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10a720270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10a7206e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10a720b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10a720fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10a721430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10a7218a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10a721d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10a722180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10a7228a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10a722d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10a723330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10a7238e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10a723e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10a724440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10a7249f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10a724fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10a725550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10a725b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10a7260b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10a726660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10a726c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10a7271c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10a727770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10a727d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10a728220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10a728720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10a728c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10a729120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10a729620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10a729b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10a72a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10a72a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10a72aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10a72af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10a72b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10a72b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10a72be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10a72c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10a72c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10a72cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10a72d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10a72d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10a72dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10a72e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10a72e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10a72eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10a72f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10a72f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10a72fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10a72ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10a730420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10a730920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10a730e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10a731320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10a731820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10a731d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10a732220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10a732720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10a732c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10a733120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10a733620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10a733b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10a734020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10a734520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10a734a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10a734f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10a735420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10a735920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10a735e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10a736320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10a736820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10a736d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10a737220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10a737720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10a737c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10a738120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10a738620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10a738b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10a739020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10a739520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10a739a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10a739f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10a73a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10a73a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10a73ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10a73b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10a73b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10a73bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10a73c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10a73c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10a73cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10a73d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10a73d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10a73db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10a73e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10a73e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10a73ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10a73ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10a73f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10a73f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10a73fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10a740320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10a740820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10a740d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10a7412d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10a741880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10a741e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10a7423e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10a7428e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10a742de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10a7432e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10a7439c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10a743e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10a744120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10a7446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10a744bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10a7452b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10a745750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10a745bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10a746090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10a7468e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10a746ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10a747150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10a747700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10a747cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10a748260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10a748810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10a748dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10a749370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10a749920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10a749ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10a74a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10a74aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10a74afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10a74b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10a74bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10a74c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10a74c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10a74cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10a74d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10a74d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10a74dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10a74e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10a74e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10a74ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10a74f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10a74f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10a74ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10a750530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10a750ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10a751090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10a751640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10a751bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10a7521a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10a752750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10a752d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10a7532b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10a753860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10a753e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10a7543c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10a754970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10a754f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10a7554d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10a755a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10a756030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10a7565e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10a756b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10a757140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10a7576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10a757ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10a758250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10a758800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10a758db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10a759360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10a759910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10a759ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10a75a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10a75aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10a75af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10a75b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10a75b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10a75be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10a75c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10a75c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10a75cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10a75d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10a75d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10a75dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10a75e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10a75e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10a75eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10a75f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x10a75f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x10a75fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x10a75ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x10a760420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x10a760920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x10a760e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x10a761320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x10a761820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x10a761d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x10a762220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10a762720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10a763130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10a763850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10a763f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10a764690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10a764950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10a7650e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10a765580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10a765a20 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.817s
user	0m0.281s
sys	0m0.328s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4864 (6ef79a67)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x139f0d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x139f0d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x139f0dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x139f0e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x139f0e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x139f0ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x139f0f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x139f0f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x139f0ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x139f10460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x139f10960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x139f10e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x139f11980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x139f12130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x139f12940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x139f13060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x139f13780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x139f13ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x139f145c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x139f14d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x139f154b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x139f15bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x139f162f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x139f16b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x139f172b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x139f17750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x139f17bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x139f18290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x139f18730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x139f18bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x139f18e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x139f19580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x139f19840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x139f19ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x139f1a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x139f1a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x139f1aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x139f1af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x139f1b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x139f1b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x139f1bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x139f1c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x139f1c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x139f1cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x139f1cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x139f1d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x139f1d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x139f1e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x139f1e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x139f1eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x139f1efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x139f1f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x139f1f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x139f1fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x139f20260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x139f20700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x139f20ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x139f21040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x139f21590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x139f21a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x139f21cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x139f22190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x139f22630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x139f22ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x139f22f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x139f23410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x139f238b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x139f23d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x139f241f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x139f24690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x139f24b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x139f24fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x139f25470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x139f259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x139f25f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x139f26460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x139f269b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x139f26f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x139f27450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x139f279a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x139f27ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x139f28440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x139f28990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x139f28ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x139f29430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x139f29980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x139f29ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x139f2a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x139f2a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x139f2aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x139f2b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x139f2b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x139f2beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x139f2c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x139f2c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x139f2cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x139f2d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x139f1dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x139f2d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x139f2e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x139f2e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x139f2eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x139f2f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x139f2f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x139f2faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x139f2fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x139f30540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x139f30a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x139f30fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x139f31530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x139f31a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x139f31fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x139f32520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x139f329c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x139f32e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x139f33300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x139f337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x139f33c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x139f340e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x139f34580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x139f34a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x139f34ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x139f35360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x139f35800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x139f35ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x139f36140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x139f365e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x139f36a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x139f36f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x139f373c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x139f37860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x139f37d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x139f381a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x139f38640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x139f38ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x139f38f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x139f39420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x139f398c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x139f39d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x139f3a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x139f3a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x139f3ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x139f3afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x139f3b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x139f3b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x139f3bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x139f3c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x139f3c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x139f3cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x139f3d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x139f3d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x139f3d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x139f3de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x139f3e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x139f3e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x139f3ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x139f3f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x139f3f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x139f3f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x139f3fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x139f40320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x139f407c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x139f40c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x139f41100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x139f415a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x139f41a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x139f41ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x139f42380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x139f42820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x139f42cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x139f43160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x139f43600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x139f43aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x139f43f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x139f443e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x139f44880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x139f44d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x139f451c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x139f45660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x139f45b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x139f45fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x139f46440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x139f468e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x139f46d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x139f47220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x139f476c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x139f47b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x139f48000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x139f484a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x139f48940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x139f48de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x139f49280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x139f49720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x139f49c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x139f4a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x139f4a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x139f4ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x139f4b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x139f4b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x139f4ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x139f4bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x139f4c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x139f4c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x139f4cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x139f4d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x139f4d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x139f4db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x139f4dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x139f4e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x139f4e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x139f4f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x139f4f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x139f4f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x139f4ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x139f504c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x139f50a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x139f51020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x139f515d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x139f51b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x139f52130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x139f526e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x139f52c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x139f53240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x139f537f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x139f53da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x139f54350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x139f54900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x139f54eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x139f55460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x139f55a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x139f55fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x139f56570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x139f56b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x139f570d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x139f57680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x139f57c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x139f581e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x139f58790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x139f58d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x139f592f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x139f598a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x139f59e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x139f5a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x139f5a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x139f5af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x139f5b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x139f5bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x139f5c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x139f5c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x139f5cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x139f5d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x139f5d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x139f5dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x139f5e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x139f5e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x139f5edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x139f5f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x139f5f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x139f5ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x139f604b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x139f60a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x139f61010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x139f615c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x139f61b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x139f62120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x139f626d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x139f62c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x139f63230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x139f63730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x139f63c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x139f64130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x139f64630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x139f64b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x139f65030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x139f65530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x139f65a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x139f65f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x139f66430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x139f66930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x139f66e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x139f67330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x139f67830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x139f67d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x139f68230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x139f68730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x139f68c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x139f69130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x139f69630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x139f69b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x139f6a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x139f6a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x139f6aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x139f6af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x139f6b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x139f6c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x139f6c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x139f6cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x139f6d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x139f6d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x139f6dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x139f6e230 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.103.500 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.103.505 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15e904bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15e905040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15e9054b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15e905920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15e905d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15e906200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15e906670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15e906ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15e906f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15e9073c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15e907830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15e907f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15e908a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15e9091f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15e909a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15e90a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15e90a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15e90af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15e90b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15e90bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15e90c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15e90cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15e90d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15e90da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15e90e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15e90e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15e90e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15e90eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15e90efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15e90f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15e90f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15e90fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15e9105d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15e910a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15e910f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15e9113b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15e911850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15e911cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15e912190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15e912630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15e912ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15e912f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15e913410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15e9138b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15e913d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15e9141f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15e914690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15e914b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15e914fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15e915470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15e915910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15e915db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15e916250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15e9166f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15e916b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15e917030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15e9174d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15e917790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15e917a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15e917ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15e918330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15e9187a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15e918c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15e919080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15e9194f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15e919960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15e919dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15e91a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13b806130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13b807110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13b807580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13b807b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13b808030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13b808540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13b808b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13b8090c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13b809680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13b809c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13b80a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13b80a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13b80ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13b80b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13b80b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13b80bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13b80c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13b80ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13b80d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13b80d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13b80db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13b80e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13b80e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13b80ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13b80f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13b80f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13b80fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13b8103c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13b810980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13b810f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13b811500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13b811ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13b812080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13b812640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13b812c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13b8131c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13b813780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13b813d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13b814300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13b8148c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13b814e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13b815440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13b815a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13b815fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13b816580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13b816b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13b817050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13b817560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13b817a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13b817f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13b818490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13b8189a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13b818eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13b8193c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13b8198d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13b819de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13b81a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13b81a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13b81ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13b81b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13b81b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13b81bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13b81c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13b81c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13b81cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13b81d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13b81d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13b81daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13b81dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13b81e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13b81e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13b81eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13b81f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13b81f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13b81fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13b820320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13b820830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13b820d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13b821250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13b821760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13b821c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13b822180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13b822680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13b822b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13b823090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13b8235a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13b823ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13b823fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13b8244d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13b8249e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13b824ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13b825400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13b825910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13b825e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13b826330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13b826840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13b826d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13b827260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13b827770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13b827c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13b828190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13b8286a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13b828bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13b8290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13b8295d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13b829ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13b829ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13b82a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13b82aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13b82af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13b82b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13b82b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13b82be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13b82c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13b82c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13b82cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13b82d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13b82d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13b82dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13b82e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13b82e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13b82ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13b82f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13b82f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13b82fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13b830020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13b8305d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13b830b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13b831130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13b8316e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13b831be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13b8320e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13b8325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13b832ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13b832ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13b833500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13b833ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13b833fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13b8344c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13b834bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13b834e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13b835380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13b835e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13b8360e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13b8366a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13b836c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13b837220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13b8377e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13b837da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13b838360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13b838920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13b838ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13b8394a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13b839a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13b83a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13b83a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13b83aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13b83b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13b83b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13b83bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13b83c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13b83c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13b83ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13b83d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13b83d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13b83df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13b83e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13b83eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13b83f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13b83f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13b83fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13b8401e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13b8407a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13b840d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13b841320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13b8418e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13b841ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13b842460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13b842a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13b842fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13b8435a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13b843b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13b844120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13b8446e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13b844ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13b845260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13b845820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13b845de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13b8463a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13b846960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13b846f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13b8474e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13b847aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13b848060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13b848620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13b848be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13b8491a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13b849760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13b849d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13b84a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13b84a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13b84ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13b84b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13b84b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13b84bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13b84c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13b84c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13b84cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13b84cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13b84d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13b84d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13b84dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13b84e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13b84e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x13b84ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x13b84f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x13b84f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x13b84fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x13b8501e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x13b8506e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x13b850be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x13b8510e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x13b8515e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x13b851ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13b851fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13b8529f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13b853110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13b853830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13b853f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13b854210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13b8549a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13b854c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13b855170 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x139e063e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x139e06850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x139e06cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x139e07130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x139e075a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x139e07a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x139e07e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x139e082f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x139e08760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x139e08bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x139e09040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x139e09720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x139e0a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x139e0a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x139e0b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x139e0b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x139e0c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x139e0c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x139e0ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x139e0d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x139e0dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x139e0e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x139e0ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x139e0f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x139e0f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x139e0fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x139e0ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x139e103e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x139e10850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x139e10cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x139e11280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x139e11790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x139e11e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x139e12310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x139e127b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x139e12c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x139e130f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x139e13590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x139e13a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x139e13ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x139e14370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x139e14810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x139e14cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x139e15150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x139e155f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x139e15a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x139e15f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x139e163d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x139e16870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x139e16d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x139e171b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x139e17650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x139e17af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x139e17f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x139e18430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x139e188d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x139e18d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x139e19030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x139e192f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x139e19760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x139e19bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x139e1a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x139e1a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x139e1a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x139e1ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x139e1b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x139e1b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x139e1bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x139e1bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x139e1c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x139e1c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x139e1cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x139e1d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x139e1d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x139e1d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x139e1de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x139e1e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x139e1e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x139e1ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x139e1f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x139e1f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x139e1f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x139e1fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x139e201e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x139e20650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x139e20ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x139e20f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x139e213a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x139e21810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x139e21c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x139e220f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x139e22560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x139e229d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x139e22e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x139e232b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x139e23720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x139e23b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x139e24000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x139e24720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x139e24c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x139e251b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x139e25760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x139e25d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x139e262c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x139e26870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x139e26e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x139e273d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x139e27980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x139e27f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x139e284e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x139e28a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x139e29040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x139e295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x139e29ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x139e2a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x139e2a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x139e2aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x139e2afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x139e2b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x139e2b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x139e2bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x139e2c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x139e2c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x139e2cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x139e2d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x139e2d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x139e2dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x139e2e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x139e2e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x139e2eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x139e2f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x139e2f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x139e2faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x139e2ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x139e304a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x139e309a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x139e30ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x139e313a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x139e318a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x139e31da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x139e322a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x139e327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x139e32ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x139e331a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x139e336a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x139e33ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x139e340a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x139e345a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x139e34aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x139e34fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x139e354a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x139e359a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x139e35ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x139e363a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x139e368a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x139e36da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x139e372a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x139e377a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x139e37ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x139e381a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x139e386a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x139e38ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x139e390a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x139e395a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x139e39aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x139e39fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x139e3a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x139e3a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x139e3aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x139e3b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x139e3b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x139e3bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x139e3c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x139e3c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x139e3cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x139e3d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x139e3d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x139e3dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x139e3e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x139e3e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x139e3eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x139e3efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x139e3f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x139e3f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x139e3fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x139e403a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x139e408a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x139e40da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x139e412a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x139e417a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x139e41ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x139e421a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x139e426a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x139e42ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x139e43150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x139e43700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x139e43cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x139e44260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x139e44760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x139e44c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x139e45160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x139e45840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x139e45ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x139e45fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x139e46550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x139e46a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x139e47130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x139e475d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x139e47a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x139e47f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x139e48760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x139e48a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x139e48fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x139e49580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x139e49b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x139e4a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x139e4a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x139e4ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x139e4b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x139e4b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x139e4bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x139e4c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x139e4c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x139e4ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x139e4d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x139e4d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x139e4df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x139e4e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x139e4ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x139e4f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x139e4f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x139e4fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x139e50190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x139e50740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x139e50cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x139e512a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x139e51850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x139e51e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x139e523b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x139e52960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x139e52f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x139e534c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x139e53a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x139e54020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x139e545d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x139e54b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x139e55130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x139e556e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x139e55c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x139e56240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x139e567f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x139e56da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x139e57350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x139e57900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x139e57eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x139e58460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x139e58a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x139e58fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x139e59570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x139e59b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x139e5a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x139e5a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x139e5ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x139e5b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x139e5b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x139e5bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x139e5c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x139e5c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x139e5cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x139e5d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x139e5d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x139e5dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x139e5e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x139e5e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x139e5eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x139e5f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x139e5f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x139e5faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x139e5ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x139e604a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x139e609a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x139e60ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x139e613a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x139e618a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x139e61da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x139e622a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x139e627a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x139e62ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x139e631a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x139e636a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x139e63ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x139e640a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x139e645a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x139e64fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x139e656d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x139e65df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x139e66510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x139e667d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x139e66f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x139e67400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x139e678a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.975s
user	0m0.234s
sys	0m0.200s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.43 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.42 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.85 sec*proc (2 tests)

Total Test time (real) =   1.87 sec
        1.89 real         0.51 user         0.23 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.22 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.30 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.52 sec*proc (2 tests)

Total Test time (real) =   0.54 sec
        0.54 real         0.12 user         0.08 sys
```
