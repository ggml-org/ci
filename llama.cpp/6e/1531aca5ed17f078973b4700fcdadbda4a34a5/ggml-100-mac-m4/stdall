Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu
Requirement already satisfied: numpy~=1.26.4 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from -r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 1)) (1.26.4)
Requirement already satisfied: sentencepiece~=0.2.0 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from -r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 2)) (0.2.0)
Requirement already satisfied: transformers<5.0.0,>=4.45.1 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from -r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (4.46.3)
Requirement already satisfied: gguf>=0.1.0 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from -r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 4)) (0.13.0)
Requirement already satisfied: protobuf<5.0.0,>=4.21.0 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from -r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 5)) (4.25.5)
Requirement already satisfied: torch~=2.2.1 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from -r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2.2.2)
Requirement already satisfied: filelock in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (3.16.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (0.26.2)
Requirement already satisfied: packaging>=20.0 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (24.2)
Requirement already satisfied: pyyaml>=5.1 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2024.11.6)
Requirement already satisfied: requests in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2.32.3)
Requirement already satisfied: tokenizers<0.21,>=0.20 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (0.20.3)
Requirement already satisfied: safetensors>=0.4.1 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (0.4.5)
Requirement already satisfied: tqdm>=4.27 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (4.67.0)
Requirement already satisfied: typing-extensions>=4.8.0 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from torch~=2.2.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (4.12.2)
Requirement already satisfied: sympy in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from torch~=2.2.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.13.3)
Requirement already satisfied: networkx in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from torch~=2.2.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.4.2)
Requirement already satisfied: jinja2 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from torch~=2.2.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.1.4)
Requirement already satisfied: fsspec in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from torch~=2.2.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2024.10.0)
Requirement already satisfied: MarkupSafe>=2.0 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from jinja2->torch~=2.2.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.0.2)
Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (3.4.0)
Requirement already satisfied: idna<4,>=2.5 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2.2.3)
Requirement already satisfied: certifi>=2017.4.17 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2024.8.30)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from sympy->torch~=2.2.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.3.0)
Obtaining file:///Users/ggml/work/llama.cpp/gguf-py
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Checking if build backend supports build_editable: started
  Checking if build backend supports build_editable: finished with status 'done'
  Getting requirements to build editable: started
  Getting requirements to build editable: finished with status 'done'
  Preparing editable metadata (pyproject.toml): started
  Preparing editable metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: numpy>=1.17 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from gguf==0.13.0) (1.26.4)
Requirement already satisfied: pyyaml>=5.1 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from gguf==0.13.0) (6.0.2)
Requirement already satisfied: sentencepiece<=0.2.0,>=0.1.98 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from gguf==0.13.0) (0.2.0)
Requirement already satisfied: tqdm>=4.27 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from gguf==0.13.0) (4.67.0)
Building wheels for collected packages: gguf
  Building editable for gguf (pyproject.toml): started
  Building editable for gguf (pyproject.toml): finished with status 'done'
  Created wheel for gguf: filename=gguf-0.13.0-py3-none-any.whl size=3397 sha256=d56c772ff3e56e809af65837b3afbb9e8186c3d46d9f9803440b5fae6acdd9d0
  Stored in directory: /private/var/folders/3z/y7wjb5257l1dj_dxrng2zq0h0000gn/T/pip-ephem-wheel-cache-kmol8pr9/wheels/57/e8/ad/c90c2fcd445a1780e6969131e27698e4f239a16d34ccd95852
Successfully built gguf
Installing collected packages: gguf
  Attempting uninstall: gguf
    Found existing installation: gguf 0.13.0
    Uninstalling gguf-0.13.0:
      Successfully uninstalled gguf-0.13.0
Successfully installed gguf-0.13.0
+ gg_run_ctest_debug
+ cd /Users/ggml/work/llama.cpp
+ rm -rf build-ci-debug
+ tee /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/ctest_debug.log
+ mkdir build-ci-debug
+ cd build-ci-debug
+ set -e
+ gg_check_build_requirements
+ command -v cmake
+ command -v make
+ command -v ctest
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/ctest_debug-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Debug -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:299 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.8s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-debug

real	0m3.078s
user	0m0.875s
sys	0m1.377s
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/ctest_debug-make.log
++ nproc
+ make -j10
[  0%] Generating build details from Git
[  2%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
[  5%] Built target sha1
[  5%] Built target sha256
[  5%] Built target xxhash
[  6%] Linking CXX shared library libggml-base.dylib
[  7%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  7%] Built target ggml-base
[  8%] Generate assembly for embedded Metal library
Embedding Metal library
[  8%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 13%] Built target build_info
[ 14%] Linking CXX shared library libggml-blas.dylib
[ 14%] Linking CXX shared library libggml-cpu.dylib
[ 14%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 15%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 15%] Built target ggml-blas
[ 15%] Built target ggml-cpu
[ 15%] Linking C shared library libggml-metal.dylib
[ 15%] Built target ggml-metal
[ 16%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 16%] Linking CXX shared library libggml.dylib
[ 16%] Built target ggml
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 19%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 19%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Linking CXX shared library libllama.dylib
[ 22%] Built target llama-gguf
[ 22%] Built target llama-gguf-hash
[ 22%] Built target llama
[ 23%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 24%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 24%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 28%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 28%] Linking CXX executable ../../bin/llama-simple
[ 28%] Linking CXX executable ../../bin/llama-quantize-stats
[ 28%] Linking CXX executable ../../bin/llama-simple-chat
[ 29%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 29%] Linking C executable ../bin/test-c
[ 29%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 31%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 31%] Built target llava
[ 31%] Built target llama-simple
[ 31%] Built target test-c
[ 31%] Built target llama-quantize-stats
[ 31%] Built target llama-simple-chat
[ 31%] Linking CXX shared library libllava_shared.dylib
[ 32%] Linking CXX static library libllava_static.a
[ 32%] Built target llava_shared
[ 32%] Built target llava_static
[ 32%] Linking CXX static library libcommon.a
[ 32%] Built target common
[ 32%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 39%] Linking CXX executable ../bin/test-tokenizer-0
[ 39%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 39%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 40%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-llama-grammar
[ 44%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 45%] Linking CXX executable ../bin/test-grammar-integration
[ 46%] Linking CXX executable ../bin/test-sampling
[ 46%] Linking CXX executable ../bin/test-arg-parser
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-log
[ 46%] Built target test-tokenizer-1-spm
[ 46%] Built target test-tokenizer-1-bpe
[ 46%] Built target test-tokenizer-0
[ 46%] Built target test-json-schema-to-grammar
[ 46%] Built target test-llama-grammar
[ 46%] Built target test-sampling
[ 46%] Built target test-grammar-integration
[ 46%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 47%] Built target test-log
[ 47%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 47%] Built target test-grammar-parser
[ 47%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 48%] Built target test-arg-parser
[ 49%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 52%] Linking CXX executable ../bin/test-chat-template
[ 53%] Linking CXX executable ../bin/test-backend-ops
[ 53%] Linking CXX executable ../bin/test-gguf
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-model-load-cancel
[ 58%] Linking CXX executable ../bin/test-autorelease
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 58%] Linking CXX executable ../bin/test-quantize-fns
[ 58%] Linking CXX executable ../bin/test-barrier
[ 58%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 59%] Linking CXX executable ../bin/test-quantize-perf
[ 59%] Built target test-gguf
[ 59%] Built target test-chat-template
[ 59%] Built target test-backend-ops
[ 60%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 60%] Built target test-model-load-cancel
[ 60%] Built target test-autorelease
[ 61%] Linking CXX executable ../../bin/llama-batched-bench
[ 61%] Built target test-quantize-fns
[ 62%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 63%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-barrier
[ 64%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 65%] Built target test-quantize-perf
[ 65%] Linking CXX executable ../../bin/llama-embedding
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 66%] Linking CXX executable ../../bin/llama-eval-callback
[ 67%] Linking CXX executable ../../bin/llama-batched
[ 67%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 67%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-gguf-split
[ 67%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-gritlm
[ 68%] Built target test-rope
[ 68%] Built target llama-batched-bench
[ 69%] Linking CXX executable ../../bin/llama-imatrix
[ 70%] Linking CXX executable ../../bin/llama-infill
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Built target llama-embedding
[ 71%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 71%] Built target llama-batched
[ 71%] Built target llama-gbnf-validator
[ 71%] Built target llama-eval-callback
[ 71%] Built target llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-bench
[ 71%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 71%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-lookahead
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 72%] Built target llama-imatrix
[ 72%] Built target llama-gritlm
[ 73%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 74%] Linking CXX executable ../../bin/llama-lookup
[ 75%] Linking CXX executable ../../bin/llama-lookup-create
[ 76%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 77%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 77%] Built target llama-infill
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-cli
[ 77%] Built target llama-bench
[ 78%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 81%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Built target llama-lookup
[ 81%] Built target llama-lookup-merge
[ 81%] Built target llama-cli
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Built target llama-lookup-stats
[ 81%] Built target llama-lookahead
[ 81%] Generating loading.html.hpp
[ 82%] Generating index.html.gz.hpp
[ 83%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Built target llama-lookup-create
[ 83%] Built target llama-parallel
[ 83%] Built target llama-passkey
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-retrieval
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Built target llama-perplexity
[ 87%] Linking CXX executable ../../bin/llama-speculative-simple
[ 87%] Built target llama-quantize
[ 88%] Linking CXX executable ../../bin/llama-speculative
[ 89%] Linking CXX executable ../../bin/llama-tts
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 91%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 92%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 92%] Built target llama-retrieval
[ 92%] Linking CXX executable ../../bin/llama-gen-docs
[ 92%] Built target llama-save-load-state
[ 92%] Built target llama-speculative-simple
[ 92%] Built target llama-speculative
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Built target llama-tokenize
[ 93%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Built target llama-tts
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 94%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-export-lora
[ 95%] Built target llama-gen-docs
[ 95%] Built target llama-convert-llama2c-to-ggml
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 97%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 99%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-llava-cli
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server
[100%] Linking CXX executable ../../bin/llama-run
[100%] Built target llama-run

real	0m4.192s
user	0m8.708s
sys	0m9.080s
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/ctest_debug-ctest.log
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.18 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.77 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.68 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.32 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.44 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.98 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.32 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.33 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.12 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.21 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.02 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.21 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.26 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.18 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.42 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  178.61 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.91 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   26.04 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.37 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 221.29 sec*proc (28 tests)

Total Test time (real) = 221.30 sec

real	3m41.333s
user	7m40.091s
sys	0m6.822s
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_ctest_release
+ cd /Users/ggml/work/llama.cpp
+ rm -rf build-ci-release
+ tee /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/ctest_release.log
+ mkdir build-ci-release
+ cd build-ci-release
+ set -e
+ gg_check_build_requirements
+ command -v cmake
+ command -v make
+ command -v ctest
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/ctest_release-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:299 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.535s
user	0m0.874s
sys	0m1.239s
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/ctest_release-make.log
++ nproc
+ make -j10
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  6%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  6%] Built target sha1
[  6%] Built target xxhash
[  6%] Built target sha256
[  7%] Linking CXX shared library libggml-base.dylib
[  7%] Built target build_info
[  7%] Built target ggml-base
[  8%] Generate assembly for embedded Metal library
Embedding Metal library
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  9%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 12%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 14%] Linking CXX shared library libggml-blas.dylib
[ 14%] Linking CXX shared library libggml-cpu.dylib
[ 14%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 15%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 15%] Built target ggml-blas
[ 15%] Built target ggml-cpu
[ 15%] Linking C shared library libggml-metal.dylib
[ 15%] Built target ggml-metal
[ 16%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 16%] Linking CXX shared library libggml.dylib
[ 16%] Built target ggml
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 17%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 19%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Linking CXX shared library libllama.dylib
[ 22%] Built target llama-gguf-hash
[ 22%] Built target llama-gguf
[ 22%] Built target llama
[ 22%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 22%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 23%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 24%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 24%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 26%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 28%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 29%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 29%] Linking C executable ../bin/test-c
[ 29%] Linking CXX executable ../../bin/llama-quantize-stats
[ 31%] Built target llava
[ 31%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-simple
[ 31%] Linking CXX executable ../../bin/llama-simple-chat
[ 32%] Linking CXX static library libllava_static.a
[ 32%] Linking CXX shared library libllava_shared.dylib
[ 32%] Built target llama-quantize-stats
[ 32%] Built target test-c
[ 32%] Built target llama-simple-chat
[ 32%] Built target llama-simple
[ 32%] Built target llava_static
[ 32%] Built target llava_shared
[ 32%] Linking CXX static library libcommon.a
[ 32%] Built target common
[ 33%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 37%] Linking CXX executable ../bin/test-tokenizer-0
[ 38%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 40%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 41%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 41%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-log
[ 43%] Linking CXX executable ../bin/test-llama-grammar
[ 44%] Linking CXX executable ../bin/test-grammar-integration
[ 45%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 45%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-sampling
[ 46%] Linking CXX executable ../bin/test-arg-parser
[ 46%] Built target test-tokenizer-0
[ 46%] Built target test-tokenizer-1-spm
[ 46%] Built target test-tokenizer-1-bpe
[ 46%] Built target test-llama-grammar
[ 46%] Built target test-log
[ 46%] Built target test-grammar-integration
[ 47%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 47%] Built target test-sampling
[ 47%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 47%] Built target test-json-schema-to-grammar
[ 47%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 47%] Built target test-grammar-parser
[ 47%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 47%] Built target test-arg-parser
[ 48%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 50%] Linking CXX executable ../bin/test-chat-template
[ 50%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 53%] Linking CXX executable ../bin/test-gguf
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 55%] Linking CXX executable ../bin/test-backend-ops
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-model-load-cancel
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-quantize-perf
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 59%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 59%] Linking CXX executable ../bin/test-quantize-fns
[ 59%] Built target test-chat-template
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 60%] Linking CXX executable ../bin/test-barrier
[ 60%] Linking CXX executable ../bin/test-rope
[ 60%] Built target test-backend-ops
[ 60%] Built target test-gguf
[ 60%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 61%] Linking CXX executable ../../bin/llama-batched-bench
[ 61%] Built target test-quantize-perf
[ 61%] Built target test-model-load-cancel
[ 61%] Built target test-autorelease
[ 61%] Built target test-quantize-fns
[ 62%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 63%] Linking CXX executable ../../bin/llama-batched
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 64%] Built target test-rope
[ 64%] Built target test-barrier
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 67%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-embedding
[ 67%] Built target llama-batched-bench
[ 67%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 68%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-gguf-split
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Linking CXX executable ../../bin/llama-imatrix
[ 70%] Built target llama-batched
[ 70%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-bench
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Built target llama-eval-callback
[ 71%] Built target llama-embedding
[ 72%] Linking CXX executable ../../bin/llama-lookahead
[ 72%] Built target llama-gguf-split
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 72%] Built target llama-gbnf-validator
[ 72%] Built target llama-gritlm
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 72%] Built target llama-imatrix
[ 72%] Built target llama-infill
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 72%] Built target llama-bench
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 73%] Linking CXX executable ../../bin/llama-lookup
[ 73%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 74%] Linking CXX executable ../../bin/llama-lookup-create
[ 74%] Built target llama-lookahead
[ 75%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 75%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 76%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Linking CXX executable ../../bin/llama-cli
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 82%] Linking CXX executable ../../bin/llama-passkey
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 82%] Built target llama-lookup
[ 82%] Built target llama-lookup-merge
[ 82%] Built target llama-lookup-create
[ 82%] Built target llama-lookup-stats
[ 82%] Generating loading.html.hpp
[ 82%] Linking CXX executable ../../bin/llama-retrieval
[ 82%] Built target llama-parallel
[ 83%] Generating index.html.gz.hpp
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Built target llama-perplexity
[ 86%] Built target llama-passkey
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Built target llama-quantize
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Built target llama-retrieval
[ 86%] Built target llama-cli
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 87%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 87%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-save-load-state
[ 88%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-speculative-simple
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-tokenize
[ 92%] Linking CXX executable ../../bin/llama-tts
[ 93%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Linking CXX executable ../../bin/llama-gen-docs
[ 93%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Built target llama-save-load-state
[ 93%] Built target llama-speculative
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-speculative-simple
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Built target llama-tts
[ 94%] Built target llama-tokenize
[ 94%] Built target llama-gen-docs
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Built target llama-convert-llama2c-to-ggml
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Built target llama-cvector-generator
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 98%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-vdot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server
[100%] Linking CXX executable ../../bin/llama-run
[100%] Built target llama-run

real	0m4.317s
user	0m7.805s
sys	0m8.986s
+ '[' -z ']'
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/ctest_release-ctest.log
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.16 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.31 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.94 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.17 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.14 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.33 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.34 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.37 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.07 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.22 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  51.33 sec*proc (28 tests)

Total Test time (real) =  51.34 sec

real	0m51.349s
user	1m11.664s
sys	0m5.664s
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_embd_bge_small
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
+ tee /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/embd_bge_small.log
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2024-12-30 16:51:05 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json [743/743] -> "config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer.json
Last-modified header missing -- time-stamps turned off.
2024-12-30 16:51:06 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer.json [711396/711396] -> "tokenizer.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json
Last-modified header missing -- time-stamps turned off.
2024-12-30 16:51:07 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json [366/366] -> "tokenizer_config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json
Last-modified header missing -- time-stamps turned off.
2024-12-30 16:51:07 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json [125/125] -> "special_tokens_map.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/pytorch_model.bin
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/pytorch_model.bin
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/pytorch_model.bin
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json
Last-modified header missing -- time-stamps turned off.
2024-12-30 16:51:08 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json [52/52] -> "sentence_bert_config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt
Last-modified header missing -- time-stamps turned off.
2024-12-30 16:51:08 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt [231508/231508] -> "vocab.txt" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json
Last-modified header missing -- time-stamps turned off.
2024-12-30 16:51:09 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json [349/349] -> "modules.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2024-12-30 16:51:09 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json [743/743] -> "config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/1_Pooling https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json
+ local out=models-mnt/bge-small/1_Pooling
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/1_Pooling
+ cd models-mnt/bge-small/1_Pooling
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json
Last-modified header missing -- time-stamps turned off.
2024-12-30 16:51:10 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json [190/190] -> "config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ path_models=../models-mnt/bge-small
+ rm -rf build-ci-release
+ mkdir build-ci-release
+ cd build-ci-release
+ set -e
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/embd_bge_small-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:299 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.518s
user	0m0.885s
sys	0m1.216s
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/embd_bge_small-make.log
++ nproc
+ make -j10
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  3%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  6%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  6%] Built target sha256
[  6%] Built target sha1
[  6%] Built target build_info
[  7%] Linking CXX shared library libggml-base.dylib
[  7%] Built target xxhash
[  7%] Built target ggml-base
[  8%] Generate assembly for embedded Metal library
Embedding Metal library
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 11%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 14%] Linking CXX shared library libggml-blas.dylib
[ 14%] Linking CXX shared library libggml-cpu.dylib
[ 14%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 15%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 15%] Built target ggml-blas
[ 15%] Built target ggml-cpu
[ 15%] Linking C shared library libggml-metal.dylib
[ 15%] Built target ggml-metal
[ 16%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 16%] Linking CXX shared library libggml.dylib
[ 16%] Built target ggml
[ 16%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 20%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Linking CXX shared library libllama.dylib
[ 22%] Built target llama-gguf
[ 22%] Built target llama
[ 22%] Built target llama-gguf-hash
[ 23%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 23%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 24%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 27%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 28%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 28%] Linking C executable ../bin/test-c
[ 28%] Linking CXX executable ../../bin/llama-simple
[ 29%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 30%] Linking CXX executable ../../bin/llama-quantize-stats
[ 30%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-simple-chat
[ 31%] Built target llava
[ 31%] Linking CXX static library libcommon.a
[ 32%] Linking CXX static library libllava_static.a
[ 32%] Built target llama-simple
[ 32%] Built target llama-simple-chat
[ 32%] Linking CXX shared library libllava_shared.dylib
[ 32%] Built target test-c
[ 32%] Built target llama-quantize-stats
[ 32%] Built target common
[ 32%] Built target llava_static
[ 32%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 35%] Built target llava_shared
[ 36%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 39%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 39%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 40%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 41%] Linking CXX executable ../bin/test-tokenizer-0
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-grammar-parser
[ 43%] Linking CXX executable ../bin/test-sampling
[ 44%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 44%] Linking CXX executable ../bin/test-llama-grammar
[ 45%] Linking CXX executable ../bin/test-grammar-integration
[ 45%] Linking CXX executable ../bin/test-log
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 46%] Built target test-tokenizer-1-bpe
[ 46%] Built target test-tokenizer-1-spm
[ 46%] Built target test-tokenizer-0
[ 46%] Linking CXX executable ../bin/test-arg-parser
[ 46%] Built target test-sampling
[ 46%] Built target test-grammar-parser
[ 46%] Built target test-llama-grammar
[ 46%] Built target test-grammar-integration
[ 47%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 47%] Built target test-json-schema-to-grammar
[ 47%] Built target test-log
[ 47%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 51%] Linking CXX executable ../bin/test-gguf
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 53%] Linking CXX executable ../bin/test-chat-template
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 55%] Linking CXX executable ../bin/test-barrier
[ 56%] Linking CXX executable ../bin/test-backend-ops
[ 56%] Built target test-arg-parser
[ 57%] Linking CXX executable ../bin/test-model-load-cancel
[ 57%] Linking CXX executable ../bin/test-quantize-fns
[ 58%] Linking CXX executable ../bin/test-autorelease
[ 58%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 58%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 58%] Built target test-gguf
[ 58%] Built target test-chat-template
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 59%] Built target test-barrier
[ 60%] Linking CXX executable ../bin/test-quantize-perf
[ 61%] Linking CXX executable ../../bin/llama-batched-bench
[ 61%] Built target test-backend-ops
[ 62%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 62%] Built target test-quantize-fns
[ 62%] Linking CXX executable ../bin/test-rope
[ 62%] Built target test-model-load-cancel
[ 62%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 63%] Built target test-autorelease
[ 64%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 65%] Linking CXX executable ../../bin/llama-eval-callback
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 66%] Linking CXX executable ../../bin/llama-embedding
[ 67%] Linking CXX executable ../../bin/llama-batched
[ 67%] Built target test-quantize-perf
[ 67%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 67%] Built target test-rope
[ 67%] Built target llama-batched-bench
[ 67%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 68%] Linking CXX executable ../../bin/llama-gguf-split
[ 68%] Linking CXX executable ../../bin/llama-gritlm
[ 69%] Linking CXX executable ../../bin/llama-imatrix
[ 69%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 70%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 70%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 70%] Built target llama-eval-callback
[ 70%] Built target llama-embedding
[ 70%] Built target llama-batched
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Built target llama-gbnf-validator
[ 71%] Linking CXX executable ../../bin/llama-bench
[ 71%] Built target llama-gritlm
[ 71%] Built target llama-gguf-split
[ 72%] Linking CXX executable ../../bin/llama-lookahead
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 72%] Built target llama-imatrix
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 72%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 73%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 74%] Linking CXX executable ../../bin/llama-lookup-create
[ 75%] Linking CXX executable ../../bin/llama-lookup
[ 76%] Linking CXX executable ../../bin/llama-lookup-stats
[ 76%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 76%] Built target llama-bench
[ 76%] Built target llama-infill
[ 77%] Linking CXX executable ../../bin/llama-parallel
[ 78%] Linking CXX executable ../../bin/llama-lookup-merge
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 78%] Built target llama-lookahead
[ 79%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 82%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 82%] Built target llama-lookup-stats
[ 82%] Built target llama-lookup
[ 82%] Built target llama-lookup-create
[ 82%] Built target llama-lookup-merge
[ 82%] Generating loading.html.hpp
[ 82%] Built target llama-parallel
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Generating index.html.gz.hpp
[ 83%] Built target llama-cli
[ 83%] Linking CXX executable ../../bin/llama-perplexity
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Built target llama-passkey
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-run
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Built target llama-retrieval
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 87%] Built target llama-perplexity
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Built target llama-quantize
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 92%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 92%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 93%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Built target llama-save-load-state
[ 93%] Built target llama-speculative
[ 93%] Built target llama-speculative-simple
[ 93%] Built target llama-run
[ 93%] Linking CXX executable ../../bin/llama-gen-docs
[ 93%] Built target llama-tokenize
[ 93%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 95%] Built target llama-tts
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 98%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 98%] Built target llama-convert-llama2c-to-ggml
[ 98%] Built target llama-gen-docs
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.857s
user	0m5.874s
sys	0m9.192s
+ python3 ../convert_hf_to_gguf.py ../models-mnt/bge-small --outfile ../models-mnt/bge-small/ggml-model-f16.gguf
INFO:hf-to-gguf:Loading model: bge-small
INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only
INFO:hf-to-gguf:Exporting model...
INFO:hf-to-gguf:gguf: loading model part 'pytorch_model.bin'
INFO:hf-to-gguf:token_embd.weight,               torch.float32 --> F16, shape = {384, 30522}
INFO:hf-to-gguf:position_embd.weight,            torch.float32 --> F32, shape = {384, 512}
INFO:hf-to-gguf:token_types.weight,              torch.float32 --> F32, shape = {384, 2}
INFO:hf-to-gguf:token_embd_norm.weight,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:token_embd_norm.bias,            torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.0.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.0.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.0.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.0.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.0.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.0.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.0.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.1.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.1.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.1.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.1.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.1.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.1.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.1.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.2.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.2.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.2.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.2.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.2.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.2.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.2.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.3.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.3.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.3.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.3.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.3.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.3.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.3.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.4.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.4.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.4.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.4.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.4.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.4.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.4.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.5.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.5.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.5.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.5.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.5.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.5.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.5.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.6.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.6.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.6.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.6.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.6.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.6.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.6.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.7.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.7.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.7.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.7.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.7.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.7.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.7.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.8.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.8.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.8.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.8.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.8.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.8.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.8.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.9.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.9.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.9.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.9.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.9.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.9.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.9.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.attn_q.weight,            torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.10.attn_q.bias,              torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.attn_k.weight,            torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.10.attn_k.bias,              torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.attn_v.weight,            torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.10.attn_v.bias,              torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.attn_output.weight,       torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.10.attn_output.bias,         torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.attn_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.attn_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.ffn_up.weight,            torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.10.ffn_up.bias,              torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.10.ffn_down.weight,          torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.10.ffn_down.bias,            torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.layer_output_norm.weight, torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.layer_output_norm.bias,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.attn_q.weight,            torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.11.attn_q.bias,              torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.attn_k.weight,            torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.11.attn_k.bias,              torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.attn_v.weight,            torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.11.attn_v.bias,              torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.attn_output.weight,       torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.11.attn_output.bias,         torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.attn_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.attn_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.ffn_up.weight,            torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.11.ffn_up.bias,              torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.11.ffn_down.weight,          torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.11.ffn_down.bias,            torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.layer_output_norm.weight, torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.layer_output_norm.bias,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:Set meta model
INFO:hf-to-gguf:Set model parameters
INFO:hf-to-gguf:gguf: context length = 512
INFO:hf-to-gguf:gguf: embedding length = 384
INFO:hf-to-gguf:gguf: feed forward length = 1536
INFO:hf-to-gguf:gguf: head count = 12
INFO:hf-to-gguf:gguf: layer norm epsilon = 1e-12
INFO:hf-to-gguf:gguf: file type = 1
INFO:hf-to-gguf:Set model tokenizer
INFO:gguf.vocab:Setting special token type unk to 100
INFO:gguf.vocab:Setting special token type sep to 102
INFO:gguf.vocab:Setting special token type pad to 0
INFO:gguf.vocab:Setting special token type cls to 101
INFO:gguf.vocab:Setting special token type mask to 103
INFO:hf-to-gguf:Set model quantization version
INFO:gguf.gguf_writer:Writing the following files:
INFO:gguf.gguf_writer:../models-mnt/bge-small/ggml-model-f16.gguf: n_tensors = 197, total_size = 66.9M
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Writing:   0%|          | 0.00/66.9M [00:00<?, ?byte/s]Writing:  37%|      | 24.5M/66.9M [00:00<00:00, 236Mbyte/s]Writing:  73%|  | 49.1M/66.9M [00:00<00:00, 220Mbyte/s]Writing: 100%|| 66.9M/66.9M [00:00<00:00, 211Mbyte/s]
INFO:hf-to-gguf:Model successfully exported to ../models-mnt/bge-small/ggml-model-f16.gguf
+ model_f16=../models-mnt/bge-small/ggml-model-f16.gguf
+ model_q8_0=../models-mnt/bge-small/ggml-model-q8_0.gguf
+ ./bin/llama-quantize ../models-mnt/bge-small/ggml-model-f16.gguf ../models-mnt/bge-small/ggml-model-q8_0.gguf q8_0
main: build = 4400 (6e1531ac)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/bge-small/ggml-model-f16.gguf' to '../models-mnt/bge-small/ggml-model-q8_0.gguf' as Q8_0
llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = bert
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Bge Small
llama_model_loader: - kv   3:                           general.basename str              = bge
llama_model_loader: - kv   4:                         general.size_label str              = small
llama_model_loader: - kv   5:                           bert.block_count u32              = 12
llama_model_loader: - kv   6:                        bert.context_length u32              = 512
llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  124 tensors
llama_model_loader: - type  f16:   73 tensors
[   1/ 197]                 position_embd.weight - [  384,   512,     1,     1], type =    f32, size =    0.750 MB
[   2/ 197]                    token_embd.weight - [  384, 30522,     1,     1], type =    f16, converting to q8_0 .. size =    22.35 MiB ->    11.88 MiB
[   3/ 197]                 token_embd_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[   4/ 197]               token_embd_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[   5/ 197]                   token_types.weight - [  384,     2,     1,     1], type =    f32, size =    0.003 MB
[   6/ 197]                    blk.0.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[   7/ 197]                  blk.0.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[   8/ 197]               blk.0.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[   9/ 197]             blk.0.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  10/ 197]          blk.0.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  11/ 197]        blk.0.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  12/ 197]                    blk.0.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  13/ 197]                  blk.0.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  14/ 197]                    blk.0.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  15/ 197]                  blk.0.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  16/ 197]                  blk.0.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  17/ 197]                blk.0.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  18/ 197]                    blk.0.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  19/ 197]                  blk.0.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  20/ 197]         blk.0.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  21/ 197]       blk.0.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  22/ 197]                    blk.1.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  23/ 197]                  blk.1.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  24/ 197]               blk.1.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  25/ 197]             blk.1.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  26/ 197]          blk.1.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  27/ 197]        blk.1.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  28/ 197]                    blk.1.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  29/ 197]                  blk.1.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  30/ 197]                    blk.1.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  31/ 197]                  blk.1.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  32/ 197]                  blk.1.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  33/ 197]                blk.1.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  34/ 197]                    blk.1.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  35/ 197]                  blk.1.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  36/ 197]         blk.1.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  37/ 197]       blk.1.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  38/ 197]                    blk.2.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  39/ 197]                  blk.2.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  40/ 197]               blk.2.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  41/ 197]             blk.2.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  42/ 197]          blk.2.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  43/ 197]        blk.2.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  44/ 197]                    blk.2.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  45/ 197]                  blk.2.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  46/ 197]                    blk.2.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  47/ 197]                  blk.2.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  48/ 197]                  blk.2.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  49/ 197]                blk.2.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  50/ 197]                    blk.2.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  51/ 197]                  blk.2.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  52/ 197]         blk.2.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  53/ 197]       blk.2.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  54/ 197]                    blk.3.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  55/ 197]                  blk.3.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  56/ 197]               blk.3.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  57/ 197]             blk.3.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  58/ 197]          blk.3.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  59/ 197]        blk.3.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  60/ 197]                    blk.3.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  61/ 197]                  blk.3.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  62/ 197]                    blk.3.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  63/ 197]                  blk.3.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  64/ 197]                  blk.3.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  65/ 197]                blk.3.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  66/ 197]                    blk.3.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  67/ 197]                  blk.3.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  68/ 197]         blk.3.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  69/ 197]       blk.3.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  70/ 197]                    blk.4.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  71/ 197]                  blk.4.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  72/ 197]               blk.4.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  73/ 197]             blk.4.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  74/ 197]          blk.4.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  75/ 197]        blk.4.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  76/ 197]                    blk.4.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  77/ 197]                  blk.4.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  78/ 197]                    blk.4.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  79/ 197]                  blk.4.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  80/ 197]                  blk.4.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  81/ 197]                blk.4.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  82/ 197]                    blk.4.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  83/ 197]                  blk.4.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  84/ 197]         blk.4.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  85/ 197]       blk.4.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  86/ 197]                    blk.5.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  87/ 197]                  blk.5.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  88/ 197]               blk.5.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  89/ 197]             blk.5.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  90/ 197]          blk.5.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  91/ 197]        blk.5.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  92/ 197]                    blk.5.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  93/ 197]                  blk.5.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  94/ 197]                    blk.5.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  95/ 197]                  blk.5.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  96/ 197]                  blk.5.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  97/ 197]                blk.5.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  98/ 197]                    blk.5.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  99/ 197]                  blk.5.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 100/ 197]         blk.5.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 101/ 197]       blk.5.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 102/ 197]                    blk.6.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 103/ 197]                  blk.6.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 104/ 197]               blk.6.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 105/ 197]             blk.6.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 106/ 197]          blk.6.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 107/ 197]        blk.6.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 108/ 197]                    blk.6.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 109/ 197]                  blk.6.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 110/ 197]                    blk.6.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 111/ 197]                  blk.6.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 112/ 197]                  blk.6.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 113/ 197]                blk.6.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 114/ 197]                    blk.6.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 115/ 197]                  blk.6.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 116/ 197]         blk.6.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 117/ 197]       blk.6.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 118/ 197]                    blk.7.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 119/ 197]                  blk.7.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 120/ 197]               blk.7.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 121/ 197]             blk.7.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 122/ 197]          blk.7.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 123/ 197]        blk.7.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 124/ 197]                    blk.7.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 125/ 197]                  blk.7.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 126/ 197]                    blk.7.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 127/ 197]                  blk.7.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 128/ 197]                  blk.7.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 129/ 197]                blk.7.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 130/ 197]                    blk.7.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 131/ 197]                  blk.7.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 132/ 197]         blk.7.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 133/ 197]       blk.7.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 134/ 197]                    blk.8.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 135/ 197]                  blk.8.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 136/ 197]               blk.8.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 137/ 197]             blk.8.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 138/ 197]          blk.8.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 139/ 197]        blk.8.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 140/ 197]                    blk.8.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 141/ 197]                  blk.8.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 142/ 197]                    blk.8.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 143/ 197]                  blk.8.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 144/ 197]                  blk.8.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 145/ 197]                blk.8.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 146/ 197]                    blk.8.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 147/ 197]                  blk.8.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 148/ 197]         blk.8.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 149/ 197]       blk.8.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 150/ 197]                    blk.9.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 151/ 197]                  blk.9.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 152/ 197]               blk.9.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 153/ 197]             blk.9.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 154/ 197]          blk.9.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 155/ 197]        blk.9.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 156/ 197]                    blk.9.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 157/ 197]                  blk.9.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 158/ 197]                    blk.9.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 159/ 197]                  blk.9.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 160/ 197]                  blk.9.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 161/ 197]                blk.9.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 162/ 197]                    blk.9.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 163/ 197]                  blk.9.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 164/ 197]         blk.9.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 165/ 197]       blk.9.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 166/ 197]                   blk.10.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 167/ 197]                 blk.10.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 168/ 197]              blk.10.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 169/ 197]            blk.10.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 170/ 197]         blk.10.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 171/ 197]       blk.10.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 172/ 197]                   blk.10.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 173/ 197]                 blk.10.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 174/ 197]                   blk.10.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 175/ 197]                 blk.10.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 176/ 197]                 blk.10.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 177/ 197]               blk.10.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 178/ 197]                   blk.10.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 179/ 197]                 blk.10.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 180/ 197]        blk.10.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 181/ 197]      blk.10.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 182/ 197]                   blk.11.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 183/ 197]                 blk.11.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 184/ 197]              blk.11.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 185/ 197]            blk.11.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 186/ 197]         blk.11.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 187/ 197]       blk.11.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 188/ 197]                   blk.11.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 189/ 197]                 blk.11.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 190/ 197]                   blk.11.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 191/ 197]                 blk.11.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 192/ 197]                 blk.11.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 193/ 197]               blk.11.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 194/ 197]                   blk.11.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 195/ 197]                 blk.11.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 196/ 197]        blk.11.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 197/ 197]      blk.11.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
llama_model_quantize_internal: model size  =    63.84 MB
llama_model_quantize_internal: quant size  =    34.38 MB

main: quantize time =    74.87 ms
main:    total time =    74.87 ms
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/embd_bge_small-tg-f16.log
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.067 I build: 4400 (6e1531ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.016.142 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.308 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.020.315 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.318 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.020.319 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.319 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.020.320 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.020.321 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.020.326 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.020.327 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.020.327 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.020.328 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.020.329 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.020.332 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.020.333 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.020.334 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.020.334 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.020.335 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.020.335 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.020.336 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.024.990 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.026.187 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.189 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.026.190 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.026.190 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.026.191 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.026.191 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.026.191 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.026.192 I llama_model_loader: - type  f32:  124 tensors
0.00.026.193 I llama_model_loader: - type  f16:   73 tensors
0.00.030.371 I llm_load_vocab: special tokens cache size = 5
0.00.032.610 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.032.638 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.032.640 I llm_load_print_meta: arch             = bert
0.00.032.641 I llm_load_print_meta: vocab type       = WPM
0.00.032.641 I llm_load_print_meta: n_vocab          = 30522
0.00.032.641 I llm_load_print_meta: n_merges         = 0
0.00.032.642 I llm_load_print_meta: vocab_only       = 0
0.00.032.642 I llm_load_print_meta: n_ctx_train      = 512
0.00.032.642 I llm_load_print_meta: n_embd           = 384
0.00.032.642 I llm_load_print_meta: n_layer          = 12
0.00.032.645 I llm_load_print_meta: n_head           = 12
0.00.032.646 I llm_load_print_meta: n_head_kv        = 12
0.00.032.646 I llm_load_print_meta: n_rot            = 32
0.00.032.647 I llm_load_print_meta: n_swa            = 0
0.00.032.647 I llm_load_print_meta: n_embd_head_k    = 32
0.00.032.647 I llm_load_print_meta: n_embd_head_v    = 32
0.00.032.648 I llm_load_print_meta: n_gqa            = 1
0.00.032.649 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.032.650 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.032.651 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.032.651 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.032.652 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.032.652 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.032.652 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.032.653 I llm_load_print_meta: n_ff             = 1536
0.00.032.654 I llm_load_print_meta: n_expert         = 0
0.00.032.654 I llm_load_print_meta: n_expert_used    = 0
0.00.032.654 I llm_load_print_meta: causal attn      = 0
0.00.032.654 I llm_load_print_meta: pooling type     = 2
0.00.032.654 I llm_load_print_meta: rope type        = 2
0.00.032.655 I llm_load_print_meta: rope scaling     = linear
0.00.032.655 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.032.656 I llm_load_print_meta: freq_scale_train = 1
0.00.032.656 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.032.656 I llm_load_print_meta: rope_finetuned   = unknown
0.00.032.656 I llm_load_print_meta: ssm_d_conv       = 0
0.00.032.657 I llm_load_print_meta: ssm_d_inner      = 0
0.00.032.657 I llm_load_print_meta: ssm_d_state      = 0
0.00.032.657 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.032.657 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.032.658 I llm_load_print_meta: model type       = 33M
0.00.032.658 I llm_load_print_meta: model ftype      = F16
0.00.032.659 I llm_load_print_meta: model params     = 33.21 M
0.00.032.660 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.032.660 I llm_load_print_meta: general.name     = Bge Small
0.00.032.660 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.032.661 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.032.661 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.032.661 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.032.661 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.032.662 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.032.662 I llm_load_print_meta: max token length = 21
0.00.034.699 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.034.701 I llm_load_tensors: offloading output layer to GPU
0.00.034.701 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.034.729 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.034.731 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.035.325 I llama_new_context_with_model: n_seq_max     = 1
0.00.035.327 I llama_new_context_with_model: n_ctx         = 512
0.00.035.327 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.035.327 I llama_new_context_with_model: n_batch       = 2048
0.00.035.327 I llama_new_context_with_model: n_ubatch      = 2048
0.00.035.328 I llama_new_context_with_model: flash_attn    = 0
0.00.035.328 I llama_new_context_with_model: freq_base     = 10000.0
0.00.035.329 I llama_new_context_with_model: freq_scale    = 1
0.00.035.329 I ggml_metal_init: allocating
0.00.035.334 I ggml_metal_init: found device: Apple M4
0.00.035.337 I ggml_metal_init: picking default device: Apple M4
0.00.036.213 I ggml_metal_init: using embedded metal library
0.00.040.232 I ggml_metal_init: GPU name:   Apple M4
0.00.040.234 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.040.235 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.040.235 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.040.236 I ggml_metal_init: simdgroup reduction   = true
0.00.040.236 I ggml_metal_init: simdgroup matrix mul. = true
0.00.040.236 I ggml_metal_init: has bfloat            = true
0.00.040.236 I ggml_metal_init: use bfloat            = true
0.00.040.237 I ggml_metal_init: hasUnifiedMemory      = true
0.00.040.238 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.052.458 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12
0.00.053.035 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.053.038 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.053.039 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.053.811 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.053.813 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.053.813 I llama_new_context_with_model: graph nodes  = 429
0.00.053.814 I llama_new_context_with_model: graph splits = 2
0.00.053.834 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.053.835 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.060.244 I 
0.00.060.259 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.060.925 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.065.974 I llama_perf_context_print:        load time =      44.10 ms
0.00.065.976 I llama_perf_context_print: prompt eval time =       4.92 ms /     9 tokens (    0.55 ms per token,  1831.13 tokens per second)
0.00.065.977 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.065.977 I llama_perf_context_print:       total time =       5.73 ms /    10 tokens
0.00.066.111 I ggml_metal_free: deallocating

real	0m0.251s
user	0m0.048s
sys	0m0.031s
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/embd_bge_small-tg-q8_0.log
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.035 I build: 4400 (6e1531ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.495 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.633 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.636 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.638 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.638 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.640 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.640 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.640 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.641 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.642 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.644 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.644 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.644 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.646 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.646 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.647 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.647 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.647 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.648 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.648 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.148 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.821 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.822 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.823 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.823 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.823 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.824 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.824 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.014.825 I llama_model_loader: - type  f32:  124 tensors
0.00.014.825 I llama_model_loader: - type q8_0:   73 tensors
0.00.017.429 I llm_load_vocab: special tokens cache size = 5
0.00.018.802 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.018.812 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.018.813 I llm_load_print_meta: arch             = bert
0.00.018.813 I llm_load_print_meta: vocab type       = WPM
0.00.018.814 I llm_load_print_meta: n_vocab          = 30522
0.00.018.814 I llm_load_print_meta: n_merges         = 0
0.00.018.814 I llm_load_print_meta: vocab_only       = 0
0.00.018.814 I llm_load_print_meta: n_ctx_train      = 512
0.00.018.814 I llm_load_print_meta: n_embd           = 384
0.00.018.814 I llm_load_print_meta: n_layer          = 12
0.00.018.817 I llm_load_print_meta: n_head           = 12
0.00.018.818 I llm_load_print_meta: n_head_kv        = 12
0.00.018.818 I llm_load_print_meta: n_rot            = 32
0.00.018.818 I llm_load_print_meta: n_swa            = 0
0.00.018.818 I llm_load_print_meta: n_embd_head_k    = 32
0.00.018.820 I llm_load_print_meta: n_embd_head_v    = 32
0.00.018.820 I llm_load_print_meta: n_gqa            = 1
0.00.018.821 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.018.822 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.018.823 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.018.827 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.018.828 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.018.828 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.018.828 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.018.829 I llm_load_print_meta: n_ff             = 1536
0.00.018.829 I llm_load_print_meta: n_expert         = 0
0.00.018.829 I llm_load_print_meta: n_expert_used    = 0
0.00.018.829 I llm_load_print_meta: causal attn      = 0
0.00.018.829 I llm_load_print_meta: pooling type     = 2
0.00.018.829 I llm_load_print_meta: rope type        = 2
0.00.018.830 I llm_load_print_meta: rope scaling     = linear
0.00.018.830 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.018.830 I llm_load_print_meta: freq_scale_train = 1
0.00.018.830 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.018.831 I llm_load_print_meta: rope_finetuned   = unknown
0.00.018.831 I llm_load_print_meta: ssm_d_conv       = 0
0.00.018.831 I llm_load_print_meta: ssm_d_inner      = 0
0.00.018.831 I llm_load_print_meta: ssm_d_state      = 0
0.00.018.831 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.018.831 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.018.832 I llm_load_print_meta: model type       = 33M
0.00.018.832 I llm_load_print_meta: model ftype      = Q8_0
0.00.018.833 I llm_load_print_meta: model params     = 33.21 M
0.00.018.833 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.018.833 I llm_load_print_meta: general.name     = Bge Small
0.00.018.833 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.018.834 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.018.834 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.018.835 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.018.835 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.018.836 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.018.836 I llm_load_print_meta: max token length = 21
0.00.020.152 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.020.152 I llm_load_tensors: offloading output layer to GPU
0.00.020.152 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.020.161 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.162 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.535 I llama_new_context_with_model: n_seq_max     = 1
0.00.020.536 I llama_new_context_with_model: n_ctx         = 512
0.00.020.536 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.020.536 I llama_new_context_with_model: n_batch       = 2048
0.00.020.537 I llama_new_context_with_model: n_ubatch      = 2048
0.00.020.537 I llama_new_context_with_model: flash_attn    = 0
0.00.020.537 I llama_new_context_with_model: freq_base     = 10000.0
0.00.020.538 I llama_new_context_with_model: freq_scale    = 1
0.00.020.538 I ggml_metal_init: allocating
0.00.020.541 I ggml_metal_init: found device: Apple M4
0.00.020.543 I ggml_metal_init: picking default device: Apple M4
0.00.021.167 I ggml_metal_init: using embedded metal library
0.00.023.739 I ggml_metal_init: GPU name:   Apple M4
0.00.023.741 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.741 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.742 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.742 I ggml_metal_init: simdgroup reduction   = true
0.00.023.742 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.742 I ggml_metal_init: has bfloat            = true
0.00.023.742 I ggml_metal_init: use bfloat            = true
0.00.023.743 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.744 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.426 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12
0.00.033.939 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.033.941 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.033.943 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.034.490 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.034.492 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.034.492 I llama_new_context_with_model: graph nodes  = 429
0.00.034.492 I llama_new_context_with_model: graph splits = 2
0.00.034.505 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.034.506 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.039.452 I 
0.00.039.471 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.040.034 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.044.500 I llama_perf_context_print:        load time =      29.95 ms
0.00.044.501 I llama_perf_context_print: prompt eval time =       4.33 ms /     9 tokens (    0.48 ms per token,  2076.60 tokens per second)
0.00.044.502 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.044.502 I llama_perf_context_print:       total time =       5.05 ms /    10 tokens
0.00.044.677 I ggml_metal_free: deallocating

real	0m0.057s
user	0m0.030s
sys	0m0.016s
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_rerank_tiny
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/ https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/config.json
+ local out=models-mnt/rerank-tiny/
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/config.json
+ tee /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/rerank_tiny.log
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/
+ cd models-mnt/rerank-tiny/
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2024-12-30 16:51:18 URL:https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/config.json [1206/1206] -> "config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/ https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/tokenizer.json
+ local out=models-mnt/rerank-tiny/
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/tokenizer.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/
+ cd models-mnt/rerank-tiny/
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/tokenizer.json
Last-modified header missing -- time-stamps turned off.
2024-12-30 16:51:19 URL:https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/tokenizer.json [2030772/2030772] -> "tokenizer.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/ https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/tokenizer_config.json
+ local out=models-mnt/rerank-tiny/
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/tokenizer_config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/
+ cd models-mnt/rerank-tiny/
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/tokenizer_config.json
Last-modified header missing -- time-stamps turned off.
2024-12-30 16:51:19 URL:https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/tokenizer_config.json [1215/1215] -> "tokenizer_config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/ https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/special_tokens_map.json
+ local out=models-mnt/rerank-tiny/
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/special_tokens_map.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/
+ cd models-mnt/rerank-tiny/
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/special_tokens_map.json
Last-modified header missing -- time-stamps turned off.
2024-12-30 16:51:20 URL:https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/special_tokens_map.json [280/280] -> "special_tokens_map.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/ https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/resolve/main/pytorch_model.bin
+ local out=models-mnt/rerank-tiny/
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/resolve/main/pytorch_model.bin
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/
+ cd models-mnt/rerank-tiny/
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/resolve/main/pytorch_model.bin
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/ https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/sentence_bert_config.json
+ local out=models-mnt/rerank-tiny/
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/sentence_bert_config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/
+ cd models-mnt/rerank-tiny/
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/sentence_bert_config.json
https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/sentence_bert_config.json:
2024-12-30 16:51:21 ERROR 404: Not Found.
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/ https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/vocab.txt
+ local out=models-mnt/rerank-tiny/
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/vocab.txt
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/
+ cd models-mnt/rerank-tiny/
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/vocab.txt
https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/vocab.txt:
2024-12-30 16:51:22 ERROR 404: Not Found.
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/ https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/modules.json
+ local out=models-mnt/rerank-tiny/
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/modules.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/
+ cd models-mnt/rerank-tiny/
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/modules.json
https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/modules.json:
2024-12-30 16:51:22 ERROR 404: Not Found.
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/ https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/config.json
+ local out=models-mnt/rerank-tiny/
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/
+ cd models-mnt/rerank-tiny/
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2024-12-30 16:51:23 URL:https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/config.json [1206/1206] -> "config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/1_Pooling https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/1_Pooling/config.json
+ local out=models-mnt/rerank-tiny/1_Pooling
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/1_Pooling/config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/1_Pooling
+ cd models-mnt/rerank-tiny/1_Pooling
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/1_Pooling/config.json
https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/1_Pooling/config.json:
2024-12-30 16:51:23 ERROR 404: Not Found.
+ cd /Users/ggml/work/llama.cpp
+ path_models=../models-mnt/rerank-tiny
+ rm -rf build-ci-release
+ mkdir build-ci-release
+ cd build-ci-release
+ set -e
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/rerank_tiny-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:299 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.2s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.488s
user	0m0.888s
sys	0m1.191s
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/rerank_tiny-make.log
++ nproc
+ make -j10
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  3%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  6%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  6%] Built target xxhash
[  6%] Built target sha256
[  6%] Built target build_info
[  6%] Built target sha1
[  7%] Linking CXX shared library libggml-base.dylib
[  7%] Built target ggml-base
[  8%] Generate assembly for embedded Metal library
Embedding Metal library
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 11%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 11%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 14%] Linking CXX shared library libggml-blas.dylib
[ 14%] Linking CXX shared library libggml-cpu.dylib
[ 14%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 15%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 15%] Built target ggml-blas
[ 15%] Built target ggml-cpu
[ 15%] Linking C shared library libggml-metal.dylib
[ 15%] Built target ggml-metal
[ 16%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 16%] Linking CXX shared library libggml.dylib
[ 16%] Built target ggml
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 19%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 21%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Linking CXX shared library libllama.dylib
[ 22%] Built target llama-gguf
[ 22%] Built target llama-gguf-hash
[ 22%] Built target llama
[ 22%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 23%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 23%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 24%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 25%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 28%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 28%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 29%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 30%] Linking C executable ../bin/test-c
[ 30%] Built target llava
[ 30%] Linking CXX executable ../../bin/llama-simple-chat
[ 30%] Linking CXX executable ../../bin/llama-quantize-stats
[ 31%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-simple
[ 31%] Linking CXX shared library libllava_shared.dylib
[ 31%] Linking CXX static library libcommon.a
[ 32%] Linking CXX static library libllava_static.a
[ 32%] Built target test-c
[ 32%] Built target llama-simple
[ 32%] Built target llama-quantize-stats
[ 32%] Built target llama-simple-chat
[ 32%] Built target llava_static
[ 32%] Built target common
[ 32%] Built target llava_shared
[ 32%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 39%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 39%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 39%] Linking CXX executable ../bin/test-tokenizer-0
[ 40%] Linking CXX executable ../bin/test-grammar-integration
[ 40%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 41%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-sampling
[ 44%] Linking CXX executable ../bin/test-llama-grammar
[ 45%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 45%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 45%] Linking CXX executable ../bin/test-log
[ 45%] Linking CXX executable ../bin/test-grammar-parser
[ 45%] Built target test-tokenizer-1-bpe
[ 45%] Built target test-tokenizer-0
[ 45%] Built target test-grammar-integration
[ 45%] Built target test-tokenizer-1-spm
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 47%] Built target test-sampling
[ 47%] Built target test-llama-grammar
[ 47%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 47%] Built target test-grammar-parser
[ 47%] Built target test-log
[ 47%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 48%] Linking CXX executable ../bin/test-arg-parser
[ 48%] Built target test-json-schema-to-grammar
[ 48%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 51%] Linking CXX executable ../bin/test-chat-template
[ 51%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 55%] Linking CXX executable ../bin/test-model-load-cancel
[ 55%] Linking CXX executable ../bin/test-backend-ops
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 55%] Linking CXX executable ../bin/test-gguf
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-autorelease
[ 57%] Linking CXX executable ../bin/test-barrier
[ 57%] Built target test-arg-parser
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 58%] Built target test-chat-template
[ 58%] Linking CXX executable ../bin/test-quantize-fns
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 59%] Built target test-model-load-cancel
[ 59%] Built target test-gguf
[ 59%] Built target test-backend-ops
[ 60%] Linking CXX executable ../bin/test-quantize-perf
[ 60%] Linking CXX executable ../bin/test-rope
[ 60%] Built target test-barrier
[ 60%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 60%] Built target test-autorelease
[ 61%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 62%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 62%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 62%] Built target test-quantize-fns
[ 63%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 64%] Linking CXX executable ../../bin/llama-batched-bench
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 66%] Linking CXX executable ../../bin/llama-embedding
[ 66%] Built target test-quantize-perf
[ 66%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 66%] Linking CXX executable ../../bin/llama-eval-callback
[ 66%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-batched
[ 67%] Built target test-rope
[ 67%] Linking CXX executable ../../bin/llama-gguf-split
[ 68%] Linking CXX executable ../../bin/llama-gritlm
[ 69%] Linking CXX executable ../../bin/llama-imatrix
[ 69%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 70%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 70%] Built target llama-batched-bench
[ 70%] Built target llama-gbnf-validator
[ 70%] Built target llama-embedding
[ 70%] Built target llama-eval-callback
[ 70%] Built target llama-batched
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Built target llama-gguf-split
[ 71%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 71%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-bench
[ 71%] Built target llama-imatrix
[ 71%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 71%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 71%] Built target llama-gritlm
[ 71%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 72%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 74%] Linking CXX executable ../../bin/llama-lookup
[ 75%] Linking CXX executable ../../bin/llama-lookup-create
[ 75%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 76%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 77%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 77%] Built target llama-bench
[ 77%] Linking CXX executable ../../bin/llama-cli
[ 77%] Built target llama-infill
[ 78%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Linking CXX executable ../../bin/llama-passkey
[ 79%] Built target llama-lookahead
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Built target llama-lookup
[ 80%] Built target llama-lookup-create
[ 80%] Built target llama-lookup-merge
[ 81%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Built target llama-lookup-stats
[ 81%] Generating loading.html.hpp
[ 81%] Built target llama-cli
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Built target llama-parallel
[ 82%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 84%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 85%] Generating index.html.gz.hpp
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-quantize
[ 86%] Built target llama-passkey
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-run
[ 86%] Linking CXX executable ../../bin/llama-retrieval
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 88%] Built target llama-perplexity
[ 88%] Built target llama-quantize
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Built target llama-run
[ 91%] Built target llama-retrieval
[ 92%] Linking CXX executable ../../bin/llama-tts
[ 92%] Built target llama-speculative
[ 92%] Built target llama-save-load-state
[ 92%] Built target llama-speculative-simple
[ 93%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Built target llama-tokenize
[ 93%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Linking CXX executable ../../bin/llama-gen-docs
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-cvector-generator
[ 95%] Built target llama-tts
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 97%] Linking CXX executable ../../bin/llama-llava-cli
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Built target llama-gen-docs
[ 97%] Built target llama-convert-llama2c-to-ggml
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-export-lora
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.882s
user	0m6.007s
sys	0m9.346s
+ python3 ../convert_hf_to_gguf.py ../models-mnt/rerank-tiny --outfile ../models-mnt/rerank-tiny/ggml-model-f16.gguf
INFO:hf-to-gguf:Loading model: rerank-tiny
INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only
INFO:hf-to-gguf:Exporting model...
INFO:hf-to-gguf:gguf: loading model part 'pytorch_model.bin'
INFO:hf-to-gguf:token_embd.weight,              torch.bfloat16 --> F16, shape = {384, 61056}
INFO:hf-to-gguf:token_types.weight,             torch.bfloat16 --> F32, shape = {384, 2}
INFO:hf-to-gguf:token_embd_norm.weight,         torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:token_embd_norm.bias,           torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_q.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.0.attn_q.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_k.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.0.attn_k.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_v.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.0.attn_v.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_output.weight,       torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.0.attn_output.bias,         torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_output_norm.weight,  torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_output_norm.bias,    torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.ffn_gate.weight,          torch.bfloat16 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.0.ffn_up.weight,            torch.bfloat16 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.0.ffn_down.weight,          torch.bfloat16 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.0.ffn_down.bias,            torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.layer_output_norm.weight, torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.layer_output_norm.bias,   torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_q.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.1.attn_q.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_k.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.1.attn_k.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_v.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.1.attn_v.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_output.weight,       torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.1.attn_output.bias,         torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_output_norm.weight,  torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_output_norm.bias,    torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.ffn_gate.weight,          torch.bfloat16 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.1.ffn_up.weight,            torch.bfloat16 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.1.ffn_down.weight,          torch.bfloat16 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.1.ffn_down.bias,            torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.layer_output_norm.weight, torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.layer_output_norm.bias,   torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_q.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.2.attn_q.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_k.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.2.attn_k.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_v.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.2.attn_v.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_output.weight,       torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.2.attn_output.bias,         torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_output_norm.weight,  torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_output_norm.bias,    torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.ffn_gate.weight,          torch.bfloat16 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.2.ffn_up.weight,            torch.bfloat16 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.2.ffn_down.weight,          torch.bfloat16 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.2.ffn_down.bias,            torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.layer_output_norm.weight, torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.layer_output_norm.bias,   torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_q.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.3.attn_q.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_k.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.3.attn_k.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_v.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.3.attn_v.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_output.weight,       torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.3.attn_output.bias,         torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_output_norm.weight,  torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_output_norm.bias,    torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.ffn_gate.weight,          torch.bfloat16 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.3.ffn_up.weight,            torch.bfloat16 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.3.ffn_down.weight,          torch.bfloat16 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.3.ffn_down.bias,            torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.layer_output_norm.weight, torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.layer_output_norm.bias,   torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:cls.weight,                     torch.bfloat16 --> F16, shape = {384, 1}
INFO:hf-to-gguf:cls.bias,                       torch.bfloat16 --> F32, shape = {1}
INFO:hf-to-gguf:Set meta model
INFO:hf-to-gguf:Set model parameters
INFO:hf-to-gguf:gguf: context length = 8192
INFO:hf-to-gguf:gguf: embedding length = 384
INFO:hf-to-gguf:gguf: feed forward length = 1536
INFO:hf-to-gguf:gguf: head count = 12
INFO:hf-to-gguf:gguf: layer norm epsilon = 1e-12
INFO:hf-to-gguf:gguf: file type = 1
INFO:hf-to-gguf:Set model tokenizer
INFO:gguf.vocab:Adding 39382 merge(s).
INFO:gguf.vocab:Setting special token type bos to 0
INFO:gguf.vocab:Setting special token type eos to 2
INFO:gguf.vocab:Setting special token type unk to 3
INFO:gguf.vocab:Setting special token type sep to 2
INFO:gguf.vocab:Setting special token type pad to 1
INFO:gguf.vocab:Setting special token type cls to 0
INFO:gguf.vocab:Setting special token type mask to 4
INFO:hf-to-gguf:Set model quantization version
INFO:gguf.gguf_writer:Writing the following files:
INFO:gguf.gguf_writer:../models-mnt/rerank-tiny/ggml-model-f16.gguf: n_tensors = 70, total_size = 65.8M
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Writing:   0%|          | 0.00/65.8M [00:00<?, ?byte/s]Writing:  79%|  | 51.9M/65.8M [00:00<00:00, 513Mbyte/s]Writing: 100%|| 65.8M/65.8M [00:00<00:00, 476Mbyte/s]
INFO:hf-to-gguf:Model successfully exported to ../models-mnt/rerank-tiny/ggml-model-f16.gguf
+ model_f16=../models-mnt/rerank-tiny/ggml-model-f16.gguf
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/rerank_tiny-rk-f16.log
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.082 I build: 4400 (6e1531ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.016.311 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.719 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.020.723 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.724 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.020.725 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.726 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.020.726 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.020.728 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.020.729 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.020.730 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.020.730 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.020.730 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.020.731 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.020.733 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.020.733 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.020.734 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.020.735 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.735 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.024.688 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.025.891 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.630 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.028.631 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.631 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.028.632 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.028.632 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.028.632 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.028.633 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.028.633 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.028.633 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.028.633 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.028.634 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.028.634 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.028.635 I llama_model_loader: - type  f32:   40 tensors
0.00.028.635 I llama_model_loader: - type  f16:   30 tensors
0.00.040.701 W llm_load_vocab: empty token at index 5
0.00.044.337 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.045.418 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.045.451 I llm_load_vocab: special tokens cache size = 5
0.00.305.444 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.305.453 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.305.458 I llm_load_print_meta: arch             = jina-bert-v2
0.00.305.459 I llm_load_print_meta: vocab type       = BPE
0.00.305.459 I llm_load_print_meta: n_vocab          = 61056
0.00.305.459 I llm_load_print_meta: n_merges         = 39382
0.00.305.459 I llm_load_print_meta: vocab_only       = 0
0.00.305.459 I llm_load_print_meta: n_ctx_train      = 8192
0.00.305.460 I llm_load_print_meta: n_embd           = 384
0.00.305.460 I llm_load_print_meta: n_layer          = 4
0.00.305.465 I llm_load_print_meta: n_head           = 12
0.00.305.466 I llm_load_print_meta: n_head_kv        = 12
0.00.305.466 I llm_load_print_meta: n_rot            = 32
0.00.305.466 I llm_load_print_meta: n_swa            = 0
0.00.305.466 I llm_load_print_meta: n_embd_head_k    = 32
0.00.305.468 I llm_load_print_meta: n_embd_head_v    = 32
0.00.305.469 I llm_load_print_meta: n_gqa            = 1
0.00.305.469 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.305.470 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.305.472 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.305.472 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.305.473 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.305.474 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.305.474 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.305.474 I llm_load_print_meta: n_ff             = 1536
0.00.305.474 I llm_load_print_meta: n_expert         = 0
0.00.305.475 I llm_load_print_meta: n_expert_used    = 0
0.00.305.475 I llm_load_print_meta: causal attn      = 0
0.00.305.475 I llm_load_print_meta: pooling type     = -1
0.00.305.475 I llm_load_print_meta: rope type        = -1
0.00.305.475 I llm_load_print_meta: rope scaling     = linear
0.00.305.476 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.305.476 I llm_load_print_meta: freq_scale_train = 1
0.00.305.476 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.305.477 I llm_load_print_meta: rope_finetuned   = unknown
0.00.305.477 I llm_load_print_meta: ssm_d_conv       = 0
0.00.305.477 I llm_load_print_meta: ssm_d_inner      = 0
0.00.305.477 I llm_load_print_meta: ssm_d_state      = 0
0.00.305.477 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.305.479 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.305.479 I llm_load_print_meta: model type       = 33M
0.00.305.480 I llm_load_print_meta: model ftype      = F16
0.00.305.481 I llm_load_print_meta: model params     = 32.90 M
0.00.305.481 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.305.482 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.305.482 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.305.483 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.305.483 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.305.483 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.305.484 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.305.484 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.305.484 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.305.485 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.305.485 I llm_load_print_meta: max token length = 45
0.00.306.451 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.306.451 I llm_load_tensors: offloading output layer to GPU
0.00.306.451 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.306.471 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.306.471 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.307.193 I llama_new_context_with_model: n_seq_max     = 1
0.00.307.194 I llama_new_context_with_model: n_ctx         = 8192
0.00.307.194 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.307.194 I llama_new_context_with_model: n_batch       = 2048
0.00.307.194 I llama_new_context_with_model: n_ubatch      = 2048
0.00.307.195 I llama_new_context_with_model: flash_attn    = 0
0.00.307.195 I llama_new_context_with_model: freq_base     = 10000.0
0.00.307.195 I llama_new_context_with_model: freq_scale    = 1
0.00.307.196 I ggml_metal_init: allocating
0.00.307.199 I ggml_metal_init: found device: Apple M4
0.00.307.201 I ggml_metal_init: picking default device: Apple M4
0.00.307.897 I ggml_metal_init: using embedded metal library
0.00.310.466 I ggml_metal_init: GPU name:   Apple M4
0.00.310.468 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.310.468 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.310.469 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.310.469 I ggml_metal_init: simdgroup reduction   = true
0.00.310.469 I ggml_metal_init: simdgroup matrix mul. = true
0.00.310.470 I ggml_metal_init: has bfloat            = true
0.00.310.470 I ggml_metal_init: use bfloat            = true
0.00.310.470 I ggml_metal_init: hasUnifiedMemory      = true
0.00.310.471 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.320.716 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4
0.00.323.139 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.323.141 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.323.143 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.323.718 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.323.719 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.323.720 I llama_new_context_with_model: graph nodes  = 154
0.00.323.720 I llama_new_context_with_model: graph splits = 2
0.00.323.739 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.323.741 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.334.147 I 
0.00.334.172 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.334.321 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.334.321 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.334.324 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.334.324 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.334.327 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.334.328 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.334.816 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.338.320 I llama_perf_context_print:        load time =     317.83 ms
0.00.338.321 I llama_perf_context_print: prompt eval time =       3.50 ms /    62 tokens (    0.06 ms per token, 17734.55 tokens per second)
0.00.338.322 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.338.322 I llama_perf_context_print:       total time =       4.17 ms /    63 tokens
0.00.338.531 I ggml_metal_free: deallocating

real	0m1.064s
user	0m0.317s
sys	0m0.035s
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/rerank_tiny-rk-f16.log
++ cat /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/rerank_tiny-rk-f16.log
++ grep 'rerank score 0'
+ check_score 'rerank score 0' 'rerank score 0:    0.023' 0.00 0.05
+ qnt='rerank score 0'
++ echo 'rerank score 0:    0.023'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ score=0.023
++ echo '0.023 < 0.00'
++ bc
+ '[' 0 -eq 1 ']'
++ echo '0.023 > 0.05'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' 'rerank score 0' 0.023
+ return 0
  - rerank score 0 @ 0.023 OK
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/rerank_tiny-rk-f16.log
++ cat /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/rerank_tiny-rk-f16.log
++ grep 'rerank score 1'
+ check_score 'rerank score 1' 'rerank score 1:    0.024' 0.00 0.05
+ qnt='rerank score 1'
++ echo 'rerank score 1:    0.024'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ score=0.024
++ echo '0.024 < 0.00'
++ bc
+ '[' 0 -eq 1 ']'
++ echo '0.024 > 0.05'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' 'rerank score 1' 0.024
+ return 0
  - rerank score 1 @ 0.024 OK
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/rerank_tiny-rk-f16.log
++ cat /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/rerank_tiny-rk-f16.log
++ grep 'rerank score 2'
+ check_score 'rerank score 2' 'rerank score 2:    0.199' 0.10 0.30
+ qnt='rerank score 2'
++ echo 'rerank score 2:    0.199'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ score=0.199
++ echo '0.199 < 0.10'
++ bc
+ '[' 0 -eq 1 ']'
++ echo '0.199 > 0.30'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' 'rerank score 2' 0.199
+ return 0
  - rerank score 2 @ 0.199 OK
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_pythia_1_4b
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/pythia/1.4B/ https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/config.json
+ local out=models-mnt/pythia/1.4B/
+ local url=https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/config.json
+ tee /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b.log
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/pythia/1.4B/
+ cd models-mnt/pythia/1.4B/
+ wget -nv -N https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2024-12-30 16:51:31 URL:https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/config.json [570/570] -> "config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/pythia/1.4B/ https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/tokenizer.json
+ local out=models-mnt/pythia/1.4B/
+ local url=https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/tokenizer.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/pythia/1.4B/
+ cd models-mnt/pythia/1.4B/
+ wget -nv -N https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/tokenizer.json
Last-modified header missing -- time-stamps turned off.
2024-12-30 16:51:32 URL:https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/tokenizer.json [2113710/2113710] -> "tokenizer.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/pythia/1.4B/ https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/tokenizer_config.json
+ local out=models-mnt/pythia/1.4B/
+ local url=https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/tokenizer_config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/pythia/1.4B/
+ cd models-mnt/pythia/1.4B/
+ wget -nv -N https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/tokenizer_config.json
Last-modified header missing -- time-stamps turned off.
2024-12-30 16:51:32 URL:https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/tokenizer_config.json [396/396] -> "tokenizer_config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/pythia/1.4B/ https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/special_tokens_map.json
+ local out=models-mnt/pythia/1.4B/
+ local url=https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/special_tokens_map.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/pythia/1.4B/
+ cd models-mnt/pythia/1.4B/
+ wget -nv -N https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/special_tokens_map.json
Last-modified header missing -- time-stamps turned off.
2024-12-30 16:51:33 URL:https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/special_tokens_map.json [99/99] -> "special_tokens_map.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/pythia/1.4B/ https://huggingface.co/EleutherAI/pythia-1.4b/resolve/main/pytorch_model.bin
+ local out=models-mnt/pythia/1.4B/
+ local url=https://huggingface.co/EleutherAI/pythia-1.4b/resolve/main/pytorch_model.bin
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/pythia/1.4B/
+ cd models-mnt/pythia/1.4B/
+ wget -nv -N https://huggingface.co/EleutherAI/pythia-1.4b/resolve/main/pytorch_model.bin
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/wikitext/ https://huggingface.co/datasets/ggml-org/ci/resolve/main/wikitext-2-raw-v1.zip
+ local out=models-mnt/wikitext/
+ local url=https://huggingface.co/datasets/ggml-org/ci/resolve/main/wikitext-2-raw-v1.zip
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/wikitext/
+ cd models-mnt/wikitext/
+ wget -nv -N https://huggingface.co/datasets/ggml-org/ci/resolve/main/wikitext-2-raw-v1.zip
+ cd /Users/ggml/work/llama.cpp
+ unzip -o models-mnt/wikitext/wikitext-2-raw-v1.zip -d models-mnt/wikitext/
Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ head -n 60 models-mnt/wikitext/wikitext-2-raw/wiki.test.raw
+ path_models=../models-mnt/pythia/1.4B
+ path_wiki=../models-mnt/wikitext/wikitext-2-raw
+ rm -rf build-ci-release
+ mkdir build-ci-release
+ cd build-ci-release
+ set -e
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:299 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.2s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.497s
user	0m0.903s
sys	0m1.198s
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-make.log
++ nproc
+ make -j10
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  6%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  6%] Built target sha1
[  6%] Built target build_info
[  6%] Built target sha256
[  6%] Built target xxhash
[  7%] Linking CXX shared library libggml-base.dylib
[  7%] Built target ggml-base
[  8%] Generate assembly for embedded Metal library
Embedding Metal library
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  8%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 12%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 14%] Linking CXX shared library libggml-blas.dylib
[ 14%] Linking CXX shared library libggml-cpu.dylib
[ 14%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 15%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 15%] Built target ggml-blas
[ 15%] Built target ggml-cpu
[ 15%] Linking C shared library libggml-metal.dylib
[ 15%] Built target ggml-metal
[ 16%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 16%] Linking CXX shared library libggml.dylib
[ 16%] Built target ggml
[ 17%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 19%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Linking CXX shared library libllama.dylib
[ 22%] Built target llama-gguf-hash
[ 22%] Built target llama-gguf
[ 22%] Built target llama
[ 22%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 23%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 24%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 25%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 27%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 28%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 29%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 29%] Linking C executable ../bin/test-c
[ 30%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-simple-chat
[ 31%] Built target llava
[ 31%] Linking CXX executable ../../bin/llama-simple
[ 31%] Linking CXX executable ../../bin/llama-quantize-stats
[ 31%] Linking CXX shared library libllava_shared.dylib
[ 31%] Linking CXX static library libcommon.a
[ 32%] Linking CXX static library libllava_static.a
[ 32%] Built target test-c
[ 32%] Built target llama-simple-chat
[ 32%] Built target llama-quantize-stats
[ 32%] Built target llama-simple
[ 32%] Built target llava_shared
[ 32%] Built target llava_static
[ 32%] Built target common
[ 34%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 40%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 41%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 43%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-0
[ 44%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 44%] Linking CXX executable ../bin/test-grammar-parser
[ 45%] Linking CXX executable ../bin/test-grammar-integration
[ 45%] Linking CXX executable ../bin/test-log
[ 45%] Linking CXX executable ../bin/test-arg-parser
[ 46%] Linking CXX executable ../bin/test-sampling
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 46%] Built target test-tokenizer-0
[ 46%] Built target test-tokenizer-1-spm
[ 46%] Built target test-tokenizer-1-bpe
[ 46%] Built target test-grammar-parser
[ 46%] Built target test-grammar-integration
[ 46%] Built target test-log
[ 46%] Built target test-sampling
[ 46%] Built target test-arg-parser
[ 47%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 47%] Built target test-llama-grammar
[ 47%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 47%] Built target test-json-schema-to-grammar
[ 49%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 52%] Linking CXX executable ../bin/test-chat-template
[ 53%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 54%] Linking CXX executable ../bin/test-gguf
[ 55%] Linking CXX executable ../bin/test-model-load-cancel
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-backend-ops
[ 57%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 58%] Linking CXX executable ../bin/test-autorelease
[ 58%] Linking CXX executable ../bin/test-quantize-fns
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-barrier
[ 60%] Linking CXX executable ../bin/test-quantize-perf
[ 60%] Built target test-chat-template
[ 60%] Linking CXX executable ../bin/test-rope
[ 61%] Linking CXX executable ../../bin/llama-batched-bench
[ 61%] Built target test-gguf
[ 61%] Built target test-model-load-cancel
[ 61%] Built target test-backend-ops
[ 61%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 61%] Built target test-autorelease
[ 61%] Built target test-quantize-fns
[ 62%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 62%] Built target test-barrier
[ 62%] Built target test-quantize-perf
[ 63%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 63%] Built target test-rope
[ 64%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 64%] Built target llama-batched-bench
[ 65%] Linking CXX executable ../../bin/llama-batched
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 67%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 67%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-embedding
[ 67%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-eval-callback
[ 67%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 68%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 68%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-gguf-split
[ 69%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 72%] Linking CXX executable ../../bin/llama-lookahead
[ 72%] Linking CXX executable ../../bin/llama-bench
[ 72%] Built target llama-batched
[ 72%] Built target llama-embedding
[ 72%] Built target llama-gbnf-validator
[ 72%] Built target llama-eval-callback
[ 72%] Built target llama-gritlm
[ 72%] Built target llama-gguf-split
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 72%] Built target llama-imatrix
[ 72%] Built target llama-infill
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 72%] Built target llama-lookahead
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 72%] Built target llama-bench
[ 73%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 73%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 74%] Linking CXX executable ../../bin/llama-lookup
[ 74%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 76%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 76%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 78%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-lookup-merge
[ 80%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-cli
[ 81%] Linking CXX executable ../../bin/llama-parallel
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 82%] Linking CXX executable ../../bin/llama-passkey
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 82%] Linking CXX executable ../../bin/llama-retrieval
[ 82%] Built target llama-lookup
[ 82%] Built target llama-lookup-create
[ 82%] Built target llama-lookup-stats
[ 82%] Built target llama-lookup-merge
[ 82%] Built target llama-cli
[ 83%] Generating index.html.gz.hpp
[ 83%] Generating loading.html.hpp
[ 83%] Built target llama-parallel
[ 83%] Built target llama-perplexity
[ 83%] Built target llama-passkey
[ 84%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 84%] Built target llama-quantize
[ 84%] Built target llama-retrieval
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 87%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-save-load-state
[ 88%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-run
[ 89%] Linking CXX executable ../../bin/llama-speculative
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-speculative-simple
[ 92%] Linking CXX executable ../../bin/llama-tokenize
[ 92%] Linking CXX executable ../../bin/llama-gen-docs
[ 93%] Linking CXX executable ../../bin/llama-tts
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Built target llama-run
[ 93%] Built target llama-save-load-state
[ 93%] Built target llama-speculative
[ 93%] Built target llama-tokenize
[ 93%] Built target llama-gen-docs
[ 93%] Built target llama-speculative-simple
[ 93%] Built target llama-tts
[ 93%] Built target llama-convert-llama2c-to-ggml
[ 93%] Built target llama-cvector-generator
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 97%] Linking CXX executable ../../bin/llama-llava-cli
[ 98%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 98%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-qwen2vl-cli
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.863s
user	0m6.029s
sys	0m9.243s
+ python3 ../convert_hf_to_gguf.py ../models-mnt/pythia/1.4B --outfile ../models-mnt/pythia/1.4B/ggml-model-f16.gguf
INFO:hf-to-gguf:Loading model: 1.4B
INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only
INFO:hf-to-gguf:Exporting model...
INFO:hf-to-gguf:gguf: loading model part 'pytorch_model.bin'
INFO:hf-to-gguf:token_embd.weight,         torch.float16 --> F16, shape = {2048, 50304}
INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.0.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.0.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.0.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.0.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.0.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.0.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.0.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.1.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.1.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.1.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.1.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.1.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.1.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.1.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.2.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.2.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.2.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.2.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.2.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.2.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.2.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.3.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.3.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.3.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.3.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.3.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.3.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.3.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.4.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.4.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.4.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.4.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.4.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.4.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.4.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.5.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.5.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.5.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.5.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.5.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.5.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.5.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.6.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.6.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.6.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.6.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.6.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.6.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.6.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.7.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.7.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.7.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.7.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.7.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.7.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.7.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.8.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.8.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.8.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.8.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.8.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.8.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.8.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.9.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.9.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.9.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.9.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.9.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.9.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.9.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.10.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.10.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.10.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.10.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.10.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.10.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.10.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.10.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.11.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.11.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.11.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.11.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.11.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.11.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.11.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.11.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.12.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.12.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.12.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.12.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.12.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.12.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.12.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.12.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.13.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.13.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.13.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.13.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.13.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.13.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.13.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.13.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.14.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.14.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.14.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.14.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.14.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.14.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.14.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.14.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.15.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.15.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.15.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.15.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.15.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.15.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.15.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.15.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.16.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.16.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.16.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.16.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.16.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.16.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.16.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.16.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.17.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.17.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.17.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.17.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.17.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.17.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.17.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.17.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.18.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.18.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.18.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.18.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.18.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.18.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.18.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.18.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.19.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.19.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.19.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.19.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.19.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.19.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.19.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.19.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.20.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.20.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.20.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.20.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.20.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.20.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.20.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.20.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.21.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.21.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.21.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.21.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.21.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.21.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.21.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.21.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.22.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.22.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.22.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.22.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.22.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.22.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.22.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.22.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.23.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.23.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.23.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.23.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.23.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.23.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.23.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.23.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:output_norm.weight,        torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:output_norm.bias,          torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:output.weight,             torch.float16 --> F16, shape = {2048, 50304}
INFO:hf-to-gguf:Set meta model
INFO:hf-to-gguf:Set model parameters
INFO:hf-to-gguf:Set model tokenizer
INFO:gguf.vocab:Adding 50009 merge(s).
INFO:gguf.vocab:Setting special token type bos to 0
INFO:gguf.vocab:Setting special token type eos to 0
INFO:gguf.vocab:Setting special token type unk to 0
INFO:hf-to-gguf:Set model quantization version
INFO:gguf.gguf_writer:Writing the following files:
INFO:gguf.gguf_writer:../models-mnt/pythia/1.4B/ggml-model-f16.gguf: n_tensors = 292, total_size = 2.8G
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Writing:   0%|          | 0.00/2.83G [00:00<?, ?byte/s]Writing:   7%|         | 206M/2.83G [00:00<00:07, 375Mbyte/s]Writing:  10%|         | 273M/2.83G [00:00<00:06, 397Mbyte/s]Writing:  12%|        | 340M/2.83G [00:00<00:05, 427Mbyte/s]Writing:  14%|        | 408M/2.83G [00:00<00:05, 435Mbyte/s]Writing:  17%|        | 475M/2.83G [00:01<00:05, 467Mbyte/s]Writing:  19%|        | 542M/2.83G [00:01<00:04, 495Mbyte/s]Writing:  22%|       | 609M/2.83G [00:01<00:04, 483Mbyte/s]Writing:  24%|       | 676M/2.83G [00:01<00:04, 499Mbyte/s]Writing:  26%|       | 744M/2.83G [00:01<00:04, 505Mbyte/s]Writing:  29%|       | 811M/2.83G [00:01<00:04, 492Mbyte/s]Writing:  31%|       | 878M/2.83G [00:01<00:03, 495Mbyte/s]Writing:  33%|      | 945M/2.83G [00:02<00:03, 506Mbyte/s]Writing:  36%|      | 1.01G/2.83G [00:02<00:03, 494Mbyte/s]Writing:  38%|      | 1.08G/2.83G [00:02<00:03, 497Mbyte/s]Writing:  41%|      | 1.15G/2.83G [00:02<00:03, 507Mbyte/s]Writing:  43%|     | 1.21G/2.83G [00:02<00:03, 487Mbyte/s]Writing:  45%|     | 1.28G/2.83G [00:02<00:03, 500Mbyte/s]Writing:  48%|     | 1.35G/2.83G [00:02<00:02, 507Mbyte/s]Writing:  50%|     | 1.42G/2.83G [00:02<00:02, 491Mbyte/s]Writing:  52%|    | 1.48G/2.83G [00:03<00:02, 514Mbyte/s]Writing:  55%|    | 1.55G/2.83G [00:03<00:02, 522Mbyte/s]Writing:  57%|    | 1.62G/2.83G [00:03<00:02, 505Mbyte/s]Writing:  59%|    | 1.68G/2.83G [00:03<00:02, 511Mbyte/s]Writing:  62%|   | 1.75G/2.83G [00:03<00:02, 520Mbyte/s]Writing:  64%|   | 1.82G/2.83G [00:03<00:02, 502Mbyte/s]Writing:  67%|   | 1.89G/2.83G [00:03<00:01, 510Mbyte/s]Writing:  69%|   | 1.95G/2.83G [00:04<00:01, 517Mbyte/s]Writing:  71%|  | 2.02G/2.83G [00:04<00:01, 495Mbyte/s]Writing:  74%|  | 2.09G/2.83G [00:04<00:01, 499Mbyte/s]Writing:  76%|  | 2.15G/2.83G [00:04<00:01, 499Mbyte/s]Writing:  78%|  | 2.22G/2.83G [00:04<00:01, 470Mbyte/s]Writing:  81%|  | 2.29G/2.83G [00:04<00:01, 471Mbyte/s]Writing:  83%| | 2.35G/2.83G [00:04<00:01, 481Mbyte/s]Writing:  86%| | 2.42G/2.83G [00:05<00:00, 450Mbyte/s]Writing:  88%| | 2.49G/2.83G [00:05<00:00, 454Mbyte/s]Writing:  90%| | 2.55G/2.83G [00:05<00:00, 470Mbyte/s]Writing:  93%|| 2.62G/2.83G [00:05<00:00, 445Mbyte/s]Writing: 100%|| 2.83G/2.83G [00:06<00:00, 376Mbyte/s]Writing: 100%|| 2.83G/2.83G [00:06<00:00, 463Mbyte/s]
INFO:hf-to-gguf:Model successfully exported to ../models-mnt/pythia/1.4B/ggml-model-f16.gguf
+ model_f16=../models-mnt/pythia/1.4B/ggml-model-f16.gguf
+ model_q8_0=../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf
+ model_q4_0=../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf
+ model_q4_1=../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf
+ model_q5_0=../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf
+ model_q5_1=../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf
+ model_q2_k=../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf
+ model_q3_k=../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf
+ model_q4_k=../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf
+ model_q5_k=../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf
+ model_q6_k=../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf
+ wiki_test_60=../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf q8_0
main: build = 4400 (6e1531ac)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf' as Q8_0
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q8_0 .. size =   196.50 MiB ->   104.39 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q8_0 .. size =   196.50 MiB ->   104.39 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
llama_model_quantize_internal: model size  =  2699.45 MB
llama_model_quantize_internal: quant size  =  1435.23 MB

main: quantize time =  3126.86 ms
main:    total time =  3126.86 ms
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf q4_0
main: build = 4400 (6e1531ac)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf' as Q4_0
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q4_0 .. size =   196.50 MiB ->    55.27 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
llama_model_quantize_internal: model size  =  2699.45 MB
llama_model_quantize_internal: quant size  =   786.31 MB
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf q4_1

main: quantize time =  1357.70 ms
main:    total time =  1357.70 ms
main: build = 4400 (6e1531ac)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf' as Q4_1
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q4_1 .. size =   196.50 MiB ->    61.41 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
llama_model_quantize_internal: model size  =  2699.45 MB
llama_model_quantize_internal: quant size  =   864.46 MB
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf q5_0

main: quantize time =  1302.33 ms
main:    total time =  1302.33 ms
main: build = 4400 (6e1531ac)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf' as Q5_0
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q5_0 .. size =   196.50 MiB ->    67.55 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
llama_model_quantize_internal: model size  =  2699.45 MB
llama_model_quantize_internal: quant size  =   942.60 MB

main: quantize time =  1476.91 ms
main:    total time =  1476.91 ms
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf q5_1
main: build = 4400 (6e1531ac)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf' as Q5_1
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q5_1 .. size =   196.50 MiB ->    73.69 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
llama_model_quantize_internal: model size  =  2699.45 MB
llama_model_quantize_internal: quant size  =  1020.74 MB

main: quantize time =  2174.05 ms
main:    total time =  2174.05 ms
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf q2_k
main: build = 4400 (6e1531ac)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf' as Q2_K
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q2_K .. size =   196.50 MiB ->    32.24 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
llama_model_quantize_internal: model size  =  2699.45 MB
llama_model_quantize_internal: quant size  =   542.04 MB

main: quantize time =  4956.52 ms
main:    total time =  4956.52 ms
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf q3_k
main: build = 4400 (6e1531ac)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf' as Q3_K
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q3_K .. size =   196.50 MiB ->    42.22 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
llama_model_quantize_internal: model size  =  2699.45 MB
llama_model_quantize_internal: quant size  =   724.27 MB
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf q4_k

main: quantize time =  5549.05 ms
main:    total time =  5549.05 ms
main: build = 4400 (6e1531ac)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf' as Q4_K
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q4_K .. size =   196.50 MiB ->    55.27 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
llama_model_quantize_internal: model size  =  2699.45 MB
llama_model_quantize_internal: quant size  =   871.81 MB

main: quantize time =  6918.52 ms
main:    total time =  6918.52 ms
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf q5_k
main: build = 4400 (6e1531ac)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf' as Q5_K
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q5_K .. size =   196.50 MiB ->    67.55 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
llama_model_quantize_internal: model size  =  2699.45 MB
llama_model_quantize_internal: quant size  =  1006.35 MB
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf q6_k

main: quantize time =  5848.88 ms
main:    total time =  5848.88 ms
main: build = 4400 (6e1531ac)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf' as Q6_K
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
llama_model_quantize_internal: model size  =  2699.45 MB
llama_model_quantize_internal: quant size  =  1108.64 MB
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-tg-f16.log

main: quantize time =  4652.19 ms
main:    total time =  4652.19 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.113 I build: 4400 (6e1531ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.228 I main: llama backend init
0.00.000.235 I main: load the model and apply lora adapter, if any
0.00.041.445 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.052.436 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.052.453 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.052.457 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.052.458 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.052.458 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.052.459 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.052.459 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.052.462 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.052.462 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.052.463 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.052.464 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.052.464 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.052.465 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.052.469 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.052.474 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.052.475 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.052.475 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.059.971 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.062.706 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.070.891 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.070.894 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.070.895 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.070.895 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.070.896 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.070.897 I llama_model_loader: - type  f32:  194 tensors
0.00.070.897 I llama_model_loader: - type  f16:   98 tensors
0.00.103.536 I llm_load_vocab: special tokens cache size = 25
0.00.110.466 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.110.469 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.110.469 I llm_load_print_meta: arch             = gptneox
0.00.110.470 I llm_load_print_meta: vocab type       = BPE
0.00.110.470 I llm_load_print_meta: n_vocab          = 50304
0.00.110.470 I llm_load_print_meta: n_merges         = 50009
0.00.110.470 I llm_load_print_meta: vocab_only       = 0
0.00.110.470 I llm_load_print_meta: n_ctx_train      = 2048
0.00.110.470 I llm_load_print_meta: n_embd           = 2048
0.00.110.470 I llm_load_print_meta: n_layer          = 24
0.00.110.474 I llm_load_print_meta: n_head           = 16
0.00.110.475 I llm_load_print_meta: n_head_kv        = 16
0.00.110.475 I llm_load_print_meta: n_rot            = 32
0.00.110.475 I llm_load_print_meta: n_swa            = 0
0.00.110.475 I llm_load_print_meta: n_embd_head_k    = 128
0.00.110.475 I llm_load_print_meta: n_embd_head_v    = 128
0.00.110.476 I llm_load_print_meta: n_gqa            = 1
0.00.110.477 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.110.477 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.110.478 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.110.478 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.110.478 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.110.479 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.110.479 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.110.479 I llm_load_print_meta: n_ff             = 8192
0.00.110.480 I llm_load_print_meta: n_expert         = 0
0.00.110.480 I llm_load_print_meta: n_expert_used    = 0
0.00.110.480 I llm_load_print_meta: causal attn      = 1
0.00.110.480 I llm_load_print_meta: pooling type     = 0
0.00.110.480 I llm_load_print_meta: rope type        = 2
0.00.110.480 I llm_load_print_meta: rope scaling     = linear
0.00.110.481 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.110.481 I llm_load_print_meta: freq_scale_train = 1
0.00.110.481 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.110.481 I llm_load_print_meta: rope_finetuned   = unknown
0.00.110.481 I llm_load_print_meta: ssm_d_conv       = 0
0.00.110.482 I llm_load_print_meta: ssm_d_inner      = 0
0.00.110.482 I llm_load_print_meta: ssm_d_state      = 0
0.00.110.482 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.110.483 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.110.484 I llm_load_print_meta: model type       = 1.4B
0.00.110.484 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.110.485 I llm_load_print_meta: model params     = 1.41 B
0.00.110.485 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.110.485 I llm_load_print_meta: general.name     = 1.4B
0.00.110.485 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.110.486 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.110.486 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.110.486 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.110.486 I llm_load_print_meta: LF token         = 128 ''
0.00.110.486 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.110.487 I llm_load_print_meta: max token length = 1024
0.00.113.077 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.113.077 I llm_load_tensors: offloading output layer to GPU
0.00.113.078 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.113.097 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.113.098 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.114.098 I llama_new_context_with_model: n_seq_max     = 1
0.00.114.099 I llama_new_context_with_model: n_ctx         = 2048
0.00.114.099 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.114.099 I llama_new_context_with_model: n_batch       = 2048
0.00.114.099 I llama_new_context_with_model: n_ubatch      = 512
0.00.114.099 I llama_new_context_with_model: flash_attn    = 0
0.00.114.100 I llama_new_context_with_model: freq_base     = 10000.0
0.00.114.100 I llama_new_context_with_model: freq_scale    = 1
0.00.114.101 I ggml_metal_init: allocating
0.00.114.111 I ggml_metal_init: found device: Apple M4
0.00.114.113 I ggml_metal_init: picking default device: Apple M4
0.00.114.825 I ggml_metal_init: using embedded metal library
0.00.142.617 I ggml_metal_init: GPU name:   Apple M4
0.00.142.619 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.142.620 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.142.620 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.142.620 I ggml_metal_init: simdgroup reduction   = true
0.00.142.621 I ggml_metal_init: simdgroup matrix mul. = true
0.00.142.621 I ggml_metal_init: has bfloat            = true
0.00.142.621 I ggml_metal_init: use bfloat            = true
0.00.142.621 I ggml_metal_init: hasUnifiedMemory      = true
0.00.142.622 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.246.305 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.266.292 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.266.297 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.266.318 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.267.290 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.267.292 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.267.292 I llama_new_context_with_model: graph nodes  = 967
0.00.267.292 I llama_new_context_with_model: graph splits = 2
0.00.267.316 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.267.451 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.267.452 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.346.052 I main: llama threadpool init, n_threads = 4
0.00.346.087 I 
0.00.346.107 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.346.107 I 
0.00.346.179 I sampler seed: 1234
0.00.346.184 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.346.207 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.346.208 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.346.208 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.187.709 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54157.13 tokens per second)
0.02.187.710 I llama_perf_context_print:        load time =     304.60 ms
0.02.187.711 I llama_perf_context_print: prompt eval time =      44.17 ms /     7 tokens (    6.31 ms per token,   158.49 tokens per second)
0.02.187.712 I llama_perf_context_print:        eval time =    1794.42 ms /    63 runs   (   28.48 ms per token,    35.11 tokens per second)
0.02.187.712 I llama_perf_context_print:       total time =    1841.66 ms /    70 tokens
0.02.187.900 I ggml_metal_free: deallocating

real	0m2.509s
user	0m0.147s
sys	0m0.101s
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-tg-q8_0.log
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4400 (6e1531ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.009.590 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.863 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.868 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.871 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.871 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.871 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.872 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.872 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.873 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.873 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.874 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.874 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.874 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.875 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.875 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.878 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.878 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.879 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.762 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.823 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.629 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.631 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.631 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.632 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.632 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.632 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.027.633 I llama_model_loader: - type  f32:  194 tensors
0.00.027.633 I llama_model_loader: - type q8_0:   98 tensors
0.00.049.070 I llm_load_vocab: special tokens cache size = 25
0.00.055.052 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.055.056 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.055.056 I llm_load_print_meta: arch             = gptneox
0.00.055.057 I llm_load_print_meta: vocab type       = BPE
0.00.055.057 I llm_load_print_meta: n_vocab          = 50304
0.00.055.057 I llm_load_print_meta: n_merges         = 50009
0.00.055.057 I llm_load_print_meta: vocab_only       = 0
0.00.055.058 I llm_load_print_meta: n_ctx_train      = 2048
0.00.055.060 I llm_load_print_meta: n_embd           = 2048
0.00.055.060 I llm_load_print_meta: n_layer          = 24
0.00.055.063 I llm_load_print_meta: n_head           = 16
0.00.055.064 I llm_load_print_meta: n_head_kv        = 16
0.00.055.064 I llm_load_print_meta: n_rot            = 32
0.00.055.064 I llm_load_print_meta: n_swa            = 0
0.00.055.064 I llm_load_print_meta: n_embd_head_k    = 128
0.00.055.065 I llm_load_print_meta: n_embd_head_v    = 128
0.00.055.065 I llm_load_print_meta: n_gqa            = 1
0.00.055.066 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.055.067 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.055.068 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.055.068 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.055.068 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.055.068 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.055.068 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.055.069 I llm_load_print_meta: n_ff             = 8192
0.00.055.069 I llm_load_print_meta: n_expert         = 0
0.00.055.069 I llm_load_print_meta: n_expert_used    = 0
0.00.055.070 I llm_load_print_meta: causal attn      = 1
0.00.055.070 I llm_load_print_meta: pooling type     = 0
0.00.055.070 I llm_load_print_meta: rope type        = 2
0.00.055.070 I llm_load_print_meta: rope scaling     = linear
0.00.055.071 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.055.071 I llm_load_print_meta: freq_scale_train = 1
0.00.055.073 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.055.074 I llm_load_print_meta: rope_finetuned   = unknown
0.00.055.074 I llm_load_print_meta: ssm_d_conv       = 0
0.00.055.074 I llm_load_print_meta: ssm_d_inner      = 0
0.00.055.074 I llm_load_print_meta: ssm_d_state      = 0
0.00.055.074 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.055.074 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.055.075 I llm_load_print_meta: model type       = 1.4B
0.00.055.075 I llm_load_print_meta: model ftype      = Q8_0
0.00.055.075 I llm_load_print_meta: model params     = 1.41 B
0.00.055.076 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.055.076 I llm_load_print_meta: general.name     = 1.4B
0.00.055.077 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.055.077 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.055.077 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.055.077 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.055.077 I llm_load_print_meta: LF token         = 128 ''
0.00.055.078 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.055.078 I llm_load_print_meta: max token length = 1024
0.00.057.518 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.057.519 I llm_load_tensors: offloading output layer to GPU
0.00.057.519 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.057.530 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.057.532 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.058.548 I llama_new_context_with_model: n_seq_max     = 1
0.00.058.549 I llama_new_context_with_model: n_ctx         = 2048
0.00.058.549 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.058.549 I llama_new_context_with_model: n_batch       = 2048
0.00.058.549 I llama_new_context_with_model: n_ubatch      = 512
0.00.058.549 I llama_new_context_with_model: flash_attn    = 0
0.00.058.550 I llama_new_context_with_model: freq_base     = 10000.0
0.00.058.550 I llama_new_context_with_model: freq_scale    = 1
0.00.058.551 I ggml_metal_init: allocating
0.00.058.554 I ggml_metal_init: found device: Apple M4
0.00.058.556 I ggml_metal_init: picking default device: Apple M4
0.00.059.303 I ggml_metal_init: using embedded metal library
0.00.061.852 I ggml_metal_init: GPU name:   Apple M4
0.00.061.853 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.853 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.854 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.854 I ggml_metal_init: simdgroup reduction   = true
0.00.061.854 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.855 I ggml_metal_init: has bfloat            = true
0.00.061.855 I ggml_metal_init: use bfloat            = true
0.00.061.855 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.856 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.072.223 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.096.947 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.096.963 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.096.993 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.098.027 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.098.030 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.098.030 I llama_new_context_with_model: graph nodes  = 967
0.00.098.030 I llama_new_context_with_model: graph splits = 2
0.00.098.042 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.098.182 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.098.183 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.154.490 I main: llama threadpool init, n_threads = 4
0.01.154.524 I 
0.01.154.547 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.154.548 I 
0.01.154.805 I sampler seed: 1234
0.01.154.810 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.154.857 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.154.858 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.154.858 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.250.067 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51486.58 tokens per second)
0.02.250.068 I llama_perf_context_print:        load time =    1144.89 ms
0.02.250.069 I llama_perf_context_print: prompt eval time =      43.86 ms /     7 tokens (    6.27 ms per token,   159.61 tokens per second)
0.02.250.070 I llama_perf_context_print:        eval time =    1048.55 ms /    63 runs   (   16.64 ms per token,    60.08 tokens per second)
0.02.250.070 I llama_perf_context_print:       total time =    1095.58 ms /    70 tokens
0.02.250.293 I ggml_metal_free: deallocating

real	0m2.269s
user	0m0.112s
sys	0m0.209s
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-tg-q4_0.log
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4400 (6e1531ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.016.875 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.032.152 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.032.159 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.160 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.161 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.161 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.161 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.162 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.165 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.165 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.165 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.165 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.166 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.166 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.166 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.171 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.171 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.171 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.036.245 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.037.369 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.041.764 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.041.766 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.041.766 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.041.766 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.041.767 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.041.767 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.041.767 I llama_model_loader: - type  f32:  194 tensors
0.00.041.768 I llama_model_loader: - type q4_0:   97 tensors
0.00.041.768 I llama_model_loader: - type q6_K:    1 tensors
0.00.067.307 I llm_load_vocab: special tokens cache size = 25
0.00.075.647 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.075.650 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.075.651 I llm_load_print_meta: arch             = gptneox
0.00.075.651 I llm_load_print_meta: vocab type       = BPE
0.00.075.652 I llm_load_print_meta: n_vocab          = 50304
0.00.075.652 I llm_load_print_meta: n_merges         = 50009
0.00.075.652 I llm_load_print_meta: vocab_only       = 0
0.00.075.652 I llm_load_print_meta: n_ctx_train      = 2048
0.00.075.652 I llm_load_print_meta: n_embd           = 2048
0.00.075.653 I llm_load_print_meta: n_layer          = 24
0.00.075.659 I llm_load_print_meta: n_head           = 16
0.00.075.660 I llm_load_print_meta: n_head_kv        = 16
0.00.075.661 I llm_load_print_meta: n_rot            = 32
0.00.075.661 I llm_load_print_meta: n_swa            = 0
0.00.075.661 I llm_load_print_meta: n_embd_head_k    = 128
0.00.075.661 I llm_load_print_meta: n_embd_head_v    = 128
0.00.075.662 I llm_load_print_meta: n_gqa            = 1
0.00.075.663 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.075.664 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.075.665 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.075.665 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.075.665 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.075.665 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.075.665 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.075.666 I llm_load_print_meta: n_ff             = 8192
0.00.075.667 I llm_load_print_meta: n_expert         = 0
0.00.075.667 I llm_load_print_meta: n_expert_used    = 0
0.00.075.669 I llm_load_print_meta: causal attn      = 1
0.00.075.671 I llm_load_print_meta: pooling type     = 0
0.00.075.671 I llm_load_print_meta: rope type        = 2
0.00.075.671 I llm_load_print_meta: rope scaling     = linear
0.00.075.672 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.075.672 I llm_load_print_meta: freq_scale_train = 1
0.00.075.672 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.075.673 I llm_load_print_meta: rope_finetuned   = unknown
0.00.075.673 I llm_load_print_meta: ssm_d_conv       = 0
0.00.075.673 I llm_load_print_meta: ssm_d_inner      = 0
0.00.075.673 I llm_load_print_meta: ssm_d_state      = 0
0.00.075.674 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.075.674 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.075.674 I llm_load_print_meta: model type       = 1.4B
0.00.075.674 I llm_load_print_meta: model ftype      = Q4_0
0.00.075.675 I llm_load_print_meta: model params     = 1.41 B
0.00.075.675 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.075.676 I llm_load_print_meta: general.name     = 1.4B
0.00.075.676 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.075.676 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.075.681 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.075.681 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.075.681 I llm_load_print_meta: LF token         = 128 ''
0.00.075.681 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.075.682 I llm_load_print_meta: max token length = 1024
0.00.078.396 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.078.396 I llm_load_tensors: offloading output layer to GPU
0.00.078.396 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.078.409 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.078.410 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.079.519 I llama_new_context_with_model: n_seq_max     = 1
0.00.079.520 I llama_new_context_with_model: n_ctx         = 2048
0.00.079.521 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.079.521 I llama_new_context_with_model: n_batch       = 2048
0.00.079.521 I llama_new_context_with_model: n_ubatch      = 512
0.00.079.521 I llama_new_context_with_model: flash_attn    = 0
0.00.079.522 I llama_new_context_with_model: freq_base     = 10000.0
0.00.079.522 I llama_new_context_with_model: freq_scale    = 1
0.00.079.523 I ggml_metal_init: allocating
0.00.079.526 I ggml_metal_init: found device: Apple M4
0.00.079.528 I ggml_metal_init: picking default device: Apple M4
0.00.080.358 I ggml_metal_init: using embedded metal library
0.00.083.481 I ggml_metal_init: GPU name:   Apple M4
0.00.083.483 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.083.483 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.083.483 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.083.484 I ggml_metal_init: simdgroup reduction   = true
0.00.083.484 I ggml_metal_init: simdgroup matrix mul. = true
0.00.083.484 I ggml_metal_init: has bfloat            = true
0.00.083.486 I ggml_metal_init: use bfloat            = true
0.00.083.487 I ggml_metal_init: hasUnifiedMemory      = true
0.00.083.489 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.096.024 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.121.170 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.121.180 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.121.217 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.122.394 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.122.396 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.122.397 I llama_new_context_with_model: graph nodes  = 967
0.00.122.397 I llama_new_context_with_model: graph splits = 2
0.00.122.415 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.122.544 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.122.545 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.822.581 I main: llama threadpool init, n_threads = 4
0.00.822.619 I 
0.00.822.639 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.822.639 I 
0.00.822.863 I sampler seed: 1234
0.00.822.867 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.822.879 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.822.879 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.822.879 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.499.615 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56709.27 tokens per second)
0.01.499.616 I llama_perf_context_print:        load time =     805.70 ms
0.01.499.617 I llama_perf_context_print: prompt eval time =      43.67 ms /     7 tokens (    6.24 ms per token,   160.28 tokens per second)
0.01.499.618 I llama_perf_context_print:        eval time =     630.00 ms /    63 runs   (   10.00 ms per token,   100.00 tokens per second)
0.01.499.618 I llama_perf_context_print:       total time =     677.04 ms /    70 tokens
0.01.499.793 I ggml_metal_free: deallocating

real	0m1.516s
user	0m0.123s
sys	0m0.166s
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-tg-q4_1.log
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4400 (6e1531ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.060 I main: llama backend init
0.00.000.062 I main: load the model and apply lora adapter, if any
0.00.008.904 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.478 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.023.482 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.483 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.484 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.485 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.485 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.486 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.486 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.487 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.487 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.487 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.488 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.488 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.490 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.493 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.497 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.497 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.253 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.295 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.019 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.032.020 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.020 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.021 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.021 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.021 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.032.022 I llama_model_loader: - type  f32:  194 tensors
0.00.032.022 I llama_model_loader: - type q4_1:   97 tensors
0.00.032.022 I llama_model_loader: - type q6_K:    1 tensors
0.00.052.997 I llm_load_vocab: special tokens cache size = 25
0.00.058.864 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.058.866 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.058.866 I llm_load_print_meta: arch             = gptneox
0.00.058.867 I llm_load_print_meta: vocab type       = BPE
0.00.058.867 I llm_load_print_meta: n_vocab          = 50304
0.00.058.867 I llm_load_print_meta: n_merges         = 50009
0.00.058.867 I llm_load_print_meta: vocab_only       = 0
0.00.058.867 I llm_load_print_meta: n_ctx_train      = 2048
0.00.058.868 I llm_load_print_meta: n_embd           = 2048
0.00.058.868 I llm_load_print_meta: n_layer          = 24
0.00.058.871 I llm_load_print_meta: n_head           = 16
0.00.058.872 I llm_load_print_meta: n_head_kv        = 16
0.00.058.872 I llm_load_print_meta: n_rot            = 32
0.00.058.872 I llm_load_print_meta: n_swa            = 0
0.00.058.872 I llm_load_print_meta: n_embd_head_k    = 128
0.00.058.872 I llm_load_print_meta: n_embd_head_v    = 128
0.00.058.873 I llm_load_print_meta: n_gqa            = 1
0.00.058.874 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.058.874 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.058.875 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.058.875 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.058.875 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.058.879 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.058.880 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.058.880 I llm_load_print_meta: n_ff             = 8192
0.00.058.881 I llm_load_print_meta: n_expert         = 0
0.00.058.881 I llm_load_print_meta: n_expert_used    = 0
0.00.058.882 I llm_load_print_meta: causal attn      = 1
0.00.058.883 I llm_load_print_meta: pooling type     = 0
0.00.058.884 I llm_load_print_meta: rope type        = 2
0.00.058.884 I llm_load_print_meta: rope scaling     = linear
0.00.058.884 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.058.885 I llm_load_print_meta: freq_scale_train = 1
0.00.058.885 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.058.885 I llm_load_print_meta: rope_finetuned   = unknown
0.00.058.885 I llm_load_print_meta: ssm_d_conv       = 0
0.00.058.885 I llm_load_print_meta: ssm_d_inner      = 0
0.00.058.885 I llm_load_print_meta: ssm_d_state      = 0
0.00.058.886 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.058.886 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.058.886 I llm_load_print_meta: model type       = 1.4B
0.00.058.886 I llm_load_print_meta: model ftype      = Q4_1
0.00.058.889 I llm_load_print_meta: model params     = 1.41 B
0.00.058.890 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.058.890 I llm_load_print_meta: general.name     = 1.4B
0.00.058.890 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.058.891 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.058.891 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.058.891 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.058.891 I llm_load_print_meta: LF token         = 128 ''
0.00.058.891 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.058.892 I llm_load_print_meta: max token length = 1024
0.00.060.830 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.060.830 I llm_load_tensors: offloading output layer to GPU
0.00.060.830 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.060.840 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.060.841 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.061.734 I llama_new_context_with_model: n_seq_max     = 1
0.00.061.735 I llama_new_context_with_model: n_ctx         = 2048
0.00.061.735 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.061.735 I llama_new_context_with_model: n_batch       = 2048
0.00.061.735 I llama_new_context_with_model: n_ubatch      = 512
0.00.061.735 I llama_new_context_with_model: flash_attn    = 0
0.00.061.736 I llama_new_context_with_model: freq_base     = 10000.0
0.00.061.736 I llama_new_context_with_model: freq_scale    = 1
0.00.061.737 I ggml_metal_init: allocating
0.00.061.745 I ggml_metal_init: found device: Apple M4
0.00.061.747 I ggml_metal_init: picking default device: Apple M4
0.00.062.367 I ggml_metal_init: using embedded metal library
0.00.064.739 I ggml_metal_init: GPU name:   Apple M4
0.00.064.740 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.064.741 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.064.741 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.064.741 I ggml_metal_init: simdgroup reduction   = true
0.00.064.742 I ggml_metal_init: simdgroup matrix mul. = true
0.00.064.742 I ggml_metal_init: has bfloat            = true
0.00.064.743 I ggml_metal_init: use bfloat            = true
0.00.064.743 I ggml_metal_init: hasUnifiedMemory      = true
0.00.064.744 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.074.350 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.096.094 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.096.102 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.096.127 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.097.259 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.097.260 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.097.261 I llama_new_context_with_model: graph nodes  = 967
0.00.097.261 I llama_new_context_with_model: graph splits = 2
0.00.097.269 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.097.431 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.097.431 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.930.304 I main: llama threadpool init, n_threads = 4
0.00.930.385 I 
0.00.930.434 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.930.434 I 
0.00.930.961 I sampler seed: 1234
0.00.930.968 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.931.041 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.931.064 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.931.065 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.670.476 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57258.06 tokens per second)
0.01.670.477 I llama_perf_context_print:        load time =     921.39 ms
0.01.670.477 I llama_perf_context_print: prompt eval time =      50.49 ms /     7 tokens (    7.21 ms per token,   138.63 tokens per second)
0.01.670.478 I llama_perf_context_print:        eval time =     685.94 ms /    63 runs   (   10.89 ms per token,    91.85 tokens per second)
0.01.670.478 I llama_perf_context_print:       total time =     740.18 ms /    70 tokens
0.01.670.677 I ggml_metal_free: deallocating

real	0m1.692s
user	0m0.121s
sys	0m0.178s
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-tg-q5_0.log
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4400 (6e1531ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.010.231 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.632 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.028.636 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.642 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.643 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.643 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.643 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.645 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.646 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.646 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.647 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.647 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.647 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.650 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.650 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.652 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.652 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.653 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.759 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.926 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.980 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.037.982 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.982 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.982 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.983 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.983 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.037.983 I llama_model_loader: - type  f32:  194 tensors
0.00.037.984 I llama_model_loader: - type q5_0:   97 tensors
0.00.037.984 I llama_model_loader: - type q6_K:    1 tensors
0.00.063.157 I llm_load_vocab: special tokens cache size = 25
0.00.071.236 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.071.240 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.071.240 I llm_load_print_meta: arch             = gptneox
0.00.071.241 I llm_load_print_meta: vocab type       = BPE
0.00.071.241 I llm_load_print_meta: n_vocab          = 50304
0.00.071.241 I llm_load_print_meta: n_merges         = 50009
0.00.071.241 I llm_load_print_meta: vocab_only       = 0
0.00.071.241 I llm_load_print_meta: n_ctx_train      = 2048
0.00.071.242 I llm_load_print_meta: n_embd           = 2048
0.00.071.242 I llm_load_print_meta: n_layer          = 24
0.00.071.245 I llm_load_print_meta: n_head           = 16
0.00.071.246 I llm_load_print_meta: n_head_kv        = 16
0.00.071.246 I llm_load_print_meta: n_rot            = 32
0.00.071.247 I llm_load_print_meta: n_swa            = 0
0.00.071.247 I llm_load_print_meta: n_embd_head_k    = 128
0.00.071.247 I llm_load_print_meta: n_embd_head_v    = 128
0.00.071.248 I llm_load_print_meta: n_gqa            = 1
0.00.071.249 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.071.249 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.071.250 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.071.251 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.071.251 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.071.251 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.071.251 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.071.252 I llm_load_print_meta: n_ff             = 8192
0.00.071.252 I llm_load_print_meta: n_expert         = 0
0.00.071.255 I llm_load_print_meta: n_expert_used    = 0
0.00.071.255 I llm_load_print_meta: causal attn      = 1
0.00.071.255 I llm_load_print_meta: pooling type     = 0
0.00.071.255 I llm_load_print_meta: rope type        = 2
0.00.071.255 I llm_load_print_meta: rope scaling     = linear
0.00.071.256 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.071.256 I llm_load_print_meta: freq_scale_train = 1
0.00.071.256 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.071.256 I llm_load_print_meta: rope_finetuned   = unknown
0.00.071.257 I llm_load_print_meta: ssm_d_conv       = 0
0.00.071.257 I llm_load_print_meta: ssm_d_inner      = 0
0.00.071.257 I llm_load_print_meta: ssm_d_state      = 0
0.00.071.257 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.071.257 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.071.257 I llm_load_print_meta: model type       = 1.4B
0.00.071.258 I llm_load_print_meta: model ftype      = Q5_0
0.00.071.258 I llm_load_print_meta: model params     = 1.41 B
0.00.071.259 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.071.259 I llm_load_print_meta: general.name     = 1.4B
0.00.071.259 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.071.260 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.071.260 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.071.261 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.071.262 I llm_load_print_meta: LF token         = 128 ''
0.00.071.262 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.071.262 I llm_load_print_meta: max token length = 1024
0.00.073.636 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.073.637 I llm_load_tensors: offloading output layer to GPU
0.00.073.637 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.073.643 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.073.643 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.074.852 I llama_new_context_with_model: n_seq_max     = 1
0.00.074.853 I llama_new_context_with_model: n_ctx         = 2048
0.00.074.854 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.074.854 I llama_new_context_with_model: n_batch       = 2048
0.00.074.854 I llama_new_context_with_model: n_ubatch      = 512
0.00.074.854 I llama_new_context_with_model: flash_attn    = 0
0.00.074.855 I llama_new_context_with_model: freq_base     = 10000.0
0.00.074.855 I llama_new_context_with_model: freq_scale    = 1
0.00.074.856 I ggml_metal_init: allocating
0.00.074.859 I ggml_metal_init: found device: Apple M4
0.00.074.862 I ggml_metal_init: picking default device: Apple M4
0.00.075.648 I ggml_metal_init: using embedded metal library
0.00.079.277 I ggml_metal_init: GPU name:   Apple M4
0.00.079.279 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.079.280 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.079.280 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.079.281 I ggml_metal_init: simdgroup reduction   = true
0.00.079.281 I ggml_metal_init: simdgroup matrix mul. = true
0.00.079.281 I ggml_metal_init: has bfloat            = true
0.00.079.281 I ggml_metal_init: use bfloat            = true
0.00.079.282 I ggml_metal_init: hasUnifiedMemory      = true
0.00.079.283 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.091.446 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.113.683 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.113.692 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.113.715 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.114.716 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.114.717 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.114.717 I llama_new_context_with_model: graph nodes  = 967
0.00.114.718 I llama_new_context_with_model: graph splits = 2
0.00.114.734 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.114.863 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.114.863 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.886.919 I main: llama threadpool init, n_threads = 4
0.00.886.960 I 
0.00.886.980 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.886.980 I 
0.00.887.203 I sampler seed: 1234
0.00.887.208 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.887.229 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.887.230 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.887.230 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.679.698 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59865.09 tokens per second)
0.01.679.699 I llama_perf_context_print:        load time =     876.68 ms
0.01.679.699 I llama_perf_context_print: prompt eval time =      43.15 ms /     7 tokens (    6.16 ms per token,   162.22 tokens per second)
0.01.679.700 I llama_perf_context_print:        eval time =     746.35 ms /    63 runs   (   11.85 ms per token,    84.41 tokens per second)
0.01.679.700 I llama_perf_context_print:       total time =     792.78 ms /    70 tokens
0.01.679.866 I ggml_metal_free: deallocating

real	0m1.699s
user	0m0.124s
sys	0m0.165s
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-tg-q5_1.log
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4400 (6e1531ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.008.722 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.803 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.807 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.809 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.809 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.809 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.810 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.815 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.816 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.816 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.816 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.817 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.817 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.817 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.818 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.820 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.821 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.821 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.694 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.733 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.544 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.545 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.546 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.546 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.546 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.546 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.547 I llama_model_loader: - type  f32:  194 tensors
0.00.023.547 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.548 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.882 I llm_load_vocab: special tokens cache size = 25
0.00.049.708 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.711 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.711 I llm_load_print_meta: arch             = gptneox
0.00.049.711 I llm_load_print_meta: vocab type       = BPE
0.00.049.712 I llm_load_print_meta: n_vocab          = 50304
0.00.049.712 I llm_load_print_meta: n_merges         = 50009
0.00.049.712 I llm_load_print_meta: vocab_only       = 0
0.00.049.712 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.712 I llm_load_print_meta: n_embd           = 2048
0.00.049.712 I llm_load_print_meta: n_layer          = 24
0.00.049.715 I llm_load_print_meta: n_head           = 16
0.00.049.716 I llm_load_print_meta: n_head_kv        = 16
0.00.049.716 I llm_load_print_meta: n_rot            = 32
0.00.049.716 I llm_load_print_meta: n_swa            = 0
0.00.049.719 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.719 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.720 I llm_load_print_meta: n_gqa            = 1
0.00.049.721 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.721 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.722 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.722 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.722 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.723 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.723 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.723 I llm_load_print_meta: n_ff             = 8192
0.00.049.724 I llm_load_print_meta: n_expert         = 0
0.00.049.724 I llm_load_print_meta: n_expert_used    = 0
0.00.049.725 I llm_load_print_meta: causal attn      = 1
0.00.049.727 I llm_load_print_meta: pooling type     = 0
0.00.049.727 I llm_load_print_meta: rope type        = 2
0.00.049.727 I llm_load_print_meta: rope scaling     = linear
0.00.049.728 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.728 I llm_load_print_meta: freq_scale_train = 1
0.00.049.728 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.728 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.729 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.729 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.729 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.729 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.729 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.733 I llm_load_print_meta: model type       = 1.4B
0.00.049.733 I llm_load_print_meta: model ftype      = Q5_1
0.00.049.734 I llm_load_print_meta: model params     = 1.41 B
0.00.049.734 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.049.734 I llm_load_print_meta: general.name     = 1.4B
0.00.049.734 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.735 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.735 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.736 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.736 I llm_load_print_meta: LF token         = 128 ''
0.00.049.737 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.737 I llm_load_print_meta: max token length = 1024
0.00.051.755 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.756 I llm_load_tensors: offloading output layer to GPU
0.00.051.756 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.766 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.767 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.676 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.677 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.677 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.677 I llama_new_context_with_model: n_batch       = 2048
0.00.052.677 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.677 I llama_new_context_with_model: flash_attn    = 0
0.00.052.678 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.678 I llama_new_context_with_model: freq_scale    = 1
0.00.052.679 I ggml_metal_init: allocating
0.00.052.682 I ggml_metal_init: found device: Apple M4
0.00.052.684 I ggml_metal_init: picking default device: Apple M4
0.00.053.283 I ggml_metal_init: using embedded metal library
0.00.055.593 I ggml_metal_init: GPU name:   Apple M4
0.00.055.595 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.595 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.595 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.596 I ggml_metal_init: simdgroup reduction   = true
0.00.055.597 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.597 I ggml_metal_init: has bfloat            = true
0.00.055.597 I ggml_metal_init: use bfloat            = true
0.00.055.598 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.598 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.270 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.085.103 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.112 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.136 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.102 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.104 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.104 I llama_new_context_with_model: graph nodes  = 967
0.00.086.104 I llama_new_context_with_model: graph splits = 2
0.00.086.119 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.260 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.260 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.728.794 I main: llama threadpool init, n_threads = 4
0.00.728.832 I 
0.00.728.851 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.728.851 I 
0.00.729.083 I sampler seed: 1234
0.00.729.087 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.729.109 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.729.109 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.729.109 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.559.420 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59216.01 tokens per second)
0.01.559.420 I llama_perf_context_print:        load time =     720.07 ms
0.01.559.421 I llama_perf_context_print: prompt eval time =      42.28 ms /     7 tokens (    6.04 ms per token,   165.56 tokens per second)
0.01.559.422 I llama_perf_context_print:        eval time =     785.05 ms /    63 runs   (   12.46 ms per token,    80.25 tokens per second)
0.01.559.422 I llama_perf_context_print:       total time =     830.63 ms /    70 tokens
0.01.559.590 I ggml_metal_free: deallocating

real	0m1.576s
user	0m0.108s
sys	0m0.160s
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-tg-q2_k.log
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4400 (6e1531ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.010.407 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.986 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.991 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.993 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.993 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.993 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.994 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.994 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.995 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.995 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.996 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.996 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.996 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.997 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.997 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.999 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.001 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.001 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.820 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.918 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.758 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.759 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.759 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.760 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.760 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.760 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.761 I llama_model_loader: - type  f32:  194 tensors
0.00.024.761 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.761 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.761 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.184 I llm_load_vocab: special tokens cache size = 25
0.00.051.138 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.140 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.141 I llm_load_print_meta: arch             = gptneox
0.00.051.141 I llm_load_print_meta: vocab type       = BPE
0.00.051.141 I llm_load_print_meta: n_vocab          = 50304
0.00.051.142 I llm_load_print_meta: n_merges         = 50009
0.00.051.142 I llm_load_print_meta: vocab_only       = 0
0.00.051.142 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.142 I llm_load_print_meta: n_embd           = 2048
0.00.051.142 I llm_load_print_meta: n_layer          = 24
0.00.051.145 I llm_load_print_meta: n_head           = 16
0.00.051.146 I llm_load_print_meta: n_head_kv        = 16
0.00.051.146 I llm_load_print_meta: n_rot            = 32
0.00.051.146 I llm_load_print_meta: n_swa            = 0
0.00.051.146 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.147 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.147 I llm_load_print_meta: n_gqa            = 1
0.00.051.148 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.149 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.150 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.150 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.150 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.150 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.151 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.151 I llm_load_print_meta: n_ff             = 8192
0.00.051.151 I llm_load_print_meta: n_expert         = 0
0.00.051.152 I llm_load_print_meta: n_expert_used    = 0
0.00.051.152 I llm_load_print_meta: causal attn      = 1
0.00.051.152 I llm_load_print_meta: pooling type     = 0
0.00.051.152 I llm_load_print_meta: rope type        = 2
0.00.051.152 I llm_load_print_meta: rope scaling     = linear
0.00.051.153 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.153 I llm_load_print_meta: freq_scale_train = 1
0.00.051.153 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.154 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.154 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.154 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.154 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.154 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.154 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.155 I llm_load_print_meta: model type       = 1.4B
0.00.051.155 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.155 I llm_load_print_meta: model params     = 1.41 B
0.00.051.156 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.156 I llm_load_print_meta: general.name     = 1.4B
0.00.051.159 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.159 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.159 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.159 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.159 I llm_load_print_meta: LF token         = 128 ''
0.00.051.160 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.160 I llm_load_print_meta: max token length = 1024
0.00.053.111 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.111 I llm_load_tensors: offloading output layer to GPU
0.00.053.111 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.121 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.123 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.093 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.093 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.094 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.094 I llama_new_context_with_model: n_batch       = 2048
0.00.054.094 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.094 I llama_new_context_with_model: flash_attn    = 0
0.00.054.095 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.095 I llama_new_context_with_model: freq_scale    = 1
0.00.054.095 I ggml_metal_init: allocating
0.00.054.098 I ggml_metal_init: found device: Apple M4
0.00.054.101 I ggml_metal_init: picking default device: Apple M4
0.00.054.678 I ggml_metal_init: using embedded metal library
0.00.057.017 I ggml_metal_init: GPU name:   Apple M4
0.00.057.019 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.019 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.019 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.020 I ggml_metal_init: simdgroup reduction   = true
0.00.057.020 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.020 I ggml_metal_init: has bfloat            = true
0.00.057.020 I ggml_metal_init: use bfloat            = true
0.00.057.021 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.021 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.751 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.087.468 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.473 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.499 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.509 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.510 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.511 I llama_new_context_with_model: graph nodes  = 967
0.00.088.511 I llama_new_context_with_model: graph splits = 2
0.00.088.526 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.667 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.668 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.442.687 I main: llama threadpool init, n_threads = 4
0.00.442.724 I 
0.00.442.751 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.442.751 I 
0.00.442.967 I sampler seed: 1234
0.00.442.972 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.443.011 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.443.013 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.443.013 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.120.977 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59314.95 tokens per second)
0.01.120.978 I llama_perf_context_print:        load time =     432.28 ms
0.01.120.979 I llama_perf_context_print: prompt eval time =      35.81 ms /     7 tokens (    5.12 ms per token,   195.47 tokens per second)
0.01.120.979 I llama_perf_context_print:        eval time =     639.21 ms /    63 runs   (   10.15 ms per token,    98.56 tokens per second)
0.01.120.980 I llama_perf_context_print:       total time =     678.29 ms /    70 tokens
0.01.121.163 I ggml_metal_free: deallocating

real	0m1.140s
user	0m0.109s
sys	0m0.111s
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-tg-q3_k.log
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4400 (6e1531ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.009.901 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.326 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.331 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.332 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.333 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.333 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.333 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.334 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.335 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.335 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.335 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.336 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.336 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.336 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.337 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.338 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.338 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.338 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.222 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.316 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.192 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.193 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.194 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.194 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.194 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.195 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.195 I llama_model_loader: - type  f32:  194 tensors
0.00.024.195 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.195 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.196 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.196 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.530 I llm_load_vocab: special tokens cache size = 25
0.00.050.376 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.379 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.379 I llm_load_print_meta: arch             = gptneox
0.00.050.379 I llm_load_print_meta: vocab type       = BPE
0.00.050.380 I llm_load_print_meta: n_vocab          = 50304
0.00.050.380 I llm_load_print_meta: n_merges         = 50009
0.00.050.380 I llm_load_print_meta: vocab_only       = 0
0.00.050.380 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.380 I llm_load_print_meta: n_embd           = 2048
0.00.050.381 I llm_load_print_meta: n_layer          = 24
0.00.050.383 I llm_load_print_meta: n_head           = 16
0.00.050.384 I llm_load_print_meta: n_head_kv        = 16
0.00.050.384 I llm_load_print_meta: n_rot            = 32
0.00.050.384 I llm_load_print_meta: n_swa            = 0
0.00.050.384 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.384 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.385 I llm_load_print_meta: n_gqa            = 1
0.00.050.386 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.387 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.387 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.388 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.388 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.388 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.388 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.389 I llm_load_print_meta: n_ff             = 8192
0.00.050.389 I llm_load_print_meta: n_expert         = 0
0.00.050.389 I llm_load_print_meta: n_expert_used    = 0
0.00.050.389 I llm_load_print_meta: causal attn      = 1
0.00.050.390 I llm_load_print_meta: pooling type     = 0
0.00.050.390 I llm_load_print_meta: rope type        = 2
0.00.050.390 I llm_load_print_meta: rope scaling     = linear
0.00.050.390 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.391 I llm_load_print_meta: freq_scale_train = 1
0.00.050.391 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.391 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.391 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.394 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.394 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.394 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.394 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.394 I llm_load_print_meta: model type       = 1.4B
0.00.050.395 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.395 I llm_load_print_meta: model params     = 1.41 B
0.00.050.396 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.396 I llm_load_print_meta: general.name     = 1.4B
0.00.050.396 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.396 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.396 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.401 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.401 I llm_load_print_meta: LF token         = 128 ''
0.00.050.401 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.401 I llm_load_print_meta: max token length = 1024
0.00.052.335 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.335 I llm_load_tensors: offloading output layer to GPU
0.00.052.336 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.346 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.347 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.290 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.291 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.291 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.291 I llama_new_context_with_model: n_batch       = 2048
0.00.053.291 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.292 I llama_new_context_with_model: flash_attn    = 0
0.00.053.292 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.292 I llama_new_context_with_model: freq_scale    = 1
0.00.053.293 I ggml_metal_init: allocating
0.00.053.296 I ggml_metal_init: found device: Apple M4
0.00.053.298 I ggml_metal_init: picking default device: Apple M4
0.00.053.905 I ggml_metal_init: using embedded metal library
0.00.056.230 I ggml_metal_init: GPU name:   Apple M4
0.00.056.232 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.232 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.232 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.233 I ggml_metal_init: simdgroup reduction   = true
0.00.056.233 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.233 I ggml_metal_init: has bfloat            = true
0.00.056.233 I ggml_metal_init: use bfloat            = true
0.00.056.234 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.234 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.908 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.085.169 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.174 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.193 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.159 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.161 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.161 I llama_new_context_with_model: graph nodes  = 967
0.00.086.161 I llama_new_context_with_model: graph splits = 2
0.00.086.169 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.300 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.301 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.529.084 I main: llama threadpool init, n_threads = 4
0.00.529.126 I 
0.00.529.149 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.529.149 I 
0.00.529.373 I sampler seed: 1234
0.00.529.379 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.529.390 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.529.390 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.529.390 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.275.479 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56349.21 tokens per second)
0.01.275.479 I llama_perf_context_print:        load time =     519.18 ms
0.01.275.480 I llama_perf_context_print: prompt eval time =      40.49 ms /     7 tokens (    5.78 ms per token,   172.90 tokens per second)
0.01.275.481 I llama_perf_context_print:        eval time =     702.50 ms /    63 runs   (   11.15 ms per token,    89.68 tokens per second)
0.01.275.481 I llama_perf_context_print:       total time =     746.40 ms /    70 tokens
0.01.275.678 I ggml_metal_free: deallocating

real	0m1.291s
user	0m0.109s
sys	0m0.125s
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-tg-q4_k.log
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4400 (6e1531ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.008.771 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.297 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.302 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.303 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.304 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.304 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.304 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.305 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.306 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.306 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.306 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.309 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.309 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.309 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.310 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.312 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.312 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.312 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.140 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.182 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.989 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.990 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.990 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.990 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.991 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.991 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.022.991 I llama_model_loader: - type  f32:  194 tensors
0.00.022.992 I llama_model_loader: - type q4_K:   61 tensors
0.00.022.992 I llama_model_loader: - type q5_K:   24 tensors
0.00.022.992 I llama_model_loader: - type q6_K:   13 tensors
0.00.043.329 I llm_load_vocab: special tokens cache size = 25
0.00.049.175 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.178 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.178 I llm_load_print_meta: arch             = gptneox
0.00.049.179 I llm_load_print_meta: vocab type       = BPE
0.00.049.179 I llm_load_print_meta: n_vocab          = 50304
0.00.049.179 I llm_load_print_meta: n_merges         = 50009
0.00.049.179 I llm_load_print_meta: vocab_only       = 0
0.00.049.179 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.179 I llm_load_print_meta: n_embd           = 2048
0.00.049.180 I llm_load_print_meta: n_layer          = 24
0.00.049.182 I llm_load_print_meta: n_head           = 16
0.00.049.183 I llm_load_print_meta: n_head_kv        = 16
0.00.049.183 I llm_load_print_meta: n_rot            = 32
0.00.049.183 I llm_load_print_meta: n_swa            = 0
0.00.049.184 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.184 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.184 I llm_load_print_meta: n_gqa            = 1
0.00.049.185 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.188 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.188 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.189 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.189 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.189 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.189 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.190 I llm_load_print_meta: n_ff             = 8192
0.00.049.190 I llm_load_print_meta: n_expert         = 0
0.00.049.191 I llm_load_print_meta: n_expert_used    = 0
0.00.049.191 I llm_load_print_meta: causal attn      = 1
0.00.049.191 I llm_load_print_meta: pooling type     = 0
0.00.049.191 I llm_load_print_meta: rope type        = 2
0.00.049.191 I llm_load_print_meta: rope scaling     = linear
0.00.049.192 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.192 I llm_load_print_meta: freq_scale_train = 1
0.00.049.192 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.192 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.193 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.193 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.199 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.201 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.201 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.201 I llm_load_print_meta: model type       = 1.4B
0.00.049.202 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.049.202 I llm_load_print_meta: model params     = 1.41 B
0.00.049.203 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.049.203 I llm_load_print_meta: general.name     = 1.4B
0.00.049.203 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.204 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.204 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.205 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.205 I llm_load_print_meta: LF token         = 128 ''
0.00.049.205 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.205 I llm_load_print_meta: max token length = 1024
0.00.051.181 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.181 I llm_load_tensors: offloading output layer to GPU
0.00.051.181 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.192 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.193 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.090 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.090 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.091 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.091 I llama_new_context_with_model: n_batch       = 2048
0.00.052.091 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.091 I llama_new_context_with_model: flash_attn    = 0
0.00.052.092 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.092 I llama_new_context_with_model: freq_scale    = 1
0.00.052.092 I ggml_metal_init: allocating
0.00.052.096 I ggml_metal_init: found device: Apple M4
0.00.052.098 I ggml_metal_init: picking default device: Apple M4
0.00.052.671 I ggml_metal_init: using embedded metal library
0.00.054.999 I ggml_metal_init: GPU name:   Apple M4
0.00.055.001 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.001 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.001 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.001 I ggml_metal_init: simdgroup reduction   = true
0.00.055.002 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.002 I ggml_metal_init: has bfloat            = true
0.00.055.002 I ggml_metal_init: use bfloat            = true
0.00.055.002 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.003 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.679 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.083.566 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.574 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.600 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.558 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.559 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.559 I llama_new_context_with_model: graph nodes  = 967
0.00.084.559 I llama_new_context_with_model: graph splits = 2
0.00.084.575 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.715 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.715 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.603.501 I main: llama threadpool init, n_threads = 4
0.00.603.544 I 
0.00.603.584 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.603.585 I 
0.00.603.818 I sampler seed: 1234
0.00.603.823 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.603.868 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.603.870 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.603.870 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.364.231 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55081.46 tokens per second)
0.01.364.232 I llama_perf_context_print:        load time =     594.72 ms
0.01.364.233 I llama_perf_context_print: prompt eval time =      47.08 ms /     7 tokens (    6.73 ms per token,   148.69 tokens per second)
0.01.364.233 I llama_perf_context_print:        eval time =     710.17 ms /    63 runs   (   11.27 ms per token,    88.71 tokens per second)
0.01.364.234 I llama_perf_context_print:       total time =     760.74 ms /    70 tokens
0.01.364.406 I ggml_metal_free: deallocating

real	0m1.380s
user	0m0.109s
sys	0m0.135s
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-tg-q5_k.log
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4400 (6e1531ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.009.840 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.362 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.366 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.372 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.372 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.373 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.373 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.373 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.374 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.375 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.375 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.375 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.376 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.376 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.376 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.378 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.378 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.378 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.392 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.480 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.438 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.440 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.440 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.440 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.441 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.441 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.441 I llama_model_loader: - type  f32:  194 tensors
0.00.025.442 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.442 I llama_model_loader: - type q6_K:   37 tensors
0.00.046.572 I llm_load_vocab: special tokens cache size = 25
0.00.052.587 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.590 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.590 I llm_load_print_meta: arch             = gptneox
0.00.052.590 I llm_load_print_meta: vocab type       = BPE
0.00.052.591 I llm_load_print_meta: n_vocab          = 50304
0.00.052.591 I llm_load_print_meta: n_merges         = 50009
0.00.052.591 I llm_load_print_meta: vocab_only       = 0
0.00.052.591 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.591 I llm_load_print_meta: n_embd           = 2048
0.00.052.591 I llm_load_print_meta: n_layer          = 24
0.00.052.594 I llm_load_print_meta: n_head           = 16
0.00.052.595 I llm_load_print_meta: n_head_kv        = 16
0.00.052.597 I llm_load_print_meta: n_rot            = 32
0.00.052.597 I llm_load_print_meta: n_swa            = 0
0.00.052.597 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.597 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.598 I llm_load_print_meta: n_gqa            = 1
0.00.052.599 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.599 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.600 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.600 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.600 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.601 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.601 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.601 I llm_load_print_meta: n_ff             = 8192
0.00.052.602 I llm_load_print_meta: n_expert         = 0
0.00.052.602 I llm_load_print_meta: n_expert_used    = 0
0.00.052.602 I llm_load_print_meta: causal attn      = 1
0.00.052.602 I llm_load_print_meta: pooling type     = 0
0.00.052.602 I llm_load_print_meta: rope type        = 2
0.00.052.603 I llm_load_print_meta: rope scaling     = linear
0.00.052.603 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.603 I llm_load_print_meta: freq_scale_train = 1
0.00.052.604 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.604 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.604 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.604 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.605 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.605 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.605 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.605 I llm_load_print_meta: model type       = 1.4B
0.00.052.606 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.052.606 I llm_load_print_meta: model params     = 1.41 B
0.00.052.607 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.052.607 I llm_load_print_meta: general.name     = 1.4B
0.00.052.607 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.608 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.608 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.609 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.610 I llm_load_print_meta: LF token         = 128 ''
0.00.052.610 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.610 I llm_load_print_meta: max token length = 1024
0.00.054.687 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.688 I llm_load_tensors: offloading output layer to GPU
0.00.054.688 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.698 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.054.699 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.055.719 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.719 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.720 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.720 I llama_new_context_with_model: n_batch       = 2048
0.00.055.720 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.720 I llama_new_context_with_model: flash_attn    = 0
0.00.055.721 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.721 I llama_new_context_with_model: freq_scale    = 1
0.00.055.721 I ggml_metal_init: allocating
0.00.055.725 I ggml_metal_init: found device: Apple M4
0.00.055.727 I ggml_metal_init: picking default device: Apple M4
0.00.056.329 I ggml_metal_init: using embedded metal library
0.00.058.643 I ggml_metal_init: GPU name:   Apple M4
0.00.058.645 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.645 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.646 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.646 I ggml_metal_init: simdgroup reduction   = true
0.00.058.646 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.646 I ggml_metal_init: has bfloat            = true
0.00.058.647 I ggml_metal_init: use bfloat            = true
0.00.058.647 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.648 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.611 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.087.732 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.739 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.758 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.738 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.740 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.740 I llama_new_context_with_model: graph nodes  = 967
0.00.088.741 I llama_new_context_with_model: graph splits = 2
0.00.088.759 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.906 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.907 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.699.413 I main: llama threadpool init, n_threads = 4
0.00.699.460 I 
0.00.699.484 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.699.484 I 
0.00.699.711 I sampler seed: 1234
0.00.699.715 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.699.740 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.699.741 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.699.741 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.544.982 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60016.91 tokens per second)
0.01.544.982 I llama_perf_context_print:        load time =     689.57 ms
0.01.544.983 I llama_perf_context_print: prompt eval time =      51.51 ms /     7 tokens (    7.36 ms per token,   135.89 tokens per second)
0.01.544.984 I llama_perf_context_print:        eval time =     790.75 ms /    63 runs   (   12.55 ms per token,    79.67 tokens per second)
0.01.544.984 I llama_perf_context_print:       total time =     845.57 ms /    70 tokens
0.01.545.158 I ggml_metal_free: deallocating

real	0m1.561s
user	0m0.109s
sys	0m0.158s
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-tg-q6_k.log
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4400 (6e1531ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.008.774 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.174 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.179 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.180 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.181 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.181 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.182 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.182 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.183 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.183 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.184 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.184 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.184 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.185 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.185 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.188 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.188 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.188 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.164 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.283 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.207 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.208 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.208 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.209 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.209 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.209 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.210 I llama_model_loader: - type  f32:  194 tensors
0.00.025.210 I llama_model_loader: - type q6_K:   98 tensors
0.00.046.434 I llm_load_vocab: special tokens cache size = 25
0.00.052.293 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.296 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.296 I llm_load_print_meta: arch             = gptneox
0.00.052.297 I llm_load_print_meta: vocab type       = BPE
0.00.052.297 I llm_load_print_meta: n_vocab          = 50304
0.00.052.297 I llm_load_print_meta: n_merges         = 50009
0.00.052.297 I llm_load_print_meta: vocab_only       = 0
0.00.052.298 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.298 I llm_load_print_meta: n_embd           = 2048
0.00.052.298 I llm_load_print_meta: n_layer          = 24
0.00.052.301 I llm_load_print_meta: n_head           = 16
0.00.052.301 I llm_load_print_meta: n_head_kv        = 16
0.00.052.301 I llm_load_print_meta: n_rot            = 32
0.00.052.302 I llm_load_print_meta: n_swa            = 0
0.00.052.302 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.302 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.303 I llm_load_print_meta: n_gqa            = 1
0.00.052.303 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.305 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.306 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.306 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.306 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.307 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.307 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.307 I llm_load_print_meta: n_ff             = 8192
0.00.052.308 I llm_load_print_meta: n_expert         = 0
0.00.052.308 I llm_load_print_meta: n_expert_used    = 0
0.00.052.308 I llm_load_print_meta: causal attn      = 1
0.00.052.310 I llm_load_print_meta: pooling type     = 0
0.00.052.311 I llm_load_print_meta: rope type        = 2
0.00.052.311 I llm_load_print_meta: rope scaling     = linear
0.00.052.312 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.312 I llm_load_print_meta: freq_scale_train = 1
0.00.052.312 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.312 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.313 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.313 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.313 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.313 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.313 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.313 I llm_load_print_meta: model type       = 1.4B
0.00.052.314 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.314 I llm_load_print_meta: model params     = 1.41 B
0.00.052.315 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.315 I llm_load_print_meta: general.name     = 1.4B
0.00.052.315 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.315 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.319 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.320 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.320 I llm_load_print_meta: LF token         = 128 ''
0.00.052.321 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.321 I llm_load_print_meta: max token length = 1024
0.00.054.435 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.435 I llm_load_tensors: offloading output layer to GPU
0.00.054.435 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.445 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.446 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.055.404 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.404 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.405 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.405 I llama_new_context_with_model: n_batch       = 2048
0.00.055.405 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.405 I llama_new_context_with_model: flash_attn    = 0
0.00.055.406 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.406 I llama_new_context_with_model: freq_scale    = 1
0.00.055.406 I ggml_metal_init: allocating
0.00.055.410 I ggml_metal_init: found device: Apple M4
0.00.055.412 I ggml_metal_init: picking default device: Apple M4
0.00.056.030 I ggml_metal_init: using embedded metal library
0.00.058.436 I ggml_metal_init: GPU name:   Apple M4
0.00.058.438 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.438 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.439 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.439 I ggml_metal_init: simdgroup reduction   = true
0.00.058.441 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.441 I ggml_metal_init: has bfloat            = true
0.00.058.441 I ggml_metal_init: use bfloat            = true
0.00.058.441 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.442 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.340 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.088.429 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.438 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.458 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.519 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.520 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.520 I llama_new_context_with_model: graph nodes  = 967
0.00.089.521 I llama_new_context_with_model: graph splits = 2
0.00.089.536 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.677 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.678 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.811.101 I main: llama threadpool init, n_threads = 4
0.00.811.146 I 
0.00.811.169 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.811.169 I 
0.00.811.397 I sampler seed: 1234
0.00.811.403 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.811.447 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.811.450 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.811.450 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.695.567 I llama_perf_sampler_print:    sampling time =       1.47 ms /    71 runs   (    0.02 ms per token, 48464.16 tokens per second)
0.01.695.567 I llama_perf_context_print:        load time =     802.32 ms
0.01.695.569 I llama_perf_context_print: prompt eval time =      54.80 ms /     7 tokens (    7.83 ms per token,   127.74 tokens per second)
0.01.695.570 I llama_perf_context_print:        eval time =     826.82 ms /    63 runs   (   13.12 ms per token,    76.20 tokens per second)
0.01.695.571 I llama_perf_context_print:       total time =     884.47 ms /    70 tokens
0.01.695.817 I ggml_metal_free: deallocating

real	0m1.715s
user	0m0.110s
sys	0m0.154s
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-tg-f16.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.811 I build: 4400 (6e1531ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.474 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.707 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.715 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.717 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.717 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.718 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.718 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.718 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.720 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.720 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.721 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.721 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.721 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.722 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.722 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.725 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.726 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.726 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.465 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.460 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.233 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.054.235 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.235 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.236 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.236 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.237 I llama_model_loader: - type  f32:  194 tensors
0.00.054.237 I llama_model_loader: - type  f16:   98 tensors
0.00.082.503 I llm_load_vocab: special tokens cache size = 25
0.00.088.961 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.088.964 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.088.964 I llm_load_print_meta: arch             = gptneox
0.00.088.965 I llm_load_print_meta: vocab type       = BPE
0.00.088.965 I llm_load_print_meta: n_vocab          = 50304
0.00.088.965 I llm_load_print_meta: n_merges         = 50009
0.00.088.965 I llm_load_print_meta: vocab_only       = 0
0.00.088.965 I llm_load_print_meta: n_ctx_train      = 2048
0.00.088.965 I llm_load_print_meta: n_embd           = 2048
0.00.088.966 I llm_load_print_meta: n_layer          = 24
0.00.088.969 I llm_load_print_meta: n_head           = 16
0.00.088.969 I llm_load_print_meta: n_head_kv        = 16
0.00.088.970 I llm_load_print_meta: n_rot            = 32
0.00.088.970 I llm_load_print_meta: n_swa            = 0
0.00.088.970 I llm_load_print_meta: n_embd_head_k    = 128
0.00.088.970 I llm_load_print_meta: n_embd_head_v    = 128
0.00.088.971 I llm_load_print_meta: n_gqa            = 1
0.00.088.971 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.088.972 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.088.972 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.088.975 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.088.975 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.088.975 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.088.975 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.088.976 I llm_load_print_meta: n_ff             = 8192
0.00.088.976 I llm_load_print_meta: n_expert         = 0
0.00.088.976 I llm_load_print_meta: n_expert_used    = 0
0.00.088.976 I llm_load_print_meta: causal attn      = 1
0.00.088.976 I llm_load_print_meta: pooling type     = 0
0.00.088.976 I llm_load_print_meta: rope type        = 2
0.00.088.977 I llm_load_print_meta: rope scaling     = linear
0.00.088.977 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.088.977 I llm_load_print_meta: freq_scale_train = 1
0.00.088.977 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.088.978 I llm_load_print_meta: rope_finetuned   = unknown
0.00.088.978 I llm_load_print_meta: ssm_d_conv       = 0
0.00.088.979 I llm_load_print_meta: ssm_d_inner      = 0
0.00.088.979 I llm_load_print_meta: ssm_d_state      = 0
0.00.088.979 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.088.979 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.088.980 I llm_load_print_meta: model type       = 1.4B
0.00.088.980 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.088.981 I llm_load_print_meta: model params     = 1.41 B
0.00.088.982 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.088.982 I llm_load_print_meta: general.name     = 1.4B
0.00.088.982 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.088.982 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.088.982 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.088.983 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.088.987 I llm_load_print_meta: LF token         = 128 ''
0.00.088.987 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.088.987 I llm_load_print_meta: max token length = 1024
0.00.090.759 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.090.759 I llm_load_tensors: offloading output layer to GPU
0.00.090.759 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.090.769 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.090.771 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.091.651 I llama_new_context_with_model: n_seq_max     = 1
0.00.091.652 I llama_new_context_with_model: n_ctx         = 128
0.00.091.652 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.091.652 I llama_new_context_with_model: n_batch       = 128
0.00.091.652 I llama_new_context_with_model: n_ubatch      = 128
0.00.091.653 I llama_new_context_with_model: flash_attn    = 0
0.00.091.653 I llama_new_context_with_model: freq_base     = 10000.0
0.00.091.653 I llama_new_context_with_model: freq_scale    = 1
0.00.091.654 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.091.654 I ggml_metal_init: allocating
0.00.091.661 I ggml_metal_init: found device: Apple M4
0.00.091.663 I ggml_metal_init: picking default device: Apple M4
0.00.092.268 I ggml_metal_init: using embedded metal library
0.00.094.808 I ggml_metal_init: GPU name:   Apple M4
0.00.094.809 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.094.810 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.094.810 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.094.810 I ggml_metal_init: simdgroup reduction   = true
0.00.094.810 I ggml_metal_init: simdgroup matrix mul. = true
0.00.094.810 I ggml_metal_init: has bfloat            = true
0.00.094.811 I ggml_metal_init: use bfloat            = true
0.00.094.811 I ggml_metal_init: hasUnifiedMemory      = true
0.00.094.812 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.104.230 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.105.590 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.105.593 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.105.609 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.106.507 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.106.508 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.106.509 I llama_new_context_with_model: graph nodes  = 967
0.00.106.509 I llama_new_context_with_model: graph splits = 2
0.00.106.521 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.106.522 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.269.243 I 
0.01.269.292 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.269.333 I perplexity: tokenizing the input ..
0.01.282.261 I perplexity: tokenization took 12.925 ms
0.01.282.266 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.404.896 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.406.762 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.406.788 I llama_perf_context_print:        load time =    1244.75 ms
0.01.406.791 I llama_perf_context_print: prompt eval time =     121.77 ms /   128 tokens (    0.95 ms per token,  1051.21 tokens per second)
0.01.406.792 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.406.793 I llama_perf_context_print:       total time =     137.55 ms /   129 tokens
0.01.407.563 I ggml_metal_free: deallocating

real	0m1.601s
user	0m0.123s
sys	0m0.226s
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-tg-q8_0.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.118 I build: 4400 (6e1531ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.329 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.018 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.020.024 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.026 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.027 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.027 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.027 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.028 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.029 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.029 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.030 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.031 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.031 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.032 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.032 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.034 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.034 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.035 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.548 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.037 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.454 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.032.456 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.457 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.457 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.458 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.458 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.032.459 I llama_model_loader: - type  f32:  194 tensors
0.00.032.459 I llama_model_loader: - type q8_0:   98 tensors
0.00.057.842 I llm_load_vocab: special tokens cache size = 25
0.00.064.254 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.064.257 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.064.257 I llm_load_print_meta: arch             = gptneox
0.00.064.258 I llm_load_print_meta: vocab type       = BPE
0.00.064.258 I llm_load_print_meta: n_vocab          = 50304
0.00.064.258 I llm_load_print_meta: n_merges         = 50009
0.00.064.258 I llm_load_print_meta: vocab_only       = 0
0.00.064.259 I llm_load_print_meta: n_ctx_train      = 2048
0.00.064.259 I llm_load_print_meta: n_embd           = 2048
0.00.064.259 I llm_load_print_meta: n_layer          = 24
0.00.064.263 I llm_load_print_meta: n_head           = 16
0.00.064.263 I llm_load_print_meta: n_head_kv        = 16
0.00.064.264 I llm_load_print_meta: n_rot            = 32
0.00.064.264 I llm_load_print_meta: n_swa            = 0
0.00.064.264 I llm_load_print_meta: n_embd_head_k    = 128
0.00.064.264 I llm_load_print_meta: n_embd_head_v    = 128
0.00.064.265 I llm_load_print_meta: n_gqa            = 1
0.00.064.266 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.064.269 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.064.270 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.064.270 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.064.271 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.064.271 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.064.271 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.064.272 I llm_load_print_meta: n_ff             = 8192
0.00.064.272 I llm_load_print_meta: n_expert         = 0
0.00.064.272 I llm_load_print_meta: n_expert_used    = 0
0.00.064.272 I llm_load_print_meta: causal attn      = 1
0.00.064.272 I llm_load_print_meta: pooling type     = 0
0.00.064.272 I llm_load_print_meta: rope type        = 2
0.00.064.273 I llm_load_print_meta: rope scaling     = linear
0.00.064.273 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.064.274 I llm_load_print_meta: freq_scale_train = 1
0.00.064.274 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.064.274 I llm_load_print_meta: rope_finetuned   = unknown
0.00.064.274 I llm_load_print_meta: ssm_d_conv       = 0
0.00.064.274 I llm_load_print_meta: ssm_d_inner      = 0
0.00.064.274 I llm_load_print_meta: ssm_d_state      = 0
0.00.064.275 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.064.275 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.064.276 I llm_load_print_meta: model type       = 1.4B
0.00.064.277 I llm_load_print_meta: model ftype      = Q8_0
0.00.064.277 I llm_load_print_meta: model params     = 1.41 B
0.00.064.278 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.064.279 I llm_load_print_meta: general.name     = 1.4B
0.00.064.279 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.064.279 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.064.279 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.064.280 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.064.280 I llm_load_print_meta: LF token         = 128 ''
0.00.064.281 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.064.281 I llm_load_print_meta: max token length = 1024
0.00.066.595 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.066.596 I llm_load_tensors: offloading output layer to GPU
0.00.066.596 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.066.607 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.066.608 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.067.595 I llama_new_context_with_model: n_seq_max     = 1
0.00.067.596 I llama_new_context_with_model: n_ctx         = 128
0.00.067.597 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.067.597 I llama_new_context_with_model: n_batch       = 128
0.00.067.597 I llama_new_context_with_model: n_ubatch      = 128
0.00.067.597 I llama_new_context_with_model: flash_attn    = 0
0.00.067.598 I llama_new_context_with_model: freq_base     = 10000.0
0.00.067.598 I llama_new_context_with_model: freq_scale    = 1
0.00.067.598 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.067.599 I ggml_metal_init: allocating
0.00.067.605 I ggml_metal_init: found device: Apple M4
0.00.067.608 I ggml_metal_init: picking default device: Apple M4
0.00.068.241 I ggml_metal_init: using embedded metal library
0.00.070.807 I ggml_metal_init: GPU name:   Apple M4
0.00.070.809 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.809 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.810 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.810 I ggml_metal_init: simdgroup reduction   = true
0.00.070.810 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.810 I ggml_metal_init: has bfloat            = true
0.00.070.810 I ggml_metal_init: use bfloat            = true
0.00.070.811 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.812 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.081.006 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.082.340 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.082.343 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.082.359 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.273 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.083.274 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.083.274 I llama_new_context_with_model: graph nodes  = 967
0.00.083.274 I llama_new_context_with_model: graph splits = 2
0.00.083.287 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.083.288 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.962.887 I 
0.00.962.908 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.962.945 I perplexity: tokenizing the input ..
0.00.970.768 I perplexity: tokenization took 7.821 ms
0.00.970.772 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.095.316 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.096.463 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.096.476 I llama_perf_context_print:        load time =     951.55 ms
0.01.096.477 I llama_perf_context_print: prompt eval time =     124.32 ms /   128 tokens (    0.97 ms per token,  1029.61 tokens per second)
0.01.096.478 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.096.478 I llama_perf_context_print:       total time =     133.59 ms /   129 tokens
0.01.096.938 I ggml_metal_free: deallocating

real	0m1.115s
user	0m0.093s
sys	0m0.155s
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-tg-q4_0.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4400 (6e1531ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.303 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.429 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.433 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.440 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.440 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.441 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.441 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.441 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.444 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.444 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.445 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.445 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.446 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.446 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.446 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.448 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.448 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.448 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.287 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.326 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.086 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.088 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.088 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.088 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.089 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.089 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.089 I llama_model_loader: - type  f32:  194 tensors
0.00.025.090 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.090 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.154 I llm_load_vocab: special tokens cache size = 25
0.00.051.939 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.942 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.942 I llm_load_print_meta: arch             = gptneox
0.00.051.942 I llm_load_print_meta: vocab type       = BPE
0.00.051.943 I llm_load_print_meta: n_vocab          = 50304
0.00.051.943 I llm_load_print_meta: n_merges         = 50009
0.00.051.943 I llm_load_print_meta: vocab_only       = 0
0.00.051.943 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.943 I llm_load_print_meta: n_embd           = 2048
0.00.051.943 I llm_load_print_meta: n_layer          = 24
0.00.051.946 I llm_load_print_meta: n_head           = 16
0.00.051.947 I llm_load_print_meta: n_head_kv        = 16
0.00.051.947 I llm_load_print_meta: n_rot            = 32
0.00.051.947 I llm_load_print_meta: n_swa            = 0
0.00.051.947 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.948 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.948 I llm_load_print_meta: n_gqa            = 1
0.00.051.949 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.950 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.950 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.951 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.952 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.952 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.959 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.964 I llm_load_print_meta: n_ff             = 8192
0.00.051.965 I llm_load_print_meta: n_expert         = 0
0.00.051.965 I llm_load_print_meta: n_expert_used    = 0
0.00.051.965 I llm_load_print_meta: causal attn      = 1
0.00.051.965 I llm_load_print_meta: pooling type     = 0
0.00.051.966 I llm_load_print_meta: rope type        = 2
0.00.051.966 I llm_load_print_meta: rope scaling     = linear
0.00.051.966 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.968 I llm_load_print_meta: freq_scale_train = 1
0.00.051.968 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.968 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.968 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.968 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.968 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.969 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.970 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.970 I llm_load_print_meta: model type       = 1.4B
0.00.051.970 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.970 I llm_load_print_meta: model params     = 1.41 B
0.00.051.971 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.971 I llm_load_print_meta: general.name     = 1.4B
0.00.051.971 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.972 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.972 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.972 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.972 I llm_load_print_meta: LF token         = 128 ''
0.00.051.974 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.975 I llm_load_print_meta: max token length = 1024
0.00.053.872 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.872 I llm_load_tensors: offloading output layer to GPU
0.00.053.873 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.883 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.884 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.054.749 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.750 I llama_new_context_with_model: n_ctx         = 128
0.00.054.750 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.750 I llama_new_context_with_model: n_batch       = 128
0.00.054.750 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.750 I llama_new_context_with_model: flash_attn    = 0
0.00.054.751 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.751 I llama_new_context_with_model: freq_scale    = 1
0.00.054.751 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.752 I ggml_metal_init: allocating
0.00.054.755 I ggml_metal_init: found device: Apple M4
0.00.054.757 I ggml_metal_init: picking default device: Apple M4
0.00.055.303 I ggml_metal_init: using embedded metal library
0.00.057.599 I ggml_metal_init: GPU name:   Apple M4
0.00.057.600 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.601 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.601 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.601 I ggml_metal_init: simdgroup reduction   = true
0.00.057.601 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.601 I ggml_metal_init: has bfloat            = true
0.00.057.602 I ggml_metal_init: use bfloat            = true
0.00.057.602 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.603 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.124 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.360 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.364 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.377 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.257 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.258 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.259 I llama_new_context_with_model: graph nodes  = 967
0.00.069.259 I llama_new_context_with_model: graph splits = 2
0.00.069.271 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.272 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.617.635 I 
0.00.617.673 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.617.685 I perplexity: tokenizing the input ..
0.00.625.558 I perplexity: tokenization took 7.87 ms
0.00.625.561 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.748.172 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.749.388 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.749.401 I llama_perf_context_print:        load time =     607.33 ms
0.00.749.403 I llama_perf_context_print: prompt eval time =     122.38 ms /   128 tokens (    0.96 ms per token,  1045.89 tokens per second)
0.00.749.403 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.749.404 I llama_perf_context_print:       total time =     131.77 ms /   129 tokens
0.00.749.874 I ggml_metal_free: deallocating

real	0m0.765s
user	0m0.078s
sys	0m0.094s
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-tg-q4_1.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4400 (6e1531ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.939 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.609 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.614 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.617 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.617 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.618 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.618 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.618 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.619 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.620 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.620 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.621 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.621 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.621 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.622 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.625 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.625 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.625 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.460 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.505 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.325 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.327 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.327 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.327 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.328 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.328 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.329 I llama_model_loader: - type  f32:  194 tensors
0.00.023.329 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.329 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.397 I llm_load_vocab: special tokens cache size = 25
0.00.050.277 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.280 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.280 I llm_load_print_meta: arch             = gptneox
0.00.050.280 I llm_load_print_meta: vocab type       = BPE
0.00.050.281 I llm_load_print_meta: n_vocab          = 50304
0.00.050.281 I llm_load_print_meta: n_merges         = 50009
0.00.050.281 I llm_load_print_meta: vocab_only       = 0
0.00.050.281 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.281 I llm_load_print_meta: n_embd           = 2048
0.00.050.282 I llm_load_print_meta: n_layer          = 24
0.00.050.285 I llm_load_print_meta: n_head           = 16
0.00.050.285 I llm_load_print_meta: n_head_kv        = 16
0.00.050.285 I llm_load_print_meta: n_rot            = 32
0.00.050.286 I llm_load_print_meta: n_swa            = 0
0.00.050.286 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.287 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.288 I llm_load_print_meta: n_gqa            = 1
0.00.050.288 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.289 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.290 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.290 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.290 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.290 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.290 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.291 I llm_load_print_meta: n_ff             = 8192
0.00.050.291 I llm_load_print_meta: n_expert         = 0
0.00.050.291 I llm_load_print_meta: n_expert_used    = 0
0.00.050.292 I llm_load_print_meta: causal attn      = 1
0.00.050.292 I llm_load_print_meta: pooling type     = 0
0.00.050.292 I llm_load_print_meta: rope type        = 2
0.00.050.292 I llm_load_print_meta: rope scaling     = linear
0.00.050.292 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.293 I llm_load_print_meta: freq_scale_train = 1
0.00.050.293 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.293 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.293 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.295 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.295 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.295 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.295 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.295 I llm_load_print_meta: model type       = 1.4B
0.00.050.296 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.296 I llm_load_print_meta: model params     = 1.41 B
0.00.050.297 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.297 I llm_load_print_meta: general.name     = 1.4B
0.00.050.297 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.297 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.299 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.299 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.300 I llm_load_print_meta: LF token         = 128 ''
0.00.050.300 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.300 I llm_load_print_meta: max token length = 1024
0.00.052.335 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.335 I llm_load_tensors: offloading output layer to GPU
0.00.052.336 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.346 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.347 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.298 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.298 I llama_new_context_with_model: n_ctx         = 128
0.00.053.299 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.299 I llama_new_context_with_model: n_batch       = 128
0.00.053.299 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.299 I llama_new_context_with_model: flash_attn    = 0
0.00.053.300 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.300 I llama_new_context_with_model: freq_scale    = 1
0.00.053.300 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.301 I ggml_metal_init: allocating
0.00.053.304 I ggml_metal_init: found device: Apple M4
0.00.053.306 I ggml_metal_init: picking default device: Apple M4
0.00.053.886 I ggml_metal_init: using embedded metal library
0.00.056.214 I ggml_metal_init: GPU name:   Apple M4
0.00.056.215 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.216 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.216 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.216 I ggml_metal_init: simdgroup reduction   = true
0.00.056.217 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.217 I ggml_metal_init: has bfloat            = true
0.00.056.217 I ggml_metal_init: use bfloat            = true
0.00.056.217 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.218 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.053 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.366 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.368 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.381 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.258 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.259 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.259 I llama_new_context_with_model: graph nodes  = 967
0.00.068.259 I llama_new_context_with_model: graph splits = 2
0.00.068.272 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.272 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.664.032 I 
0.00.664.062 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.664.076 I perplexity: tokenizing the input ..
0.00.672.088 I perplexity: tokenization took 8.01 ms
0.00.672.091 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.794.961 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.796.129 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.796.151 I llama_perf_context_print:        load time =     655.09 ms
0.00.796.152 I llama_perf_context_print: prompt eval time =     122.63 ms /   128 tokens (    0.96 ms per token,  1043.76 tokens per second)
0.00.796.152 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.796.153 I llama_perf_context_print:       total time =     132.12 ms /   129 tokens
0.00.796.599 I ggml_metal_free: deallocating

real	0m0.811s
user	0m0.079s
sys	0m0.100s
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-tg-q5_0.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4400 (6e1531ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.177 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.781 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.786 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.792 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.793 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.793 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.794 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.794 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.795 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.795 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.795 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.796 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.796 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.796 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.797 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.799 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.799 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.799 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.657 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.683 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.503 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.504 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.504 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.505 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.505 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.505 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.506 I llama_model_loader: - type  f32:  194 tensors
0.00.024.506 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.507 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.651 I llm_load_vocab: special tokens cache size = 25
0.00.050.498 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.501 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.501 I llm_load_print_meta: arch             = gptneox
0.00.050.502 I llm_load_print_meta: vocab type       = BPE
0.00.050.502 I llm_load_print_meta: n_vocab          = 50304
0.00.050.502 I llm_load_print_meta: n_merges         = 50009
0.00.050.502 I llm_load_print_meta: vocab_only       = 0
0.00.050.502 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.503 I llm_load_print_meta: n_embd           = 2048
0.00.050.503 I llm_load_print_meta: n_layer          = 24
0.00.050.506 I llm_load_print_meta: n_head           = 16
0.00.050.507 I llm_load_print_meta: n_head_kv        = 16
0.00.050.507 I llm_load_print_meta: n_rot            = 32
0.00.050.508 I llm_load_print_meta: n_swa            = 0
0.00.050.508 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.509 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.509 I llm_load_print_meta: n_gqa            = 1
0.00.050.510 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.511 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.511 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.512 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.512 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.512 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.512 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.514 I llm_load_print_meta: n_ff             = 8192
0.00.050.514 I llm_load_print_meta: n_expert         = 0
0.00.050.514 I llm_load_print_meta: n_expert_used    = 0
0.00.050.514 I llm_load_print_meta: causal attn      = 1
0.00.050.514 I llm_load_print_meta: pooling type     = 0
0.00.050.515 I llm_load_print_meta: rope type        = 2
0.00.050.515 I llm_load_print_meta: rope scaling     = linear
0.00.050.515 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.516 I llm_load_print_meta: freq_scale_train = 1
0.00.050.516 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.516 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.516 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.516 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.516 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.516 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.518 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.518 I llm_load_print_meta: model type       = 1.4B
0.00.050.519 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.519 I llm_load_print_meta: model params     = 1.41 B
0.00.050.520 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.520 I llm_load_print_meta: general.name     = 1.4B
0.00.050.520 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.520 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.520 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.521 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.521 I llm_load_print_meta: LF token         = 128 ''
0.00.050.521 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.522 I llm_load_print_meta: max token length = 1024
0.00.052.469 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.469 I llm_load_tensors: offloading output layer to GPU
0.00.052.469 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.480 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.481 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.403 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.404 I llama_new_context_with_model: n_ctx         = 128
0.00.053.404 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.404 I llama_new_context_with_model: n_batch       = 128
0.00.053.405 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.405 I llama_new_context_with_model: flash_attn    = 0
0.00.053.405 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.406 I llama_new_context_with_model: freq_scale    = 1
0.00.053.406 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.406 I ggml_metal_init: allocating
0.00.053.412 I ggml_metal_init: found device: Apple M4
0.00.053.416 I ggml_metal_init: picking default device: Apple M4
0.00.053.970 I ggml_metal_init: using embedded metal library
0.00.056.327 I ggml_metal_init: GPU name:   Apple M4
0.00.056.329 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.329 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.330 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.330 I ggml_metal_init: simdgroup reduction   = true
0.00.056.330 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.330 I ggml_metal_init: has bfloat            = true
0.00.056.330 I ggml_metal_init: use bfloat            = true
0.00.056.331 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.332 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.767 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.073 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.076 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.092 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.977 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.978 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.979 I llama_new_context_with_model: graph nodes  = 967
0.00.067.979 I llama_new_context_with_model: graph splits = 2
0.00.067.991 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.992 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.703.849 I 
0.00.703.881 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.703.896 I perplexity: tokenizing the input ..
0.00.712.177 I perplexity: tokenization took 8.279 ms
0.00.712.180 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.847.416 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.848.600 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.848.613 I llama_perf_context_print:        load time =     693.67 ms
0.00.848.614 I llama_perf_context_print: prompt eval time =     135.00 ms /   128 tokens (    1.05 ms per token,   948.12 tokens per second)
0.00.848.615 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.848.615 I llama_perf_context_print:       total time =     144.77 ms /   129 tokens
0.00.849.124 I ggml_metal_free: deallocating

real	0m0.864s
user	0m0.078s
sys	0m0.111s
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-tg-q5_1.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4400 (6e1531ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.855 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.623 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.627 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.633 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.633 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.634 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.634 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.634 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.635 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.636 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.638 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.638 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.639 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.639 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.639 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.643 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.644 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.644 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.572 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.667 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.652 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.653 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.653 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.654 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.654 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.654 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.655 I llama_model_loader: - type  f32:  194 tensors
0.00.023.655 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.655 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.731 I llm_load_vocab: special tokens cache size = 25
0.00.050.694 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.696 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.697 I llm_load_print_meta: arch             = gptneox
0.00.050.697 I llm_load_print_meta: vocab type       = BPE
0.00.050.697 I llm_load_print_meta: n_vocab          = 50304
0.00.050.697 I llm_load_print_meta: n_merges         = 50009
0.00.050.698 I llm_load_print_meta: vocab_only       = 0
0.00.050.698 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.698 I llm_load_print_meta: n_embd           = 2048
0.00.050.698 I llm_load_print_meta: n_layer          = 24
0.00.050.701 I llm_load_print_meta: n_head           = 16
0.00.050.702 I llm_load_print_meta: n_head_kv        = 16
0.00.050.702 I llm_load_print_meta: n_rot            = 32
0.00.050.702 I llm_load_print_meta: n_swa            = 0
0.00.050.703 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.703 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.705 I llm_load_print_meta: n_gqa            = 1
0.00.050.706 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.707 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.708 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.709 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.709 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.709 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.709 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.710 I llm_load_print_meta: n_ff             = 8192
0.00.050.710 I llm_load_print_meta: n_expert         = 0
0.00.050.710 I llm_load_print_meta: n_expert_used    = 0
0.00.050.710 I llm_load_print_meta: causal attn      = 1
0.00.050.711 I llm_load_print_meta: pooling type     = 0
0.00.050.711 I llm_load_print_meta: rope type        = 2
0.00.050.711 I llm_load_print_meta: rope scaling     = linear
0.00.050.711 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.712 I llm_load_print_meta: freq_scale_train = 1
0.00.050.712 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.712 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.712 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.712 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.712 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.712 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.713 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.713 I llm_load_print_meta: model type       = 1.4B
0.00.050.718 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.718 I llm_load_print_meta: model params     = 1.41 B
0.00.050.718 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.720 I llm_load_print_meta: general.name     = 1.4B
0.00.050.720 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.721 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.721 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.721 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.721 I llm_load_print_meta: LF token         = 128 ''
0.00.050.721 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.722 I llm_load_print_meta: max token length = 1024
0.00.052.763 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.763 I llm_load_tensors: offloading output layer to GPU
0.00.052.764 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.774 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.775 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.698 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.698 I llama_new_context_with_model: n_ctx         = 128
0.00.053.699 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.699 I llama_new_context_with_model: n_batch       = 128
0.00.053.699 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.699 I llama_new_context_with_model: flash_attn    = 0
0.00.053.699 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.700 I llama_new_context_with_model: freq_scale    = 1
0.00.053.700 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.700 I ggml_metal_init: allocating
0.00.053.703 I ggml_metal_init: found device: Apple M4
0.00.053.705 I ggml_metal_init: picking default device: Apple M4
0.00.054.299 I ggml_metal_init: using embedded metal library
0.00.056.667 I ggml_metal_init: GPU name:   Apple M4
0.00.056.668 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.669 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.669 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.669 I ggml_metal_init: simdgroup reduction   = true
0.00.056.669 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.670 I ggml_metal_init: has bfloat            = true
0.00.056.670 I ggml_metal_init: use bfloat            = true
0.00.056.670 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.671 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.527 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.819 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.821 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.835 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.786 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.788 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.788 I llama_new_context_with_model: graph nodes  = 967
0.00.068.788 I llama_new_context_with_model: graph splits = 2
0.00.068.801 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.802 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.646.968 I 
0.00.646.994 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.647.007 I perplexity: tokenizing the input ..
0.00.655.139 I perplexity: tokenization took 8.131 ms
0.00.655.143 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.789.963 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.791.113 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.791.131 I llama_perf_context_print:        load time =     638.11 ms
0.00.791.132 I llama_perf_context_print: prompt eval time =     134.59 ms /   128 tokens (    1.05 ms per token,   951.01 tokens per second)
0.00.791.133 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.791.133 I llama_perf_context_print:       total time =     144.16 ms /   129 tokens
0.00.791.448 I ggml_metal_free: deallocating

real	0m0.805s
user	0m0.079s
sys	0m0.112s
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-tg-q2_k.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4400 (6e1531ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.434 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.856 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.014.861 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.867 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.868 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.868 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.868 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.868 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.871 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.871 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.871 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.872 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.872 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.872 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.873 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.874 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.874 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.875 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.653 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.744 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.535 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.537 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.537 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.537 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.537 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.538 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.538 I llama_model_loader: - type  f32:  194 tensors
0.00.023.539 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.539 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.539 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.594 I llm_load_vocab: special tokens cache size = 25
0.00.049.364 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.367 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.367 I llm_load_print_meta: arch             = gptneox
0.00.049.368 I llm_load_print_meta: vocab type       = BPE
0.00.049.368 I llm_load_print_meta: n_vocab          = 50304
0.00.049.368 I llm_load_print_meta: n_merges         = 50009
0.00.049.368 I llm_load_print_meta: vocab_only       = 0
0.00.049.369 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.369 I llm_load_print_meta: n_embd           = 2048
0.00.049.369 I llm_load_print_meta: n_layer          = 24
0.00.049.371 I llm_load_print_meta: n_head           = 16
0.00.049.372 I llm_load_print_meta: n_head_kv        = 16
0.00.049.372 I llm_load_print_meta: n_rot            = 32
0.00.049.373 I llm_load_print_meta: n_swa            = 0
0.00.049.373 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.374 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.374 I llm_load_print_meta: n_gqa            = 1
0.00.049.375 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.376 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.376 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.377 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.378 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.378 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.378 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.379 I llm_load_print_meta: n_ff             = 8192
0.00.049.379 I llm_load_print_meta: n_expert         = 0
0.00.049.380 I llm_load_print_meta: n_expert_used    = 0
0.00.049.380 I llm_load_print_meta: causal attn      = 1
0.00.049.381 I llm_load_print_meta: pooling type     = 0
0.00.049.381 I llm_load_print_meta: rope type        = 2
0.00.049.381 I llm_load_print_meta: rope scaling     = linear
0.00.049.381 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.382 I llm_load_print_meta: freq_scale_train = 1
0.00.049.382 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.382 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.382 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.382 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.382 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.383 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.383 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.383 I llm_load_print_meta: model type       = 1.4B
0.00.049.383 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.384 I llm_load_print_meta: model params     = 1.41 B
0.00.049.385 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.385 I llm_load_print_meta: general.name     = 1.4B
0.00.049.385 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.385 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.386 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.387 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.387 I llm_load_print_meta: LF token         = 128 ''
0.00.049.388 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.388 I llm_load_print_meta: max token length = 1024
0.00.050.975 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.975 I llm_load_tensors: offloading output layer to GPU
0.00.050.976 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.985 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.050.986 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.051.867 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.868 I llama_new_context_with_model: n_ctx         = 128
0.00.051.868 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.868 I llama_new_context_with_model: n_batch       = 128
0.00.051.868 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.869 I llama_new_context_with_model: flash_attn    = 0
0.00.051.869 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.869 I llama_new_context_with_model: freq_scale    = 1
0.00.051.870 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.870 I ggml_metal_init: allocating
0.00.051.876 I ggml_metal_init: found device: Apple M4
0.00.051.878 I ggml_metal_init: picking default device: Apple M4
0.00.052.450 I ggml_metal_init: using embedded metal library
0.00.054.776 I ggml_metal_init: GPU name:   Apple M4
0.00.054.777 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.778 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.778 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.778 I ggml_metal_init: simdgroup reduction   = true
0.00.054.778 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.778 I ggml_metal_init: has bfloat            = true
0.00.054.779 I ggml_metal_init: use bfloat            = true
0.00.054.779 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.780 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.500 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.065.725 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.730 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.743 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.593 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.594 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.595 I llama_new_context_with_model: graph nodes  = 967
0.00.066.595 I llama_new_context_with_model: graph splits = 2
0.00.066.607 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.608 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.432.151 I 
0.00.432.181 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.432.198 I perplexity: tokenizing the input ..
0.00.440.121 I perplexity: tokenization took 7.921 ms
0.00.440.125 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.572.768 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.573.930 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.573.947 I llama_perf_context_print:        load time =     422.71 ms
0.00.573.947 I llama_perf_context_print: prompt eval time =     132.42 ms /   128 tokens (    1.03 ms per token,   966.64 tokens per second)
0.00.573.948 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.573.950 I llama_perf_context_print:       total time =     141.80 ms /   129 tokens
0.00.574.376 I ggml_metal_free: deallocating

real	0m0.589s
user	0m0.078s
sys	0m0.070s
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-tg-q3_k.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4400 (6e1531ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.825 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.414 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.419 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.421 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.421 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.422 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.422 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.422 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.423 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.423 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.424 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.424 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.424 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.425 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.425 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.427 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.427 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.427 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.347 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.444 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.362 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.363 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.363 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.364 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.364 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.364 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.365 I llama_model_loader: - type  f32:  194 tensors
0.00.023.365 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.366 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.366 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.366 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.374 I llm_load_vocab: special tokens cache size = 25
0.00.050.051 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.054 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.054 I llm_load_print_meta: arch             = gptneox
0.00.050.055 I llm_load_print_meta: vocab type       = BPE
0.00.050.055 I llm_load_print_meta: n_vocab          = 50304
0.00.050.055 I llm_load_print_meta: n_merges         = 50009
0.00.050.055 I llm_load_print_meta: vocab_only       = 0
0.00.050.055 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.056 I llm_load_print_meta: n_embd           = 2048
0.00.050.056 I llm_load_print_meta: n_layer          = 24
0.00.050.059 I llm_load_print_meta: n_head           = 16
0.00.050.059 I llm_load_print_meta: n_head_kv        = 16
0.00.050.059 I llm_load_print_meta: n_rot            = 32
0.00.050.060 I llm_load_print_meta: n_swa            = 0
0.00.050.060 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.062 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.063 I llm_load_print_meta: n_gqa            = 1
0.00.050.064 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.065 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.072 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.075 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.075 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.075 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.076 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.082 I llm_load_print_meta: n_ff             = 8192
0.00.050.082 I llm_load_print_meta: n_expert         = 0
0.00.050.083 I llm_load_print_meta: n_expert_used    = 0
0.00.050.083 I llm_load_print_meta: causal attn      = 1
0.00.050.083 I llm_load_print_meta: pooling type     = 0
0.00.050.083 I llm_load_print_meta: rope type        = 2
0.00.050.083 I llm_load_print_meta: rope scaling     = linear
0.00.050.084 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.084 I llm_load_print_meta: freq_scale_train = 1
0.00.050.084 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.084 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.084 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.085 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.085 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.085 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.085 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.085 I llm_load_print_meta: model type       = 1.4B
0.00.050.086 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.086 I llm_load_print_meta: model params     = 1.41 B
0.00.050.086 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.086 I llm_load_print_meta: general.name     = 1.4B
0.00.050.087 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.087 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.087 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.087 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.087 I llm_load_print_meta: LF token         = 128 ''
0.00.050.088 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.088 I llm_load_print_meta: max token length = 1024
0.00.052.052 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.053 I llm_load_tensors: offloading output layer to GPU
0.00.052.053 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.063 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.065 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.923 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.924 I llama_new_context_with_model: n_ctx         = 128
0.00.052.924 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.924 I llama_new_context_with_model: n_batch       = 128
0.00.052.924 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.924 I llama_new_context_with_model: flash_attn    = 0
0.00.052.925 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.925 I llama_new_context_with_model: freq_scale    = 1
0.00.052.925 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.926 I ggml_metal_init: allocating
0.00.052.928 I ggml_metal_init: found device: Apple M4
0.00.052.930 I ggml_metal_init: picking default device: Apple M4
0.00.053.495 I ggml_metal_init: using embedded metal library
0.00.055.806 I ggml_metal_init: GPU name:   Apple M4
0.00.055.807 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.808 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.808 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.808 I ggml_metal_init: simdgroup reduction   = true
0.00.055.809 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.809 I ggml_metal_init: has bfloat            = true
0.00.055.809 I ggml_metal_init: use bfloat            = true
0.00.055.809 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.810 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.187 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.424 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.427 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.443 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.327 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.328 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.328 I llama_new_context_with_model: graph nodes  = 967
0.00.067.328 I llama_new_context_with_model: graph splits = 2
0.00.067.340 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.341 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.489.376 I 
0.00.489.408 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.489.422 I perplexity: tokenizing the input ..
0.00.497.657 I perplexity: tokenization took 8.233 ms
0.00.497.661 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.629.406 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.630.873 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.630.889 I llama_perf_context_print:        load time =     480.55 ms
0.00.630.890 I llama_perf_context_print: prompt eval time =     131.52 ms /   128 tokens (    1.03 ms per token,   973.23 tokens per second)
0.00.630.891 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.630.892 I llama_perf_context_print:       total time =     141.51 ms /   129 tokens
0.00.631.405 I ggml_metal_free: deallocating

real	0m0.644s
user	0m0.077s
sys	0m0.086s
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-tg-q4_k.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4400 (6e1531ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.869 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.609 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.614 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.615 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.616 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.616 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.616 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.617 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.618 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.618 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.618 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.619 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.619 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.619 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.620 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.621 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.622 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.622 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.506 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.544 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.379 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.380 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.380 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.381 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.381 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.381 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.382 I llama_model_loader: - type  f32:  194 tensors
0.00.023.382 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.383 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.383 I llama_model_loader: - type q6_K:   13 tensors
0.00.043.429 I llm_load_vocab: special tokens cache size = 25
0.00.049.241 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.243 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.243 I llm_load_print_meta: arch             = gptneox
0.00.049.244 I llm_load_print_meta: vocab type       = BPE
0.00.049.244 I llm_load_print_meta: n_vocab          = 50304
0.00.049.244 I llm_load_print_meta: n_merges         = 50009
0.00.049.245 I llm_load_print_meta: vocab_only       = 0
0.00.049.245 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.245 I llm_load_print_meta: n_embd           = 2048
0.00.049.245 I llm_load_print_meta: n_layer          = 24
0.00.049.248 I llm_load_print_meta: n_head           = 16
0.00.049.248 I llm_load_print_meta: n_head_kv        = 16
0.00.049.248 I llm_load_print_meta: n_rot            = 32
0.00.049.249 I llm_load_print_meta: n_swa            = 0
0.00.049.249 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.249 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.250 I llm_load_print_meta: n_gqa            = 1
0.00.049.250 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.252 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.253 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.253 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.253 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.264 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.268 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.276 I llm_load_print_meta: n_ff             = 8192
0.00.049.276 I llm_load_print_meta: n_expert         = 0
0.00.049.277 I llm_load_print_meta: n_expert_used    = 0
0.00.049.277 I llm_load_print_meta: causal attn      = 1
0.00.049.277 I llm_load_print_meta: pooling type     = 0
0.00.049.277 I llm_load_print_meta: rope type        = 2
0.00.049.277 I llm_load_print_meta: rope scaling     = linear
0.00.049.278 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.278 I llm_load_print_meta: freq_scale_train = 1
0.00.049.279 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.279 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.279 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.280 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.281 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.281 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.281 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.281 I llm_load_print_meta: model type       = 1.4B
0.00.049.282 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.049.283 I llm_load_print_meta: model params     = 1.41 B
0.00.049.283 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.049.284 I llm_load_print_meta: general.name     = 1.4B
0.00.049.284 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.286 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.286 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.286 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.286 I llm_load_print_meta: LF token         = 128 ''
0.00.049.287 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.287 I llm_load_print_meta: max token length = 1024
0.00.051.074 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.074 I llm_load_tensors: offloading output layer to GPU
0.00.051.074 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.080 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.080 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.051.984 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.985 I llama_new_context_with_model: n_ctx         = 128
0.00.051.985 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.985 I llama_new_context_with_model: n_batch       = 128
0.00.051.985 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.986 I llama_new_context_with_model: flash_attn    = 0
0.00.051.986 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.986 I llama_new_context_with_model: freq_scale    = 1
0.00.051.987 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.987 I ggml_metal_init: allocating
0.00.051.991 I ggml_metal_init: found device: Apple M4
0.00.051.993 I ggml_metal_init: picking default device: Apple M4
0.00.052.571 I ggml_metal_init: using embedded metal library
0.00.054.887 I ggml_metal_init: GPU name:   Apple M4
0.00.054.889 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.889 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.889 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.890 I ggml_metal_init: simdgroup reduction   = true
0.00.054.890 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.890 I ggml_metal_init: has bfloat            = true
0.00.054.890 I ggml_metal_init: use bfloat            = true
0.00.054.890 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.891 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.732 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.383 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.397 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.422 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.300 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.301 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.302 I llama_new_context_with_model: graph nodes  = 967
0.00.067.302 I llama_new_context_with_model: graph splits = 2
0.00.067.309 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.309 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.637.643 I 
0.00.637.729 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.637.767 I perplexity: tokenizing the input ..
0.00.646.109 I perplexity: tokenization took 8.34 ms
0.00.646.113 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.779.483 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.780.794 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.780.806 I llama_perf_context_print:        load time =     628.77 ms
0.00.780.807 I llama_perf_context_print: prompt eval time =     133.13 ms /   128 tokens (    1.04 ms per token,   961.44 tokens per second)
0.00.780.807 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.780.808 I llama_perf_context_print:       total time =     143.17 ms /   129 tokens
0.00.781.141 I ggml_metal_free: deallocating

real	0m0.795s
user	0m0.078s
sys	0m0.117s
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-tg-q5_k.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4400 (6e1531ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.532 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.541 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.547 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.549 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.549 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.550 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.550 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.550 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.551 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.552 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.552 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.552 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.553 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.553 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.553 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.556 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.556 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.556 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.635 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.672 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.563 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.565 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.566 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.566 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.567 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.568 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.568 I llama_model_loader: - type  f32:  194 tensors
0.00.025.569 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.569 I llama_model_loader: - type q6_K:   37 tensors
0.00.046.279 I llm_load_vocab: special tokens cache size = 25
0.00.052.132 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.137 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.137 I llm_load_print_meta: arch             = gptneox
0.00.052.137 I llm_load_print_meta: vocab type       = BPE
0.00.052.138 I llm_load_print_meta: n_vocab          = 50304
0.00.052.138 I llm_load_print_meta: n_merges         = 50009
0.00.052.138 I llm_load_print_meta: vocab_only       = 0
0.00.052.138 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.138 I llm_load_print_meta: n_embd           = 2048
0.00.052.138 I llm_load_print_meta: n_layer          = 24
0.00.052.143 I llm_load_print_meta: n_head           = 16
0.00.052.143 I llm_load_print_meta: n_head_kv        = 16
0.00.052.143 I llm_load_print_meta: n_rot            = 32
0.00.052.144 I llm_load_print_meta: n_swa            = 0
0.00.052.144 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.144 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.146 I llm_load_print_meta: n_gqa            = 1
0.00.052.147 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.147 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.148 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.148 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.148 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.148 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.149 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.149 I llm_load_print_meta: n_ff             = 8192
0.00.052.149 I llm_load_print_meta: n_expert         = 0
0.00.052.149 I llm_load_print_meta: n_expert_used    = 0
0.00.052.150 I llm_load_print_meta: causal attn      = 1
0.00.052.150 I llm_load_print_meta: pooling type     = 0
0.00.052.150 I llm_load_print_meta: rope type        = 2
0.00.052.150 I llm_load_print_meta: rope scaling     = linear
0.00.052.151 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.151 I llm_load_print_meta: freq_scale_train = 1
0.00.052.151 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.152 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.152 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.152 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.154 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.154 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.154 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.154 I llm_load_print_meta: model type       = 1.4B
0.00.052.155 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.052.155 I llm_load_print_meta: model params     = 1.41 B
0.00.052.155 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.052.156 I llm_load_print_meta: general.name     = 1.4B
0.00.052.156 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.156 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.156 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.156 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.157 I llm_load_print_meta: LF token         = 128 ''
0.00.052.157 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.157 I llm_load_print_meta: max token length = 1024
0.00.054.132 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.133 I llm_load_tensors: offloading output layer to GPU
0.00.054.133 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.144 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.054.145 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.055.082 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.083 I llama_new_context_with_model: n_ctx         = 128
0.00.055.083 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.083 I llama_new_context_with_model: n_batch       = 128
0.00.055.083 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.083 I llama_new_context_with_model: flash_attn    = 0
0.00.055.084 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.084 I llama_new_context_with_model: freq_scale    = 1
0.00.055.084 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.085 I ggml_metal_init: allocating
0.00.055.089 I ggml_metal_init: found device: Apple M4
0.00.055.090 I ggml_metal_init: picking default device: Apple M4
0.00.055.711 I ggml_metal_init: using embedded metal library
0.00.058.463 I ggml_metal_init: GPU name:   Apple M4
0.00.058.465 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.466 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.466 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.466 I ggml_metal_init: simdgroup reduction   = true
0.00.058.466 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.467 I ggml_metal_init: has bfloat            = true
0.00.058.467 I ggml_metal_init: use bfloat            = true
0.00.058.468 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.468 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.452 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.782 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.786 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.801 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.793 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.794 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.795 I llama_new_context_with_model: graph nodes  = 967
0.00.069.795 I llama_new_context_with_model: graph splits = 2
0.00.069.808 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.810 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.625.622 I 
0.00.625.658 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.625.674 I perplexity: tokenizing the input ..
0.00.633.469 I perplexity: tokenization took 7.794 ms
0.00.633.472 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.774.421 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.775.677 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.775.692 I llama_perf_context_print:        load time =     615.08 ms
0.00.775.698 I llama_perf_context_print: prompt eval time =     140.71 ms /   128 tokens (    1.10 ms per token,   909.70 tokens per second)
0.00.775.700 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.775.700 I llama_perf_context_print:       total time =     150.07 ms /   129 tokens
0.00.776.180 I ggml_metal_free: deallocating

real	0m0.791s
user	0m0.078s
sys	0m0.112s
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-tg-q6_k.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4400 (6e1531ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.977 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.779 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.783 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.785 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.785 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.786 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.786 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.786 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.787 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.787 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.788 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.788 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.789 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.789 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.789 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.791 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.793 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.793 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.679 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.673 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.516 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.517 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.517 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.517 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.518 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.518 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.519 I llama_model_loader: - type  f32:  194 tensors
0.00.023.519 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.456 I llm_load_vocab: special tokens cache size = 25
0.00.050.125 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.128 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.128 I llm_load_print_meta: arch             = gptneox
0.00.050.128 I llm_load_print_meta: vocab type       = BPE
0.00.050.128 I llm_load_print_meta: n_vocab          = 50304
0.00.050.129 I llm_load_print_meta: n_merges         = 50009
0.00.050.129 I llm_load_print_meta: vocab_only       = 0
0.00.050.129 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.129 I llm_load_print_meta: n_embd           = 2048
0.00.050.129 I llm_load_print_meta: n_layer          = 24
0.00.050.132 I llm_load_print_meta: n_head           = 16
0.00.050.133 I llm_load_print_meta: n_head_kv        = 16
0.00.050.133 I llm_load_print_meta: n_rot            = 32
0.00.050.133 I llm_load_print_meta: n_swa            = 0
0.00.050.134 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.136 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.137 I llm_load_print_meta: n_gqa            = 1
0.00.050.137 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.138 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.139 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.139 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.139 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.140 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.140 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.140 I llm_load_print_meta: n_ff             = 8192
0.00.050.141 I llm_load_print_meta: n_expert         = 0
0.00.050.141 I llm_load_print_meta: n_expert_used    = 0
0.00.050.141 I llm_load_print_meta: causal attn      = 1
0.00.050.141 I llm_load_print_meta: pooling type     = 0
0.00.050.141 I llm_load_print_meta: rope type        = 2
0.00.050.141 I llm_load_print_meta: rope scaling     = linear
0.00.050.142 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.142 I llm_load_print_meta: freq_scale_train = 1
0.00.050.142 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.143 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.143 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.143 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.144 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.144 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.145 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.145 I llm_load_print_meta: model type       = 1.4B
0.00.050.146 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.146 I llm_load_print_meta: model params     = 1.41 B
0.00.050.146 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.148 I llm_load_print_meta: general.name     = 1.4B
0.00.050.148 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.148 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.148 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.149 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.149 I llm_load_print_meta: LF token         = 128 ''
0.00.050.149 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.149 I llm_load_print_meta: max token length = 1024
0.00.052.176 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.176 I llm_load_tensors: offloading output layer to GPU
0.00.052.177 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.187 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.189 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.107 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.107 I llama_new_context_with_model: n_ctx         = 128
0.00.053.108 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.108 I llama_new_context_with_model: n_batch       = 128
0.00.053.108 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.108 I llama_new_context_with_model: flash_attn    = 0
0.00.053.109 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.109 I llama_new_context_with_model: freq_scale    = 1
0.00.053.109 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.110 I ggml_metal_init: allocating
0.00.053.115 I ggml_metal_init: found device: Apple M4
0.00.053.117 I ggml_metal_init: picking default device: Apple M4
0.00.053.662 I ggml_metal_init: using embedded metal library
0.00.056.001 I ggml_metal_init: GPU name:   Apple M4
0.00.056.003 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.003 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.004 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.004 I ggml_metal_init: simdgroup reduction   = true
0.00.056.004 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.004 I ggml_metal_init: has bfloat            = true
0.00.056.004 I ggml_metal_init: use bfloat            = true
0.00.056.004 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.005 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.553 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.805 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.806 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.821 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.691 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.692 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.692 I llama_new_context_with_model: graph nodes  = 967
0.00.067.692 I llama_new_context_with_model: graph splits = 2
0.00.067.704 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.705 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.521.458 I 
0.00.521.493 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.521.507 I perplexity: tokenizing the input ..
0.00.529.177 I perplexity: tokenization took 7.667 ms
0.00.529.182 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.669.174 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.670.341 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.670.358 I llama_perf_context_print:        load time =     512.47 ms
0.00.670.359 I llama_perf_context_print: prompt eval time =     139.76 ms /   128 tokens (    1.09 ms per token,   915.84 tokens per second)
0.00.670.360 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.670.360 I llama_perf_context_print:       total time =     148.90 ms /   129 tokens
0.00.670.818 I ggml_metal_free: deallocating

real	0m0.684s
user	0m0.078s
sys	0m0.100s
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-imatrix.log
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.344 I build: 4400 (6e1531ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.034 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.518 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.524 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.526 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.533 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.533 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.534 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.536 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.538 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.539 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.539 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.540 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.541 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.542 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.543 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.546 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.547 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.548 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.579 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.773 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.481 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.054.483 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.484 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.484 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.485 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.486 I llama_model_loader: - type  f32:  194 tensors
0.00.054.486 I llama_model_loader: - type  f16:   98 tensors
0.00.084.606 I llm_load_vocab: special tokens cache size = 25
0.00.091.461 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.091.463 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.091.464 I llm_load_print_meta: arch             = gptneox
0.00.091.464 I llm_load_print_meta: vocab type       = BPE
0.00.091.464 I llm_load_print_meta: n_vocab          = 50304
0.00.091.465 I llm_load_print_meta: n_merges         = 50009
0.00.091.465 I llm_load_print_meta: vocab_only       = 0
0.00.091.465 I llm_load_print_meta: n_ctx_train      = 2048
0.00.091.465 I llm_load_print_meta: n_embd           = 2048
0.00.091.465 I llm_load_print_meta: n_layer          = 24
0.00.091.468 I llm_load_print_meta: n_head           = 16
0.00.091.468 I llm_load_print_meta: n_head_kv        = 16
0.00.091.469 I llm_load_print_meta: n_rot            = 32
0.00.091.469 I llm_load_print_meta: n_swa            = 0
0.00.091.469 I llm_load_print_meta: n_embd_head_k    = 128
0.00.091.469 I llm_load_print_meta: n_embd_head_v    = 128
0.00.091.470 I llm_load_print_meta: n_gqa            = 1
0.00.091.471 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.091.471 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.091.472 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.091.472 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.091.472 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.091.472 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.091.472 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.091.473 I llm_load_print_meta: n_ff             = 8192
0.00.091.473 I llm_load_print_meta: n_expert         = 0
0.00.091.473 I llm_load_print_meta: n_expert_used    = 0
0.00.091.474 I llm_load_print_meta: causal attn      = 1
0.00.091.474 I llm_load_print_meta: pooling type     = 0
0.00.091.475 I llm_load_print_meta: rope type        = 2
0.00.091.475 I llm_load_print_meta: rope scaling     = linear
0.00.091.475 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.091.475 I llm_load_print_meta: freq_scale_train = 1
0.00.091.477 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.091.477 I llm_load_print_meta: rope_finetuned   = unknown
0.00.091.477 I llm_load_print_meta: ssm_d_conv       = 0
0.00.091.477 I llm_load_print_meta: ssm_d_inner      = 0
0.00.091.477 I llm_load_print_meta: ssm_d_state      = 0
0.00.091.478 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.091.478 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.091.478 I llm_load_print_meta: model type       = 1.4B
0.00.091.478 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.091.479 I llm_load_print_meta: model params     = 1.41 B
0.00.091.479 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.091.479 I llm_load_print_meta: general.name     = 1.4B
0.00.091.480 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.091.483 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.091.483 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.091.484 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.091.484 I llm_load_print_meta: LF token         = 128 ''
0.00.091.484 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.091.484 I llm_load_print_meta: max token length = 1024
0.00.093.497 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.093.497 I llm_load_tensors: offloading output layer to GPU
0.00.093.497 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.093.502 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.093.503 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.094.420 I llama_new_context_with_model: n_seq_max     = 1
0.00.094.421 I llama_new_context_with_model: n_ctx         = 128
0.00.094.421 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.094.421 I llama_new_context_with_model: n_batch       = 128
0.00.094.421 I llama_new_context_with_model: n_ubatch      = 128
0.00.094.421 I llama_new_context_with_model: flash_attn    = 0
0.00.094.422 I llama_new_context_with_model: freq_base     = 10000.0
0.00.094.422 I llama_new_context_with_model: freq_scale    = 1
0.00.094.423 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.094.423 I ggml_metal_init: allocating
0.00.094.429 I ggml_metal_init: found device: Apple M4
0.00.094.431 I ggml_metal_init: picking default device: Apple M4
0.00.095.046 I ggml_metal_init: using embedded metal library
0.00.097.618 I ggml_metal_init: GPU name:   Apple M4
0.00.097.619 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.620 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.620 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.620 I ggml_metal_init: simdgroup reduction   = true
0.00.097.621 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.621 I ggml_metal_init: has bfloat            = true
0.00.097.621 I ggml_metal_init: use bfloat            = true
0.00.097.621 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.622 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.106.657 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.107.933 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.107.935 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.107.948 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.108.778 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.108.779 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.108.780 I llama_new_context_with_model: graph nodes  = 967
0.00.108.780 I llama_new_context_with_model: graph splits = 2
0.00.108.788 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.108.789 I 
0.00.108.807 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.108.808 I compute_imatrix: tokenizing the input ..
0.00.115.597 I compute_imatrix: tokenization took 6.788 ms
0.00.115.599 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.605.201 I compute_imatrix: 1.49 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.607.700 I llama_perf_context_print:        load time =    1581.16 ms
0.01.607.701 I llama_perf_context_print: prompt eval time =    1487.58 ms /   128 tokens (   11.62 ms per token,    86.05 tokens per second)
0.01.607.702 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.607.702 I llama_perf_context_print:       total time =    1583.65 ms /   129 tokens
0.01.608.284 I ggml_metal_free: deallocating

real	0m1.797s
user	0m0.170s
sys	0m0.233s
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-save-load-state.log
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4400 (6e1531ac)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13860a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13860a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13860aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13860b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13860ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13860bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13860c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13860cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13860d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13860d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13860daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13860dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13860eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13860f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13860fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1386101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x138610910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x138611030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x138611750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x138611f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x138612640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x138612d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x138613480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x138613d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x138614440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x138614700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x138614d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x138615980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x138615ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x138616180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x138616620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1386168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x138617170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1386176b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x138617970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x138617e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1386182b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x138618750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x138618bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x138619090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x138619530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1386199d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x138619e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13861a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13861a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13861abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13861b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13861bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13861c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13861c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13861cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13861d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13861d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13861df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13861e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13861ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13861f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13861f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13861f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x138620160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x138620420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1386208c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x138620d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x138621200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1386216a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x138621b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x138621fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x138622480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x138622920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x138622dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x138623260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x138623700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x138623ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1386240f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x138624640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x138624b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1386250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x138625630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x138625b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1386260d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x138626620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x138626b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1386270c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x138627610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x138627b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1386280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x138628600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x138628b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1386290a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1386295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x138629b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13862a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13862a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13862ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13862b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13862b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13862bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13861b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13862bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13862c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13862cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13862d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13862d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13862dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13862e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13862e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13862ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13862f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13862f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13862fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1386301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x138630700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x138630c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1386310f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x138631590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x138631a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x138631ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x138632370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x138632810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x138632cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x138633150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1386335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x138633a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x138633f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1386343d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x138634870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x138634d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1386351b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x138635650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x138635af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x138635f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x138636430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1386368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x138636d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x138637210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1386376b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x138637b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x138637ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x138638490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x138638930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x138638dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x138639270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x138639710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x138639bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13863a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13863a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13863a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13863ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13863b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13863b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13863bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13863c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13863c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13863c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13863ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13863d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13863d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13863dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13863e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13863e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13863ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13863eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13863f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13863f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13863fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x138640170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x138640610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x138640ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x138640f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1386413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x138641890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x138641d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1386421d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x138642670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x138642b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x138642fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x138643450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1386438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x138643d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x138644230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1386446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x138644b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x138645010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1386454b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x138645950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x138645df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x138646290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x138646730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x138646bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x138647070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x138647510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1386479b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x138647e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1386483a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1386488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x138648e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x138649390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x138649650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x138649c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13864a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13864a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13864b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13864b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13864b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13864bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13864c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13864cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13864d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13864d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13864d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13864e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13864e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13864ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13864f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13864f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13864fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x138650150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1386506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x138650bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x138651140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x138651690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x138651be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x138652130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x138652680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x138652bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x138653120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x138653670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x138653bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x138654110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x138654660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x138654bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x138655100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x138655650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x138655ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1386560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x138656640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x138656b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1386570e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x138657630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x138657b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1386580d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x138658620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x138658b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1386590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x138659610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x138659b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13865a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13865a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13865ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13865b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13865b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13865bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13865c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13865c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13865cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13865d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13865d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13865db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13865e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13865e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13865eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13865f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13865f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13865fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x138660050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1386605a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x138660af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x138660f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x138661430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1386618d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x138661d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x138662210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1386626b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x138662b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x138662ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x138663490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x138663930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x138663dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x138664270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x138664710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x138664bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x138665050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1386655a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x138665cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1386663e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x138666b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x138667220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1386674e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x138667cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x138667f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1386685a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.143.874 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.143.877 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x149306120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x149306590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x149306a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x149306e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1493072e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x149307750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x149307bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x149304080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1493044f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x149304960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x149308030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x149308610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x149309140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1493098f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14930a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14930a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14930af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14930b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14930bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14930c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14930cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14930d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14930dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14930e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14930e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14930ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14930f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14930f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14930fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1493105d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x149310a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x149310d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1493115c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x149311b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x149311dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x149312260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x149312700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x149312ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x149313040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1493134e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x149313980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x149313e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1493142c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x149314760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x149314a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x149315030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x149315640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x149315c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x149316260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x149316870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x149316e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x149317490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x149317aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1493180b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1493188a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x149318d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1493191e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1493194a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x149319ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14931a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14931a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14931abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14931b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14931b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14931b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14931be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14931c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14931c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14931cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14931d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14931d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14931da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14931dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14931e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14931e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14931eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14931f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14931f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14931fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1493203f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x149320940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x149320e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1493213e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x149321930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x149321e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1493223d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x149322920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x149322e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1493233c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x149323910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x149323e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1493243b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x149324900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x149324e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1493253a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1493258f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x149325e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x149326390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1493268e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x149326e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x149327380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1493278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x149327e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x149328370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1493288c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x149328e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x149329360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1493298b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x149329e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14932a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14932a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14932adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14932b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14932b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14932bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14932c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14932c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14932ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14932cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14932d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14932d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14932dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14932e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14932e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14932eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14932ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14932f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14932f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14932fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1493301e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x149330680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x149330b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x149330fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x149331460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x149331900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x149331da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x149332240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1493326e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x149332b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x149333020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1493334c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x149333960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x149333e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1493342a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x149334740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x149334be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x149335080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x149335520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1493359c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x149335e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x149336300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1493367a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x149336c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1493370e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x149337580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x149337a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x149337ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x149338360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x149338800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x149338ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x149339140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1493395e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x149339a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x149339f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14933a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14933a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14933ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14933b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14933b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14933bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14933bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14933c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14933c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14933cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14933d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14933d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14933db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14933dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14933e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14933e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14933edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14933f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14933f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14933fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x149340040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1493404e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x149340980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x149340e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1493412c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x149341760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x149341c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1493420a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x149342540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x149342a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x149342fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x149343530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x149343a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x149343d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x149344350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x149344960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x149344f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x149345760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x149345c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x149345ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1493464d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x149346ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1493472d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x149347770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x149347c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1493480b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x149348860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x149348db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x149349300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x149349850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x149349da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14934a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14934a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14934ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14934b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14934b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14934bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14934c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14934c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14934cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14934d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14934d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14934dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14934e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14934e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14934ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14934f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14934f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14934fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x149350290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1493507e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x149350d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x149351280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1493517d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x149351d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x149352270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1493527c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x149352d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x149353260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1493537b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x149353d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x149354250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1493547a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x149354cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x149355240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x149355790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x149355ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x149356230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x149356780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x149356cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x149357220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x149357770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x149357cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x149358210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x149358760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x149358cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x149359200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x149359750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x149359ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14935a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14935a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14935ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14935b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14935b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14935bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14935bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14935c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14935c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14935cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14935d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14935d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14935db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14935e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14935e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14935e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14935ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14935f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14935f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14935fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1493603b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x149360ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1493611f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x149361910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x149361bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1493623c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x149362680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x149362c90 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x149306580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1493069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x149306e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1493072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x149307740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x149307bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x149308020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x149308490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x149308900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x149308d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1493091e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1493097c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14930a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14930a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14930b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14930b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14930bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14930c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14930cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14930d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14930dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14930e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14930ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14930f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14930f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14930fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1493100e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x149310550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1493109c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x149310e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1493112a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x149311710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x149311b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x149311e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1493122b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x149312720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x149312b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x149313000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x149313470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1493138e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x149313d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1493141c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x149314630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x149314aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x149314f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x149315380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1493157f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x149315c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1493160d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x149316540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1493169b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x149316e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x149317290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x149317700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x149317b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x149317fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x149318450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1493188c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x149318d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1493191a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x149319610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x149319a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x149319ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14931a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14931a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14931ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14931b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14931b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14931b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14931be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14931c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14931c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14931cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14931cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14931d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14931d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14931dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14931e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14931e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14931ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14931eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14931f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14931f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14931fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x149320090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x149320500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x149320970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x149320de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x149321250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1493216c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x149321b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x149321fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x149322410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x149322880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x149322cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x149323160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1493235d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x149323a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x149323eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x149324320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x149324790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x149324c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x149325070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1493254e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x149325950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x149325dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x149326230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1493266a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x149326b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x149326f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1493273f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x149327860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x149327cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x149328140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1493285b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x149328a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x149328e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x149329300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x149329770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x149329be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14932a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14932a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14932a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14932ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14932b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14932b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14932baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14932bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14932c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14932c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14932ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14932d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14932d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14932da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14932de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14932e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14932e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14932ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14932f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14932f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14932f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14932fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1493301f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x149330660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x149330ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x149330f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1493313b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x149331820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x149331c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x149332100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x149332570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1493329e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x149332e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1493332c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x149333730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x149333ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x149334010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x149334480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1493348f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x149334d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1493351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x149335640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x149335ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x149335f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x149336390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x149336800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x149336c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1493370e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x149337550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1493379c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x149337e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1493382a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x149338710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x149338b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x149338ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x149339460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1493398d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x149339d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14933a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14933a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14933aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14933af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14933b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14933b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14933bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14933c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14933c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14933c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14933ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14933d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14933d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14933db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14933dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14933e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14933e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14933ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14933f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14933f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14933fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14933fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x149340350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1493407c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x149340c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1493410a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x149341510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x149341980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x149341df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x149342260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1493426d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x149342b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1493432c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x149343730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x149343ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x149344010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x149344480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1493448f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x149344d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1493451d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x149345640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x149345ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x149345f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x149346390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x149346800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x149346c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1493470e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x149347550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1493479c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x149347e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1493482a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x149348710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x149348b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x149348ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x149349460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1493498d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x149349d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14934a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14934a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14934aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14934af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14934b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14934b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14934bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14934c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14934c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14934c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14934ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14934d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14934d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14934db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14934dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14934e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14934e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14934ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14934f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14934f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14934fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14934fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x149350350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1493507c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x149350c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1493510a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x149351510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x149351980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x149351df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x149352260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1493526d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x149352b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x149352fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x149353420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x149353890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x149353d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x149354170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1493545e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x149354a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x149354ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x149355330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1493557a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x149355c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x149356080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1493564f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x149356960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x149356dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x149357240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x149357aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x149358190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x149358880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x149358f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1493593e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x149359850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x149359cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14935a130 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.777s
user	0m0.292s
sys	0m0.301s
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-save-load-state.log
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4400 (6e1531ac)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15b808260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15b808970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15b808f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15b8094d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15b809a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15b80a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15b80a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15b80ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15b80b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15b80b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15b80bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15b80c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15b80cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15b80d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15b80db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15b80e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15b80e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15b80f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15b80f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15b80ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15b810690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15b810db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15b8114d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15b811d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15b812490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15b812750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15b812d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15b8139d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15b813f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15b8141d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15b814670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15b814930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15b8151c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15b815700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15b8159c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15b815e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15b816300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15b8167a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15b816c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15b8170e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15b817580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15b817a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15b817ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15b818360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15b818620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15b818c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15b819240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15b819b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15b81a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15b81a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15b81ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15b81b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15b81b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15b81bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15b81c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15b81cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15b81d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15b81d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15b81d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15b81e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15b81e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15b81e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15b81edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15b81f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15b81f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15b81fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15b820030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15b8204d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15b820970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15b820e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15b8212b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15b821750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15b821bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15b822140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15b822690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15b822be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15b823130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15b823680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15b823bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15b824120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15b824670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15b824bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15b825110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15b825660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15b825bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15b826100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15b826650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15b826ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15b8270f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15b827640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15b827b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15b8280e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15b828630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15b828b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15b8290d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15b829620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15b829b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15b819850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15b829fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15b82a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15b82ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15b82b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15b82b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15b82bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15b82c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15b82c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15b82ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15b82d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15b82d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15b82dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15b82e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15b82e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15b82eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15b82f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15b82f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15b82fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15b82ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15b8303c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15b830860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15b830d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15b8311a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15b831640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15b831ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15b831f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15b832420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15b8328c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15b832d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15b833200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15b8336a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15b833b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15b833fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15b834480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15b834920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15b834dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15b835260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15b835700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15b835ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15b836040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15b8364e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15b836980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15b836e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15b8372c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15b837760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15b837c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15b8380a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15b838540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15b8389e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15b838e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15b839320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15b8397c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15b839c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15b83a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15b83a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15b83aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15b83aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15b83b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15b83b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15b83bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15b83c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15b83c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15b83caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15b83cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15b83d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15b83d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15b83dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15b83e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15b83e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15b83eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15b83efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15b83f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15b83f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15b83fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15b840220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15b8406c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15b840b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15b841000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15b8414a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15b841940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15b841de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15b842280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15b842720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15b842bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15b843060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15b843500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15b8439a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15b843e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15b8442e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15b844780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15b844c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15b8450c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15b845560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15b845a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15b845ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15b8463f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15b846940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15b846e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15b8473e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15b8476a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15b847cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15b8482c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15b8488d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15b8490c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15b849560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15b849820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15b849e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15b84a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15b84ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15b84b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15b84b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15b84ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15b84c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15b84c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15b84cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15b84d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15b84d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15b84dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15b84e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15b84e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15b84ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15b84f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15b84f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15b84fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15b850180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15b8506d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15b850c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15b851170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15b8516c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15b851c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15b852160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15b8526b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15b852c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15b853150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15b8536a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15b853bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15b854140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15b854690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15b854be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15b855130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15b855680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15b855bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15b856120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15b856670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15b856bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15b857110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15b857660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15b857bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15b858100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15b858650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15b858ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15b8590f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15b859640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15b859b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15b85a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15b85a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15b85ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15b85b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15b85b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15b85bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15b85c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15b85c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15b85cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15b85d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15b85d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15b85db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15b85e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15b85e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15b85eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15b85efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15b85f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15b85f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15b85fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15b860260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15b860700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15b860ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15b861040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15b8614e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15b861980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15b861e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15b8622c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15b862760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15b862c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15b8630a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15b8635f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15b863d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15b864430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15b864b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15b865270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15b865530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15b865d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15b865fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15b8665f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.093.963 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.093.966 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15b8098d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15b809d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15b80a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15b80a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15b80aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15b80af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15b807890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15b808410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15b822d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15b823170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15b8235e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15b823a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15b8241d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15b824950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15b825130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15b825820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15b825f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15b826600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15b826cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15b827670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15b827d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15b828450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15b828b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15b829230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15b829920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15b829d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15b82a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15b82a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15b82aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15b82af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15b82b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15b82b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15b82bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15b82bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15b82c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15b82c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15b82ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15b82d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15b82d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15b82da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15b82de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15b82e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15b82e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15b82ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15b82f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15b82f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15b82f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15b82fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15b8301f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15b830660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15b830ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15b830f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15b8313b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15b831820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15b831c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15b832100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15b832570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15b8329e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15b832e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15b8332c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15b833730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15b833ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15b834010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15b834480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15b8348f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15b834d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15b8351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15b835640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15b835ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15b835f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15b836390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15b836800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15b836c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15b8370e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15b837550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15b8379c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15b837e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15b8382a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15b838710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15b838b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15b838ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15b839460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15b8398d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15b839d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15b83a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15b83a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15b83aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15b83af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15b83b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15b83b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15b83bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15b83c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15b83c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15b83c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15b83ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15b83d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15b83d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15b83db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15b83dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15b83e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15b83e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15b83ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15b83f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15b83f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15b83fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15b83fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15b840350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15b8407c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15b840c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15b8410a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15b841510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15b841980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15b841df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15b842260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15b8426d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15b842b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15b842fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15b843420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15b843890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15b843d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15b844170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15b8445e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15b844a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15b844ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15b845330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15b8457a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15b845c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15b846080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15b8464f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15b846960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15b846dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15b847240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15b8476b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15b847b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15b847f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15b848400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15b848870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15b848ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15b849150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15b8495c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15b849a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15b849ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15b84a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15b84a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15b84abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15b84b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15b84b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15b84b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15b84bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15b84c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15b84c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15b84cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15b84cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15b84d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15b84d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15b84dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15b84e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15b84e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15b84ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15b84ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15b84f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15b84f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15b84fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15b850040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15b8504b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15b850920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15b850d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15b851200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15b851670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15b851ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15b851f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15b8523c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15b852830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15b852ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15b853110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15b853580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15b8539f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15b853e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15b8542d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15b854740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15b854bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15b855020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15b855490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15b855900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15b855d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15b8561e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15b856650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15b856ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15b856f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15b8573a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15b857810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15b857c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15b8580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15b858560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15b8589d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15b858e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15b8592b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15b859720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15b859b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15b85a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15b85a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15b85a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15b85ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15b85b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15b85b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15b85baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15b85bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15b85c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15b85c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15b85cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15b85d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15b85d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15b85dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15b85e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15b85e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15b85ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15b85ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15b85f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15b85f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15b85fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15b860040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15b8604b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15b860920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15b860d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15b861200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15b861670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15b861ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15b861f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15b8623c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15b862830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15b862ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15b863110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15b863580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15b8639f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15b863e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15b8642d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15b864740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15b864bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15b865020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15b865490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15b865900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15b865d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15b8661e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15b866650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15b8158a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15b815d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15b816180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15b8165f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15b816a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15b816ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15b817340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15b8177b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15b817c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15b818090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15b818500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15b818970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15b818de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15b819250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15b8196c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15b819b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15b819fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15b81a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15b81a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15b81acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15b81b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15b81b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15b81ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15b81beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15b81c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15b81c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15b81cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15b81d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15b81d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15b81d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15b81ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15b81e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15b81e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15b81eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15b81ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15b81f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15b81f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15b81fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15b820140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15b820830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15b820f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15b821610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15b821d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15b822170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15b8225e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15b8140b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15b814520 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x159e09380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x159e097f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x159e09c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x159e0a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x159e0a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x159e0a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x159e0ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x159e0b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x159e0b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x159e0bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x159e0c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x159e0c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x159e0d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x159e0da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x159e0e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x159e0e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x159e0f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x159e0f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x159e0fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x159e10690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x159e10db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x159e114d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x159e11bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x159e12310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x159e12a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x159e12cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x159e12fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x159e13420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x159e13890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x159e13d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x159e14170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x159e146a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x159e14b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x159e14dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x159e15240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x159e156b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x159e15b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x159e15f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x159e16400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x159e16870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x159e16ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x159e17150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x159e175c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x159e17a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x159e17ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x159e18310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x159e18780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x159e18bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x159e19060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x159e194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x159e19940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x159e19db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x159e1a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x159e1a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x159e1ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x159e1af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x159e1b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x159e1b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x159e1be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x159e1c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x159e1c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x159e1cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x159e1d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x159e1d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x159e1d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x159e1dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x159e1e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x159e1e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x159e1eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x159e1ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x159e1f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x159e1f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x159e1fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x159e200e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x159e20550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x159e209c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x159e20e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x159e212a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x159e21710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x159e21b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x159e21ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x159e22460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x159e228d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x159e22d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x159e231b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x159e23620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x159e23a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x159e23f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x159e24370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x159e247e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x159e24c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x159e250c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x159e25530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x159e259a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x159e25e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x159e26280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x159e266f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x159e26b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x159e26fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x159e27440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x159e278b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x159e27d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x159e28190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x159e28600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x159e28a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x159e28ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x159e29350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x159e297c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x159e29c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x159e2a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x159e2a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x159e2a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x159e2adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x159e2b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x159e2b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x159e2bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x159e2bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x159e2c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x159e2c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x159e2cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x159e2d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x159e2d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x159e2da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x159e2dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x159e2e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x159e2e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x159e2ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x159e2f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x159e2f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x159e2f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x159e2fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x159e30240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x159e306b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x159e30b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x159e30f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x159e31400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x159e31870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x159e31ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x159e32150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x159e325c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x159e32a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x159e32ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x159e33310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x159e33780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x159e33bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x159e34060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x159e344d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x159e34940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x159e34db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x159e35220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x159e35690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x159e35b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x159e35f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x159e363e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x159e36850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x159e36cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x159e37130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x159e375a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x159e37a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x159e37e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x159e382f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x159e38760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x159e38bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x159e39040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x159e394b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x159e39920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x159e39d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x159e3a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x159e3a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x159e3aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x159e3af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x159e3b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x159e3b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x159e3bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x159e3c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x159e3c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x159e3c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x159e3ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x159e3d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x159e3d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x159e3dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x159e3e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x159e3e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x159e3e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x159e3ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x159e3f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x159e3f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x159e3fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x159e3ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x159e403a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x159e40810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x159e40c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x159e410f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x159e41560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x159e419d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x159e41e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x159e422b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x159e42720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x159e42b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x159e43000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x159e43470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x159e438e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x159e43d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x159e441c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x159e44630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x159e44aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x159e44f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x159e454a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x159e45910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x159e45d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x159e468d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x159e46b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x159e46e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x159e472c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x159e47730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x159e47ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x159e48010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x159e48480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x159e488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x159e48d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x159e491d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x159e49640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x159e49ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x159e49f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x159e4a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x159e4a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x159e4ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x159e4b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x159e4b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x159e4b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x159e4be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x159e4c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x159e4c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x159e4cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x159e4cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x159e4d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x159e4d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x159e4dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x159e4e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x159e4e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x159e4ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x159e4ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x159e4f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x159e4f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x159e50150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x159e50410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x159e50880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x159e50cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x159e51160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x159e515d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x159e51a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x159e51eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x159e52320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x159e52790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x159e52c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x159e53070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x159e534e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x159e53950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x159e53dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x159e54230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x159e546a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x159e54b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x159e54f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x159e553f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x159e55860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x159e55cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x159e56140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x159e565b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x159e56a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x159e56e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x159e57300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x159e57770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x159e57be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x159e58050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x159e584c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x159e58930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x159e58da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x159e59210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x159e59680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x159e59af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x159e59f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x159e5a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x159e5a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x159e5b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x159e5b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x159e5c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x159e5c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x159e5cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x159e5cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x159e5d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x159e5db50 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.944s
user	0m0.242s
sys	0m0.148s
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-tg-f16.log
++ grep '^\[1\]'
+ check_ppl f16 '[1]10.1498,'
+ qnt=f16
++ echo '[1]10.1498,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=10.1498
++ echo '10.1498 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' f16 10.1498
+ return 0
  - f16 @ 10.1498 OK
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-tg-q8_0.log
++ grep '^\[1\]'
+ check_ppl q8_0 '[1]10.1362,'
+ qnt=q8_0
++ echo '[1]10.1362,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=10.1362
++ echo '10.1362 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q8_0 10.1362
+ return 0
  - q8_0 @ 10.1362 OK
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-tg-q4_0.log
++ grep '^\[1\]'
+ check_ppl q4_0 '[1]11.1740,'
+ qnt=q4_0
++ echo '[1]11.1740,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=11.1740
++ echo '11.1740 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q4_0 11.1740
+ return 0
  - q4_0 @ 11.1740 OK
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-tg-q4_1.log
++ grep '^\[1\]'
+ check_ppl q4_1 '[1]10.5507,'
+ qnt=q4_1
++ echo '[1]10.5507,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=10.5507
++ echo '10.5507 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q4_1 10.5507
+ return 0
  - q4_1 @ 10.5507 OK
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-tg-q5_0.log
++ grep '^\[1\]'
+ check_ppl q5_0 '[1]10.0972,'
+ qnt=q5_0
++ echo '[1]10.0972,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=10.0972
++ echo '10.0972 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q5_0 10.0972
+ return 0
  - q5_0 @ 10.0972 OK
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-tg-q5_1.log
++ grep '^\[1\]'
+ check_ppl q5_1 '[1]10.1971,'
+ qnt=q5_1
++ echo '[1]10.1971,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=10.1971
++ echo '10.1971 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q5_1 10.1971
+ return 0
  - q5_1 @ 10.1971 OK
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-tg-q3_k.log
++ grep '^\[1\]'
+ check_ppl q3_k '[1]12.0517,'
+ qnt=q3_k
++ echo '[1]12.0517,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=12.0517
++ echo '12.0517 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q3_k 12.0517
+ return 0
  - q3_k @ 12.0517 OK
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-tg-q4_k.log
++ grep '^\[1\]'
+ check_ppl q4_k '[1]10.1031,'
+ qnt=q4_k
++ echo '[1]10.1031,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=10.1031
++ echo '10.1031 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q4_k 10.1031
+ return 0
  - q4_k @ 10.1031 OK
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-tg-q5_k.log
++ grep '^\[1\]'
+ check_ppl q5_k '[1]10.2433,'
+ qnt=q5_k
++ echo '[1]10.2433,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=10.2433
++ echo '10.2433 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q5_k 10.2433
+ return 0
  - q5_k @ 10.2433 OK
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-tg-q6_k.log
++ grep '^\[1\]'
+ check_ppl q6_k '[1]10.3179,'
+ qnt=q6_k
++ echo '[1]10.3179,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=10.3179
++ echo '10.3179 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q6_k 10.3179
+ return 0
  - q6_k @ 10.3179 OK
+ cat /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/pythia_1_4b-imatrix.log
+ grep Final
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_ctest_with_model_debug
+ cd /Users/ggml/work/llama.cpp
+ local model
+ tee /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/ctest_with_model_debug.log
++ gg_get_model
++ local gguf_0=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
++ local gguf_1=/Users/ggml/mnt/llama.cpp/models/pythia/2.8B/ggml-model-f16.gguf
++ local gguf_2=/Users/ggml/mnt/llama.cpp/models/open-llama/7B-v2/ggml-model-f16.gguf
++ [[ -s /Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf ]]
++ echo -n /Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ model=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ cd build-ci-debug
+ set -e
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/ctest_with_model_debug-ctest.log
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.54 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.59 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.12 sec*proc (2 tests)

Total Test time (real) =   1.14 sec
        1.16 real         0.73 user         0.05 sys
+ set +e
+ cd ..
+ cur=0
+ echo 0
+ set +x
+ gg_run_ctest_with_model_release
+ cd /Users/ggml/work/llama.cpp
+ local model
+ tee /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/ctest_with_model_release.log
++ gg_get_model
++ local gguf_0=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
++ local gguf_1=/Users/ggml/mnt/llama.cpp/models/pythia/2.8B/ggml-model-f16.gguf
++ local gguf_2=/Users/ggml/mnt/llama.cpp/models/open-llama/7B-v2/ggml-model-f16.gguf
++ [[ -s /Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf ]]
++ echo -n /Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ model=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ cd build-ci-release
+ set -e
+ tee -a /Users/ggml/results/llama.cpp/6e/1531aca5ed17f078973b4700fcdadbda4a34a5/ggml-100-mac-m4/ctest_with_model_release-ctest.log
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.24 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.50 sec*proc (2 tests)

Total Test time (real) =   0.51 sec
        0.52 real         0.15 user         0.04 sys
+ set +e
+ cd ..
+ cur=0
+ echo 0
+ set +x
