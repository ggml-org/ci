### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.26 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.14 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.17 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.45 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.28 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.22 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.68 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.09 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.23 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.61 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.22 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.18 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.19 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.24 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.63 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.30 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.12 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.22 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.30 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.99 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    1.04 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  193.72 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.90 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   26.39 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.34 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 255.62 sec*proc (29 tests)

Total Test time (real) = 255.63 sec

real	4m15.726s
user	8m34.547s
sys	0m7.175s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.18 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.23 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.15 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.12 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    1.00 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.83 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.21 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.19 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.22 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.45 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.41 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   30.80 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.40 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.18 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.23 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  55.08 sec*proc (29 tests)

Total Test time (real) =  55.09 sec

real	0m55.106s
user	1m17.545s
sys	0m6.171s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.137 I build: 4729 (e08f38df) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.016.351 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.021.988 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.021.995 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.997 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.021.998 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.999 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.022.000 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.022.000 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.022.002 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.022.002 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.022.003 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.022.004 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.022.004 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.022.007 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.022.007 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.022.008 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.022.008 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.022.009 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.022.011 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.022.012 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.026.340 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.027.391 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.393 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.027.394 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.027.394 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.027.394 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.027.395 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.027.396 I llama_model_loader: - type  f32:  124 tensors
0.00.027.396 I llama_model_loader: - type  f16:   73 tensors
0.00.027.397 I print_info: file format = GGUF V3 (latest)
0.00.027.397 I print_info: file type   = F16
0.00.027.398 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.031.408 I load: special tokens cache size = 5
0.00.033.403 I load: token to piece cache size = 0.2032 MB
0.00.033.431 I print_info: arch             = bert
0.00.033.432 I print_info: vocab_only       = 0
0.00.033.433 I print_info: n_ctx_train      = 512
0.00.033.433 I print_info: n_embd           = 384
0.00.033.433 I print_info: n_layer          = 12
0.00.033.437 I print_info: n_head           = 12
0.00.033.437 I print_info: n_head_kv        = 12
0.00.033.438 I print_info: n_rot            = 32
0.00.033.438 I print_info: n_swa            = 0
0.00.033.438 I print_info: n_embd_head_k    = 32
0.00.033.441 I print_info: n_embd_head_v    = 32
0.00.033.442 I print_info: n_gqa            = 1
0.00.033.443 I print_info: n_embd_k_gqa     = 384
0.00.033.443 I print_info: n_embd_v_gqa     = 384
0.00.033.444 I print_info: f_norm_eps       = 1.0e-12
0.00.033.454 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.033.457 I print_info: f_clamp_kqv      = 0.0e+00
0.00.033.457 I print_info: f_max_alibi_bias = 0.0e+00
0.00.033.458 I print_info: f_logit_scale    = 0.0e+00
0.00.033.465 I print_info: n_ff             = 1536
0.00.033.465 I print_info: n_expert         = 0
0.00.033.466 I print_info: n_expert_used    = 0
0.00.033.466 I print_info: causal attn      = 0
0.00.033.466 I print_info: pooling type     = 2
0.00.033.466 I print_info: rope type        = 2
0.00.033.467 I print_info: rope scaling     = linear
0.00.033.468 I print_info: freq_base_train  = 10000.0
0.00.033.468 I print_info: freq_scale_train = 1
0.00.033.468 I print_info: n_ctx_orig_yarn  = 512
0.00.033.469 I print_info: rope_finetuned   = unknown
0.00.033.469 I print_info: ssm_d_conv       = 0
0.00.033.469 I print_info: ssm_d_inner      = 0
0.00.033.469 I print_info: ssm_d_state      = 0
0.00.033.469 I print_info: ssm_dt_rank      = 0
0.00.033.470 I print_info: ssm_dt_b_c_rms   = 0
0.00.033.470 I print_info: model type       = 33M
0.00.033.472 I print_info: model params     = 33.21 M
0.00.033.473 I print_info: general.name     = Bge Small
0.00.033.473 I print_info: vocab type       = WPM
0.00.033.474 I print_info: n_vocab          = 30522
0.00.033.474 I print_info: n_merges         = 0
0.00.033.474 I print_info: BOS token        = 101 '[CLS]'
0.00.033.475 I print_info: UNK token        = 100 '[UNK]'
0.00.033.475 I print_info: SEP token        = 102 '[SEP]'
0.00.033.475 I print_info: PAD token        = 0 '[PAD]'
0.00.033.476 I print_info: MASK token       = 103 '[MASK]'
0.00.033.476 I print_info: LF token         = 0 '[PAD]'
0.00.033.476 I print_info: max token length = 21
0.00.033.477 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.036.794 I load_tensors: offloading 12 repeating layers to GPU
0.00.036.796 I load_tensors: offloading output layer to GPU
0.00.036.796 I load_tensors: offloaded 13/13 layers to GPU
0.00.036.821 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.036.823 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.037.089 I llama_context_kv_self: n_seq_max     = 1
0.00.037.090 I llama_context_kv_self: n_ctx         = 512
0.00.037.090 I llama_context_kv_self: n_ctx_per_seq = 512
0.00.037.091 I llama_context_kv_self: n_batch       = 2048
0.00.037.091 I llama_context_kv_self: n_ubatch      = 2048
0.00.037.091 I llama_context_kv_self: flash_attn    = 0
0.00.037.092 I llama_context_kv_self: freq_base     = 10000.0
0.00.037.092 I llama_context_kv_self: freq_scale    = 1
0.00.037.092 I ggml_metal_init: allocating
0.00.037.098 I ggml_metal_init: found device: Apple M4
0.00.037.104 I ggml_metal_init: picking default device: Apple M4
0.00.037.848 I ggml_metal_init: using embedded metal library
0.00.041.787 I ggml_metal_init: GPU name:   Apple M4
0.00.041.790 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.041.790 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.041.791 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.041.791 I ggml_metal_init: simdgroup reduction   = true
0.00.041.791 I ggml_metal_init: simdgroup matrix mul. = true
0.00.041.791 I ggml_metal_init: has residency sets    = true
0.00.041.792 I ggml_metal_init: has bfloat            = true
0.00.041.792 I ggml_metal_init: use bfloat            = true
0.00.041.792 I ggml_metal_init: hasUnifiedMemory      = true
0.00.041.793 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.053.502 I init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.054.183 I init:      Metal KV buffer size =     9.00 MiB
0.00.054.186 I llama_context_kv_self: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.054.187 I llama_context_kv_self:        CPU  output buffer size =     0.00 MiB
0.00.055.310 I llama_context_kv_self:      Metal compute buffer size =    16.00 MiB
0.00.055.311 I llama_context_kv_self:        CPU compute buffer size =     2.51 MiB
0.00.055.312 I llama_context_kv_self: graph nodes  = 429
0.00.055.312 I llama_context_kv_self: graph splits = 2
0.00.055.313 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.055.313 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.060.764 I 
0.00.060.789 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.061.451 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.066.472 I llama_perf_context_print:        load time =      44.41 ms
0.00.066.473 I llama_perf_context_print: prompt eval time =       4.87 ms /     9 tokens (    0.54 ms per token,  1846.53 tokens per second)
0.00.066.473 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.066.474 I llama_perf_context_print:       total time =       5.71 ms /    10 tokens
0.00.066.665 I ggml_metal_free: deallocating

real	0m0.260s
user	0m0.048s
sys	0m0.031s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.044 I build: 4729 (e08f38df) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.736 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.012.553 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.556 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.558 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.558 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.562 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.563 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.563 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.565 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.565 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.566 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.566 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.566 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.568 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.569 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.012.569 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.012.570 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.570 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.012.570 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.015.058 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.724 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.725 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.726 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.726 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.726 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.727 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.015.727 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.015.727 I llama_model_loader: - type  f32:  124 tensors
0.00.015.728 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.728 I print_info: file format = GGUF V3 (latest)
0.00.015.729 I print_info: file type   = Q8_0
0.00.015.730 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.018.191 I load: special tokens cache size = 5
0.00.019.486 I load: token to piece cache size = 0.2032 MB
0.00.019.495 I print_info: arch             = bert
0.00.019.496 I print_info: vocab_only       = 0
0.00.019.496 I print_info: n_ctx_train      = 512
0.00.019.496 I print_info: n_embd           = 384
0.00.019.496 I print_info: n_layer          = 12
0.00.019.500 I print_info: n_head           = 12
0.00.019.500 I print_info: n_head_kv        = 12
0.00.019.501 I print_info: n_rot            = 32
0.00.019.501 I print_info: n_swa            = 0
0.00.019.501 I print_info: n_embd_head_k    = 32
0.00.019.501 I print_info: n_embd_head_v    = 32
0.00.019.503 I print_info: n_gqa            = 1
0.00.019.504 I print_info: n_embd_k_gqa     = 384
0.00.019.504 I print_info: n_embd_v_gqa     = 384
0.00.019.505 I print_info: f_norm_eps       = 1.0e-12
0.00.019.505 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.019.505 I print_info: f_clamp_kqv      = 0.0e+00
0.00.019.505 I print_info: f_max_alibi_bias = 0.0e+00
0.00.019.505 I print_info: f_logit_scale    = 0.0e+00
0.00.019.506 I print_info: n_ff             = 1536
0.00.019.506 I print_info: n_expert         = 0
0.00.019.506 I print_info: n_expert_used    = 0
0.00.019.506 I print_info: causal attn      = 0
0.00.019.507 I print_info: pooling type     = 2
0.00.019.507 I print_info: rope type        = 2
0.00.019.507 I print_info: rope scaling     = linear
0.00.019.507 I print_info: freq_base_train  = 10000.0
0.00.019.507 I print_info: freq_scale_train = 1
0.00.019.508 I print_info: n_ctx_orig_yarn  = 512
0.00.019.508 I print_info: rope_finetuned   = unknown
0.00.019.508 I print_info: ssm_d_conv       = 0
0.00.019.508 I print_info: ssm_d_inner      = 0
0.00.019.508 I print_info: ssm_d_state      = 0
0.00.019.508 I print_info: ssm_dt_rank      = 0
0.00.019.508 I print_info: ssm_dt_b_c_rms   = 0
0.00.019.509 I print_info: model type       = 33M
0.00.019.509 I print_info: model params     = 33.21 M
0.00.019.509 I print_info: general.name     = Bge Small
0.00.019.510 I print_info: vocab type       = WPM
0.00.019.510 I print_info: n_vocab          = 30522
0.00.019.510 I print_info: n_merges         = 0
0.00.019.510 I print_info: BOS token        = 101 '[CLS]'
0.00.019.510 I print_info: UNK token        = 100 '[UNK]'
0.00.019.510 I print_info: SEP token        = 102 '[SEP]'
0.00.019.511 I print_info: PAD token        = 0 '[PAD]'
0.00.019.511 I print_info: MASK token       = 103 '[MASK]'
0.00.019.511 I print_info: LF token         = 0 '[PAD]'
0.00.019.511 I print_info: max token length = 21
0.00.019.511 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.021.171 I load_tensors: offloading 12 repeating layers to GPU
0.00.021.172 I load_tensors: offloading output layer to GPU
0.00.021.173 I load_tensors: offloaded 13/13 layers to GPU
0.00.021.178 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.021.178 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.021.347 I llama_context_kv_self: n_seq_max     = 1
0.00.021.347 I llama_context_kv_self: n_ctx         = 512
0.00.021.348 I llama_context_kv_self: n_ctx_per_seq = 512
0.00.021.348 I llama_context_kv_self: n_batch       = 2048
0.00.021.348 I llama_context_kv_self: n_ubatch      = 2048
0.00.021.348 I llama_context_kv_self: flash_attn    = 0
0.00.021.349 I llama_context_kv_self: freq_base     = 10000.0
0.00.021.349 I llama_context_kv_self: freq_scale    = 1
0.00.021.349 I ggml_metal_init: allocating
0.00.021.352 I ggml_metal_init: found device: Apple M4
0.00.021.355 I ggml_metal_init: picking default device: Apple M4
0.00.021.867 I ggml_metal_init: using embedded metal library
0.00.024.395 I ggml_metal_init: GPU name:   Apple M4
0.00.024.396 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.024.397 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.024.397 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.024.398 I ggml_metal_init: simdgroup reduction   = true
0.00.024.398 I ggml_metal_init: simdgroup matrix mul. = true
0.00.024.398 I ggml_metal_init: has residency sets    = true
0.00.024.398 I ggml_metal_init: has bfloat            = true
0.00.024.398 I ggml_metal_init: use bfloat            = true
0.00.024.399 I ggml_metal_init: hasUnifiedMemory      = true
0.00.024.399 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.799 I init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.035.410 I init:      Metal KV buffer size =     9.00 MiB
0.00.035.412 I llama_context_kv_self: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.035.413 I llama_context_kv_self:        CPU  output buffer size =     0.00 MiB
0.00.036.367 I llama_context_kv_self:      Metal compute buffer size =    16.00 MiB
0.00.036.368 I llama_context_kv_self:        CPU compute buffer size =     2.51 MiB
0.00.036.368 I llama_context_kv_self: graph nodes  = 429
0.00.036.369 I llama_context_kv_self: graph splits = 2
0.00.036.370 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.036.370 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.040.264 I 
0.00.040.290 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.040.794 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.045.243 I llama_perf_context_print:        load time =      30.52 ms
0.00.045.245 I llama_perf_context_print: prompt eval time =       4.33 ms /     9 tokens (    0.48 ms per token,  2080.44 tokens per second)
0.00.045.246 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.045.246 I llama_perf_context_print:       total time =       4.98 ms /    10 tokens
0.00.045.445 I ggml_metal_free: deallocating

real	0m0.057s
user	0m0.031s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.396 I build: 4729 (e08f38df) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.289 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.759 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.764 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.766 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.035.767 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.768 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.035.769 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.035.770 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.035.771 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.035.772 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.035.772 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.035.773 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.035.773 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.035.777 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.035.777 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.035.778 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.035.778 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.779 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.043.174 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.045.401 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.110 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.050.112 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.113 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.050.113 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.050.113 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.050.114 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.050.114 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.050.114 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.050.115 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.050.115 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.050.115 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.050.116 I llama_model_loader: - type  f32:   40 tensors
0.00.050.116 I llama_model_loader: - type  f16:   30 tensors
0.00.050.122 I print_info: file format = GGUF V3 (latest)
0.00.050.125 I print_info: file type   = F16
0.00.050.126 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.054.472 W load: empty token at index 5
0.00.059.525 W load: model vocab missing newline token, using special_pad_id instead
0.00.060.945 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.060.979 I load: special tokens cache size = 5
0.00.325.324 I load: token to piece cache size = 1.5060 MB
0.00.325.354 I print_info: arch             = jina-bert-v2
0.00.325.355 I print_info: vocab_only       = 0
0.00.325.355 I print_info: n_ctx_train      = 8192
0.00.325.355 I print_info: n_embd           = 384
0.00.325.356 I print_info: n_layer          = 4
0.00.325.361 I print_info: n_head           = 12
0.00.325.364 I print_info: n_head_kv        = 12
0.00.325.364 I print_info: n_rot            = 32
0.00.325.365 I print_info: n_swa            = 0
0.00.325.365 I print_info: n_embd_head_k    = 32
0.00.325.365 I print_info: n_embd_head_v    = 32
0.00.325.366 I print_info: n_gqa            = 1
0.00.325.366 I print_info: n_embd_k_gqa     = 384
0.00.325.367 I print_info: n_embd_v_gqa     = 384
0.00.325.368 I print_info: f_norm_eps       = 1.0e-12
0.00.325.368 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.325.369 I print_info: f_clamp_kqv      = 0.0e+00
0.00.325.369 I print_info: f_max_alibi_bias = 8.0e+00
0.00.325.369 I print_info: f_logit_scale    = 0.0e+00
0.00.325.371 I print_info: n_ff             = 1536
0.00.325.371 I print_info: n_expert         = 0
0.00.325.371 I print_info: n_expert_used    = 0
0.00.325.372 I print_info: causal attn      = 0
0.00.325.372 I print_info: pooling type     = -1
0.00.325.372 I print_info: rope type        = -1
0.00.325.372 I print_info: rope scaling     = linear
0.00.325.372 I print_info: freq_base_train  = 10000.0
0.00.325.375 I print_info: freq_scale_train = 1
0.00.325.375 I print_info: n_ctx_orig_yarn  = 8192
0.00.325.375 I print_info: rope_finetuned   = unknown
0.00.325.375 I print_info: ssm_d_conv       = 0
0.00.325.375 I print_info: ssm_d_inner      = 0
0.00.325.375 I print_info: ssm_d_state      = 0
0.00.325.376 I print_info: ssm_dt_rank      = 0
0.00.325.376 I print_info: ssm_dt_b_c_rms   = 0
0.00.325.376 I print_info: model type       = 33M
0.00.325.376 I print_info: model params     = 32.90 M
0.00.325.377 I print_info: general.name     = Jina Bert Implementation
0.00.325.378 I print_info: vocab type       = BPE
0.00.325.378 I print_info: n_vocab          = 61056
0.00.325.378 I print_info: n_merges         = 39382
0.00.325.383 I print_info: BOS token        = 0 '<s>'
0.00.325.383 I print_info: EOS token        = 2 '</s>'
0.00.325.383 I print_info: UNK token        = 3 '<unk>'
0.00.325.383 I print_info: SEP token        = 2 '</s>'
0.00.325.384 I print_info: PAD token        = 1 '<pad>'
0.00.325.384 I print_info: MASK token       = 4 '<mask>'
0.00.325.384 I print_info: EOG token        = 2 '</s>'
0.00.325.384 I print_info: max token length = 45
0.00.325.385 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.327.471 I load_tensors: offloading 4 repeating layers to GPU
0.00.327.472 I load_tensors: offloading output layer to GPU
0.00.327.473 I load_tensors: offloaded 5/5 layers to GPU
0.00.327.498 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.327.499 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.327.792 I llama_context_kv_self: n_seq_max     = 1
0.00.327.793 I llama_context_kv_self: n_ctx         = 8192
0.00.327.793 I llama_context_kv_self: n_ctx_per_seq = 8192
0.00.327.793 I llama_context_kv_self: n_batch       = 2048
0.00.327.794 I llama_context_kv_self: n_ubatch      = 2048
0.00.327.794 I llama_context_kv_self: flash_attn    = 0
0.00.327.794 I llama_context_kv_self: freq_base     = 10000.0
0.00.327.794 I llama_context_kv_self: freq_scale    = 1
0.00.327.795 I ggml_metal_init: allocating
0.00.327.799 I ggml_metal_init: found device: Apple M4
0.00.327.802 I ggml_metal_init: picking default device: Apple M4
0.00.328.664 I ggml_metal_init: using embedded metal library
0.00.331.460 I ggml_metal_init: GPU name:   Apple M4
0.00.331.461 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.331.462 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.331.462 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.331.462 I ggml_metal_init: simdgroup reduction   = true
0.00.331.462 I ggml_metal_init: simdgroup matrix mul. = true
0.00.331.463 I ggml_metal_init: has residency sets    = true
0.00.331.463 I ggml_metal_init: has bfloat            = true
0.00.331.463 I ggml_metal_init: use bfloat            = true
0.00.331.463 I ggml_metal_init: hasUnifiedMemory      = true
0.00.331.464 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.341.054 I init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.344.036 I init:      Metal KV buffer size =    48.00 MiB
0.00.344.038 I llama_context_kv_self: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.344.039 I llama_context_kv_self:        CPU  output buffer size =     0.00 MiB
0.00.350.298 I llama_context_kv_self:      Metal compute buffer size =   220.01 MiB
0.00.350.299 I llama_context_kv_self:        CPU compute buffer size =    22.02 MiB
0.00.350.299 I llama_context_kv_self: graph nodes  = 154
0.00.350.300 I llama_context_kv_self: graph splits = 2
0.00.350.301 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.350.301 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.357.779 I 
0.00.357.817 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.357.918 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.357.919 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.357.922 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.357.923 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.357.926 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.357.926 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.358.437 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.362.346 I llama_perf_context_print:        load time =     335.48 ms
0.00.362.347 I llama_perf_context_print: prompt eval time =       3.90 ms /    62 tokens (    0.06 ms per token, 15889.29 tokens per second)
0.00.362.347 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.362.348 I llama_perf_context_print:       total time =       4.57 ms /    63 tokens
0.00.362.809 I ggml_metal_free: deallocating

real	0m1.083s
user	0m0.332s
sys	0m0.048s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.241 I build: 4729 (e08f38df) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.397 I main: llama backend init
0.00.000.403 I main: load the model and apply lora adapter, if any
0.00.054.818 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.067.977 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.067.997 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.068.001 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.068.002 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.068.002 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.068.003 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.068.004 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.068.006 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.068.006 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.068.007 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.068.008 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.068.008 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.068.009 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.068.025 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.068.029 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.068.030 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.068.030 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.075.159 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.077.524 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.086.861 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.086.874 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.086.874 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.086.875 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.086.875 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.086.877 I llama_model_loader: - type  f32:  194 tensors
0.00.086.879 I llama_model_loader: - type  f16:   98 tensors
0.00.086.883 I print_info: file format = GGUF V3 (latest)
0.00.086.885 I print_info: file type   = all F32 (guessed)
0.00.086.887 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.102.493 I load: special tokens cache size = 25
0.00.112.003 I load: token to piece cache size = 0.2984 MB
0.00.112.033 I print_info: arch             = gptneox
0.00.112.034 I print_info: vocab_only       = 0
0.00.112.034 I print_info: n_ctx_train      = 2048
0.00.112.034 I print_info: n_embd           = 2048
0.00.112.035 I print_info: n_layer          = 24
0.00.112.042 I print_info: n_head           = 16
0.00.112.043 I print_info: n_head_kv        = 16
0.00.112.043 I print_info: n_rot            = 32
0.00.112.043 I print_info: n_swa            = 0
0.00.112.043 I print_info: n_embd_head_k    = 128
0.00.112.044 I print_info: n_embd_head_v    = 128
0.00.112.044 I print_info: n_gqa            = 1
0.00.112.045 I print_info: n_embd_k_gqa     = 2048
0.00.112.046 I print_info: n_embd_v_gqa     = 2048
0.00.112.047 I print_info: f_norm_eps       = 1.0e-05
0.00.112.048 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.112.048 I print_info: f_clamp_kqv      = 0.0e+00
0.00.112.048 I print_info: f_max_alibi_bias = 0.0e+00
0.00.112.048 I print_info: f_logit_scale    = 0.0e+00
0.00.112.049 I print_info: n_ff             = 8192
0.00.112.049 I print_info: n_expert         = 0
0.00.112.052 I print_info: n_expert_used    = 0
0.00.112.052 I print_info: causal attn      = 1
0.00.112.052 I print_info: pooling type     = 0
0.00.112.052 I print_info: rope type        = 2
0.00.112.053 I print_info: rope scaling     = linear
0.00.112.053 I print_info: freq_base_train  = 10000.0
0.00.112.054 I print_info: freq_scale_train = 1
0.00.112.054 I print_info: n_ctx_orig_yarn  = 2048
0.00.112.054 I print_info: rope_finetuned   = unknown
0.00.112.054 I print_info: ssm_d_conv       = 0
0.00.112.054 I print_info: ssm_d_inner      = 0
0.00.112.054 I print_info: ssm_d_state      = 0
0.00.112.054 I print_info: ssm_dt_rank      = 0
0.00.112.055 I print_info: ssm_dt_b_c_rms   = 0
0.00.112.055 I print_info: model type       = 1.4B
0.00.112.055 I print_info: model params     = 1.41 B
0.00.112.055 I print_info: general.name     = 1.4B
0.00.112.056 I print_info: vocab type       = BPE
0.00.112.056 I print_info: n_vocab          = 50304
0.00.112.057 I print_info: n_merges         = 50009
0.00.112.057 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.112.057 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.112.057 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.112.057 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.112.058 I print_info: LF token         = 187 ''
0.00.112.058 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.112.058 I print_info: max token length = 1024
0.00.112.059 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.154.022 I load_tensors: offloading 24 repeating layers to GPU
0.00.154.027 I load_tensors: offloading output layer to GPU
0.00.154.027 I load_tensors: offloaded 25/25 layers to GPU
0.00.154.051 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.154.052 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.154.388 I llama_context_kv_self: n_seq_max     = 1
0.00.154.389 I llama_context_kv_self: n_ctx         = 2048
0.00.154.390 I llama_context_kv_self: n_ctx_per_seq = 2048
0.00.154.390 I llama_context_kv_self: n_batch       = 2048
0.00.154.390 I llama_context_kv_self: n_ubatch      = 512
0.00.154.390 I llama_context_kv_self: flash_attn    = 0
0.00.154.391 I llama_context_kv_self: freq_base     = 10000.0
0.00.154.391 I llama_context_kv_self: freq_scale    = 1
0.00.154.392 I ggml_metal_init: allocating
0.00.154.423 I ggml_metal_init: found device: Apple M4
0.00.154.429 I ggml_metal_init: picking default device: Apple M4
0.00.155.064 I ggml_metal_init: using embedded metal library
0.00.343.839 I ggml_metal_init: GPU name:   Apple M4
0.00.343.855 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.343.856 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.343.857 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.343.857 I ggml_metal_init: simdgroup reduction   = true
0.00.343.858 I ggml_metal_init: simdgroup matrix mul. = true
0.00.343.858 I ggml_metal_init: has residency sets    = true
0.00.343.858 I ggml_metal_init: has bfloat            = true
0.00.343.859 I ggml_metal_init: use bfloat            = true
0.00.343.861 I ggml_metal_init: hasUnifiedMemory      = true
0.00.343.867 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.381.141 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.420.000 I init:      Metal KV buffer size =   384.00 MiB
0.00.420.007 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.420.031 I llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
0.00.423.875 I llama_context_kv_self:      Metal compute buffer size =   102.25 MiB
0.00.423.877 I llama_context_kv_self:        CPU compute buffer size =     8.01 MiB
0.00.423.877 I llama_context_kv_self: graph nodes  = 967
0.00.423.877 I llama_context_kv_self: graph splits = 2
0.00.423.880 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.423.998 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.423.999 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.492.202 I main: llama threadpool init, n_threads = 4
0.00.492.241 I 
0.00.492.271 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.492.273 I 
0.00.492.317 I sampler seed: 1234
0.00.492.322 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.492.346 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.492.348 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.492.348 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.329.511 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58823.53 tokens per second)
0.02.329.512 I llama_perf_context_print:        load time =     436.50 ms
0.02.329.514 I llama_perf_context_print: prompt eval time =      43.76 ms /     7 tokens (    6.25 ms per token,   159.98 tokens per second)
0.02.329.515 I llama_perf_context_print:        eval time =    1790.55 ms /    63 runs   (   28.42 ms per token,    35.18 tokens per second)
0.02.329.515 I llama_perf_context_print:       total time =    1838.18 ms /    70 tokens
0.02.333.531 I ggml_metal_free: deallocating

real	0m2.675s
user	0m0.144s
sys	0m0.156s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.540 I build: 4729 (e08f38df) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.612 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.796 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.801 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.803 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.803 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.804 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.805 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.806 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.811 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.811 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.812 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.812 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.813 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.813 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.814 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.817 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.818 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.819 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.197 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.285 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.077 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.055.079 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.079 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.080 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.080 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.081 I llama_model_loader: - type  f32:  194 tensors
0.00.055.081 I llama_model_loader: - type  f16:   98 tensors
0.00.055.082 I print_info: file format = GGUF V3 (latest)
0.00.055.083 I print_info: file type   = all F32 (guessed)
0.00.055.084 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.067.490 I load: special tokens cache size = 25
0.00.075.630 I load: token to piece cache size = 0.2984 MB
0.00.075.645 I print_info: arch             = gptneox
0.00.075.646 I print_info: vocab_only       = 0
0.00.075.646 I print_info: n_ctx_train      = 2048
0.00.075.647 I print_info: n_embd           = 2048
0.00.075.647 I print_info: n_layer          = 24
0.00.075.650 I print_info: n_head           = 16
0.00.075.650 I print_info: n_head_kv        = 16
0.00.075.651 I print_info: n_rot            = 32
0.00.075.651 I print_info: n_swa            = 0
0.00.075.651 I print_info: n_embd_head_k    = 128
0.00.075.651 I print_info: n_embd_head_v    = 128
0.00.075.652 I print_info: n_gqa            = 1
0.00.075.652 I print_info: n_embd_k_gqa     = 2048
0.00.075.653 I print_info: n_embd_v_gqa     = 2048
0.00.075.654 I print_info: f_norm_eps       = 1.0e-05
0.00.075.654 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.075.654 I print_info: f_clamp_kqv      = 0.0e+00
0.00.075.654 I print_info: f_max_alibi_bias = 0.0e+00
0.00.075.655 I print_info: f_logit_scale    = 0.0e+00
0.00.075.655 I print_info: n_ff             = 8192
0.00.075.655 I print_info: n_expert         = 0
0.00.075.655 I print_info: n_expert_used    = 0
0.00.075.656 I print_info: causal attn      = 1
0.00.075.656 I print_info: pooling type     = 0
0.00.075.656 I print_info: rope type        = 2
0.00.075.656 I print_info: rope scaling     = linear
0.00.075.658 I print_info: freq_base_train  = 10000.0
0.00.075.658 I print_info: freq_scale_train = 1
0.00.075.658 I print_info: n_ctx_orig_yarn  = 2048
0.00.075.658 I print_info: rope_finetuned   = unknown
0.00.075.658 I print_info: ssm_d_conv       = 0
0.00.075.659 I print_info: ssm_d_inner      = 0
0.00.075.659 I print_info: ssm_d_state      = 0
0.00.075.659 I print_info: ssm_dt_rank      = 0
0.00.075.660 I print_info: ssm_dt_b_c_rms   = 0
0.00.075.661 I print_info: model type       = 1.4B
0.00.075.661 I print_info: model params     = 1.41 B
0.00.075.661 I print_info: general.name     = 1.4B
0.00.075.661 I print_info: vocab type       = BPE
0.00.075.662 I print_info: n_vocab          = 50304
0.00.075.662 I print_info: n_merges         = 50009
0.00.075.662 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.075.662 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.075.663 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.075.663 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.075.663 I print_info: LF token         = 187 ''
0.00.075.663 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.075.663 I print_info: max token length = 1024
0.00.075.664 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.047.145 I load_tensors: offloading 24 repeating layers to GPU
0.01.047.149 I load_tensors: offloading output layer to GPU
0.01.047.150 I load_tensors: offloaded 25/25 layers to GPU
0.01.047.173 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.047.175 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.048.169 I llama_context_kv_self: n_seq_max     = 1
0.01.048.170 I llama_context_kv_self: n_ctx         = 128
0.01.048.170 I llama_context_kv_self: n_ctx_per_seq = 128
0.01.048.171 I llama_context_kv_self: n_batch       = 128
0.01.048.171 I llama_context_kv_self: n_ubatch      = 128
0.01.048.171 I llama_context_kv_self: flash_attn    = 0
0.01.048.172 I llama_context_kv_self: freq_base     = 10000.0
0.01.048.172 I llama_context_kv_self: freq_scale    = 1
0.01.048.172 W llama_context_kv_self: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.048.173 I ggml_metal_init: allocating
0.01.048.233 I ggml_metal_init: found device: Apple M4
0.01.048.239 I ggml_metal_init: picking default device: Apple M4
0.01.049.347 I ggml_metal_init: using embedded metal library
0.01.053.146 I ggml_metal_init: GPU name:   Apple M4
0.01.053.149 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.053.150 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.053.151 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.053.151 I ggml_metal_init: simdgroup reduction   = true
0.01.053.151 I ggml_metal_init: simdgroup matrix mul. = true
0.01.053.151 I ggml_metal_init: has residency sets    = true
0.01.053.151 I ggml_metal_init: has bfloat            = true
0.01.053.151 I ggml_metal_init: use bfloat            = true
0.01.053.152 I ggml_metal_init: hasUnifiedMemory      = true
0.01.053.154 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.063.601 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.065.389 I init:      Metal KV buffer size =    24.00 MiB
0.01.065.400 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.065.428 I llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
0.01.066.957 I llama_context_kv_self:      Metal compute buffer size =    25.56 MiB
0.01.066.958 I llama_context_kv_self:        CPU compute buffer size =     1.06 MiB
0.01.066.959 I llama_context_kv_self: graph nodes  = 967
0.01.066.959 I llama_context_kv_self: graph splits = 2
0.01.066.960 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.066.962 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.102.276 I 
0.01.102.318 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.102.338 I perplexity: tokenizing the input ..
0.01.107.486 I perplexity: tokenization took 5.146 ms
0.01.107.506 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.226.439 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.227.830 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.227.843 I llama_perf_context_print:        load time =    1078.65 ms
0.01.227.844 I llama_perf_context_print: prompt eval time =     118.66 ms /   128 tokens (    0.93 ms per token,  1078.68 tokens per second)
0.01.227.845 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.227.845 I llama_perf_context_print:       total time =     125.57 ms /   129 tokens
0.01.228.412 I ggml_metal_free: deallocating

real	0m1.427s
user	0m0.097s
sys	0m0.213s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.058 I build: 4729 (e08f38df) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.094 I main: llama backend init
0.00.000.096 I main: load the model and apply lora adapter, if any
0.00.009.945 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.031.136 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.031.143 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.145 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.150 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.150 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.151 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.151 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.152 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.152 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.153 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.153 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.154 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.154 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.154 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.156 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.156 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.157 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.035.210 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.302 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.344 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.040.346 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.346 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.346 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.347 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.347 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.040.348 I llama_model_loader: - type  f32:  194 tensors
0.00.040.348 I llama_model_loader: - type q8_0:   98 tensors
0.00.040.349 I print_info: file format = GGUF V3 (latest)
0.00.040.349 I print_info: file type   = Q8_0
0.00.040.350 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.049.491 I load: special tokens cache size = 25
0.00.056.487 I load: token to piece cache size = 0.2984 MB
0.00.056.503 I print_info: arch             = gptneox
0.00.056.504 I print_info: vocab_only       = 0
0.00.056.504 I print_info: n_ctx_train      = 2048
0.00.056.505 I print_info: n_embd           = 2048
0.00.056.505 I print_info: n_layer          = 24
0.00.056.509 I print_info: n_head           = 16
0.00.056.510 I print_info: n_head_kv        = 16
0.00.056.510 I print_info: n_rot            = 32
0.00.056.510 I print_info: n_swa            = 0
0.00.056.511 I print_info: n_embd_head_k    = 128
0.00.056.511 I print_info: n_embd_head_v    = 128
0.00.056.512 I print_info: n_gqa            = 1
0.00.056.512 I print_info: n_embd_k_gqa     = 2048
0.00.056.513 I print_info: n_embd_v_gqa     = 2048
0.00.056.514 I print_info: f_norm_eps       = 1.0e-05
0.00.056.514 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.056.514 I print_info: f_clamp_kqv      = 0.0e+00
0.00.056.515 I print_info: f_max_alibi_bias = 0.0e+00
0.00.056.516 I print_info: f_logit_scale    = 0.0e+00
0.00.056.519 I print_info: n_ff             = 8192
0.00.056.519 I print_info: n_expert         = 0
0.00.056.519 I print_info: n_expert_used    = 0
0.00.056.519 I print_info: causal attn      = 1
0.00.056.519 I print_info: pooling type     = 0
0.00.056.520 I print_info: rope type        = 2
0.00.056.520 I print_info: rope scaling     = linear
0.00.056.520 I print_info: freq_base_train  = 10000.0
0.00.056.520 I print_info: freq_scale_train = 1
0.00.056.521 I print_info: n_ctx_orig_yarn  = 2048
0.00.056.521 I print_info: rope_finetuned   = unknown
0.00.056.521 I print_info: ssm_d_conv       = 0
0.00.056.522 I print_info: ssm_d_inner      = 0
0.00.056.522 I print_info: ssm_d_state      = 0
0.00.056.522 I print_info: ssm_dt_rank      = 0
0.00.056.524 I print_info: ssm_dt_b_c_rms   = 0
0.00.056.524 I print_info: model type       = 1.4B
0.00.056.524 I print_info: model params     = 1.41 B
0.00.056.524 I print_info: general.name     = 1.4B
0.00.056.526 I print_info: vocab type       = BPE
0.00.056.526 I print_info: n_vocab          = 50304
0.00.056.526 I print_info: n_merges         = 50009
0.00.056.527 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.056.527 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.056.527 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.056.527 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.056.528 I print_info: LF token         = 187 ''
0.00.056.528 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.056.528 I print_info: max token length = 1024
0.00.056.530 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.972.729 I load_tensors: offloading 24 repeating layers to GPU
0.00.972.734 I load_tensors: offloading output layer to GPU
0.00.972.735 I load_tensors: offloaded 25/25 layers to GPU
0.00.972.759 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.972.760 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.973.987 I llama_context_kv_self: n_seq_max     = 1
0.00.973.988 I llama_context_kv_self: n_ctx         = 2048
0.00.973.989 I llama_context_kv_self: n_ctx_per_seq = 2048
0.00.973.989 I llama_context_kv_self: n_batch       = 2048
0.00.973.989 I llama_context_kv_self: n_ubatch      = 512
0.00.973.990 I llama_context_kv_self: flash_attn    = 0
0.00.973.990 I llama_context_kv_self: freq_base     = 10000.0
0.00.973.991 I llama_context_kv_self: freq_scale    = 1
0.00.973.992 I ggml_metal_init: allocating
0.00.974.008 I ggml_metal_init: found device: Apple M4
0.00.974.020 I ggml_metal_init: picking default device: Apple M4
0.00.975.383 I ggml_metal_init: using embedded metal library
0.00.980.663 I ggml_metal_init: GPU name:   Apple M4
0.00.980.667 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.980.667 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.980.668 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.980.669 I ggml_metal_init: simdgroup reduction   = true
0.00.980.669 I ggml_metal_init: simdgroup matrix mul. = true
0.00.980.669 I ggml_metal_init: has residency sets    = true
0.00.980.669 I ggml_metal_init: has bfloat            = true
0.00.980.670 I ggml_metal_init: use bfloat            = true
0.00.980.670 I ggml_metal_init: hasUnifiedMemory      = true
0.00.980.672 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.996.308 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.049.498 I init:      Metal KV buffer size =   384.00 MiB
0.01.049.505 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.049.529 I llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
0.01.053.567 I llama_context_kv_self:      Metal compute buffer size =   102.25 MiB
0.01.053.569 I llama_context_kv_self:        CPU compute buffer size =     8.01 MiB
0.01.053.570 I llama_context_kv_self: graph nodes  = 967
0.01.053.570 I llama_context_kv_self: graph splits = 2
0.01.053.576 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.053.704 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.053.705 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.107.841 I main: llama threadpool init, n_threads = 4
0.01.107.888 I 
0.01.107.914 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.107.915 I 
0.01.108.079 I sampler seed: 1234
0.01.108.083 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.108.129 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.108.133 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.108.133 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.213.382 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50786.84 tokens per second)
0.02.213.383 I llama_perf_context_print:        load time =    1097.16 ms
0.02.213.384 I llama_perf_context_print: prompt eval time =      49.34 ms /     7 tokens (    7.05 ms per token,   141.88 tokens per second)
0.02.213.386 I llama_perf_context_print:        eval time =    1053.16 ms /    63 runs   (   16.72 ms per token,    59.82 tokens per second)
0.02.213.386 I llama_perf_context_print:       total time =    1106.27 ms /    70 tokens
0.02.216.140 I ggml_metal_free: deallocating

real	0m2.232s
user	0m0.109s
sys	0m0.246s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4729 (e08f38df) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.385 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.657 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.664 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.672 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.672 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.673 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.673 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.673 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.674 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.674 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.675 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.676 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.677 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.677 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.677 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.679 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.679 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.680 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.574 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.643 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.444 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.445 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.446 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.446 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.446 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.447 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.447 I llama_model_loader: - type  f32:  194 tensors
0.00.025.448 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.448 I print_info: file format = GGUF V3 (latest)
0.00.025.449 I print_info: file type   = Q8_0
0.00.025.450 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.898 I load: special tokens cache size = 25
0.00.040.045 I load: token to piece cache size = 0.2984 MB
0.00.040.062 I print_info: arch             = gptneox
0.00.040.063 I print_info: vocab_only       = 0
0.00.040.063 I print_info: n_ctx_train      = 2048
0.00.040.064 I print_info: n_embd           = 2048
0.00.040.064 I print_info: n_layer          = 24
0.00.040.068 I print_info: n_head           = 16
0.00.040.068 I print_info: n_head_kv        = 16
0.00.040.069 I print_info: n_rot            = 32
0.00.040.072 I print_info: n_swa            = 0
0.00.040.073 I print_info: n_embd_head_k    = 128
0.00.040.073 I print_info: n_embd_head_v    = 128
0.00.040.073 I print_info: n_gqa            = 1
0.00.040.074 I print_info: n_embd_k_gqa     = 2048
0.00.040.074 I print_info: n_embd_v_gqa     = 2048
0.00.040.075 I print_info: f_norm_eps       = 1.0e-05
0.00.040.075 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.075 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.075 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.076 I print_info: f_logit_scale    = 0.0e+00
0.00.040.076 I print_info: n_ff             = 8192
0.00.040.076 I print_info: n_expert         = 0
0.00.040.077 I print_info: n_expert_used    = 0
0.00.040.077 I print_info: causal attn      = 1
0.00.040.077 I print_info: pooling type     = 0
0.00.040.077 I print_info: rope type        = 2
0.00.040.077 I print_info: rope scaling     = linear
0.00.040.078 I print_info: freq_base_train  = 10000.0
0.00.040.078 I print_info: freq_scale_train = 1
0.00.040.078 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.078 I print_info: rope_finetuned   = unknown
0.00.040.078 I print_info: ssm_d_conv       = 0
0.00.040.081 I print_info: ssm_d_inner      = 0
0.00.040.081 I print_info: ssm_d_state      = 0
0.00.040.081 I print_info: ssm_dt_rank      = 0
0.00.040.081 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.081 I print_info: model type       = 1.4B
0.00.040.082 I print_info: model params     = 1.41 B
0.00.040.082 I print_info: general.name     = 1.4B
0.00.040.082 I print_info: vocab type       = BPE
0.00.040.082 I print_info: n_vocab          = 50304
0.00.040.082 I print_info: n_merges         = 50009
0.00.040.083 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.083 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.083 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.083 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.083 I print_info: LF token         = 187 ''
0.00.040.084 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.084 I print_info: max token length = 1024
0.00.040.084 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.744.157 I load_tensors: offloading 24 repeating layers to GPU
0.00.744.163 I load_tensors: offloading output layer to GPU
0.00.744.164 I load_tensors: offloaded 25/25 layers to GPU
0.00.744.191 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.744.194 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.745.582 I llama_context_kv_self: n_seq_max     = 1
0.00.745.583 I llama_context_kv_self: n_ctx         = 128
0.00.745.584 I llama_context_kv_self: n_ctx_per_seq = 128
0.00.745.584 I llama_context_kv_self: n_batch       = 128
0.00.745.584 I llama_context_kv_self: n_ubatch      = 128
0.00.745.584 I llama_context_kv_self: flash_attn    = 0
0.00.745.585 I llama_context_kv_self: freq_base     = 10000.0
0.00.745.585 I llama_context_kv_self: freq_scale    = 1
0.00.745.586 W llama_context_kv_self: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.745.587 I ggml_metal_init: allocating
0.00.745.625 I ggml_metal_init: found device: Apple M4
0.00.745.634 I ggml_metal_init: picking default device: Apple M4
0.00.746.830 I ggml_metal_init: using embedded metal library
0.00.752.114 I ggml_metal_init: GPU name:   Apple M4
0.00.752.118 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.752.118 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.752.119 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.752.120 I ggml_metal_init: simdgroup reduction   = true
0.00.752.122 I ggml_metal_init: simdgroup matrix mul. = true
0.00.752.125 I ggml_metal_init: has residency sets    = true
0.00.752.126 I ggml_metal_init: has bfloat            = true
0.00.752.126 I ggml_metal_init: use bfloat            = true
0.00.752.127 I ggml_metal_init: hasUnifiedMemory      = true
0.00.752.129 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.767.256 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.770.629 I init:      Metal KV buffer size =    24.00 MiB
0.00.770.638 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.770.686 I llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
0.00.773.795 I llama_context_kv_self:      Metal compute buffer size =    25.56 MiB
0.00.773.797 I llama_context_kv_self:        CPU compute buffer size =     1.06 MiB
0.00.773.797 I llama_context_kv_self: graph nodes  = 967
0.00.773.797 I llama_context_kv_self: graph splits = 2
0.00.773.801 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.773.801 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.800.896 I 
0.00.800.982 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.801.002 I perplexity: tokenizing the input ..
0.00.808.186 I perplexity: tokenization took 7.181 ms
0.00.808.205 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.933.566 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.934.911 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.934.936 I llama_perf_context_print:        load time =     791.50 ms
0.00.934.937 I llama_perf_context_print: prompt eval time =     124.47 ms /   128 tokens (    0.97 ms per token,  1028.34 tokens per second)
0.00.934.938 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.934.938 I llama_perf_context_print:       total time =     134.04 ms /   129 tokens
0.00.935.538 I ggml_metal_free: deallocating

real	0m0.950s
user	0m0.077s
sys	0m0.162s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.058 I build: 4729 (e08f38df) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.094 I main: llama backend init
0.00.000.096 I main: load the model and apply lora adapter, if any
0.00.010.240 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.205 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.212 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.214 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.214 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.215 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.220 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.220 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.221 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.222 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.222 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.222 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.223 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.223 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.223 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.226 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.227 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.227 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.253 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.369 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.300 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.302 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.302 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.302 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.303 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.303 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.304 I llama_model_loader: - type  f32:  194 tensors
0.00.027.304 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.304 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.305 I print_info: file format = GGUF V3 (latest)
0.00.027.306 I print_info: file type   = Q4_0
0.00.027.307 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.035.834 I load: special tokens cache size = 25
0.00.042.043 I load: token to piece cache size = 0.2984 MB
0.00.042.062 I print_info: arch             = gptneox
0.00.042.063 I print_info: vocab_only       = 0
0.00.042.063 I print_info: n_ctx_train      = 2048
0.00.042.063 I print_info: n_embd           = 2048
0.00.042.063 I print_info: n_layer          = 24
0.00.042.068 I print_info: n_head           = 16
0.00.042.069 I print_info: n_head_kv        = 16
0.00.042.069 I print_info: n_rot            = 32
0.00.042.069 I print_info: n_swa            = 0
0.00.042.069 I print_info: n_embd_head_k    = 128
0.00.042.069 I print_info: n_embd_head_v    = 128
0.00.042.070 I print_info: n_gqa            = 1
0.00.042.070 I print_info: n_embd_k_gqa     = 2048
0.00.042.071 I print_info: n_embd_v_gqa     = 2048
0.00.042.071 I print_info: f_norm_eps       = 1.0e-05
0.00.042.072 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.072 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.072 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.072 I print_info: f_logit_scale    = 0.0e+00
0.00.042.073 I print_info: n_ff             = 8192
0.00.042.073 I print_info: n_expert         = 0
0.00.042.073 I print_info: n_expert_used    = 0
0.00.042.073 I print_info: causal attn      = 1
0.00.042.074 I print_info: pooling type     = 0
0.00.042.074 I print_info: rope type        = 2
0.00.042.076 I print_info: rope scaling     = linear
0.00.042.078 I print_info: freq_base_train  = 10000.0
0.00.042.079 I print_info: freq_scale_train = 1
0.00.042.079 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.079 I print_info: rope_finetuned   = unknown
0.00.042.079 I print_info: ssm_d_conv       = 0
0.00.042.079 I print_info: ssm_d_inner      = 0
0.00.042.079 I print_info: ssm_d_state      = 0
0.00.042.079 I print_info: ssm_dt_rank      = 0
0.00.042.080 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.080 I print_info: model type       = 1.4B
0.00.042.080 I print_info: model params     = 1.41 B
0.00.042.080 I print_info: general.name     = 1.4B
0.00.042.081 I print_info: vocab type       = BPE
0.00.042.081 I print_info: n_vocab          = 50304
0.00.042.081 I print_info: n_merges         = 50009
0.00.042.081 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.081 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.082 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.082 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.082 I print_info: LF token         = 187 ''
0.00.042.082 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.082 I print_info: max token length = 1024
0.00.042.083 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.533.492 I load_tensors: offloading 24 repeating layers to GPU
0.00.533.508 I load_tensors: offloading output layer to GPU
0.00.533.509 I load_tensors: offloaded 25/25 layers to GPU
0.00.533.542 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.533.543 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.534.703 I llama_context_kv_self: n_seq_max     = 1
0.00.534.706 I llama_context_kv_self: n_ctx         = 2048
0.00.534.706 I llama_context_kv_self: n_ctx_per_seq = 2048
0.00.534.707 I llama_context_kv_self: n_batch       = 2048
0.00.534.707 I llama_context_kv_self: n_ubatch      = 512
0.00.534.708 I llama_context_kv_self: flash_attn    = 0
0.00.534.710 I llama_context_kv_self: freq_base     = 10000.0
0.00.534.711 I llama_context_kv_self: freq_scale    = 1
0.00.534.714 I ggml_metal_init: allocating
0.00.534.822 I ggml_metal_init: found device: Apple M4
0.00.534.836 I ggml_metal_init: picking default device: Apple M4
0.00.536.786 I ggml_metal_init: using embedded metal library
0.00.542.178 I ggml_metal_init: GPU name:   Apple M4
0.00.542.186 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.542.187 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.542.188 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.542.188 I ggml_metal_init: simdgroup reduction   = true
0.00.542.189 I ggml_metal_init: simdgroup matrix mul. = true
0.00.542.189 I ggml_metal_init: has residency sets    = true
0.00.542.189 I ggml_metal_init: has bfloat            = true
0.00.542.190 I ggml_metal_init: use bfloat            = true
0.00.542.191 I ggml_metal_init: hasUnifiedMemory      = true
0.00.542.199 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.562.445 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.620.193 I init:      Metal KV buffer size =   384.00 MiB
0.00.620.202 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.620.225 I llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
0.00.624.391 I llama_context_kv_self:      Metal compute buffer size =   102.25 MiB
0.00.624.393 I llama_context_kv_self:        CPU compute buffer size =     8.01 MiB
0.00.624.394 I llama_context_kv_self: graph nodes  = 967
0.00.624.394 I llama_context_kv_self: graph splits = 2
0.00.624.400 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.624.531 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.624.532 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.676.821 I main: llama threadpool init, n_threads = 4
0.00.676.864 I 
0.00.676.888 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.676.889 I 
0.00.677.066 I sampler seed: 1234
0.00.677.071 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.677.081 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.677.081 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.677.081 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.381.515 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52514.79 tokens per second)
0.01.381.516 I llama_perf_context_print:        load time =     665.80 ms
0.01.381.517 I llama_perf_context_print: prompt eval time =      39.45 ms /     7 tokens (    5.64 ms per token,   177.44 tokens per second)
0.01.381.518 I llama_perf_context_print:        eval time =     662.13 ms /    63 runs   (   10.51 ms per token,    95.15 tokens per second)
0.01.381.518 I llama_perf_context_print:       total time =     705.48 ms /    70 tokens
0.01.385.093 I ggml_metal_free: deallocating

real	0m1.401s
user	0m0.111s
sys	0m0.182s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4729 (e08f38df) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.076 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.086 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.092 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.099 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.100 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.100 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.100 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.101 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.102 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.102 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.102 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.102 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.103 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.103 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.103 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.105 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.105 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.105 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.009 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.041 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.945 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.947 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.947 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.947 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.948 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.948 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.949 I llama_model_loader: - type  f32:  194 tensors
0.00.025.949 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.949 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.950 I print_info: file format = GGUF V3 (latest)
0.00.025.950 I print_info: file type   = Q4_0
0.00.025.951 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.440 I load: special tokens cache size = 25
0.00.040.644 I load: token to piece cache size = 0.2984 MB
0.00.040.662 I print_info: arch             = gptneox
0.00.040.662 I print_info: vocab_only       = 0
0.00.040.663 I print_info: n_ctx_train      = 2048
0.00.040.663 I print_info: n_embd           = 2048
0.00.040.663 I print_info: n_layer          = 24
0.00.040.667 I print_info: n_head           = 16
0.00.040.668 I print_info: n_head_kv        = 16
0.00.040.668 I print_info: n_rot            = 32
0.00.040.668 I print_info: n_swa            = 0
0.00.040.668 I print_info: n_embd_head_k    = 128
0.00.040.668 I print_info: n_embd_head_v    = 128
0.00.040.669 I print_info: n_gqa            = 1
0.00.040.669 I print_info: n_embd_k_gqa     = 2048
0.00.040.670 I print_info: n_embd_v_gqa     = 2048
0.00.040.671 I print_info: f_norm_eps       = 1.0e-05
0.00.040.671 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.671 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.671 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.671 I print_info: f_logit_scale    = 0.0e+00
0.00.040.672 I print_info: n_ff             = 8192
0.00.040.672 I print_info: n_expert         = 0
0.00.040.672 I print_info: n_expert_used    = 0
0.00.040.673 I print_info: causal attn      = 1
0.00.040.673 I print_info: pooling type     = 0
0.00.040.674 I print_info: rope type        = 2
0.00.040.675 I print_info: rope scaling     = linear
0.00.040.677 I print_info: freq_base_train  = 10000.0
0.00.040.677 I print_info: freq_scale_train = 1
0.00.040.677 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.677 I print_info: rope_finetuned   = unknown
0.00.040.678 I print_info: ssm_d_conv       = 0
0.00.040.678 I print_info: ssm_d_inner      = 0
0.00.040.678 I print_info: ssm_d_state      = 0
0.00.040.678 I print_info: ssm_dt_rank      = 0
0.00.040.678 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.678 I print_info: model type       = 1.4B
0.00.040.679 I print_info: model params     = 1.41 B
0.00.040.679 I print_info: general.name     = 1.4B
0.00.040.680 I print_info: vocab type       = BPE
0.00.040.680 I print_info: n_vocab          = 50304
0.00.040.680 I print_info: n_merges         = 50009
0.00.040.680 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.680 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.680 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.681 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.681 I print_info: LF token         = 187 ''
0.00.040.681 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.681 I print_info: max token length = 1024
0.00.040.682 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.550.699 I load_tensors: offloading 24 repeating layers to GPU
0.00.550.710 I load_tensors: offloading output layer to GPU
0.00.550.710 I load_tensors: offloaded 25/25 layers to GPU
0.00.550.746 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.550.747 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.552.392 I llama_context_kv_self: n_seq_max     = 1
0.00.552.395 I llama_context_kv_self: n_ctx         = 128
0.00.552.396 I llama_context_kv_self: n_ctx_per_seq = 128
0.00.552.396 I llama_context_kv_self: n_batch       = 128
0.00.552.397 I llama_context_kv_self: n_ubatch      = 128
0.00.552.397 I llama_context_kv_self: flash_attn    = 0
0.00.552.399 I llama_context_kv_self: freq_base     = 10000.0
0.00.552.399 I llama_context_kv_self: freq_scale    = 1
0.00.552.400 W llama_context_kv_self: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.552.402 I ggml_metal_init: allocating
0.00.552.483 I ggml_metal_init: found device: Apple M4
0.00.552.497 I ggml_metal_init: picking default device: Apple M4
0.00.554.313 I ggml_metal_init: using embedded metal library
0.00.560.991 I ggml_metal_init: GPU name:   Apple M4
0.00.560.999 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.560.999 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.561.000 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.561.001 I ggml_metal_init: simdgroup reduction   = true
0.00.561.001 I ggml_metal_init: simdgroup matrix mul. = true
0.00.561.001 I ggml_metal_init: has residency sets    = true
0.00.561.002 I ggml_metal_init: has bfloat            = true
0.00.561.002 I ggml_metal_init: use bfloat            = true
0.00.561.003 I ggml_metal_init: hasUnifiedMemory      = true
0.00.561.013 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.578.882 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.582.500 I init:      Metal KV buffer size =    24.00 MiB
0.00.582.507 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.582.540 I llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
0.00.585.741 I llama_context_kv_self:      Metal compute buffer size =    25.56 MiB
0.00.585.743 I llama_context_kv_self:        CPU compute buffer size =     1.06 MiB
0.00.585.743 I llama_context_kv_self: graph nodes  = 967
0.00.585.744 I llama_context_kv_self: graph splits = 2
0.00.585.747 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.585.747 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.611.613 I 
0.00.611.688 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.611.710 I perplexity: tokenizing the input ..
0.00.619.606 I perplexity: tokenization took 7.892 ms
0.00.619.627 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.753.096 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.754.434 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.754.449 I llama_perf_context_print:        load time =     601.53 ms
0.00.754.450 I llama_perf_context_print: prompt eval time =     132.55 ms /   128 tokens (    1.04 ms per token,   965.70 tokens per second)
0.00.754.451 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.754.451 I llama_perf_context_print:       total time =     142.84 ms /   129 tokens
0.00.755.036 I ggml_metal_free: deallocating

real	0m0.771s
user	0m0.082s
sys	0m0.114s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4729 (e08f38df) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.010.274 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.288 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.018.292 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.294 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.294 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.294 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.295 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.295 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.296 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.296 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.296 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.297 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.297 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.298 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.298 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.300 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.301 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.301 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.085 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.095 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.853 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.854 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.854 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.854 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.855 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.855 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.855 I llama_model_loader: - type  f32:  194 tensors
0.00.026.856 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.856 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.857 I print_info: file format = GGUF V3 (latest)
0.00.026.857 I print_info: file type   = Q4_1
0.00.026.858 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.034.785 I load: special tokens cache size = 25
0.00.040.917 I load: token to piece cache size = 0.2984 MB
0.00.040.931 I print_info: arch             = gptneox
0.00.040.932 I print_info: vocab_only       = 0
0.00.040.932 I print_info: n_ctx_train      = 2048
0.00.040.933 I print_info: n_embd           = 2048
0.00.040.933 I print_info: n_layer          = 24
0.00.040.935 I print_info: n_head           = 16
0.00.040.936 I print_info: n_head_kv        = 16
0.00.040.936 I print_info: n_rot            = 32
0.00.040.936 I print_info: n_swa            = 0
0.00.040.937 I print_info: n_embd_head_k    = 128
0.00.040.937 I print_info: n_embd_head_v    = 128
0.00.040.937 I print_info: n_gqa            = 1
0.00.040.938 I print_info: n_embd_k_gqa     = 2048
0.00.040.939 I print_info: n_embd_v_gqa     = 2048
0.00.040.939 I print_info: f_norm_eps       = 1.0e-05
0.00.040.940 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.941 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.941 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.941 I print_info: f_logit_scale    = 0.0e+00
0.00.040.943 I print_info: n_ff             = 8192
0.00.040.944 I print_info: n_expert         = 0
0.00.040.944 I print_info: n_expert_used    = 0
0.00.040.944 I print_info: causal attn      = 1
0.00.040.944 I print_info: pooling type     = 0
0.00.040.945 I print_info: rope type        = 2
0.00.040.945 I print_info: rope scaling     = linear
0.00.040.945 I print_info: freq_base_train  = 10000.0
0.00.040.946 I print_info: freq_scale_train = 1
0.00.040.946 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.946 I print_info: rope_finetuned   = unknown
0.00.040.946 I print_info: ssm_d_conv       = 0
0.00.040.946 I print_info: ssm_d_inner      = 0
0.00.040.946 I print_info: ssm_d_state      = 0
0.00.040.947 I print_info: ssm_dt_rank      = 0
0.00.040.947 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.947 I print_info: model type       = 1.4B
0.00.040.947 I print_info: model params     = 1.41 B
0.00.040.948 I print_info: general.name     = 1.4B
0.00.040.948 I print_info: vocab type       = BPE
0.00.040.948 I print_info: n_vocab          = 50304
0.00.040.948 I print_info: n_merges         = 50009
0.00.040.949 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.949 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.949 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.949 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.949 I print_info: LF token         = 187 ''
0.00.040.950 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.950 I print_info: max token length = 1024
0.00.040.951 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.649.433 I load_tensors: offloading 24 repeating layers to GPU
0.00.649.454 I load_tensors: offloading output layer to GPU
0.00.649.454 I load_tensors: offloaded 25/25 layers to GPU
0.00.649.490 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.649.491 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.650.883 I llama_context_kv_self: n_seq_max     = 1
0.00.650.887 I llama_context_kv_self: n_ctx         = 2048
0.00.650.888 I llama_context_kv_self: n_ctx_per_seq = 2048
0.00.650.888 I llama_context_kv_self: n_batch       = 2048
0.00.650.889 I llama_context_kv_self: n_ubatch      = 512
0.00.650.889 I llama_context_kv_self: flash_attn    = 0
0.00.650.892 I llama_context_kv_self: freq_base     = 10000.0
0.00.650.892 I llama_context_kv_self: freq_scale    = 1
0.00.650.895 I ggml_metal_init: allocating
0.00.650.967 I ggml_metal_init: found device: Apple M4
0.00.650.980 I ggml_metal_init: picking default device: Apple M4
0.00.652.988 I ggml_metal_init: using embedded metal library
0.00.658.849 I ggml_metal_init: GPU name:   Apple M4
0.00.658.855 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.658.856 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.658.857 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.658.858 I ggml_metal_init: simdgroup reduction   = true
0.00.658.858 I ggml_metal_init: simdgroup matrix mul. = true
0.00.658.858 I ggml_metal_init: has residency sets    = true
0.00.658.859 I ggml_metal_init: has bfloat            = true
0.00.658.859 I ggml_metal_init: use bfloat            = true
0.00.658.860 I ggml_metal_init: hasUnifiedMemory      = true
0.00.658.862 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.678.549 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.739.291 I init:      Metal KV buffer size =   384.00 MiB
0.00.739.300 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.739.343 I llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
0.00.744.111 I llama_context_kv_self:      Metal compute buffer size =   102.25 MiB
0.00.744.113 I llama_context_kv_self:        CPU compute buffer size =     8.01 MiB
0.00.744.113 I llama_context_kv_self: graph nodes  = 967
0.00.744.113 I llama_context_kv_self: graph splits = 2
0.00.744.120 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.744.251 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.744.251 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.793.683 I main: llama threadpool init, n_threads = 4
0.00.793.728 I 
0.00.793.752 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.793.753 I 
0.00.793.872 I sampler seed: 1234
0.00.793.877 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.793.895 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.793.895 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.793.896 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.562.034 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59714.05 tokens per second)
0.01.562.035 I llama_perf_context_print:        load time =     782.70 ms
0.01.562.036 I llama_perf_context_print: prompt eval time =      49.53 ms /     7 tokens (    7.08 ms per token,   141.33 tokens per second)
0.01.562.037 I llama_perf_context_print:        eval time =     715.93 ms /    63 runs   (   11.36 ms per token,    88.00 tokens per second)
0.01.562.037 I llama_perf_context_print:       total time =     769.06 ms /    70 tokens
0.01.565.273 I ggml_metal_free: deallocating

real	0m1.582s
user	0m0.111s
sys	0m0.211s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.108 I build: 4729 (e08f38df) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.821 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.894 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.900 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.907 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.907 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.908 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.908 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.908 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.909 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.910 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.910 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.910 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.910 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.911 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.911 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.913 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.913 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.914 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.785 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.777 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.773 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.776 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.776 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.776 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.777 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.777 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.778 I llama_model_loader: - type  f32:  194 tensors
0.00.024.778 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.778 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.781 I print_info: file format = GGUF V3 (latest)
0.00.024.781 I print_info: file type   = Q4_1
0.00.024.782 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.832 I load: special tokens cache size = 25
0.00.038.859 I load: token to piece cache size = 0.2984 MB
0.00.038.877 I print_info: arch             = gptneox
0.00.038.878 I print_info: vocab_only       = 0
0.00.038.878 I print_info: n_ctx_train      = 2048
0.00.038.879 I print_info: n_embd           = 2048
0.00.038.879 I print_info: n_layer          = 24
0.00.038.888 I print_info: n_head           = 16
0.00.038.888 I print_info: n_head_kv        = 16
0.00.038.888 I print_info: n_rot            = 32
0.00.038.889 I print_info: n_swa            = 0
0.00.038.889 I print_info: n_embd_head_k    = 128
0.00.038.889 I print_info: n_embd_head_v    = 128
0.00.038.890 I print_info: n_gqa            = 1
0.00.038.890 I print_info: n_embd_k_gqa     = 2048
0.00.038.893 I print_info: n_embd_v_gqa     = 2048
0.00.038.894 I print_info: f_norm_eps       = 1.0e-05
0.00.038.894 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.894 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.895 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.895 I print_info: f_logit_scale    = 0.0e+00
0.00.038.895 I print_info: n_ff             = 8192
0.00.038.897 I print_info: n_expert         = 0
0.00.038.897 I print_info: n_expert_used    = 0
0.00.038.897 I print_info: causal attn      = 1
0.00.038.897 I print_info: pooling type     = 0
0.00.038.897 I print_info: rope type        = 2
0.00.038.897 I print_info: rope scaling     = linear
0.00.038.898 I print_info: freq_base_train  = 10000.0
0.00.038.898 I print_info: freq_scale_train = 1
0.00.038.898 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.899 I print_info: rope_finetuned   = unknown
0.00.038.899 I print_info: ssm_d_conv       = 0
0.00.038.899 I print_info: ssm_d_inner      = 0
0.00.038.899 I print_info: ssm_d_state      = 0
0.00.038.899 I print_info: ssm_dt_rank      = 0
0.00.038.900 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.900 I print_info: model type       = 1.4B
0.00.038.900 I print_info: model params     = 1.41 B
0.00.038.900 I print_info: general.name     = 1.4B
0.00.038.901 I print_info: vocab type       = BPE
0.00.038.901 I print_info: n_vocab          = 50304
0.00.038.901 I print_info: n_merges         = 50009
0.00.038.902 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.904 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.904 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.904 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.904 I print_info: LF token         = 187 ''
0.00.038.904 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.905 I print_info: max token length = 1024
0.00.038.905 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.648.915 I load_tensors: offloading 24 repeating layers to GPU
0.00.648.929 I load_tensors: offloading output layer to GPU
0.00.648.930 I load_tensors: offloaded 25/25 layers to GPU
0.00.648.961 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.648.962 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.650.202 I llama_context_kv_self: n_seq_max     = 1
0.00.650.204 I llama_context_kv_self: n_ctx         = 128
0.00.650.205 I llama_context_kv_self: n_ctx_per_seq = 128
0.00.650.205 I llama_context_kv_self: n_batch       = 128
0.00.650.206 I llama_context_kv_self: n_ubatch      = 128
0.00.650.206 I llama_context_kv_self: flash_attn    = 0
0.00.650.208 I llama_context_kv_self: freq_base     = 10000.0
0.00.650.208 I llama_context_kv_self: freq_scale    = 1
0.00.650.209 W llama_context_kv_self: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.650.212 I ggml_metal_init: allocating
0.00.650.304 I ggml_metal_init: found device: Apple M4
0.00.650.320 I ggml_metal_init: picking default device: Apple M4
0.00.652.150 I ggml_metal_init: using embedded metal library
0.00.657.606 I ggml_metal_init: GPU name:   Apple M4
0.00.657.617 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.657.617 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.657.618 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.657.618 I ggml_metal_init: simdgroup reduction   = true
0.00.657.619 I ggml_metal_init: simdgroup matrix mul. = true
0.00.657.619 I ggml_metal_init: has residency sets    = true
0.00.657.620 I ggml_metal_init: has bfloat            = true
0.00.657.620 I ggml_metal_init: use bfloat            = true
0.00.657.622 I ggml_metal_init: hasUnifiedMemory      = true
0.00.657.631 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.678.481 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.682.108 I init:      Metal KV buffer size =    24.00 MiB
0.00.682.115 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.682.151 I llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
0.00.685.437 I llama_context_kv_self:      Metal compute buffer size =    25.56 MiB
0.00.685.439 I llama_context_kv_self:        CPU compute buffer size =     1.06 MiB
0.00.685.440 I llama_context_kv_self: graph nodes  = 967
0.00.685.440 I llama_context_kv_self: graph splits = 2
0.00.685.443 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.685.444 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.711.236 I 
0.00.711.322 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.711.341 I perplexity: tokenizing the input ..
0.00.717.196 I perplexity: tokenization took 5.853 ms
0.00.717.208 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.848.380 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.849.717 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.849.732 I llama_perf_context_print:        load time =     702.40 ms
0.00.849.733 I llama_perf_context_print: prompt eval time =     130.94 ms /   128 tokens (    1.02 ms per token,   977.52 tokens per second)
0.00.849.733 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.849.734 I llama_perf_context_print:       total time =     138.50 ms /   129 tokens
0.00.850.248 I ggml_metal_free: deallocating

real	0m0.864s
user	0m0.079s
sys	0m0.138s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.061 I build: 4729 (e08f38df) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.096 I main: llama backend init
0.00.000.098 I main: load the model and apply lora adapter, if any
0.00.009.615 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.496 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.501 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.503 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.507 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.508 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.510 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.510 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.511 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.512 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.512 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.516 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.517 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.517 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.517 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.520 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.520 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.520 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.225 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.256 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.951 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.952 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.952 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.952 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.953 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.953 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.954 I llama_model_loader: - type  f32:  194 tensors
0.00.025.954 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.954 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.955 I print_info: file format = GGUF V3 (latest)
0.00.025.955 I print_info: file type   = Q5_0
0.00.025.956 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.871 I load: special tokens cache size = 25
0.00.040.088 I load: token to piece cache size = 0.2984 MB
0.00.040.101 I print_info: arch             = gptneox
0.00.040.103 I print_info: vocab_only       = 0
0.00.040.103 I print_info: n_ctx_train      = 2048
0.00.040.103 I print_info: n_embd           = 2048
0.00.040.103 I print_info: n_layer          = 24
0.00.040.106 I print_info: n_head           = 16
0.00.040.107 I print_info: n_head_kv        = 16
0.00.040.107 I print_info: n_rot            = 32
0.00.040.107 I print_info: n_swa            = 0
0.00.040.108 I print_info: n_embd_head_k    = 128
0.00.040.108 I print_info: n_embd_head_v    = 128
0.00.040.109 I print_info: n_gqa            = 1
0.00.040.109 I print_info: n_embd_k_gqa     = 2048
0.00.040.110 I print_info: n_embd_v_gqa     = 2048
0.00.040.111 I print_info: f_norm_eps       = 1.0e-05
0.00.040.111 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.111 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.111 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.112 I print_info: f_logit_scale    = 0.0e+00
0.00.040.112 I print_info: n_ff             = 8192
0.00.040.112 I print_info: n_expert         = 0
0.00.040.113 I print_info: n_expert_used    = 0
0.00.040.113 I print_info: causal attn      = 1
0.00.040.113 I print_info: pooling type     = 0
0.00.040.114 I print_info: rope type        = 2
0.00.040.114 I print_info: rope scaling     = linear
0.00.040.115 I print_info: freq_base_train  = 10000.0
0.00.040.115 I print_info: freq_scale_train = 1
0.00.040.115 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.115 I print_info: rope_finetuned   = unknown
0.00.040.115 I print_info: ssm_d_conv       = 0
0.00.040.116 I print_info: ssm_d_inner      = 0
0.00.040.116 I print_info: ssm_d_state      = 0
0.00.040.116 I print_info: ssm_dt_rank      = 0
0.00.040.116 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.116 I print_info: model type       = 1.4B
0.00.040.117 I print_info: model params     = 1.41 B
0.00.040.118 I print_info: general.name     = 1.4B
0.00.040.118 I print_info: vocab type       = BPE
0.00.040.119 I print_info: n_vocab          = 50304
0.00.040.119 I print_info: n_merges         = 50009
0.00.040.119 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.119 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.120 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.120 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.120 I print_info: LF token         = 187 ''
0.00.040.121 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.121 I print_info: max token length = 1024
0.00.040.121 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.664.577 I load_tensors: offloading 24 repeating layers to GPU
0.00.664.593 I load_tensors: offloading output layer to GPU
0.00.664.594 I load_tensors: offloaded 25/25 layers to GPU
0.00.664.626 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.664.627 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.665.895 I llama_context_kv_self: n_seq_max     = 1
0.00.665.899 I llama_context_kv_self: n_ctx         = 2048
0.00.665.899 I llama_context_kv_self: n_ctx_per_seq = 2048
0.00.665.900 I llama_context_kv_self: n_batch       = 2048
0.00.665.900 I llama_context_kv_self: n_ubatch      = 512
0.00.665.901 I llama_context_kv_self: flash_attn    = 0
0.00.665.903 I llama_context_kv_self: freq_base     = 10000.0
0.00.665.904 I llama_context_kv_self: freq_scale    = 1
0.00.665.907 I ggml_metal_init: allocating
0.00.665.983 I ggml_metal_init: found device: Apple M4
0.00.665.998 I ggml_metal_init: picking default device: Apple M4
0.00.667.956 I ggml_metal_init: using embedded metal library
0.00.674.242 I ggml_metal_init: GPU name:   Apple M4
0.00.674.247 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.674.248 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.674.249 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.674.250 I ggml_metal_init: simdgroup reduction   = true
0.00.674.250 I ggml_metal_init: simdgroup matrix mul. = true
0.00.674.250 I ggml_metal_init: has residency sets    = true
0.00.674.251 I ggml_metal_init: has bfloat            = true
0.00.674.251 I ggml_metal_init: use bfloat            = true
0.00.674.252 I ggml_metal_init: hasUnifiedMemory      = true
0.00.674.257 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.692.871 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.751.474 I init:      Metal KV buffer size =   384.00 MiB
0.00.751.481 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.751.503 I llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
0.00.756.099 I llama_context_kv_self:      Metal compute buffer size =   102.25 MiB
0.00.756.101 I llama_context_kv_self:        CPU compute buffer size =     8.01 MiB
0.00.756.101 I llama_context_kv_self: graph nodes  = 967
0.00.756.101 I llama_context_kv_self: graph splits = 2
0.00.756.108 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.756.237 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.756.238 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.803.680 I main: llama threadpool init, n_threads = 4
0.00.803.727 I 
0.00.803.750 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.803.750 I 
0.00.803.875 I sampler seed: 1234
0.00.803.879 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.803.899 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.803.899 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.803.899 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.627.131 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54911.06 tokens per second)
0.01.627.132 I llama_perf_context_print:        load time =     793.36 ms
0.01.627.133 I llama_perf_context_print: prompt eval time =      42.86 ms /     7 tokens (    6.12 ms per token,   163.32 tokens per second)
0.01.627.135 I llama_perf_context_print:        eval time =     777.58 ms /    63 runs   (   12.34 ms per token,    81.02 tokens per second)
0.01.627.135 I llama_perf_context_print:       total time =     824.16 ms /    70 tokens
0.01.630.381 I ggml_metal_free: deallocating

real	0m1.648s
user	0m0.111s
sys	0m0.220s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4729 (e08f38df) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.971 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.030 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.035 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.041 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.042 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.042 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.043 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.043 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.044 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.044 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.044 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.045 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.045 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.045 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.046 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.048 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.048 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.048 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.841 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.919 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.749 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.750 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.750 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.751 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.751 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.752 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.752 I llama_model_loader: - type  f32:  194 tensors
0.00.025.752 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.753 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.753 I print_info: file format = GGUF V3 (latest)
0.00.025.754 I print_info: file type   = Q5_0
0.00.025.756 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.183 I load: special tokens cache size = 25
0.00.040.079 I load: token to piece cache size = 0.2984 MB
0.00.040.096 I print_info: arch             = gptneox
0.00.040.097 I print_info: vocab_only       = 0
0.00.040.098 I print_info: n_ctx_train      = 2048
0.00.040.098 I print_info: n_embd           = 2048
0.00.040.098 I print_info: n_layer          = 24
0.00.040.102 I print_info: n_head           = 16
0.00.040.102 I print_info: n_head_kv        = 16
0.00.040.102 I print_info: n_rot            = 32
0.00.040.108 I print_info: n_swa            = 0
0.00.040.108 I print_info: n_embd_head_k    = 128
0.00.040.108 I print_info: n_embd_head_v    = 128
0.00.040.110 I print_info: n_gqa            = 1
0.00.040.111 I print_info: n_embd_k_gqa     = 2048
0.00.040.112 I print_info: n_embd_v_gqa     = 2048
0.00.040.112 I print_info: f_norm_eps       = 1.0e-05
0.00.040.113 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.113 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.113 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.115 I print_info: f_logit_scale    = 0.0e+00
0.00.040.115 I print_info: n_ff             = 8192
0.00.040.115 I print_info: n_expert         = 0
0.00.040.115 I print_info: n_expert_used    = 0
0.00.040.116 I print_info: causal attn      = 1
0.00.040.116 I print_info: pooling type     = 0
0.00.040.116 I print_info: rope type        = 2
0.00.040.116 I print_info: rope scaling     = linear
0.00.040.116 I print_info: freq_base_train  = 10000.0
0.00.040.117 I print_info: freq_scale_train = 1
0.00.040.117 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.117 I print_info: rope_finetuned   = unknown
0.00.040.117 I print_info: ssm_d_conv       = 0
0.00.040.117 I print_info: ssm_d_inner      = 0
0.00.040.117 I print_info: ssm_d_state      = 0
0.00.040.117 I print_info: ssm_dt_rank      = 0
0.00.040.117 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.118 I print_info: model type       = 1.4B
0.00.040.118 I print_info: model params     = 1.41 B
0.00.040.118 I print_info: general.name     = 1.4B
0.00.040.119 I print_info: vocab type       = BPE
0.00.040.119 I print_info: n_vocab          = 50304
0.00.040.119 I print_info: n_merges         = 50009
0.00.040.119 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.120 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.126 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.128 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.128 I print_info: LF token         = 187 ''
0.00.040.128 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.128 I print_info: max token length = 1024
0.00.040.129 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.648.601 I load_tensors: offloading 24 repeating layers to GPU
0.00.648.618 I load_tensors: offloading output layer to GPU
0.00.648.619 I load_tensors: offloaded 25/25 layers to GPU
0.00.648.653 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.648.661 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.650.391 I llama_context_kv_self: n_seq_max     = 1
0.00.650.395 I llama_context_kv_self: n_ctx         = 128
0.00.650.395 I llama_context_kv_self: n_ctx_per_seq = 128
0.00.650.396 I llama_context_kv_self: n_batch       = 128
0.00.650.396 I llama_context_kv_self: n_ubatch      = 128
0.00.650.396 I llama_context_kv_self: flash_attn    = 0
0.00.650.398 I llama_context_kv_self: freq_base     = 10000.0
0.00.650.398 I llama_context_kv_self: freq_scale    = 1
0.00.650.399 W llama_context_kv_self: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.650.401 I ggml_metal_init: allocating
0.00.650.415 I ggml_metal_init: found device: Apple M4
0.00.650.424 I ggml_metal_init: picking default device: Apple M4
0.00.651.815 I ggml_metal_init: using embedded metal library
0.00.658.132 I ggml_metal_init: GPU name:   Apple M4
0.00.658.136 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.658.137 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.658.138 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.658.138 I ggml_metal_init: simdgroup reduction   = true
0.00.658.138 I ggml_metal_init: simdgroup matrix mul. = true
0.00.658.139 I ggml_metal_init: has residency sets    = true
0.00.658.139 I ggml_metal_init: has bfloat            = true
0.00.658.139 I ggml_metal_init: use bfloat            = true
0.00.658.140 I ggml_metal_init: hasUnifiedMemory      = true
0.00.658.141 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.675.793 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.679.329 I init:      Metal KV buffer size =    24.00 MiB
0.00.679.332 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.679.357 I llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
0.00.682.518 I llama_context_kv_self:      Metal compute buffer size =    25.56 MiB
0.00.682.520 I llama_context_kv_self:        CPU compute buffer size =     1.06 MiB
0.00.682.520 I llama_context_kv_self: graph nodes  = 967
0.00.682.521 I llama_context_kv_self: graph splits = 2
0.00.682.524 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.682.526 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.711.345 I 
0.00.711.438 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.711.460 I perplexity: tokenizing the input ..
0.00.718.436 I perplexity: tokenization took 6.973 ms
0.00.718.455 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.854.479 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.855.810 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.855.826 I llama_perf_context_print:        load time =     701.37 ms
0.00.855.828 I llama_perf_context_print: prompt eval time =     135.15 ms /   128 tokens (    1.06 ms per token,   947.13 tokens per second)
0.00.855.829 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.855.829 I llama_perf_context_print:       total time =     144.48 ms /   129 tokens
0.00.856.384 I ggml_metal_free: deallocating

real	0m0.872s
user	0m0.080s
sys	0m0.136s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4729 (e08f38df) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.010.222 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.069 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.018.074 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.075 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.078 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.078 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.079 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.079 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.081 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.083 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.083 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.083 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.086 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.087 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.087 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.089 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.091 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.092 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.891 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.971 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.710 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.711 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.712 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.712 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.712 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.712 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.713 I llama_model_loader: - type  f32:  194 tensors
0.00.026.713 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.714 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.714 I print_info: file format = GGUF V3 (latest)
0.00.026.715 I print_info: file type   = Q5_1
0.00.026.716 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.960 I load: special tokens cache size = 25
0.00.040.915 I load: token to piece cache size = 0.2984 MB
0.00.040.929 I print_info: arch             = gptneox
0.00.040.930 I print_info: vocab_only       = 0
0.00.040.930 I print_info: n_ctx_train      = 2048
0.00.040.931 I print_info: n_embd           = 2048
0.00.040.931 I print_info: n_layer          = 24
0.00.040.934 I print_info: n_head           = 16
0.00.040.935 I print_info: n_head_kv        = 16
0.00.040.937 I print_info: n_rot            = 32
0.00.040.937 I print_info: n_swa            = 0
0.00.040.937 I print_info: n_embd_head_k    = 128
0.00.040.937 I print_info: n_embd_head_v    = 128
0.00.040.938 I print_info: n_gqa            = 1
0.00.040.939 I print_info: n_embd_k_gqa     = 2048
0.00.040.940 I print_info: n_embd_v_gqa     = 2048
0.00.040.941 I print_info: f_norm_eps       = 1.0e-05
0.00.040.942 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.942 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.942 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.942 I print_info: f_logit_scale    = 0.0e+00
0.00.040.943 I print_info: n_ff             = 8192
0.00.040.943 I print_info: n_expert         = 0
0.00.040.943 I print_info: n_expert_used    = 0
0.00.040.943 I print_info: causal attn      = 1
0.00.040.943 I print_info: pooling type     = 0
0.00.040.944 I print_info: rope type        = 2
0.00.040.945 I print_info: rope scaling     = linear
0.00.040.945 I print_info: freq_base_train  = 10000.0
0.00.040.945 I print_info: freq_scale_train = 1
0.00.040.945 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.945 I print_info: rope_finetuned   = unknown
0.00.040.946 I print_info: ssm_d_conv       = 0
0.00.040.946 I print_info: ssm_d_inner      = 0
0.00.040.949 I print_info: ssm_d_state      = 0
0.00.040.949 I print_info: ssm_dt_rank      = 0
0.00.040.949 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.949 I print_info: model type       = 1.4B
0.00.040.950 I print_info: model params     = 1.41 B
0.00.040.950 I print_info: general.name     = 1.4B
0.00.040.950 I print_info: vocab type       = BPE
0.00.040.951 I print_info: n_vocab          = 50304
0.00.040.951 I print_info: n_merges         = 50009
0.00.040.951 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.951 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.951 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.952 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.953 I print_info: LF token         = 187 ''
0.00.040.953 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.953 I print_info: max token length = 1024
0.00.040.953 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.770.200 I load_tensors: offloading 24 repeating layers to GPU
0.00.770.222 I load_tensors: offloading output layer to GPU
0.00.770.223 I load_tensors: offloaded 25/25 layers to GPU
0.00.770.258 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.770.259 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.771.559 I llama_context_kv_self: n_seq_max     = 1
0.00.771.564 I llama_context_kv_self: n_ctx         = 2048
0.00.771.564 I llama_context_kv_self: n_ctx_per_seq = 2048
0.00.771.565 I llama_context_kv_self: n_batch       = 2048
0.00.771.565 I llama_context_kv_self: n_ubatch      = 512
0.00.771.565 I llama_context_kv_self: flash_attn    = 0
0.00.771.568 I llama_context_kv_self: freq_base     = 10000.0
0.00.771.569 I llama_context_kv_self: freq_scale    = 1
0.00.771.571 I ggml_metal_init: allocating
0.00.771.652 I ggml_metal_init: found device: Apple M4
0.00.771.668 I ggml_metal_init: picking default device: Apple M4
0.00.773.623 I ggml_metal_init: using embedded metal library
0.00.780.004 I ggml_metal_init: GPU name:   Apple M4
0.00.780.010 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.780.011 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.780.012 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.780.012 I ggml_metal_init: simdgroup reduction   = true
0.00.780.013 I ggml_metal_init: simdgroup matrix mul. = true
0.00.780.013 I ggml_metal_init: has residency sets    = true
0.00.780.013 I ggml_metal_init: has bfloat            = true
0.00.780.013 I ggml_metal_init: use bfloat            = true
0.00.780.014 I ggml_metal_init: hasUnifiedMemory      = true
0.00.780.016 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.800.063 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.848.955 I init:      Metal KV buffer size =   384.00 MiB
0.00.848.962 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.848.985 I llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
0.00.853.565 I llama_context_kv_self:      Metal compute buffer size =   102.25 MiB
0.00.853.567 I llama_context_kv_self:        CPU compute buffer size =     8.01 MiB
0.00.853.567 I llama_context_kv_self: graph nodes  = 967
0.00.853.567 I llama_context_kv_self: graph splits = 2
0.00.853.574 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.853.702 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.853.703 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.903.555 I main: llama threadpool init, n_threads = 4
0.00.903.601 I 
0.00.903.627 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.903.628 I 
0.00.903.748 I sampler seed: 1234
0.00.903.753 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.903.773 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.903.773 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.903.773 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.784.452 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54364.47 tokens per second)
0.01.784.453 I llama_perf_context_print:        load time =     892.63 ms
0.01.784.454 I llama_perf_context_print: prompt eval time =      42.00 ms /     7 tokens (    6.00 ms per token,   166.67 tokens per second)
0.01.784.454 I llama_perf_context_print:        eval time =     835.77 ms /    63 runs   (   13.27 ms per token,    75.38 tokens per second)
0.01.784.455 I llama_perf_context_print:       total time =     881.60 ms /    70 tokens
0.01.787.924 I ggml_metal_free: deallocating

real	0m1.805s
user	0m0.111s
sys	0m0.219s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4729 (e08f38df) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.916 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.065 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.071 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.078 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.078 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.079 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.079 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.079 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.080 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.081 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.081 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.081 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.082 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.082 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.082 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.084 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.084 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.084 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.931 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.002 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.013 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.015 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.015 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.016 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.016 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.017 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.017 I llama_model_loader: - type  f32:  194 tensors
0.00.025.017 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.018 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.018 I print_info: file format = GGUF V3 (latest)
0.00.025.019 I print_info: file type   = Q5_1
0.00.025.021 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.403 I load: special tokens cache size = 25
0.00.039.466 I load: token to piece cache size = 0.2984 MB
0.00.039.483 I print_info: arch             = gptneox
0.00.039.484 I print_info: vocab_only       = 0
0.00.039.484 I print_info: n_ctx_train      = 2048
0.00.039.484 I print_info: n_embd           = 2048
0.00.039.485 I print_info: n_layer          = 24
0.00.039.488 I print_info: n_head           = 16
0.00.039.490 I print_info: n_head_kv        = 16
0.00.039.490 I print_info: n_rot            = 32
0.00.039.492 I print_info: n_swa            = 0
0.00.039.492 I print_info: n_embd_head_k    = 128
0.00.039.493 I print_info: n_embd_head_v    = 128
0.00.039.493 I print_info: n_gqa            = 1
0.00.039.494 I print_info: n_embd_k_gqa     = 2048
0.00.039.494 I print_info: n_embd_v_gqa     = 2048
0.00.039.495 I print_info: f_norm_eps       = 1.0e-05
0.00.039.495 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.495 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.496 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.496 I print_info: f_logit_scale    = 0.0e+00
0.00.039.496 I print_info: n_ff             = 8192
0.00.039.496 I print_info: n_expert         = 0
0.00.039.498 I print_info: n_expert_used    = 0
0.00.039.498 I print_info: causal attn      = 1
0.00.039.498 I print_info: pooling type     = 0
0.00.039.498 I print_info: rope type        = 2
0.00.039.498 I print_info: rope scaling     = linear
0.00.039.499 I print_info: freq_base_train  = 10000.0
0.00.039.499 I print_info: freq_scale_train = 1
0.00.039.499 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.499 I print_info: rope_finetuned   = unknown
0.00.039.499 I print_info: ssm_d_conv       = 0
0.00.039.500 I print_info: ssm_d_inner      = 0
0.00.039.500 I print_info: ssm_d_state      = 0
0.00.039.500 I print_info: ssm_dt_rank      = 0
0.00.039.500 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.500 I print_info: model type       = 1.4B
0.00.039.501 I print_info: model params     = 1.41 B
0.00.039.501 I print_info: general.name     = 1.4B
0.00.039.501 I print_info: vocab type       = BPE
0.00.039.501 I print_info: n_vocab          = 50304
0.00.039.501 I print_info: n_merges         = 50009
0.00.039.502 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.502 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.502 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.502 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.502 I print_info: LF token         = 187 ''
0.00.039.507 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.507 I print_info: max token length = 1024
0.00.039.508 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.643.532 I load_tensors: offloading 24 repeating layers to GPU
0.00.643.537 I load_tensors: offloading output layer to GPU
0.00.643.538 I load_tensors: offloaded 25/25 layers to GPU
0.00.643.562 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.643.565 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.645.053 I llama_context_kv_self: n_seq_max     = 1
0.00.645.055 I llama_context_kv_self: n_ctx         = 128
0.00.645.056 I llama_context_kv_self: n_ctx_per_seq = 128
0.00.645.056 I llama_context_kv_self: n_batch       = 128
0.00.645.057 I llama_context_kv_self: n_ubatch      = 128
0.00.645.057 I llama_context_kv_self: flash_attn    = 0
0.00.645.059 I llama_context_kv_self: freq_base     = 10000.0
0.00.645.059 I llama_context_kv_self: freq_scale    = 1
0.00.645.060 W llama_context_kv_self: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.645.062 I ggml_metal_init: allocating
0.00.645.085 I ggml_metal_init: found device: Apple M4
0.00.645.095 I ggml_metal_init: picking default device: Apple M4
0.00.646.551 I ggml_metal_init: using embedded metal library
0.00.652.415 I ggml_metal_init: GPU name:   Apple M4
0.00.652.419 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.652.419 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.652.420 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.652.421 I ggml_metal_init: simdgroup reduction   = true
0.00.652.421 I ggml_metal_init: simdgroup matrix mul. = true
0.00.652.421 I ggml_metal_init: has residency sets    = true
0.00.652.422 I ggml_metal_init: has bfloat            = true
0.00.652.422 I ggml_metal_init: use bfloat            = true
0.00.652.423 I ggml_metal_init: hasUnifiedMemory      = true
0.00.652.425 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.669.340 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.672.789 I init:      Metal KV buffer size =    24.00 MiB
0.00.672.793 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.672.818 I llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
0.00.676.059 I llama_context_kv_self:      Metal compute buffer size =    25.56 MiB
0.00.676.060 I llama_context_kv_self:        CPU compute buffer size =     1.06 MiB
0.00.676.061 I llama_context_kv_self: graph nodes  = 967
0.00.676.061 I llama_context_kv_self: graph splits = 2
0.00.676.065 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.676.065 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.705.897 I 
0.00.705.980 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.706.005 I perplexity: tokenizing the input ..
0.00.713.363 I perplexity: tokenization took 7.355 ms
0.00.713.384 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.862.318 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.863.756 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.863.767 I llama_perf_context_print:        load time =     696.97 ms
0.00.863.768 I llama_perf_context_print: prompt eval time =     148.04 ms /   128 tokens (    1.16 ms per token,   864.61 tokens per second)
0.00.863.769 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.863.770 I llama_perf_context_print:       total time =     157.88 ms /   129 tokens
0.00.864.312 I ggml_metal_free: deallocating

real	0m0.878s
user	0m0.079s
sys	0m0.127s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4729 (e08f38df) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.010.031 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.624 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.629 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.631 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.632 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.632 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.632 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.633 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.634 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.634 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.634 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.635 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.637 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.638 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.638 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.639 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.640 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.640 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.410 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.458 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.213 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.214 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.214 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.215 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.215 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.215 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.216 I llama_model_loader: - type  f32:  194 tensors
0.00.025.216 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.216 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.217 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.217 I print_info: file format = GGUF V3 (latest)
0.00.025.218 I print_info: file type   = Q2_K - Medium
0.00.025.219 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.140 I load: special tokens cache size = 25
0.00.039.396 I load: token to piece cache size = 0.2984 MB
0.00.039.410 I print_info: arch             = gptneox
0.00.039.411 I print_info: vocab_only       = 0
0.00.039.411 I print_info: n_ctx_train      = 2048
0.00.039.411 I print_info: n_embd           = 2048
0.00.039.412 I print_info: n_layer          = 24
0.00.039.415 I print_info: n_head           = 16
0.00.039.415 I print_info: n_head_kv        = 16
0.00.039.416 I print_info: n_rot            = 32
0.00.039.416 I print_info: n_swa            = 0
0.00.039.416 I print_info: n_embd_head_k    = 128
0.00.039.416 I print_info: n_embd_head_v    = 128
0.00.039.417 I print_info: n_gqa            = 1
0.00.039.418 I print_info: n_embd_k_gqa     = 2048
0.00.039.418 I print_info: n_embd_v_gqa     = 2048
0.00.039.419 I print_info: f_norm_eps       = 1.0e-05
0.00.039.419 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.419 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.420 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.420 I print_info: f_logit_scale    = 0.0e+00
0.00.039.420 I print_info: n_ff             = 8192
0.00.039.421 I print_info: n_expert         = 0
0.00.039.421 I print_info: n_expert_used    = 0
0.00.039.421 I print_info: causal attn      = 1
0.00.039.421 I print_info: pooling type     = 0
0.00.039.423 I print_info: rope type        = 2
0.00.039.423 I print_info: rope scaling     = linear
0.00.039.423 I print_info: freq_base_train  = 10000.0
0.00.039.424 I print_info: freq_scale_train = 1
0.00.039.424 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.424 I print_info: rope_finetuned   = unknown
0.00.039.424 I print_info: ssm_d_conv       = 0
0.00.039.424 I print_info: ssm_d_inner      = 0
0.00.039.424 I print_info: ssm_d_state      = 0
0.00.039.425 I print_info: ssm_dt_rank      = 0
0.00.039.425 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.425 I print_info: model type       = 1.4B
0.00.039.425 I print_info: model params     = 1.41 B
0.00.039.425 I print_info: general.name     = 1.4B
0.00.039.427 I print_info: vocab type       = BPE
0.00.039.427 I print_info: n_vocab          = 50304
0.00.039.427 I print_info: n_merges         = 50009
0.00.039.427 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.428 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.428 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.428 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.428 I print_info: LF token         = 187 ''
0.00.039.428 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.428 I print_info: max token length = 1024
0.00.039.430 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.400.556 I load_tensors: offloading 24 repeating layers to GPU
0.00.400.566 I load_tensors: offloading output layer to GPU
0.00.400.567 I load_tensors: offloaded 25/25 layers to GPU
0.00.400.606 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.400.608 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.402.005 I llama_context_kv_self: n_seq_max     = 1
0.00.402.009 I llama_context_kv_self: n_ctx         = 2048
0.00.402.010 I llama_context_kv_self: n_ctx_per_seq = 2048
0.00.402.010 I llama_context_kv_self: n_batch       = 2048
0.00.402.011 I llama_context_kv_self: n_ubatch      = 512
0.00.402.011 I llama_context_kv_self: flash_attn    = 0
0.00.402.013 I llama_context_kv_self: freq_base     = 10000.0
0.00.402.014 I llama_context_kv_self: freq_scale    = 1
0.00.402.016 I ggml_metal_init: allocating
0.00.402.086 I ggml_metal_init: found device: Apple M4
0.00.402.101 I ggml_metal_init: picking default device: Apple M4
0.00.403.917 I ggml_metal_init: using embedded metal library
0.00.409.855 I ggml_metal_init: GPU name:   Apple M4
0.00.409.870 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.409.870 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.409.871 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.409.872 I ggml_metal_init: simdgroup reduction   = true
0.00.409.872 I ggml_metal_init: simdgroup matrix mul. = true
0.00.409.872 I ggml_metal_init: has residency sets    = true
0.00.409.873 I ggml_metal_init: has bfloat            = true
0.00.409.873 I ggml_metal_init: use bfloat            = true
0.00.409.876 I ggml_metal_init: hasUnifiedMemory      = true
0.00.409.881 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.431.695 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.495.900 I init:      Metal KV buffer size =   384.00 MiB
0.00.495.907 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.495.933 I llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
0.00.500.764 I llama_context_kv_self:      Metal compute buffer size =   102.25 MiB
0.00.500.765 I llama_context_kv_self:        CPU compute buffer size =     8.01 MiB
0.00.500.766 I llama_context_kv_self: graph nodes  = 967
0.00.500.766 I llama_context_kv_self: graph splits = 2
0.00.500.772 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.500.901 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.500.902 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.550.376 I main: llama threadpool init, n_threads = 4
0.00.550.421 I 
0.00.550.443 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.550.443 I 
0.00.550.582 I sampler seed: 1234
0.00.550.587 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.550.605 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.550.606 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.550.606 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.235.611 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54447.85 tokens per second)
0.01.235.613 I llama_perf_context_print:        load time =     539.65 ms
0.01.235.614 I llama_perf_context_print: prompt eval time =      35.63 ms /     7 tokens (    5.09 ms per token,   196.49 tokens per second)
0.01.235.614 I llama_perf_context_print:        eval time =     646.65 ms /    63 runs   (   10.26 ms per token,    97.42 tokens per second)
0.01.235.615 I llama_perf_context_print:       total time =     685.92 ms /    70 tokens
0.01.239.078 I ggml_metal_free: deallocating

real	0m1.254s
user	0m0.114s
sys	0m0.180s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4729 (e08f38df) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.003 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.946 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.953 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.954 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.955 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.955 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.956 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.956 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.957 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.957 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.957 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.958 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.958 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.958 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.961 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.963 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.963 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.963 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.857 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.878 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.746 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.747 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.748 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.748 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.748 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.749 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.749 I llama_model_loader: - type  f32:  194 tensors
0.00.025.750 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.750 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.750 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.751 I print_info: file format = GGUF V3 (latest)
0.00.025.751 I print_info: file type   = Q2_K - Medium
0.00.025.752 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.057 I load: special tokens cache size = 25
0.00.040.135 I load: token to piece cache size = 0.2984 MB
0.00.040.144 I print_info: arch             = gptneox
0.00.040.145 I print_info: vocab_only       = 0
0.00.040.146 I print_info: n_ctx_train      = 2048
0.00.040.146 I print_info: n_embd           = 2048
0.00.040.146 I print_info: n_layer          = 24
0.00.040.150 I print_info: n_head           = 16
0.00.040.150 I print_info: n_head_kv        = 16
0.00.040.151 I print_info: n_rot            = 32
0.00.040.151 I print_info: n_swa            = 0
0.00.040.151 I print_info: n_embd_head_k    = 128
0.00.040.151 I print_info: n_embd_head_v    = 128
0.00.040.153 I print_info: n_gqa            = 1
0.00.040.154 I print_info: n_embd_k_gqa     = 2048
0.00.040.155 I print_info: n_embd_v_gqa     = 2048
0.00.040.155 I print_info: f_norm_eps       = 1.0e-05
0.00.040.158 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.158 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.158 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.159 I print_info: f_logit_scale    = 0.0e+00
0.00.040.159 I print_info: n_ff             = 8192
0.00.040.160 I print_info: n_expert         = 0
0.00.040.160 I print_info: n_expert_used    = 0
0.00.040.160 I print_info: causal attn      = 1
0.00.040.160 I print_info: pooling type     = 0
0.00.040.160 I print_info: rope type        = 2
0.00.040.160 I print_info: rope scaling     = linear
0.00.040.161 I print_info: freq_base_train  = 10000.0
0.00.040.161 I print_info: freq_scale_train = 1
0.00.040.161 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.161 I print_info: rope_finetuned   = unknown
0.00.040.161 I print_info: ssm_d_conv       = 0
0.00.040.161 I print_info: ssm_d_inner      = 0
0.00.040.162 I print_info: ssm_d_state      = 0
0.00.040.163 I print_info: ssm_dt_rank      = 0
0.00.040.163 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.163 I print_info: model type       = 1.4B
0.00.040.163 I print_info: model params     = 1.41 B
0.00.040.163 I print_info: general.name     = 1.4B
0.00.040.164 I print_info: vocab type       = BPE
0.00.040.164 I print_info: n_vocab          = 50304
0.00.040.164 I print_info: n_merges         = 50009
0.00.040.165 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.165 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.165 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.165 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.165 I print_info: LF token         = 187 ''
0.00.040.166 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.166 I print_info: max token length = 1024
0.00.040.167 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.409.814 I load_tensors: offloading 24 repeating layers to GPU
0.00.409.822 I load_tensors: offloading output layer to GPU
0.00.409.823 I load_tensors: offloaded 25/25 layers to GPU
0.00.409.853 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.409.854 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.411.327 I llama_context_kv_self: n_seq_max     = 1
0.00.411.335 I llama_context_kv_self: n_ctx         = 128
0.00.411.336 I llama_context_kv_self: n_ctx_per_seq = 128
0.00.411.336 I llama_context_kv_self: n_batch       = 128
0.00.411.336 I llama_context_kv_self: n_ubatch      = 128
0.00.411.337 I llama_context_kv_self: flash_attn    = 0
0.00.411.339 I llama_context_kv_self: freq_base     = 10000.0
0.00.411.339 I llama_context_kv_self: freq_scale    = 1
0.00.411.339 W llama_context_kv_self: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.411.345 I ggml_metal_init: allocating
0.00.411.397 I ggml_metal_init: found device: Apple M4
0.00.411.407 I ggml_metal_init: picking default device: Apple M4
0.00.414.006 I ggml_metal_init: using embedded metal library
0.00.419.992 I ggml_metal_init: GPU name:   Apple M4
0.00.420.004 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.420.004 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.420.010 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.420.011 I ggml_metal_init: simdgroup reduction   = true
0.00.420.012 I ggml_metal_init: simdgroup matrix mul. = true
0.00.420.013 I ggml_metal_init: has residency sets    = true
0.00.420.013 I ggml_metal_init: has bfloat            = true
0.00.420.013 I ggml_metal_init: use bfloat            = true
0.00.420.015 I ggml_metal_init: hasUnifiedMemory      = true
0.00.420.020 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.442.932 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.446.634 I init:      Metal KV buffer size =    24.00 MiB
0.00.446.644 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.446.681 I llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
0.00.450.109 I llama_context_kv_self:      Metal compute buffer size =    25.56 MiB
0.00.450.111 I llama_context_kv_self:        CPU compute buffer size =     1.06 MiB
0.00.450.111 I llama_context_kv_self: graph nodes  = 967
0.00.450.112 I llama_context_kv_self: graph splits = 2
0.00.450.117 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.450.117 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.479.098 I 
0.00.479.172 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.479.191 I perplexity: tokenizing the input ..
0.00.486.193 I perplexity: tokenization took 6.997 ms
0.00.486.213 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.619.814 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.621.223 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.621.235 I llama_perf_context_print:        load time =     469.09 ms
0.00.621.236 I llama_perf_context_print: prompt eval time =     132.64 ms /   128 tokens (    1.04 ms per token,   965.02 tokens per second)
0.00.621.236 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.621.236 I llama_perf_context_print:       total time =     142.14 ms /   129 tokens
0.00.621.748 I ggml_metal_free: deallocating

real	0m0.637s
user	0m0.082s
sys	0m0.115s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4729 (e08f38df) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.009.926 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.698 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.703 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.705 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.706 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.706 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.707 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.707 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.708 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.708 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.708 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.710 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.710 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.711 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.711 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.713 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.714 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.714 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.473 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.452 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.160 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.162 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.162 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.162 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.163 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.163 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.163 I llama_model_loader: - type  f32:  194 tensors
0.00.026.164 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.164 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.164 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.164 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.165 I print_info: file format = GGUF V3 (latest)
0.00.026.165 I print_info: file type   = Q3_K - Medium
0.00.026.166 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.034.364 I load: special tokens cache size = 25
0.00.040.550 I load: token to piece cache size = 0.2984 MB
0.00.040.565 I print_info: arch             = gptneox
0.00.040.566 I print_info: vocab_only       = 0
0.00.040.566 I print_info: n_ctx_train      = 2048
0.00.040.566 I print_info: n_embd           = 2048
0.00.040.566 I print_info: n_layer          = 24
0.00.040.569 I print_info: n_head           = 16
0.00.040.570 I print_info: n_head_kv        = 16
0.00.040.570 I print_info: n_rot            = 32
0.00.040.570 I print_info: n_swa            = 0
0.00.040.570 I print_info: n_embd_head_k    = 128
0.00.040.571 I print_info: n_embd_head_v    = 128
0.00.040.571 I print_info: n_gqa            = 1
0.00.040.572 I print_info: n_embd_k_gqa     = 2048
0.00.040.573 I print_info: n_embd_v_gqa     = 2048
0.00.040.573 I print_info: f_norm_eps       = 1.0e-05
0.00.040.574 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.574 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.574 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.574 I print_info: f_logit_scale    = 0.0e+00
0.00.040.575 I print_info: n_ff             = 8192
0.00.040.575 I print_info: n_expert         = 0
0.00.040.575 I print_info: n_expert_used    = 0
0.00.040.576 I print_info: causal attn      = 1
0.00.040.577 I print_info: pooling type     = 0
0.00.040.577 I print_info: rope type        = 2
0.00.040.579 I print_info: rope scaling     = linear
0.00.040.579 I print_info: freq_base_train  = 10000.0
0.00.040.580 I print_info: freq_scale_train = 1
0.00.040.580 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.580 I print_info: rope_finetuned   = unknown
0.00.040.580 I print_info: ssm_d_conv       = 0
0.00.040.580 I print_info: ssm_d_inner      = 0
0.00.040.584 I print_info: ssm_d_state      = 0
0.00.040.584 I print_info: ssm_dt_rank      = 0
0.00.040.585 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.585 I print_info: model type       = 1.4B
0.00.040.585 I print_info: model params     = 1.41 B
0.00.040.585 I print_info: general.name     = 1.4B
0.00.040.587 I print_info: vocab type       = BPE
0.00.040.587 I print_info: n_vocab          = 50304
0.00.040.587 I print_info: n_merges         = 50009
0.00.040.587 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.588 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.588 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.588 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.588 I print_info: LF token         = 187 ''
0.00.040.588 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.588 I print_info: max token length = 1024
0.00.040.589 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.452.168 I load_tensors: offloading 24 repeating layers to GPU
0.00.452.188 I load_tensors: offloading output layer to GPU
0.00.452.188 I load_tensors: offloaded 25/25 layers to GPU
0.00.452.222 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.452.224 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.453.648 I llama_context_kv_self: n_seq_max     = 1
0.00.453.652 I llama_context_kv_self: n_ctx         = 2048
0.00.453.653 I llama_context_kv_self: n_ctx_per_seq = 2048
0.00.453.653 I llama_context_kv_self: n_batch       = 2048
0.00.453.654 I llama_context_kv_self: n_ubatch      = 512
0.00.453.654 I llama_context_kv_self: flash_attn    = 0
0.00.453.657 I llama_context_kv_self: freq_base     = 10000.0
0.00.453.657 I llama_context_kv_self: freq_scale    = 1
0.00.453.659 I ggml_metal_init: allocating
0.00.453.737 I ggml_metal_init: found device: Apple M4
0.00.453.756 I ggml_metal_init: picking default device: Apple M4
0.00.455.764 I ggml_metal_init: using embedded metal library
0.00.461.792 I ggml_metal_init: GPU name:   Apple M4
0.00.461.811 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.461.812 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.461.813 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.461.814 I ggml_metal_init: simdgroup reduction   = true
0.00.461.814 I ggml_metal_init: simdgroup matrix mul. = true
0.00.461.814 I ggml_metal_init: has residency sets    = true
0.00.461.814 I ggml_metal_init: has bfloat            = true
0.00.461.815 I ggml_metal_init: use bfloat            = true
0.00.461.820 I ggml_metal_init: hasUnifiedMemory      = true
0.00.461.826 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.483.088 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.544.678 I init:      Metal KV buffer size =   384.00 MiB
0.00.544.688 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.544.722 I llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
0.00.549.672 I llama_context_kv_self:      Metal compute buffer size =   102.25 MiB
0.00.549.674 I llama_context_kv_self:        CPU compute buffer size =     8.01 MiB
0.00.549.675 I llama_context_kv_self: graph nodes  = 967
0.00.549.675 I llama_context_kv_self: graph splits = 2
0.00.549.681 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.549.810 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.549.811 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.601.563 I main: llama threadpool init, n_threads = 4
0.00.601.607 I 
0.00.601.627 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.601.628 I 
0.00.601.756 I sampler seed: 1234
0.00.601.760 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.601.771 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.601.771 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.601.771 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.378.296 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52052.79 tokens per second)
0.01.378.296 I llama_perf_context_print:        load time =     590.93 ms
0.01.378.297 I llama_perf_context_print: prompt eval time =      50.98 ms /     7 tokens (    7.28 ms per token,   137.31 tokens per second)
0.01.378.298 I llama_perf_context_print:        eval time =     722.57 ms /    63 runs   (   11.47 ms per token,    87.19 tokens per second)
0.01.378.298 I llama_perf_context_print:       total time =     777.44 ms /    70 tokens
0.01.381.524 I ggml_metal_free: deallocating

real	0m1.398s
user	0m0.113s
sys	0m0.188s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4729 (e08f38df) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.801 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.876 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.881 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.883 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.883 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.883 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.884 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.884 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.885 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.885 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.886 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.886 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.887 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.887 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.887 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.889 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.889 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.890 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.750 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.739 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.631 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.632 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.632 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.633 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.633 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.634 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.634 I llama_model_loader: - type  f32:  194 tensors
0.00.024.634 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.635 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.635 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.635 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.636 I print_info: file format = GGUF V3 (latest)
0.00.024.636 I print_info: file type   = Q3_K - Medium
0.00.024.637 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.741 I load: special tokens cache size = 25
0.00.038.880 I load: token to piece cache size = 0.2984 MB
0.00.038.894 I print_info: arch             = gptneox
0.00.038.896 I print_info: vocab_only       = 0
0.00.038.896 I print_info: n_ctx_train      = 2048
0.00.038.896 I print_info: n_embd           = 2048
0.00.038.896 I print_info: n_layer          = 24
0.00.038.899 I print_info: n_head           = 16
0.00.038.900 I print_info: n_head_kv        = 16
0.00.038.900 I print_info: n_rot            = 32
0.00.038.900 I print_info: n_swa            = 0
0.00.038.900 I print_info: n_embd_head_k    = 128
0.00.038.900 I print_info: n_embd_head_v    = 128
0.00.038.901 I print_info: n_gqa            = 1
0.00.038.902 I print_info: n_embd_k_gqa     = 2048
0.00.038.902 I print_info: n_embd_v_gqa     = 2048
0.00.038.902 I print_info: f_norm_eps       = 1.0e-05
0.00.038.903 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.903 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.903 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.903 I print_info: f_logit_scale    = 0.0e+00
0.00.038.904 I print_info: n_ff             = 8192
0.00.038.904 I print_info: n_expert         = 0
0.00.038.904 I print_info: n_expert_used    = 0
0.00.038.904 I print_info: causal attn      = 1
0.00.038.904 I print_info: pooling type     = 0
0.00.038.904 I print_info: rope type        = 2
0.00.038.905 I print_info: rope scaling     = linear
0.00.038.905 I print_info: freq_base_train  = 10000.0
0.00.038.905 I print_info: freq_scale_train = 1
0.00.038.905 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.906 I print_info: rope_finetuned   = unknown
0.00.038.906 I print_info: ssm_d_conv       = 0
0.00.038.906 I print_info: ssm_d_inner      = 0
0.00.038.906 I print_info: ssm_d_state      = 0
0.00.038.906 I print_info: ssm_dt_rank      = 0
0.00.038.906 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.906 I print_info: model type       = 1.4B
0.00.038.906 I print_info: model params     = 1.41 B
0.00.038.907 I print_info: general.name     = 1.4B
0.00.038.907 I print_info: vocab type       = BPE
0.00.038.908 I print_info: n_vocab          = 50304
0.00.038.908 I print_info: n_merges         = 50009
0.00.038.908 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.908 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.908 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.908 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.908 I print_info: LF token         = 187 ''
0.00.038.909 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.909 I print_info: max token length = 1024
0.00.038.909 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.423.470 I load_tensors: offloading 24 repeating layers to GPU
0.00.423.475 I load_tensors: offloading output layer to GPU
0.00.423.475 I load_tensors: offloaded 25/25 layers to GPU
0.00.423.494 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.423.495 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.424.421 I llama_context_kv_self: n_seq_max     = 1
0.00.424.423 I llama_context_kv_self: n_ctx         = 128
0.00.424.424 I llama_context_kv_self: n_ctx_per_seq = 128
0.00.424.424 I llama_context_kv_self: n_batch       = 128
0.00.424.424 I llama_context_kv_self: n_ubatch      = 128
0.00.424.425 I llama_context_kv_self: flash_attn    = 0
0.00.424.426 I llama_context_kv_self: freq_base     = 10000.0
0.00.424.427 I llama_context_kv_self: freq_scale    = 1
0.00.424.427 W llama_context_kv_self: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.424.428 I ggml_metal_init: allocating
0.00.424.467 I ggml_metal_init: found device: Apple M4
0.00.424.477 I ggml_metal_init: picking default device: Apple M4
0.00.425.500 I ggml_metal_init: using embedded metal library
0.00.429.624 I ggml_metal_init: GPU name:   Apple M4
0.00.429.631 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.429.631 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.429.632 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.429.633 I ggml_metal_init: simdgroup reduction   = true
0.00.429.633 I ggml_metal_init: simdgroup matrix mul. = true
0.00.429.633 I ggml_metal_init: has residency sets    = true
0.00.429.633 I ggml_metal_init: has bfloat            = true
0.00.429.634 I ggml_metal_init: use bfloat            = true
0.00.429.635 I ggml_metal_init: hasUnifiedMemory      = true
0.00.429.638 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.446.392 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.448.024 I init:      Metal KV buffer size =    24.00 MiB
0.00.448.028 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.448.043 I llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
0.00.449.662 I llama_context_kv_self:      Metal compute buffer size =    25.56 MiB
0.00.449.663 I llama_context_kv_self:        CPU compute buffer size =     1.06 MiB
0.00.449.664 I llama_context_kv_self: graph nodes  = 967
0.00.449.664 I llama_context_kv_self: graph splits = 2
0.00.449.666 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.449.666 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.472.671 I 
0.00.472.705 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.472.714 I perplexity: tokenizing the input ..
0.00.476.649 I perplexity: tokenization took 3.933 ms
0.00.476.660 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.607.587 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.609.004 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.609.025 I llama_perf_context_print:        load time =     463.86 ms
0.00.609.026 I llama_perf_context_print: prompt eval time =     130.70 ms /   128 tokens (    1.02 ms per token,   979.35 tokens per second)
0.00.609.027 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.609.027 I llama_perf_context_print:       total time =     136.35 ms /   129 tokens
0.00.609.556 I ggml_metal_free: deallocating

real	0m0.624s
user	0m0.071s
sys	0m0.071s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4729 (e08f38df) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.010.120 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.784 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.789 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.791 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.791 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.792 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.792 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.792 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.793 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.794 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.794 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.794 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.795 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.795 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.796 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.798 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.798 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.798 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.647 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.715 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.498 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.499 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.500 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.500 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.500 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.501 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.501 I llama_model_loader: - type  f32:  194 tensors
0.00.026.501 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.502 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.502 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.503 I print_info: file format = GGUF V3 (latest)
0.00.026.503 I print_info: file type   = Q4_K - Medium
0.00.026.504 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.351 I load: special tokens cache size = 25
0.00.040.420 I load: token to piece cache size = 0.2984 MB
0.00.040.434 I print_info: arch             = gptneox
0.00.040.435 I print_info: vocab_only       = 0
0.00.040.435 I print_info: n_ctx_train      = 2048
0.00.040.436 I print_info: n_embd           = 2048
0.00.040.436 I print_info: n_layer          = 24
0.00.040.439 I print_info: n_head           = 16
0.00.040.440 I print_info: n_head_kv        = 16
0.00.040.440 I print_info: n_rot            = 32
0.00.040.440 I print_info: n_swa            = 0
0.00.040.440 I print_info: n_embd_head_k    = 128
0.00.040.440 I print_info: n_embd_head_v    = 128
0.00.040.441 I print_info: n_gqa            = 1
0.00.040.442 I print_info: n_embd_k_gqa     = 2048
0.00.040.442 I print_info: n_embd_v_gqa     = 2048
0.00.040.443 I print_info: f_norm_eps       = 1.0e-05
0.00.040.443 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.444 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.444 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.444 I print_info: f_logit_scale    = 0.0e+00
0.00.040.445 I print_info: n_ff             = 8192
0.00.040.445 I print_info: n_expert         = 0
0.00.040.445 I print_info: n_expert_used    = 0
0.00.040.445 I print_info: causal attn      = 1
0.00.040.446 I print_info: pooling type     = 0
0.00.040.447 I print_info: rope type        = 2
0.00.040.447 I print_info: rope scaling     = linear
0.00.040.448 I print_info: freq_base_train  = 10000.0
0.00.040.448 I print_info: freq_scale_train = 1
0.00.040.448 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.448 I print_info: rope_finetuned   = unknown
0.00.040.448 I print_info: ssm_d_conv       = 0
0.00.040.449 I print_info: ssm_d_inner      = 0
0.00.040.449 I print_info: ssm_d_state      = 0
0.00.040.449 I print_info: ssm_dt_rank      = 0
0.00.040.449 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.449 I print_info: model type       = 1.4B
0.00.040.449 I print_info: model params     = 1.41 B
0.00.040.449 I print_info: general.name     = 1.4B
0.00.040.450 I print_info: vocab type       = BPE
0.00.040.450 I print_info: n_vocab          = 50304
0.00.040.450 I print_info: n_merges         = 50009
0.00.040.451 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.451 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.451 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.451 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.451 I print_info: LF token         = 187 ''
0.00.040.451 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.452 I print_info: max token length = 1024
0.00.040.452 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.552.967 I load_tensors: offloading 24 repeating layers to GPU
0.00.552.978 I load_tensors: offloading output layer to GPU
0.00.552.979 I load_tensors: offloaded 25/25 layers to GPU
0.00.553.016 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.553.018 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.554.541 I llama_context_kv_self: n_seq_max     = 1
0.00.554.547 I llama_context_kv_self: n_ctx         = 2048
0.00.554.548 I llama_context_kv_self: n_ctx_per_seq = 2048
0.00.554.548 I llama_context_kv_self: n_batch       = 2048
0.00.554.549 I llama_context_kv_self: n_ubatch      = 512
0.00.554.549 I llama_context_kv_self: flash_attn    = 0
0.00.554.551 I llama_context_kv_self: freq_base     = 10000.0
0.00.554.551 I llama_context_kv_self: freq_scale    = 1
0.00.554.553 I ggml_metal_init: allocating
0.00.554.599 I ggml_metal_init: found device: Apple M4
0.00.554.612 I ggml_metal_init: picking default device: Apple M4
0.00.556.412 I ggml_metal_init: using embedded metal library
0.00.563.231 I ggml_metal_init: GPU name:   Apple M4
0.00.563.237 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.563.237 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.563.238 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.563.239 I ggml_metal_init: simdgroup reduction   = true
0.00.563.239 I ggml_metal_init: simdgroup matrix mul. = true
0.00.563.239 I ggml_metal_init: has residency sets    = true
0.00.563.239 I ggml_metal_init: has bfloat            = true
0.00.563.240 I ggml_metal_init: use bfloat            = true
0.00.563.241 I ggml_metal_init: hasUnifiedMemory      = true
0.00.563.243 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.580.920 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.633.270 I init:      Metal KV buffer size =   384.00 MiB
0.00.633.278 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.633.310 I llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
0.00.637.882 I llama_context_kv_self:      Metal compute buffer size =   102.25 MiB
0.00.637.884 I llama_context_kv_self:        CPU compute buffer size =     8.01 MiB
0.00.637.885 I llama_context_kv_self: graph nodes  = 967
0.00.637.885 I llama_context_kv_self: graph splits = 2
0.00.637.890 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.638.021 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.638.021 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.697.658 I main: llama threadpool init, n_threads = 4
0.00.697.697 I 
0.00.697.720 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.697.720 I 
0.00.697.889 I sampler seed: 1234
0.00.697.894 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.697.912 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.697.912 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.697.912 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.452.604 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49511.85 tokens per second)
0.01.452.605 I llama_perf_context_print:        load time =     686.84 ms
0.01.452.605 I llama_perf_context_print: prompt eval time =      50.53 ms /     7 tokens (    7.22 ms per token,   138.52 tokens per second)
0.01.452.606 I llama_perf_context_print:        eval time =     701.20 ms /    63 runs   (   11.13 ms per token,    89.85 tokens per second)
0.01.452.607 I llama_perf_context_print:       total time =     755.65 ms /    70 tokens
0.01.456.451 I ggml_metal_free: deallocating

real	0m1.473s
user	0m0.111s
sys	0m0.214s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4729 (e08f38df) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.897 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.345 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.352 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.354 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.354 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.355 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.355 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.355 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.356 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.356 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.358 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.358 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.359 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.359 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.359 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.361 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.362 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.362 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.232 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.326 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.324 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.326 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.326 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.327 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.327 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.327 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.328 I llama_model_loader: - type  f32:  194 tensors
0.00.026.328 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.328 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.329 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.329 I print_info: file format = GGUF V3 (latest)
0.00.026.330 I print_info: file type   = Q4_K - Medium
0.00.026.332 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.658 I load: special tokens cache size = 25
0.00.040.858 I load: token to piece cache size = 0.2984 MB
0.00.040.876 I print_info: arch             = gptneox
0.00.040.877 I print_info: vocab_only       = 0
0.00.040.877 I print_info: n_ctx_train      = 2048
0.00.040.877 I print_info: n_embd           = 2048
0.00.040.877 I print_info: n_layer          = 24
0.00.040.880 I print_info: n_head           = 16
0.00.040.881 I print_info: n_head_kv        = 16
0.00.040.881 I print_info: n_rot            = 32
0.00.040.881 I print_info: n_swa            = 0
0.00.040.882 I print_info: n_embd_head_k    = 128
0.00.040.882 I print_info: n_embd_head_v    = 128
0.00.040.883 I print_info: n_gqa            = 1
0.00.040.883 I print_info: n_embd_k_gqa     = 2048
0.00.040.884 I print_info: n_embd_v_gqa     = 2048
0.00.040.884 I print_info: f_norm_eps       = 1.0e-05
0.00.040.885 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.885 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.885 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.885 I print_info: f_logit_scale    = 0.0e+00
0.00.040.886 I print_info: n_ff             = 8192
0.00.040.886 I print_info: n_expert         = 0
0.00.040.886 I print_info: n_expert_used    = 0
0.00.040.886 I print_info: causal attn      = 1
0.00.040.886 I print_info: pooling type     = 0
0.00.040.889 I print_info: rope type        = 2
0.00.040.889 I print_info: rope scaling     = linear
0.00.040.889 I print_info: freq_base_train  = 10000.0
0.00.040.889 I print_info: freq_scale_train = 1
0.00.040.890 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.890 I print_info: rope_finetuned   = unknown
0.00.040.890 I print_info: ssm_d_conv       = 0
0.00.040.890 I print_info: ssm_d_inner      = 0
0.00.040.890 I print_info: ssm_d_state      = 0
0.00.040.890 I print_info: ssm_dt_rank      = 0
0.00.040.890 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.891 I print_info: model type       = 1.4B
0.00.040.891 I print_info: model params     = 1.41 B
0.00.040.891 I print_info: general.name     = 1.4B
0.00.040.892 I print_info: vocab type       = BPE
0.00.040.892 I print_info: n_vocab          = 50304
0.00.040.892 I print_info: n_merges         = 50009
0.00.040.893 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.893 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.893 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.893 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.894 I print_info: LF token         = 187 ''
0.00.040.894 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.894 I print_info: max token length = 1024
0.00.040.894 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.564.351 I load_tensors: offloading 24 repeating layers to GPU
0.00.564.363 I load_tensors: offloading output layer to GPU
0.00.564.364 I load_tensors: offloaded 25/25 layers to GPU
0.00.564.399 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.564.400 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.565.569 I llama_context_kv_self: n_seq_max     = 1
0.00.565.574 I llama_context_kv_self: n_ctx         = 128
0.00.565.574 I llama_context_kv_self: n_ctx_per_seq = 128
0.00.565.575 I llama_context_kv_self: n_batch       = 128
0.00.565.575 I llama_context_kv_self: n_ubatch      = 128
0.00.565.575 I llama_context_kv_self: flash_attn    = 0
0.00.565.577 I llama_context_kv_self: freq_base     = 10000.0
0.00.565.577 I llama_context_kv_self: freq_scale    = 1
0.00.565.578 W llama_context_kv_self: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.565.585 I ggml_metal_init: allocating
0.00.565.681 I ggml_metal_init: found device: Apple M4
0.00.565.695 I ggml_metal_init: picking default device: Apple M4
0.00.567.548 I ggml_metal_init: using embedded metal library
0.00.573.891 I ggml_metal_init: GPU name:   Apple M4
0.00.573.904 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.573.905 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.573.905 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.573.906 I ggml_metal_init: simdgroup reduction   = true
0.00.573.906 I ggml_metal_init: simdgroup matrix mul. = true
0.00.573.906 I ggml_metal_init: has residency sets    = true
0.00.573.907 I ggml_metal_init: has bfloat            = true
0.00.573.907 I ggml_metal_init: use bfloat            = true
0.00.573.909 I ggml_metal_init: hasUnifiedMemory      = true
0.00.573.916 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.594.859 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.598.667 I init:      Metal KV buffer size =    24.00 MiB
0.00.598.672 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.598.707 I llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
0.00.602.160 I llama_context_kv_self:      Metal compute buffer size =    25.56 MiB
0.00.602.162 I llama_context_kv_self:        CPU compute buffer size =     1.06 MiB
0.00.602.162 I llama_context_kv_self: graph nodes  = 967
0.00.602.163 I llama_context_kv_self: graph splits = 2
0.00.602.166 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.602.169 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.630.059 I 
0.00.630.133 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.630.152 I perplexity: tokenizing the input ..
0.00.636.188 I perplexity: tokenization took 6.035 ms
0.00.636.199 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.769.733 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.771.188 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.771.202 I llama_perf_context_print:        load time =     620.15 ms
0.00.771.202 I llama_perf_context_print: prompt eval time =     133.21 ms /   128 tokens (    1.04 ms per token,   960.90 tokens per second)
0.00.771.203 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.771.203 I llama_perf_context_print:       total time =     141.15 ms /   129 tokens
0.00.771.746 I ggml_metal_free: deallocating

real	0m0.787s
user	0m0.080s
sys	0m0.128s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4729 (e08f38df) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.008.664 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.200 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.205 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.207 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.207 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.209 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.209 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.210 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.211 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.211 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.213 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.213 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.214 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.214 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.215 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.217 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.217 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.218 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.981 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.036 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.740 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.741 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.741 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.742 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.742 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.742 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.743 I llama_model_loader: - type  f32:  194 tensors
0.00.024.743 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.743 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.744 I print_info: file format = GGUF V3 (latest)
0.00.024.744 I print_info: file type   = Q5_K - Medium
0.00.024.745 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.665 I load: special tokens cache size = 25
0.00.038.604 I load: token to piece cache size = 0.2984 MB
0.00.038.618 I print_info: arch             = gptneox
0.00.038.619 I print_info: vocab_only       = 0
0.00.038.619 I print_info: n_ctx_train      = 2048
0.00.038.620 I print_info: n_embd           = 2048
0.00.038.620 I print_info: n_layer          = 24
0.00.038.623 I print_info: n_head           = 16
0.00.038.624 I print_info: n_head_kv        = 16
0.00.038.624 I print_info: n_rot            = 32
0.00.038.624 I print_info: n_swa            = 0
0.00.038.625 I print_info: n_embd_head_k    = 128
0.00.038.625 I print_info: n_embd_head_v    = 128
0.00.038.626 I print_info: n_gqa            = 1
0.00.038.627 I print_info: n_embd_k_gqa     = 2048
0.00.038.627 I print_info: n_embd_v_gqa     = 2048
0.00.038.628 I print_info: f_norm_eps       = 1.0e-05
0.00.038.628 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.629 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.629 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.629 I print_info: f_logit_scale    = 0.0e+00
0.00.038.630 I print_info: n_ff             = 8192
0.00.038.630 I print_info: n_expert         = 0
0.00.038.630 I print_info: n_expert_used    = 0
0.00.038.630 I print_info: causal attn      = 1
0.00.038.630 I print_info: pooling type     = 0
0.00.038.631 I print_info: rope type        = 2
0.00.038.633 I print_info: rope scaling     = linear
0.00.038.633 I print_info: freq_base_train  = 10000.0
0.00.038.634 I print_info: freq_scale_train = 1
0.00.038.634 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.634 I print_info: rope_finetuned   = unknown
0.00.038.634 I print_info: ssm_d_conv       = 0
0.00.038.634 I print_info: ssm_d_inner      = 0
0.00.038.635 I print_info: ssm_d_state      = 0
0.00.038.635 I print_info: ssm_dt_rank      = 0
0.00.038.635 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.636 I print_info: model type       = 1.4B
0.00.038.636 I print_info: model params     = 1.41 B
0.00.038.636 I print_info: general.name     = 1.4B
0.00.038.637 I print_info: vocab type       = BPE
0.00.038.637 I print_info: n_vocab          = 50304
0.00.038.637 I print_info: n_merges         = 50009
0.00.038.638 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.638 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.638 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.638 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.638 I print_info: LF token         = 187 ''
0.00.038.639 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.639 I print_info: max token length = 1024
0.00.038.639 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.601.792 I load_tensors: offloading 24 repeating layers to GPU
0.00.601.806 I load_tensors: offloading output layer to GPU
0.00.601.807 I load_tensors: offloaded 25/25 layers to GPU
0.00.601.842 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.601.844 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.603.402 I llama_context_kv_self: n_seq_max     = 1
0.00.603.404 I llama_context_kv_self: n_ctx         = 2048
0.00.603.404 I llama_context_kv_self: n_ctx_per_seq = 2048
0.00.603.404 I llama_context_kv_self: n_batch       = 2048
0.00.603.405 I llama_context_kv_self: n_ubatch      = 512
0.00.603.405 I llama_context_kv_self: flash_attn    = 0
0.00.603.406 I llama_context_kv_self: freq_base     = 10000.0
0.00.603.407 I llama_context_kv_self: freq_scale    = 1
0.00.603.409 I ggml_metal_init: allocating
0.00.603.421 I ggml_metal_init: found device: Apple M4
0.00.603.429 I ggml_metal_init: picking default device: Apple M4
0.00.604.891 I ggml_metal_init: using embedded metal library
0.00.611.121 I ggml_metal_init: GPU name:   Apple M4
0.00.611.124 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.611.125 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.611.126 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.611.127 I ggml_metal_init: simdgroup reduction   = true
0.00.611.127 I ggml_metal_init: simdgroup matrix mul. = true
0.00.611.127 I ggml_metal_init: has residency sets    = true
0.00.611.127 I ggml_metal_init: has bfloat            = true
0.00.611.128 I ggml_metal_init: use bfloat            = true
0.00.611.128 I ggml_metal_init: hasUnifiedMemory      = true
0.00.611.130 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.629.456 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.682.598 I init:      Metal KV buffer size =   384.00 MiB
0.00.682.606 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.682.637 I llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
0.00.687.114 I llama_context_kv_self:      Metal compute buffer size =   102.25 MiB
0.00.687.117 I llama_context_kv_self:        CPU compute buffer size =     8.01 MiB
0.00.687.117 I llama_context_kv_self: graph nodes  = 967
0.00.687.117 I llama_context_kv_self: graph splits = 2
0.00.687.122 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.687.254 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.687.255 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.741.518 I main: llama threadpool init, n_threads = 4
0.00.741.560 I 
0.00.741.581 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.741.581 I 
0.00.741.701 I sampler seed: 1234
0.00.741.706 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.741.739 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.741.742 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.741.742 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.598.122 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51711.58 tokens per second)
0.01.598.123 I llama_perf_context_print:        load time =     732.17 ms
0.01.598.124 I llama_perf_context_print: prompt eval time =      51.53 ms /     7 tokens (    7.36 ms per token,   135.84 tokens per second)
0.01.598.124 I llama_perf_context_print:        eval time =     802.15 ms /    63 runs   (   12.73 ms per token,    78.54 tokens per second)
0.01.598.125 I llama_perf_context_print:       total time =     857.29 ms /    70 tokens
0.01.602.017 I ggml_metal_free: deallocating

real	0m1.617s
user	0m0.110s
sys	0m0.205s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4729 (e08f38df) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.126 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.199 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.205 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.207 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.207 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.208 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.208 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.208 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.209 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.211 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.211 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.211 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.212 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.212 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.213 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.214 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.215 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.215 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.055 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.143 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.985 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.986 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.987 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.987 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.987 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.988 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.988 I llama_model_loader: - type  f32:  194 tensors
0.00.024.989 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.989 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.990 I print_info: file format = GGUF V3 (latest)
0.00.024.990 I print_info: file type   = Q5_K - Medium
0.00.024.991 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.152 I load: special tokens cache size = 25
0.00.039.152 I load: token to piece cache size = 0.2984 MB
0.00.039.170 I print_info: arch             = gptneox
0.00.039.170 I print_info: vocab_only       = 0
0.00.039.171 I print_info: n_ctx_train      = 2048
0.00.039.171 I print_info: n_embd           = 2048
0.00.039.171 I print_info: n_layer          = 24
0.00.039.175 I print_info: n_head           = 16
0.00.039.176 I print_info: n_head_kv        = 16
0.00.039.176 I print_info: n_rot            = 32
0.00.039.176 I print_info: n_swa            = 0
0.00.039.176 I print_info: n_embd_head_k    = 128
0.00.039.176 I print_info: n_embd_head_v    = 128
0.00.039.177 I print_info: n_gqa            = 1
0.00.039.178 I print_info: n_embd_k_gqa     = 2048
0.00.039.178 I print_info: n_embd_v_gqa     = 2048
0.00.039.179 I print_info: f_norm_eps       = 1.0e-05
0.00.039.179 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.179 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.179 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.179 I print_info: f_logit_scale    = 0.0e+00
0.00.039.180 I print_info: n_ff             = 8192
0.00.039.180 I print_info: n_expert         = 0
0.00.039.180 I print_info: n_expert_used    = 0
0.00.039.180 I print_info: causal attn      = 1
0.00.039.180 I print_info: pooling type     = 0
0.00.039.181 I print_info: rope type        = 2
0.00.039.181 I print_info: rope scaling     = linear
0.00.039.181 I print_info: freq_base_train  = 10000.0
0.00.039.181 I print_info: freq_scale_train = 1
0.00.039.182 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.182 I print_info: rope_finetuned   = unknown
0.00.039.182 I print_info: ssm_d_conv       = 0
0.00.039.182 I print_info: ssm_d_inner      = 0
0.00.039.182 I print_info: ssm_d_state      = 0
0.00.039.182 I print_info: ssm_dt_rank      = 0
0.00.039.182 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.183 I print_info: model type       = 1.4B
0.00.039.183 I print_info: model params     = 1.41 B
0.00.039.183 I print_info: general.name     = 1.4B
0.00.039.183 I print_info: vocab type       = BPE
0.00.039.184 I print_info: n_vocab          = 50304
0.00.039.184 I print_info: n_merges         = 50009
0.00.039.184 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.184 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.184 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.184 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.185 I print_info: LF token         = 187 ''
0.00.039.185 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.185 I print_info: max token length = 1024
0.00.039.185 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.626.930 I load_tensors: offloading 24 repeating layers to GPU
0.00.626.936 I load_tensors: offloading output layer to GPU
0.00.626.937 I load_tensors: offloaded 25/25 layers to GPU
0.00.626.965 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.626.967 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.628.472 I llama_context_kv_self: n_seq_max     = 1
0.00.628.474 I llama_context_kv_self: n_ctx         = 128
0.00.628.475 I llama_context_kv_self: n_ctx_per_seq = 128
0.00.628.475 I llama_context_kv_self: n_batch       = 128
0.00.628.475 I llama_context_kv_self: n_ubatch      = 128
0.00.628.476 I llama_context_kv_self: flash_attn    = 0
0.00.628.477 I llama_context_kv_self: freq_base     = 10000.0
0.00.628.477 I llama_context_kv_self: freq_scale    = 1
0.00.628.478 W llama_context_kv_self: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.628.480 I ggml_metal_init: allocating
0.00.628.537 I ggml_metal_init: found device: Apple M4
0.00.628.550 I ggml_metal_init: picking default device: Apple M4
0.00.630.033 I ggml_metal_init: using embedded metal library
0.00.635.962 I ggml_metal_init: GPU name:   Apple M4
0.00.635.965 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.635.966 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.635.967 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.635.967 I ggml_metal_init: simdgroup reduction   = true
0.00.635.968 I ggml_metal_init: simdgroup matrix mul. = true
0.00.635.968 I ggml_metal_init: has residency sets    = true
0.00.635.968 I ggml_metal_init: has bfloat            = true
0.00.635.969 I ggml_metal_init: use bfloat            = true
0.00.635.970 I ggml_metal_init: hasUnifiedMemory      = true
0.00.635.974 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.652.829 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.656.352 I init:      Metal KV buffer size =    24.00 MiB
0.00.656.356 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.656.383 I llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
0.00.659.604 I llama_context_kv_self:      Metal compute buffer size =    25.56 MiB
0.00.659.606 I llama_context_kv_self:        CPU compute buffer size =     1.06 MiB
0.00.659.606 I llama_context_kv_self: graph nodes  = 967
0.00.659.607 I llama_context_kv_self: graph splits = 2
0.00.659.618 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.659.620 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.695.172 I 
0.00.695.263 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.695.285 I perplexity: tokenizing the input ..
0.00.702.288 I perplexity: tokenization took 7 ms
0.00.702.312 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.843.780 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.845.125 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.845.138 I llama_perf_context_print:        load time =     686.04 ms
0.00.845.139 I llama_perf_context_print: prompt eval time =     140.57 ms /   128 tokens (    1.10 ms per token,   910.55 tokens per second)
0.00.845.140 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.845.140 I llama_perf_context_print:       total time =     149.97 ms /   129 tokens
0.00.845.658 I ggml_metal_free: deallocating

real	0m0.860s
user	0m0.078s
sys	0m0.134s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4729 (e08f38df) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.009.899 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.681 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.685 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.687 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.687 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.688 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.688 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.688 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.689 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.690 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.690 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.690 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.691 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.691 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.693 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.696 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.697 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.697 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.537 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.547 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.346 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.347 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.347 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.348 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.348 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.348 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.349 I llama_model_loader: - type  f32:  194 tensors
0.00.026.349 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.350 I print_info: file format = GGUF V3 (latest)
0.00.026.350 I print_info: file type   = Q6_K
0.00.026.351 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.034.503 I load: special tokens cache size = 25
0.00.040.413 I load: token to piece cache size = 0.2984 MB
0.00.040.428 I print_info: arch             = gptneox
0.00.040.429 I print_info: vocab_only       = 0
0.00.040.429 I print_info: n_ctx_train      = 2048
0.00.040.429 I print_info: n_embd           = 2048
0.00.040.429 I print_info: n_layer          = 24
0.00.040.432 I print_info: n_head           = 16
0.00.040.433 I print_info: n_head_kv        = 16
0.00.040.435 I print_info: n_rot            = 32
0.00.040.435 I print_info: n_swa            = 0
0.00.040.436 I print_info: n_embd_head_k    = 128
0.00.040.436 I print_info: n_embd_head_v    = 128
0.00.040.437 I print_info: n_gqa            = 1
0.00.040.437 I print_info: n_embd_k_gqa     = 2048
0.00.040.438 I print_info: n_embd_v_gqa     = 2048
0.00.040.439 I print_info: f_norm_eps       = 1.0e-05
0.00.040.439 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.442 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.443 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.443 I print_info: f_logit_scale    = 0.0e+00
0.00.040.444 I print_info: n_ff             = 8192
0.00.040.445 I print_info: n_expert         = 0
0.00.040.445 I print_info: n_expert_used    = 0
0.00.040.445 I print_info: causal attn      = 1
0.00.040.445 I print_info: pooling type     = 0
0.00.040.445 I print_info: rope type        = 2
0.00.040.446 I print_info: rope scaling     = linear
0.00.040.448 I print_info: freq_base_train  = 10000.0
0.00.040.448 I print_info: freq_scale_train = 1
0.00.040.448 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.449 I print_info: rope_finetuned   = unknown
0.00.040.449 I print_info: ssm_d_conv       = 0
0.00.040.449 I print_info: ssm_d_inner      = 0
0.00.040.449 I print_info: ssm_d_state      = 0
0.00.040.449 I print_info: ssm_dt_rank      = 0
0.00.040.449 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.449 I print_info: model type       = 1.4B
0.00.040.450 I print_info: model params     = 1.41 B
0.00.040.450 I print_info: general.name     = 1.4B
0.00.040.450 I print_info: vocab type       = BPE
0.00.040.450 I print_info: n_vocab          = 50304
0.00.040.451 I print_info: n_merges         = 50009
0.00.040.452 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.452 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.452 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.452 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.452 I print_info: LF token         = 187 ''
0.00.040.453 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.453 I print_info: max token length = 1024
0.00.040.453 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.652.538 I load_tensors: offloading 24 repeating layers to GPU
0.00.652.542 I load_tensors: offloading output layer to GPU
0.00.652.543 I load_tensors: offloaded 25/25 layers to GPU
0.00.652.567 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.652.568 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.654.097 I llama_context_kv_self: n_seq_max     = 1
0.00.654.099 I llama_context_kv_self: n_ctx         = 2048
0.00.654.099 I llama_context_kv_self: n_ctx_per_seq = 2048
0.00.654.100 I llama_context_kv_self: n_batch       = 2048
0.00.654.100 I llama_context_kv_self: n_ubatch      = 512
0.00.654.101 I llama_context_kv_self: flash_attn    = 0
0.00.654.101 I llama_context_kv_self: freq_base     = 10000.0
0.00.654.102 I llama_context_kv_self: freq_scale    = 1
0.00.654.103 I ggml_metal_init: allocating
0.00.654.114 I ggml_metal_init: found device: Apple M4
0.00.654.122 I ggml_metal_init: picking default device: Apple M4
0.00.655.522 I ggml_metal_init: using embedded metal library
0.00.661.282 I ggml_metal_init: GPU name:   Apple M4
0.00.661.285 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.661.286 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.661.287 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.661.287 I ggml_metal_init: simdgroup reduction   = true
0.00.661.288 I ggml_metal_init: simdgroup matrix mul. = true
0.00.661.288 I ggml_metal_init: has residency sets    = true
0.00.661.288 I ggml_metal_init: has bfloat            = true
0.00.661.288 I ggml_metal_init: use bfloat            = true
0.00.661.289 I ggml_metal_init: hasUnifiedMemory      = true
0.00.661.290 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.680.144 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.737.833 I init:      Metal KV buffer size =   384.00 MiB
0.00.737.838 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.737.859 I llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
0.00.741.873 I llama_context_kv_self:      Metal compute buffer size =   102.25 MiB
0.00.741.876 I llama_context_kv_self:        CPU compute buffer size =     8.01 MiB
0.00.741.876 I llama_context_kv_self: graph nodes  = 967
0.00.741.876 I llama_context_kv_self: graph splits = 2
0.00.741.882 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.742.010 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.742.011 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.811.138 I main: llama threadpool init, n_threads = 4
0.00.811.184 I 
0.00.811.207 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.811.208 I 
0.00.811.362 I sampler seed: 1234
0.00.811.367 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.811.378 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.811.378 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.811.378 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.694.985 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53064.28 tokens per second)
0.01.694.986 I llama_perf_context_print:        load time =     800.49 ms
0.01.694.986 I llama_perf_context_print: prompt eval time =      54.49 ms /     7 tokens (    7.78 ms per token,   128.47 tokens per second)
0.01.694.988 I llama_perf_context_print:        eval time =     826.16 ms /    63 runs   (   13.11 ms per token,    76.26 tokens per second)
0.01.694.989 I llama_perf_context_print:       total time =     884.60 ms /    70 tokens
0.01.698.924 I ggml_metal_free: deallocating

real	0m1.717s
user	0m0.111s
sys	0m0.229s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4729 (e08f38df) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.825 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.901 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.907 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.908 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.913 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.913 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.914 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.914 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.915 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.915 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.916 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.916 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.916 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.917 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.917 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.919 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.919 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.920 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.733 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.786 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.574 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.576 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.576 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.577 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.577 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.577 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.578 I llama_model_loader: - type  f32:  194 tensors
0.00.025.578 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.579 I print_info: file format = GGUF V3 (latest)
0.00.025.579 I print_info: file type   = Q6_K
0.00.025.580 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.684 I load: special tokens cache size = 25
0.00.039.623 I load: token to piece cache size = 0.2984 MB
0.00.039.638 I print_info: arch             = gptneox
0.00.039.639 I print_info: vocab_only       = 0
0.00.039.639 I print_info: n_ctx_train      = 2048
0.00.039.640 I print_info: n_embd           = 2048
0.00.039.640 I print_info: n_layer          = 24
0.00.039.644 I print_info: n_head           = 16
0.00.039.644 I print_info: n_head_kv        = 16
0.00.039.648 I print_info: n_rot            = 32
0.00.039.648 I print_info: n_swa            = 0
0.00.039.648 I print_info: n_embd_head_k    = 128
0.00.039.648 I print_info: n_embd_head_v    = 128
0.00.039.649 I print_info: n_gqa            = 1
0.00.039.651 I print_info: n_embd_k_gqa     = 2048
0.00.039.651 I print_info: n_embd_v_gqa     = 2048
0.00.039.652 I print_info: f_norm_eps       = 1.0e-05
0.00.039.652 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.652 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.653 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.653 I print_info: f_logit_scale    = 0.0e+00
0.00.039.653 I print_info: n_ff             = 8192
0.00.039.653 I print_info: n_expert         = 0
0.00.039.653 I print_info: n_expert_used    = 0
0.00.039.654 I print_info: causal attn      = 1
0.00.039.654 I print_info: pooling type     = 0
0.00.039.654 I print_info: rope type        = 2
0.00.039.654 I print_info: rope scaling     = linear
0.00.039.654 I print_info: freq_base_train  = 10000.0
0.00.039.655 I print_info: freq_scale_train = 1
0.00.039.655 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.655 I print_info: rope_finetuned   = unknown
0.00.039.655 I print_info: ssm_d_conv       = 0
0.00.039.655 I print_info: ssm_d_inner      = 0
0.00.039.655 I print_info: ssm_d_state      = 0
0.00.039.655 I print_info: ssm_dt_rank      = 0
0.00.039.656 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.656 I print_info: model type       = 1.4B
0.00.039.656 I print_info: model params     = 1.41 B
0.00.039.656 I print_info: general.name     = 1.4B
0.00.039.658 I print_info: vocab type       = BPE
0.00.039.658 I print_info: n_vocab          = 50304
0.00.039.658 I print_info: n_merges         = 50009
0.00.039.658 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.659 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.660 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.660 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.660 I print_info: LF token         = 187 ''
0.00.039.660 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.660 I print_info: max token length = 1024
0.00.039.661 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.258.773 I load_tensors: offloading 24 repeating layers to GPU
0.00.258.785 I load_tensors: offloading output layer to GPU
0.00.258.786 I load_tensors: offloaded 25/25 layers to GPU
0.00.258.813 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.258.814 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.260.368 I llama_context_kv_self: n_seq_max     = 1
0.00.260.373 I llama_context_kv_self: n_ctx         = 128
0.00.260.373 I llama_context_kv_self: n_ctx_per_seq = 128
0.00.260.374 I llama_context_kv_self: n_batch       = 128
0.00.260.374 I llama_context_kv_self: n_ubatch      = 128
0.00.260.375 I llama_context_kv_self: flash_attn    = 0
0.00.260.377 I llama_context_kv_self: freq_base     = 10000.0
0.00.260.377 I llama_context_kv_self: freq_scale    = 1
0.00.260.378 W llama_context_kv_self: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.260.386 I ggml_metal_init: allocating
0.00.260.442 I ggml_metal_init: found device: Apple M4
0.00.260.455 I ggml_metal_init: picking default device: Apple M4
0.00.262.163 I ggml_metal_init: using embedded metal library
0.00.269.168 I ggml_metal_init: GPU name:   Apple M4
0.00.269.173 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.269.174 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.269.175 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.269.176 I ggml_metal_init: simdgroup reduction   = true
0.00.269.176 I ggml_metal_init: simdgroup matrix mul. = true
0.00.269.176 I ggml_metal_init: has residency sets    = true
0.00.269.177 I ggml_metal_init: has bfloat            = true
0.00.269.177 I ggml_metal_init: use bfloat            = true
0.00.269.178 I ggml_metal_init: hasUnifiedMemory      = true
0.00.269.188 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.287.691 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.291.222 I init:      Metal KV buffer size =    24.00 MiB
0.00.291.226 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.291.254 I llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
0.00.294.439 I llama_context_kv_self:      Metal compute buffer size =    25.56 MiB
0.00.294.440 I llama_context_kv_self:        CPU compute buffer size =     1.06 MiB
0.00.294.441 I llama_context_kv_self: graph nodes  = 967
0.00.294.441 I llama_context_kv_self: graph splits = 2
0.00.294.446 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.294.448 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.332.330 I 
0.00.332.419 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.332.444 I perplexity: tokenizing the input ..
0.00.338.563 I perplexity: tokenization took 6.117 ms
0.00.338.579 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.478.004 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.479.523 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.479.539 I llama_perf_context_print:        load time =     322.50 ms
0.00.479.540 I llama_perf_context_print: prompt eval time =     139.07 ms /   128 tokens (    1.09 ms per token,   920.40 tokens per second)
0.00.479.540 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.479.541 I llama_perf_context_print:       total time =     147.21 ms /   129 tokens
0.00.480.060 I ggml_metal_free: deallocating

real	0m0.496s
user	0m0.077s
sys	0m0.092s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4729 (e08f38df)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_context_kv_self: n_seq_max     = 1
llama_context_kv_self: n_ctx         = 2048
llama_context_kv_self: n_ctx_per_seq = 2048
llama_context_kv_self: n_batch       = 2048
llama_context_kv_self: n_ubatch      = 512
llama_context_kv_self: flash_attn    = 0
llama_context_kv_self: freq_base     = 10000.0
llama_context_kv_self: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11e508d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11e509430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11e5099e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11e509f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11e50a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11e50aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11e50b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11e50b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11e50bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11e50c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11e50c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11e50cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11e50d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11e50ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11e50e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11e50ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11e50f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11e50fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11e510260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11e510a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11e511150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11e511870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11e511f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11e512830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11e512f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11e513210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11e513820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11e514490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11e5149d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11e514c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11e515130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11e5153f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11e515c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11e5161c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11e516480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11e516920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11e516dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11e517260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11e517700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11e517ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11e518040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11e5184e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11e518980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11e518e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11e5190e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11e5196f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11e519d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11e51a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11e51ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11e51b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11e51b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11e51be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11e51c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11e51ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11e51d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11e51d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11e51dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11e51de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11e51e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11e51ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11e51ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11e51f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11e51f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11e51fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11e5201b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11e520650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11e520af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11e520f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11e521430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11e5218d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11e521d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11e522210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11e5226b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11e522c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11e523150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11e5236a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11e523bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11e524140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11e524690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11e524be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11e525130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11e525680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11e525bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11e526120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11e526670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11e526bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11e527110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11e527660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11e527bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11e528100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11e528650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11e528ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11e5290f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11e529640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11e529b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11e52a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11e52a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11e51a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11e52aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11e52b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11e52b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11e52bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11e52c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11e52c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11e52cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11e52d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11e52d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11e52dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11e52e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11e52e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11e52ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11e52f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11e52f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11e52fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11e5300a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11e530540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11e5309e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11e530e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11e531320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11e5317c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11e531c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11e532100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11e5325a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11e532a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11e532ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11e533380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11e533820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11e533cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11e534160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11e534600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11e534aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11e534f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11e5353e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11e535880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11e535d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11e5361c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11e536660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11e536b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11e536fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11e537440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11e5378e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11e537d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11e538220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11e5386c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11e538b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11e539000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11e5394a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11e539940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11e539de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11e53a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11e53a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11e53abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11e53b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11e53b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11e53b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11e53be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11e53c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11e53c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11e53cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11e53d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11e53d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11e53da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11e53dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11e53e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11e53e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11e53ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11e53f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11e53f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11e53fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11e53ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11e5403a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11e540840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11e540ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11e541180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11e541620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11e541ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11e541f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11e542400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11e5428a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11e542d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11e5431e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11e543680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11e543b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11e543fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11e544460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11e544900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11e544da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11e545240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11e5456e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11e545b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11e546020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11e5464c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11e546960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11e546eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11e547400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11e547950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11e547ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11e548160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11e548770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11e548d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11e549390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11e549b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11e54a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11e54a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11e54a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11e54af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11e54b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11e54bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11e54c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11e54c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11e54cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11e54d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11e54d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11e54dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11e54e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11e54e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11e54ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11e54f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11e54f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11e54fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11e5501a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11e5506f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11e550c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11e551190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11e5516e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11e551c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11e552180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11e5526d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11e552c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11e553170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11e5536c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11e553c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11e554160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11e5546b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11e554c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11e555150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11e5556a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11e555bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11e556140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11e556690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11e556be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11e557130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11e557680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11e557bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11e558120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11e558670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11e558bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11e559110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11e559660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11e559bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11e55a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11e55a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11e55aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11e55b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11e55b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11e55bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11e55c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11e55c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11e55cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11e55d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11e55d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11e55db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11e55e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11e55e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11e55eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11e55f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11e55f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11e55faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11e55ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11e5603e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11e560880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11e560d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11e5611c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11e561660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11e561b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11e561fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11e562440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11e5628e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11e562d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11e563220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11e5636c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11e563b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11e5640b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11e5647d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11e564ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11e565610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11e565d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11e565ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11e5667e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11e566aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11e5670b0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self:      Metal compute buffer size =   102.25 MiB
llama_context_kv_self:        CPU compute buffer size =     8.01 MiB
llama_context_kv_self: graph nodes  = 967
llama_context_kv_self: graph splits = 2
0.00.696.828 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.696.832 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_context_kv_self: n_seq_max     = 1
llama_context_kv_self: n_ctx         = 2048
llama_context_kv_self: n_ctx_per_seq = 2048
llama_context_kv_self: n_batch       = 2048
llama_context_kv_self: n_ubatch      = 512
llama_context_kv_self: flash_attn    = 0
llama_context_kv_self: freq_base     = 10000.0
llama_context_kv_self: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x104c089a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x104c08e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x104c09280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x104c096f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x104c09b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x104c09fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x104c0a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x104c0a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x104c0ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x104c0b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x104c0b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x104c0bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x104c0c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x104c0cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x104c0d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x104c0dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x104c0e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x104c0ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x104c0f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x104c0fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x104c10310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x104c10a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x104c11150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x104c11870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x104c11f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x104c12250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x104c12510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x104c12980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x104c12df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x104c13260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x104c13760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x104c13c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x104c140e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x104c143a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x104c14810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x104c14c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x104c151e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x104c156e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x104c15be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x104c160e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x104c165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x104c16ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x104c16fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x104c174e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x104c179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x104c17e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x104c182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x104c18730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x104c18ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x104c19010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x104c19480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x104c198f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x104c19d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x104c1a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x104c1a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x104c1ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x104c1b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x104c1b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x104c1bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x104c1c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x104c1c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x104c1ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x104c1d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x104c1d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x104c1da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x104c1df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x104c1e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x104c1e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x104c1ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x104c1f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x104c1f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x104c1faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x104c1ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x104c204e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x104c20a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x104c20f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x104c214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x104c21a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x104c21f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x104c224c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x104c22a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x104c22f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x104c234b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x104c23a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x104c23f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x104c244a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x104c249f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x104c24f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x104c25490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x104c259e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x104c25f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x104c26480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x104c269d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x104c26f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x104c27470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x104c279c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x104c27f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x104c28460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x104c289b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x104c28f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x104c29450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x104c299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x104c29ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x104c2a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x104c2a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x104c2aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x104c2b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x104c2b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x104c2bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x104c2c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x104c2c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x104c2cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x104c2d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x104c2d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x104c2dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x104c2e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x104c2e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x104c2eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x104c2efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x104c2f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x104c2f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x104c2fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x104c30250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x104c306f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x104c30b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x104c31030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x104c314d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x104c31970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x104c31e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x104c322b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x104c32750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x104c32bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x104c33090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x104c33530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x104c339d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x104c33e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x104c34310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x104c347b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x104c34c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x104c350f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x104c35590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x104c35a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x104c35ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x104c36370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x104c36810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x104c36cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x104c37150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x104c375f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x104c37a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x104c37f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x104c383d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x104c38870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x104c38d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x104c391b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x104c39650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x104c39af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x104c39f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x104c3a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x104c3a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x104c3ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x104c3b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x104c3b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x104c3bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x104c3bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x104c3c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x104c3c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x104c3cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x104c3d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x104c3d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x104c3dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x104c3e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x104c3e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x104c3e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x104c3ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x104c3f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x104c3f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x104c3fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x104c400b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x104c40550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x104c409f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x104c40e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x104c41330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x104c417d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x104c41c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x104c42110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x104c425b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x104c42a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x104c42ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x104c43390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x104c43830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x104c43cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x104c44170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x104c44610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x104c44b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x104c450b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x104c45600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x104c45b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x104c45e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x104c46420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x104c46a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x104c47040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x104c47830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x104c47cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x104c47f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x104c485a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x104c48bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x104c493a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x104c49840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x104c49ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x104c4a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x104c4a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x104c4ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x104c4b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x104c4b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x104c4be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x104c4c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x104c4c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x104c4ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x104c4d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x104c4d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x104c4de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x104c4e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x104c4e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x104c4ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x104c4f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x104c4f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x104c4fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x104c50380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x104c508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x104c50e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x104c51370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x104c518c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x104c51e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x104c52360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x104c528b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x104c52e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x104c53350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x104c538a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x104c53df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x104c54340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x104c54890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x104c54de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x104c55330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x104c55880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x104c55dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x104c56320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x104c56870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x104c56dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x104c57310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x104c57860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x104c57db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x104c58300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x104c58850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x104c58da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x104c592f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x104c59840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x104c59d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x104c5a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x104c5a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x104c5ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x104c5b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x104c5b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x104c5bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x104c5c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x104c5c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x104c5cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x104c5d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x104c5d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x104c5dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x104c5e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x104c5e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x104c5e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x104c5ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x104c5f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x104c5f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x104c5fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x104c600f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x104c60590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x104c60a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x104c60ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x104c61370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x104c61810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x104c61d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x104c62480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x104c62ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x104c632c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x104c639e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x104c63ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x104c64490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x104c64750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x104c64d60 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self:      Metal compute buffer size =   102.25 MiB
llama_context_kv_self:        CPU compute buffer size =     8.01 MiB
llama_context_kv_self: graph nodes  = 967
llama_context_kv_self: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_context_kv_self: n_seq_max     = 1
llama_context_kv_self: n_ctx         = 2048
llama_context_kv_self: n_ctx_per_seq = 2048
llama_context_kv_self: n_batch       = 2048
llama_context_kv_self: n_ubatch      = 512
llama_context_kv_self: flash_attn    = 0
llama_context_kv_self: freq_base     = 10000.0
llama_context_kv_self: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x104f044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x104f04950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x104f04dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x104f05230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x104f056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x104f05b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x104f05f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x104f063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x104f06860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x104f06db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x104f07220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x104f078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x104f083c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x104f08b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x104f09380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x104f09aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x104f0a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x104f0a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x104f0b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x104f0b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x104f0bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x104f0c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x104f0cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x104f0d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x104f0db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x104f0de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x104f0e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x104f0e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x104f0e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x104f0ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x104f0f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x104f0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x104f0fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x104f0ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x104f10380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x104f107f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x104f10c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x104f110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x104f11540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x104f119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x104f11e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x104f12290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x104f12700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x104f12b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x104f12fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x104f13450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x104f138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x104f13d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x104f141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x104f14610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x104f14a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x104f14ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x104f15360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x104f157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x104f15c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x104f160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x104f16620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x104f16b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x104f16f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x104f17400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x104f17870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x104f17ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x104f18150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x104f185c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x104f18a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x104f18ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x104f19310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x104f19780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x104f19bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x104f1a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x104f1a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x104f1a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x104f1adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x104f1b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x104f1b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x104f1bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x104f1bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x104f1c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x104f1c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x104f1ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x104f1d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x104f1d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x104f1da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x104f1de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x104f1e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x104f1e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x104f1ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x104f1f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x104f1f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x104f1f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x104f1fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x104f20200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x104f20670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x104f20ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x104f20f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x104f213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x104f21830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x104f21ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x104f22110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x104f22580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x104f229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x104f22e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x104f232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x104f23b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x104f23e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x104f24290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x104f24700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x104f24b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x104f24fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x104f25450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x104f258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x104f25d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x104f261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x104f26610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x104f26a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x104f26ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x104f27360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x104f277d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x104f27c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x104f280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x104f28520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x104f28990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x104f28e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x104f29270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x104f296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x104f29b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x104f29fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x104f2a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x104f2a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x104f2ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x104f2b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x104f2b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x104f2ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x104f2bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x104f2c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x104f2c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x104f2cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x104f2d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x104f2d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x104f2d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x104f2dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x104f2e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x104f2e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x104f2eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x104f2efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x104f2f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x104f2f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x104f2fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x104f30160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x104f305d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x104f30a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x104f30eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x104f31320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x104f31790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x104f31c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x104f32070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x104f324e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x104f32950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x104f32dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x104f33230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x104f336a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x104f33b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x104f33f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x104f343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x104f34860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x104f34cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x104f35140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x104f355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x104f35a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x104f35e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x104f36300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x104f36770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x104f36be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x104f37050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x104f374c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x104f37930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x104f37da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x104f38210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x104f38680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x104f38af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x104f38f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x104f393d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x104f39840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x104f39cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x104f3a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x104f3a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x104f3aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x104f3ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x104f3b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x104f3b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x104f3bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x104f3c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x104f3c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x104f3c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x104f3cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x104f3d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x104f3d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x104f3dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x104f3df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x104f3e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x104f3e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x104f3ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x104f3f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x104f3f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x104f3f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x104f3fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x104f402c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x104f40730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x104f40ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x104f41010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x104f41b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x104f41e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x104f42110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x104f42580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x104f429f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x104f42e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x104f432d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x104f43740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x104f43bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x104f44020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x104f44490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x104f44900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x104f44d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x104f451e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x104f45650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x104f45ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x104f45f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x104f463a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x104f46810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x104f46c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x104f470f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x104f47560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x104f479d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x104f47e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x104f482b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x104f48720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x104f48b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x104f49000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x104f49470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x104f498e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x104f49d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x104f4a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x104f4a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x104f4aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x104f4af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x104f4b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x104f4b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x104f4bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x104f4c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x104f4c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x104f4c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x104f4ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x104f4d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x104f4d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x104f4db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x104f4dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x104f4e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x104f4e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x104f4ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x104f4f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x104f4f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x104f4fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x104f4fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x104f50360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x104f507d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x104f50c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x104f510b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x104f51520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x104f51990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x104f51e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x104f52270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x104f526e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x104f52b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x104f52fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x104f53430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x104f538a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x104f53d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x104f54180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x104f545f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x104f54a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x104f54ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x104f55340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x104f557b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x104f56220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x104f56940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x104f57060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x104f57780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x104f57a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x104f57eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x104f584b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x104f58ac0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self:      Metal compute buffer size =   102.25 MiB
llama_context_kv_self:        CPU compute buffer size =     8.01 MiB
llama_context_kv_self: graph nodes  = 967
llama_context_kv_self: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.755s
user	0m0.278s
sys	0m0.320s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4729 (e08f38df)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_context_kv_self: n_seq_max     = 1
llama_context_kv_self: n_ctx         = 2048
llama_context_kv_self: n_ctx_per_seq = 2048
llama_context_kv_self: n_batch       = 2048
llama_context_kv_self: n_ubatch      = 512
llama_context_kv_self: flash_attn    = 1
llama_context_kv_self: freq_base     = 10000.0
llama_context_kv_self: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12a710580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12a710c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12a711240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12a7117f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12a711da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12a712350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12a712900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12a712eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12a713460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12a713960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12a713e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12a714360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12a714e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12a715630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12a715e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12a716560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12a716c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12a7173a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12a717ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12a718290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12a7189b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12a7190d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12a7197f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12a71a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12a71a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12a71aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12a71b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12a71bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12a71c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12a71c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12a71c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12a71cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12a71d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12a71da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12a71dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12a71e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12a71e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12a71eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12a71ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12a71f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12a71f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12a71fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12a7201e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12a720680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12a720940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12a720f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12a721560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12a721e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12a722490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12a722aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12a7230b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12a7236c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12a723cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12a7242e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12a724ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12a724f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12a725410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12a7256d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12a725ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12a7264d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12a726790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12a726c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12a7270d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12a727570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12a727a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12a727eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12a728350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12a7287f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12a728c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12a729130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12a7295d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12a729a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12a729f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12a72a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12a72a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12a72af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12a72b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12a72b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12a72bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12a72c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12a72c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12a72cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12a72d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12a72d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12a72ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12a72e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12a72e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12a72eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12a72f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12a72f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12a72feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12a730400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12a730950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12a730ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12a7313f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12a731940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12a731e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12a721b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12a732300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12a732ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12a733000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12a733550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12a733aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12a733ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12a734540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12a734a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12a734fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12a735530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12a735a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12a735fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12a736520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12a736a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12a736fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12a737460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12a737900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12a737da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12a738240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12a7386e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12a738b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12a739020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12a7394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12a739960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12a739e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12a73a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12a73a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12a73abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12a73b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12a73b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12a73b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12a73be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12a73c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12a73c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12a73cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12a73d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12a73d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12a73da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12a73dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12a73e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12a73e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12a73eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12a73f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12a73f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12a73fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12a73ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12a7403c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12a740860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12a740d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12a7411a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12a741640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12a741ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12a741f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12a742420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12a7428c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12a742d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12a743200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12a7436a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12a743b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12a743fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12a744480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12a744920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12a744dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12a745260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12a745700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12a745ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12a746040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12a7464e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12a746980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12a746e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12a7472c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12a747760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12a747c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12a7480a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12a748540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12a7489e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12a748e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12a749320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12a7497c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12a749c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12a74a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12a74a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12a74aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12a74aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12a74b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12a74b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12a74bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12a74c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12a74c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12a74caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12a74cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12a74d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12a74d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12a74dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12a74e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12a74e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12a74ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12a74f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12a74f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12a74f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12a74ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12a7505e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12a750bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12a7513e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12a751880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12a751b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12a752150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12a752760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12a752f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12a7533f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12a753890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12a753d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12a7544e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12a754a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12a754f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12a7554d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12a755a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12a755f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12a7564c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12a756a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12a756f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12a7574b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12a757a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12a757f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12a7584a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12a7589f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12a758f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12a759490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12a7599e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12a759f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12a75a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12a75a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12a75af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12a75b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12a75b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12a75bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12a75c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12a75c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12a75cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12a75d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12a75d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12a75def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12a75e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12a75e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12a75eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12a75f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12a75f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12a75fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12a760420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12a760970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12a760ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12a761410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12a761960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12a761eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12a762400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12a762950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12a762ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12a7633f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12a763940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12a763e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12a7643e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12a764930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12a764e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12a7653d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12a765920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12a765e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12a7663c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12a766910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12a766e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12a767300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12a7677a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12a767c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12a7680e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12a768580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12a768a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12a768ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12a769360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12a769800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12a769ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12a76a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12a76a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12a76aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12a76af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12a76b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12a76b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12a76c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12a76c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12a76ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12a76d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12a76d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12a76e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12a76e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12a76e910 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self:      Metal compute buffer size =   102.25 MiB
llama_context_kv_self:        CPU compute buffer size =     8.01 MiB
llama_context_kv_self: graph nodes  = 872
llama_context_kv_self: graph splits = 2
0.00.098.834 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.098.837 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_context_kv_self: n_seq_max     = 1
llama_context_kv_self: n_ctx         = 2048
llama_context_kv_self: n_ctx_per_seq = 2048
llama_context_kv_self: n_batch       = 2048
llama_context_kv_self: n_ubatch      = 512
llama_context_kv_self: flash_attn    = 1
llama_context_kv_self: freq_base     = 10000.0
llama_context_kv_self: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x175304bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x175305040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1753054b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x175305920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x175305d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x175306200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x175306670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x175306ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x175306f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1753073c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x175307830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x175307f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x175308a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1753091f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x175309a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x17530a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x17530a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x17530af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x17530b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x17530bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x17530c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x17530cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x17530d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x17530da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x17530e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x17530e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x17530e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x17530eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x17530efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x17530f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x17530f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x17530fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x175310230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1753104f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x175310960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x175310dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x175311240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1753116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x175311b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x175311f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x175312400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x175312870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x175312ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x175313150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1753135c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x175313a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x175313ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x175314310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x175314780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x175314bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x175315060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1753154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x175315940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x175315db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x175316220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x175316690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x175316c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x175317100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x175317570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1753179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x175317e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1753182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x175318730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x175318ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x175319010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x175319480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1753198f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x175319d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x17531a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x17531a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x17531aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x17531af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x17531b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x17531b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x17531bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x17531c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x17531c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x17531c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x17531ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x17531d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x17531d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x17531db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x17531dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x17531e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x17531e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x17531ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x17531f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x17531f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x17531fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x17531ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x175320370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1753207e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x175320c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1753210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x175321530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1753219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x175321e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x175322280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1753226f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x175322b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x175322fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x175323440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1753238b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x175323d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x175324190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x175324600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x175324a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x175324ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x175325350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1753257c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x175325c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1753260a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x175326510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x175326980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x175326df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x175327260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1753276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x175327b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x175327fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x175328420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x175328890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x175328d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x175329170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1753295e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x175329a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x175329ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x17532a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x17532a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x17532ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x17532b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x17532b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x17532b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x17532bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x17532c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x17532c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x17532cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x17532cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x17532d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x17532d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x17532dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x17532e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x17532e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x17532ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x17532eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x17532f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x17532f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x17532fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x175330060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1753304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x175330940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x175330db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x175331220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x175331690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x175331b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x175331f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1753323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x175332850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x175332cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x175333130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1753335a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x175333a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x175333e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1753342f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x175334760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x175334bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x175335040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x175335c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x175335f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1753361f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x175336660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x175336ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x175336f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1753373b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x175337820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x175337c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x175338100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x175338570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1753389e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x175338e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1753392c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x175339730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x175339ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x17533a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x17533a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x17533a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x17533ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x17533b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x17533b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x17533bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x17533bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x17533c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x17533c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x17533cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x17533d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x17533d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x17533d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x17533de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x17533e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x17533e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x17533eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x17533eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x17533f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x17533f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x17533fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x175340340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1753407b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x175340c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x175341090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1753415b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x175341ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x175342630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1753428f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x175342eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x175343470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x175343a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x175343ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1753445b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x175344b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x175345130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1753456f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x175345cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x175346270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x175346830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x175346df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1753473b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x175347970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x175347f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1753484f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x175348ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x175349070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x175349630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x175349bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x17534a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x17534a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x17534ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x17534b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x17534b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x17534be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x17534c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x17534c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x17534cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x17534d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x17534db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x17534e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x17534e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x17534ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x17534f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x17534f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x17534fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x175350370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x175350930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x175350ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1753514b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x175351a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x175352030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1753525f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x175352bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x175353170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x175353730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x175353cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1753542b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x175354870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x175354e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1753553f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1753559b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x175355f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x175356530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x175356af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x175356ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1753574f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1753579f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x175357ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1753583f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1753588f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x175358df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1753592f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1753597f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x175359cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x17535a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x17535a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x17535abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x17535b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x17535b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x17535c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x17535c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x17535ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x17535d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x17535d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x17535e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x17535e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x17535e8e0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self:      Metal compute buffer size =   102.25 MiB
llama_context_kv_self:        CPU compute buffer size =     8.01 MiB
llama_context_kv_self: graph nodes  = 872
llama_context_kv_self: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_context_kv_self: n_seq_max     = 1
llama_context_kv_self: n_ctx         = 2048
llama_context_kv_self: n_ctx_per_seq = 2048
llama_context_kv_self: n_batch       = 2048
llama_context_kv_self: n_ubatch      = 512
llama_context_kv_self: flash_attn    = 1
llama_context_kv_self: freq_base     = 10000.0
llama_context_kv_self: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12a76e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12a750290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12a74fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12a7508a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12a723980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12a723370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12a725990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12a752410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12a71ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12a721820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12a722140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12a722750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12a720c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12a722d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12a719d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12a725fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12a7325c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12a76db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12a71cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12a71d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12a752a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12a750eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12a71b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12a71b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12a71b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12a76ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12a76f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12a76f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12a76f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12a76f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12a76fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12a76fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12a7700b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12a770370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12a770630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12a7708f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12a770bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12a770e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12a771130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12a7713f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12a7716b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12a771970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12a771c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12a771ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12a7721b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12a772470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12a772730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12a7729f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12a772cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12a772f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12a773230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12a7734f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12a7737b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12a773a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12a773d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12a773ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12a7742b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12a774570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12a774830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12a774af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12a774db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12a775070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12a775330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12a7755f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12a7758b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12a775b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12a775e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12a7760f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12a7763b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12a776670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12a776930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12a776bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12a776eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12a777170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12a777430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12a7776f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12a7779b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12a777c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12a777f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12a7781f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12a7784b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12a778770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12a778a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12a778cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12a778fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12a779270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12a779530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12a7797f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12a779ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12a779d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12a77a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12a77a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12a77a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12a77a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12a77ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12a77adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12a77b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12a77b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12a77b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12a77b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12a77bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12a77be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12a77c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12a77c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12a77c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12a77c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12a77cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12a77cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12a77d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12a77d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12a77d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12a77d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12a77dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12a77df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12a77e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12a77e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12a77e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12a77ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12a77ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12a77eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12a77f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12a77f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12a77f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12a77faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12a77fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12a780070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12a780330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12a7805f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12a7808b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12a780b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12a780e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12a7810f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12a7813b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12a781670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12a781930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12a781bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12a781eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12a782170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12a782430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12a7826f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12a7829b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12a782c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12a782f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12a7831f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12a7834b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12a783770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12a783a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12a783cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12a783fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12a784270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12a784530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12a7847f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12a784ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12a784d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12a785030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12a7852f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12a7855b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12a785870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12a785b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12a785df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12a7860b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12a786370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12a786630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12a7868f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12a786bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12a786e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12a787130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12a7873f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12a7876b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12a787970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12a787c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12a787ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12a7881b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12a788470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12a788730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12a7889f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12a788cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12a788f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12a789230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12a7894f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12a7897b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12a789a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12a789d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12a789ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12a78a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12a78a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12a78a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12a78aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12a78adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12a78b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12a78b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12a78b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12a78b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12a78bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12a78be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12a78c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12a78c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12a78c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12a78c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12a78cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12a78ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12a78d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12a78d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12a78d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12a78d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12a78dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12a78df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12a78e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12a78e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12a78ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12a78f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12a78f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12a78f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12a78fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12a7900a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12a790510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12a790980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12a790df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12a791260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12a7916d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12a791b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12a791fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12a792420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12a792890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12a792d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12a793170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12a7935e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12a793a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12a793ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12a794330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12a7947a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12a794c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12a795080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12a7954f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12a795960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12a795dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12a796240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12a7966b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12a796b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12a796f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12a797400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12a797870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12a797ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12a798150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12a7985c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12a798a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12a798ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12a799310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12a799780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12a799bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12a79a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12a79a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12a79a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12a79adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12a79b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12a79b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12a79bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12a79bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12a79c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12a79c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12a79ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12a79d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12a79d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12a79da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12a79de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12a79e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12a79e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12a79ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12a79f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12a79f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12a79f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12a79fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12a7a0200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12a7a0670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12a7a0ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12a7a0f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12a7a13c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12a7a1830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12a7a1ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12a7a2110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12a7a2580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12a7a29f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12a7a2e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12a7a38d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12a7a3ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12a7a4710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12a7a4e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12a7a50f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12a7a58e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12a7a5ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12a7a61b0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self:      Metal compute buffer size =   102.25 MiB
llama_context_kv_self:        CPU compute buffer size =     8.01 MiB
llama_context_kv_self: graph nodes  = 872
llama_context_kv_self: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.952s
user	0m0.232s
sys	0m0.187s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.47 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.80 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   2.26 sec*proc (2 tests)

Total Test time (real) =   2.27 sec
        2.30 real         0.52 user         0.27 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.24 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.31 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.55 sec*proc (2 tests)

Total Test time (real) =   0.56 sec
        0.57 real         0.13 user         0.09 sys
```
