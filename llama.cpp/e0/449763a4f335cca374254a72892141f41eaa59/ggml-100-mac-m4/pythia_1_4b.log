Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:312 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.5s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.780s
user	0m0.897s
sys	0m1.263s
++ nproc
+ make -j10
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  5%] Built target build_info
[  5%] Built target sha256
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target xxhash
[  5%] Built target sha1
[  6%] Linking CXX shared library ../../bin/libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  7%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-cpu
[ 13%] Built target ggml-blas
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 17%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 17%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 23%] Linking CXX executable ../../bin/llama-gguf
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Built target llama-gguf
[ 26%] Linking CXX shared library ../bin/libllama.dylib
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama
[ 26%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 26%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 27%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 28%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 28%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 30%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 33%] Linking C executable ../bin/test-c
[ 33%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-simple
[ 33%] Built target llava
[ 33%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 35%] Linking CXX executable ../../bin/llama-quantize-stats
[ 35%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 35%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Linking CXX static library libllava_static.a
[ 37%] Linking CXX static library libcommon.a
[ 37%] Built target llama-simple
[ 37%] Built target llama-simple-chat
[ 37%] Built target llama-quantize-stats
[ 37%] Built target test-c
[ 37%] Built target llava_static
[ 37%] Built target common
[ 37%] Built target llava_shared
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-0
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Linking CXX executable ../bin/test-sampling
[ 47%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Linking CXX executable ../bin/test-arg-parser
[ 49%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 49%] Linking CXX executable ../bin/test-grammar-parser
[ 49%] Built target test-tokenizer-0
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Built target test-sampling
[ 49%] Built target test-grammar-integration
[ 49%] Built target test-llama-grammar
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 49%] Built target test-log
[ 49%] Built target test-grammar-parser
[ 49%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Built target test-json-schema-to-grammar
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 51%] Built target test-arg-parser
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 54%] Linking CXX executable ../bin/test-chat-template
[ 54%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 55%] Linking CXX executable ../bin/test-gguf
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 56%] Linking CXX executable ../bin/test-backend-ops
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 57%] Linking CXX executable ../bin/test-model-load-cancel
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 58%] Linking CXX executable ../bin/test-autorelease
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-barrier
[ 60%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 61%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 61%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Built target test-chat-template
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Built target test-gguf
[ 62%] Built target test-backend-ops
[ 62%] Built target test-model-load-cancel
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-barrier
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 63%] Linking CXX executable ../../bin/llama-batched-bench
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 64%] Built target test-autorelease
[ 65%] Linking CXX executable ../bin/test-rope
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 67%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Built target test-quantize-fns
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 69%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Built target llama-batched-bench
[ 69%] Built target test-quantize-perf
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 69%] Built target test-rope
[ 70%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Built target llama-gbnf-validator
[ 71%] Built target llama-eval-callback
[ 71%] Built target llama-batched
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Built target llama-embedding
[ 72%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-gguf-split
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 77%] Built target llama-gritlm
[ 77%] Linking CXX executable ../../bin/llama-lookahead
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Built target llama-infill
[ 77%] Built target llama-imatrix
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 78%] Linking CXX executable ../../bin/llama-lookup-stats
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Built target llama-bench
[ 78%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 78%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 78%] Built target llama-lookahead
[ 78%] Built target llama-lookup-merge
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Built target llama-lookup-create
[ 79%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Generating loading.html.hpp
[ 80%] Built target llama-lookup
[ 81%] Built target llama-cli
[ 81%] Linking CXX executable ../../bin/llama-passkey
[ 81%] Built target llama-lookup-stats
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 84%] Generating index.html.gz.hpp
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Built target llama-parallel
[ 87%] Linking CXX executable ../../bin/llama-retrieval
[ 87%] Linking CXX executable ../../bin/llama-save-load-state
[ 87%] Built target llama-passkey
[ 88%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 88%] Built target llama-perplexity
[ 88%] Linking CXX executable ../../bin/llama-speculative
[ 88%] Built target llama-quantize
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 88%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 89%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-run
[ 89%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Built target llama-save-load-state
[ 90%] Built target llama-retrieval
[ 90%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Built target llama-speculative
[ 91%] Built target llama-speculative-simple
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 95%] Built target llama-tokenize
[ 96%] Linking CXX executable ../../bin/llama-cvector-generator
[ 95%] Built target llama-run
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Built target llama-tts
[ 96%] Built target llama-gen-docs
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Built target llama-convert-llama2c-to-ggml
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Built target llama-llava-cli
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-vdot
[ 99%] Built target llama-qwen2vl-cli
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.070s
user	0m6.248s
sys	0m9.470s

main: quantize time =  3150.04 ms
main:    total time =  3150.04 ms

main: quantize time =  1483.35 ms
main:    total time =  1483.35 ms

main: quantize time =  2398.61 ms
main:    total time =  2398.61 ms

main: quantize time =  2219.11 ms
main:    total time =  2219.11 ms

main: quantize time =  1709.07 ms
main:    total time =  1709.07 ms

main: quantize time =  4996.56 ms
main:    total time =  4996.56 ms

main: quantize time =  5843.44 ms
main:    total time =  5843.44 ms

main: quantize time =  7096.26 ms
main:    total time =  7096.26 ms

main: quantize time =  5800.62 ms
main:    total time =  5800.62 ms

main: quantize time =  5006.76 ms
main:    total time =  5006.76 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.141 I build: 4590 (e0449763) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.314 I main: llama backend init
0.00.000.322 I main: load the model and apply lora adapter, if any
0.00.063.339 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.076.481 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.076.508 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.076.511 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.076.512 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.076.512 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.076.513 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.076.513 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.076.517 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.076.517 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.076.518 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.076.519 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.076.520 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.076.520 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.076.521 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.076.526 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.076.526 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.076.527 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.083.613 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.085.815 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.093.728 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.093.738 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.093.739 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.093.740 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.093.740 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.093.741 I llama_model_loader: - type  f32:  194 tensors
0.00.093.742 I llama_model_loader: - type  f16:   98 tensors
0.00.093.744 I print_info: file format = GGUF V3 (latest)
0.00.093.756 I print_info: file type   = all F32 (guessed)
0.00.093.759 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.110.776 I load: special tokens cache size = 25
0.00.120.349 I load: token to piece cache size = 0.2984 MB
0.00.120.353 I print_info: arch             = gptneox
0.00.120.353 I print_info: vocab_only       = 0
0.00.120.353 I print_info: n_ctx_train      = 2048
0.00.120.353 I print_info: n_embd           = 2048
0.00.120.354 I print_info: n_layer          = 24
0.00.120.358 I print_info: n_head           = 16
0.00.120.359 I print_info: n_head_kv        = 16
0.00.120.359 I print_info: n_rot            = 32
0.00.120.359 I print_info: n_swa            = 0
0.00.120.360 I print_info: n_embd_head_k    = 128
0.00.120.360 I print_info: n_embd_head_v    = 128
0.00.120.363 I print_info: n_gqa            = 1
0.00.120.364 I print_info: n_embd_k_gqa     = 2048
0.00.120.365 I print_info: n_embd_v_gqa     = 2048
0.00.120.366 I print_info: f_norm_eps       = 1.0e-05
0.00.120.368 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.120.368 I print_info: f_clamp_kqv      = 0.0e+00
0.00.120.368 I print_info: f_max_alibi_bias = 0.0e+00
0.00.120.368 I print_info: f_logit_scale    = 0.0e+00
0.00.120.369 I print_info: n_ff             = 8192
0.00.120.369 I print_info: n_expert         = 0
0.00.120.370 I print_info: n_expert_used    = 0
0.00.120.370 I print_info: causal attn      = 1
0.00.120.370 I print_info: pooling type     = 0
0.00.120.370 I print_info: rope type        = 2
0.00.120.370 I print_info: rope scaling     = linear
0.00.120.371 I print_info: freq_base_train  = 10000.0
0.00.120.371 I print_info: freq_scale_train = 1
0.00.120.372 I print_info: n_ctx_orig_yarn  = 2048
0.00.120.372 I print_info: rope_finetuned   = unknown
0.00.120.372 I print_info: ssm_d_conv       = 0
0.00.120.372 I print_info: ssm_d_inner      = 0
0.00.120.372 I print_info: ssm_d_state      = 0
0.00.120.374 I print_info: ssm_dt_rank      = 0
0.00.120.374 I print_info: ssm_dt_b_c_rms   = 0
0.00.120.375 I print_info: model type       = 1.4B
0.00.120.375 I print_info: model params     = 1.41 B
0.00.120.375 I print_info: general.name     = 1.4B
0.00.120.376 I print_info: vocab type       = BPE
0.00.120.376 I print_info: n_vocab          = 50304
0.00.120.376 I print_info: n_merges         = 50009
0.00.120.377 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.120.377 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.120.377 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.120.377 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.120.379 I print_info: LF token         = 128 'Ä'
0.00.120.380 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.120.380 I print_info: max token length = 1024
0.00.165.555 I load_tensors: offloading 24 repeating layers to GPU
0.00.165.559 I load_tensors: offloading output layer to GPU
0.00.165.559 I load_tensors: offloaded 25/25 layers to GPU
0.00.165.588 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.165.589 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.166.164 I llama_init_from_model: n_seq_max     = 1
0.00.166.166 I llama_init_from_model: n_ctx         = 2048
0.00.166.166 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.166.167 I llama_init_from_model: n_batch       = 2048
0.00.166.167 I llama_init_from_model: n_ubatch      = 512
0.00.166.167 I llama_init_from_model: flash_attn    = 0
0.00.166.168 I llama_init_from_model: freq_base     = 10000.0
0.00.166.168 I llama_init_from_model: freq_scale    = 1
0.00.166.170 I ggml_metal_init: allocating
0.00.166.216 I ggml_metal_init: found device: Apple M4
0.00.166.222 I ggml_metal_init: picking default device: Apple M4
0.00.166.893 I ggml_metal_init: using embedded metal library
0.00.176.600 I ggml_metal_init: GPU name:   Apple M4
0.00.176.602 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.176.602 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.176.603 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.176.603 I ggml_metal_init: simdgroup reduction   = true
0.00.176.603 I ggml_metal_init: simdgroup matrix mul. = true
0.00.176.603 I ggml_metal_init: has residency sets    = true
0.00.176.603 I ggml_metal_init: has bfloat            = true
0.00.176.604 I ggml_metal_init: use bfloat            = true
0.00.176.604 I ggml_metal_init: hasUnifiedMemory      = true
0.00.176.605 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.206.733 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.236.040 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.236.046 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.236.070 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.239.720 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.239.722 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.239.722 I llama_init_from_model: graph nodes  = 967
0.00.239.723 I llama_init_from_model: graph splits = 2
0.00.239.728 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.239.861 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.239.862 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.305.252 I main: llama threadpool init, n_threads = 4
0.00.305.294 I 
0.00.305.328 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.305.330 I 
0.00.305.507 I sampler seed: 1234
0.00.305.512 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.305.537 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.305.538 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.305.538 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.139.474 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60271.65 tokens per second)
0.02.139.474 I llama_perf_context_print:        load time =     240.91 ms
0.02.139.475 I llama_perf_context_print: prompt eval time =      43.62 ms /     7 tokens (    6.23 ms per token,   160.47 tokens per second)
0.02.139.476 I llama_perf_context_print:        eval time =    1787.43 ms /    63 runs   (   28.37 ms per token,    35.25 tokens per second)
0.02.139.476 I llama_perf_context_print:       total time =    1835.22 ms /    70 tokens
0.02.139.681 I ggml_metal_free: deallocating

real	0m2.481s
user	0m0.134s
sys	0m0.139s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4590 (e0449763) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.009.943 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.844 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.028.851 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.853 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.854 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.856 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.856 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.857 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.858 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.858 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.859 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.859 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.859 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.860 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.860 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.862 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.863 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.863 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.718 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.744 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.538 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.540 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.540 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.541 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.541 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.541 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.037.542 I llama_model_loader: - type  f32:  194 tensors
0.00.037.542 I llama_model_loader: - type q8_0:   98 tensors
0.00.037.544 I print_info: file format = GGUF V3 (latest)
0.00.037.544 I print_info: file type   = Q8_0
0.00.037.547 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.046.018 I load: special tokens cache size = 25
0.00.053.026 I load: token to piece cache size = 0.2984 MB
0.00.053.031 I print_info: arch             = gptneox
0.00.053.031 I print_info: vocab_only       = 0
0.00.053.033 I print_info: n_ctx_train      = 2048
0.00.053.033 I print_info: n_embd           = 2048
0.00.053.034 I print_info: n_layer          = 24
0.00.053.041 I print_info: n_head           = 16
0.00.053.042 I print_info: n_head_kv        = 16
0.00.053.042 I print_info: n_rot            = 32
0.00.053.042 I print_info: n_swa            = 0
0.00.053.043 I print_info: n_embd_head_k    = 128
0.00.053.043 I print_info: n_embd_head_v    = 128
0.00.053.043 I print_info: n_gqa            = 1
0.00.053.044 I print_info: n_embd_k_gqa     = 2048
0.00.053.045 I print_info: n_embd_v_gqa     = 2048
0.00.053.046 I print_info: f_norm_eps       = 1.0e-05
0.00.053.046 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.046 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.046 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.047 I print_info: f_logit_scale    = 0.0e+00
0.00.053.050 I print_info: n_ff             = 8192
0.00.053.051 I print_info: n_expert         = 0
0.00.053.051 I print_info: n_expert_used    = 0
0.00.053.051 I print_info: causal attn      = 1
0.00.053.051 I print_info: pooling type     = 0
0.00.053.051 I print_info: rope type        = 2
0.00.053.051 I print_info: rope scaling     = linear
0.00.053.052 I print_info: freq_base_train  = 10000.0
0.00.053.052 I print_info: freq_scale_train = 1
0.00.053.052 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.053 I print_info: rope_finetuned   = unknown
0.00.053.053 I print_info: ssm_d_conv       = 0
0.00.053.053 I print_info: ssm_d_inner      = 0
0.00.053.053 I print_info: ssm_d_state      = 0
0.00.053.053 I print_info: ssm_dt_rank      = 0
0.00.053.058 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.058 I print_info: model type       = 1.4B
0.00.053.059 I print_info: model params     = 1.41 B
0.00.053.059 I print_info: general.name     = 1.4B
0.00.053.060 I print_info: vocab type       = BPE
0.00.053.060 I print_info: n_vocab          = 50304
0.00.053.060 I print_info: n_merges         = 50009
0.00.053.061 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.061 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.061 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.063 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.063 I print_info: LF token         = 128 'Ä'
0.00.053.063 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.063 I print_info: max token length = 1024
0.01.211.092 I load_tensors: offloading 24 repeating layers to GPU
0.01.211.098 I load_tensors: offloading output layer to GPU
0.01.211.100 I load_tensors: offloaded 25/25 layers to GPU
0.01.211.121 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.211.123 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.212.418 I llama_init_from_model: n_seq_max     = 1
0.01.212.420 I llama_init_from_model: n_ctx         = 2048
0.01.212.421 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.212.421 I llama_init_from_model: n_batch       = 2048
0.01.212.422 I llama_init_from_model: n_ubatch      = 512
0.01.212.422 I llama_init_from_model: flash_attn    = 0
0.01.212.423 I llama_init_from_model: freq_base     = 10000.0
0.01.212.423 I llama_init_from_model: freq_scale    = 1
0.01.212.425 I ggml_metal_init: allocating
0.01.212.438 I ggml_metal_init: found device: Apple M4
0.01.212.447 I ggml_metal_init: picking default device: Apple M4
0.01.213.641 I ggml_metal_init: using embedded metal library
0.01.219.053 I ggml_metal_init: GPU name:   Apple M4
0.01.219.057 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.219.057 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.219.059 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.219.059 I ggml_metal_init: simdgroup reduction   = true
0.01.219.059 I ggml_metal_init: simdgroup matrix mul. = true
0.01.219.060 I ggml_metal_init: has residency sets    = true
0.01.219.060 I ggml_metal_init: has bfloat            = true
0.01.219.060 I ggml_metal_init: use bfloat            = true
0.01.219.061 I ggml_metal_init: hasUnifiedMemory      = true
0.01.219.064 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.235.009 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.294.929 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.294.934 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.294.955 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.299.811 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.299.813 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.299.813 I llama_init_from_model: graph nodes  = 967
0.01.299.814 I llama_init_from_model: graph splits = 2
0.01.299.820 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.299.952 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.299.953 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.355.180 I main: llama threadpool init, n_threads = 4
0.01.355.225 I 
0.01.355.248 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.355.249 I 
0.01.355.422 I sampler seed: 1234
0.01.355.426 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.355.437 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.355.438 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.355.438 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.447.594 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 57028.11 tokens per second)
0.02.447.594 I llama_perf_context_print:        load time =    1344.30 ms
0.02.447.595 I llama_perf_context_print: prompt eval time =      48.91 ms /     7 tokens (    6.99 ms per token,   143.12 tokens per second)
0.02.447.596 I llama_perf_context_print:        eval time =    1040.42 ms /    63 runs   (   16.51 ms per token,    60.55 tokens per second)
0.02.447.597 I llama_perf_context_print:       total time =    1093.35 ms /    70 tokens
0.02.447.863 I ggml_metal_free: deallocating

real	0m2.467s
user	0m0.109s
sys	0m0.272s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4590 (e0449763) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.018.233 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.033.791 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.033.797 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.799 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.799 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.799 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.800 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.800 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.801 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.802 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.802 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.802 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.803 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.803 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.804 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.806 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.806 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.806 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.038.602 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.040.020 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.045.330 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.045.332 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.045.333 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.045.333 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.045.333 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.045.334 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.045.334 I llama_model_loader: - type  f32:  194 tensors
0.00.045.335 I llama_model_loader: - type q4_0:   97 tensors
0.00.045.335 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.336 I print_info: file format = GGUF V3 (latest)
0.00.045.338 I print_info: file type   = Q4_0
0.00.045.339 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.057.601 I load: special tokens cache size = 25
0.00.068.399 I load: token to piece cache size = 0.2984 MB
0.00.068.405 I print_info: arch             = gptneox
0.00.068.405 I print_info: vocab_only       = 0
0.00.068.406 I print_info: n_ctx_train      = 2048
0.00.068.406 I print_info: n_embd           = 2048
0.00.068.406 I print_info: n_layer          = 24
0.00.068.412 I print_info: n_head           = 16
0.00.068.413 I print_info: n_head_kv        = 16
0.00.068.414 I print_info: n_rot            = 32
0.00.068.418 I print_info: n_swa            = 0
0.00.068.418 I print_info: n_embd_head_k    = 128
0.00.068.419 I print_info: n_embd_head_v    = 128
0.00.068.420 I print_info: n_gqa            = 1
0.00.068.422 I print_info: n_embd_k_gqa     = 2048
0.00.068.423 I print_info: n_embd_v_gqa     = 2048
0.00.068.426 I print_info: f_norm_eps       = 1.0e-05
0.00.068.427 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.068.427 I print_info: f_clamp_kqv      = 0.0e+00
0.00.068.427 I print_info: f_max_alibi_bias = 0.0e+00
0.00.068.427 I print_info: f_logit_scale    = 0.0e+00
0.00.068.429 I print_info: n_ff             = 8192
0.00.068.429 I print_info: n_expert         = 0
0.00.068.429 I print_info: n_expert_used    = 0
0.00.068.429 I print_info: causal attn      = 1
0.00.068.430 I print_info: pooling type     = 0
0.00.068.430 I print_info: rope type        = 2
0.00.068.430 I print_info: rope scaling     = linear
0.00.068.431 I print_info: freq_base_train  = 10000.0
0.00.068.432 I print_info: freq_scale_train = 1
0.00.068.432 I print_info: n_ctx_orig_yarn  = 2048
0.00.068.432 I print_info: rope_finetuned   = unknown
0.00.068.432 I print_info: ssm_d_conv       = 0
0.00.068.433 I print_info: ssm_d_inner      = 0
0.00.068.433 I print_info: ssm_d_state      = 0
0.00.068.433 I print_info: ssm_dt_rank      = 0
0.00.068.433 I print_info: ssm_dt_b_c_rms   = 0
0.00.068.434 I print_info: model type       = 1.4B
0.00.068.440 I print_info: model params     = 1.41 B
0.00.068.440 I print_info: general.name     = 1.4B
0.00.068.441 I print_info: vocab type       = BPE
0.00.068.442 I print_info: n_vocab          = 50304
0.00.068.442 I print_info: n_merges         = 50009
0.00.068.442 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.068.443 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.068.443 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.068.443 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.068.444 I print_info: LF token         = 128 'Ä'
0.00.068.444 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.068.445 I print_info: max token length = 1024
0.00.673.819 I load_tensors: offloading 24 repeating layers to GPU
0.00.673.836 I load_tensors: offloading output layer to GPU
0.00.673.836 I load_tensors: offloaded 25/25 layers to GPU
0.00.673.877 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.673.879 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.675.436 I llama_init_from_model: n_seq_max     = 1
0.00.675.443 I llama_init_from_model: n_ctx         = 2048
0.00.675.444 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.675.444 I llama_init_from_model: n_batch       = 2048
0.00.675.445 I llama_init_from_model: n_ubatch      = 512
0.00.675.445 I llama_init_from_model: flash_attn    = 0
0.00.675.448 I llama_init_from_model: freq_base     = 10000.0
0.00.675.448 I llama_init_from_model: freq_scale    = 1
0.00.675.453 I ggml_metal_init: allocating
0.00.675.587 I ggml_metal_init: found device: Apple M4
0.00.675.602 I ggml_metal_init: picking default device: Apple M4
0.00.677.520 I ggml_metal_init: using embedded metal library
0.00.684.027 I ggml_metal_init: GPU name:   Apple M4
0.00.684.032 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.684.033 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.684.034 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.684.035 I ggml_metal_init: simdgroup reduction   = true
0.00.684.035 I ggml_metal_init: simdgroup matrix mul. = true
0.00.684.035 I ggml_metal_init: has residency sets    = true
0.00.684.036 I ggml_metal_init: has bfloat            = true
0.00.684.036 I ggml_metal_init: use bfloat            = true
0.00.684.037 I ggml_metal_init: hasUnifiedMemory      = true
0.00.684.046 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.702.165 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.756.468 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.756.476 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.756.501 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.760.993 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.760.995 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.760.995 I llama_init_from_model: graph nodes  = 967
0.00.760.996 I llama_init_from_model: graph splits = 2
0.00.761.001 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.761.116 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.761.116 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.818.502 I main: llama threadpool init, n_threads = 4
0.00.818.549 I 
0.00.818.575 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.818.577 I 
0.00.818.758 I sampler seed: 1234
0.00.818.763 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.818.782 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.818.782 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.818.782 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.514.782 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49650.35 tokens per second)
0.01.514.783 I llama_perf_context_print:        load time =     799.38 ms
0.01.514.784 I llama_perf_context_print: prompt eval time =      49.30 ms /     7 tokens (    7.04 ms per token,   141.98 tokens per second)
0.01.514.784 I llama_perf_context_print:        eval time =     643.92 ms /    63 runs   (   10.22 ms per token,    97.84 tokens per second)
0.01.514.785 I llama_perf_context_print:       total time =     697.16 ms /    70 tokens
0.01.515.073 I ggml_metal_free: deallocating

real	0m1.546s
user	0m0.124s
sys	0m0.200s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4590 (e0449763) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.698 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.027.598 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.027.605 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.606 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.607 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.607 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.608 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.608 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.609 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.609 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.609 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.610 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.610 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.611 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.612 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.615 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.615 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.615 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.169 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.171 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.962 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.963 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.963 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.964 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.964 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.964 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.035.965 I llama_model_loader: - type  f32:  194 tensors
0.00.035.965 I llama_model_loader: - type q4_1:   97 tensors
0.00.035.966 I llama_model_loader: - type q6_K:    1 tensors
0.00.035.966 I print_info: file format = GGUF V3 (latest)
0.00.035.967 I print_info: file type   = Q4_1
0.00.035.968 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.044.022 I load: special tokens cache size = 25
0.00.049.594 I load: token to piece cache size = 0.2984 MB
0.00.049.598 I print_info: arch             = gptneox
0.00.049.598 I print_info: vocab_only       = 0
0.00.049.598 I print_info: n_ctx_train      = 2048
0.00.049.599 I print_info: n_embd           = 2048
0.00.049.599 I print_info: n_layer          = 24
0.00.049.602 I print_info: n_head           = 16
0.00.049.604 I print_info: n_head_kv        = 16
0.00.049.604 I print_info: n_rot            = 32
0.00.049.604 I print_info: n_swa            = 0
0.00.049.604 I print_info: n_embd_head_k    = 128
0.00.049.605 I print_info: n_embd_head_v    = 128
0.00.049.607 I print_info: n_gqa            = 1
0.00.049.608 I print_info: n_embd_k_gqa     = 2048
0.00.049.609 I print_info: n_embd_v_gqa     = 2048
0.00.049.610 I print_info: f_norm_eps       = 1.0e-05
0.00.049.610 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.610 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.611 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.611 I print_info: f_logit_scale    = 0.0e+00
0.00.049.611 I print_info: n_ff             = 8192
0.00.049.613 I print_info: n_expert         = 0
0.00.049.613 I print_info: n_expert_used    = 0
0.00.049.613 I print_info: causal attn      = 1
0.00.049.613 I print_info: pooling type     = 0
0.00.049.615 I print_info: rope type        = 2
0.00.049.615 I print_info: rope scaling     = linear
0.00.049.616 I print_info: freq_base_train  = 10000.0
0.00.049.616 I print_info: freq_scale_train = 1
0.00.049.616 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.616 I print_info: rope_finetuned   = unknown
0.00.049.616 I print_info: ssm_d_conv       = 0
0.00.049.616 I print_info: ssm_d_inner      = 0
0.00.049.616 I print_info: ssm_d_state      = 0
0.00.049.617 I print_info: ssm_dt_rank      = 0
0.00.049.617 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.617 I print_info: model type       = 1.4B
0.00.049.617 I print_info: model params     = 1.41 B
0.00.049.617 I print_info: general.name     = 1.4B
0.00.049.618 I print_info: vocab type       = BPE
0.00.049.618 I print_info: n_vocab          = 50304
0.00.049.618 I print_info: n_merges         = 50009
0.00.049.618 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.618 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.619 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.619 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.619 I print_info: LF token         = 128 'Ä'
0.00.049.619 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.619 I print_info: max token length = 1024
0.00.955.778 I load_tensors: offloading 24 repeating layers to GPU
0.00.955.795 I load_tensors: offloading output layer to GPU
0.00.955.795 I load_tensors: offloaded 25/25 layers to GPU
0.00.955.828 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.955.829 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.956.658 I llama_init_from_model: n_seq_max     = 1
0.00.956.665 I llama_init_from_model: n_ctx         = 2048
0.00.956.666 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.956.666 I llama_init_from_model: n_batch       = 2048
0.00.956.666 I llama_init_from_model: n_ubatch      = 512
0.00.956.666 I llama_init_from_model: flash_attn    = 0
0.00.956.668 I llama_init_from_model: freq_base     = 10000.0
0.00.956.668 I llama_init_from_model: freq_scale    = 1
0.00.956.674 I ggml_metal_init: allocating
0.00.956.727 I ggml_metal_init: found device: Apple M4
0.00.956.739 I ggml_metal_init: picking default device: Apple M4
0.00.958.337 I ggml_metal_init: using embedded metal library
0.00.962.916 I ggml_metal_init: GPU name:   Apple M4
0.00.962.921 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.962.921 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.962.922 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.962.922 I ggml_metal_init: simdgroup reduction   = true
0.00.962.923 I ggml_metal_init: simdgroup matrix mul. = true
0.00.962.923 I ggml_metal_init: has residency sets    = true
0.00.962.923 I ggml_metal_init: has bfloat            = true
0.00.962.923 I ggml_metal_init: use bfloat            = true
0.00.962.925 I ggml_metal_init: hasUnifiedMemory      = true
0.00.962.927 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.976.888 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.008.234 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.008.239 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.008.272 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.012.387 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.012.389 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.012.389 I llama_init_from_model: graph nodes  = 967
0.01.012.389 I llama_init_from_model: graph splits = 2
0.01.012.394 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.012.527 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.012.528 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.068.124 I main: llama threadpool init, n_threads = 4
0.01.068.163 I 
0.01.068.187 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.068.187 I 
0.01.068.349 I sampler seed: 1234
0.01.068.354 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.068.364 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.068.365 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.068.365 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.790.102 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54198.47 tokens per second)
0.01.790.103 I llama_perf_context_print:        load time =    1058.56 ms
0.01.790.103 I llama_perf_context_print: prompt eval time =      48.81 ms /     7 tokens (    6.97 ms per token,   143.41 tokens per second)
0.01.790.104 I llama_perf_context_print:        eval time =     670.69 ms /    63 runs   (   10.65 ms per token,    93.93 tokens per second)
0.01.790.104 I llama_perf_context_print:       total time =     722.85 ms /    70 tokens
0.01.790.318 I ggml_metal_free: deallocating

real	0m1.813s
user	0m0.102s
sys	0m0.171s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4590 (e0449763) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.846 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.140 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.144 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.146 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.146 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.147 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.147 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.147 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.148 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.148 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.149 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.149 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.150 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.150 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.153 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.154 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.155 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.155 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.968 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.009 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.707 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.708 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.708 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.708 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.709 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.709 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.709 I llama_model_loader: - type  f32:  194 tensors
0.00.025.709 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.710 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.710 I print_info: file format = GGUF V3 (latest)
0.00.025.711 I print_info: file type   = Q5_0
0.00.025.711 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.679 I load: special tokens cache size = 25
0.00.039.625 I load: token to piece cache size = 0.2984 MB
0.00.039.629 I print_info: arch             = gptneox
0.00.039.629 I print_info: vocab_only       = 0
0.00.039.629 I print_info: n_ctx_train      = 2048
0.00.039.629 I print_info: n_embd           = 2048
0.00.039.629 I print_info: n_layer          = 24
0.00.039.632 I print_info: n_head           = 16
0.00.039.633 I print_info: n_head_kv        = 16
0.00.039.633 I print_info: n_rot            = 32
0.00.039.633 I print_info: n_swa            = 0
0.00.039.633 I print_info: n_embd_head_k    = 128
0.00.039.634 I print_info: n_embd_head_v    = 128
0.00.039.634 I print_info: n_gqa            = 1
0.00.039.635 I print_info: n_embd_k_gqa     = 2048
0.00.039.635 I print_info: n_embd_v_gqa     = 2048
0.00.039.636 I print_info: f_norm_eps       = 1.0e-05
0.00.039.636 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.637 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.637 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.637 I print_info: f_logit_scale    = 0.0e+00
0.00.039.638 I print_info: n_ff             = 8192
0.00.039.638 I print_info: n_expert         = 0
0.00.039.638 I print_info: n_expert_used    = 0
0.00.039.638 I print_info: causal attn      = 1
0.00.039.638 I print_info: pooling type     = 0
0.00.039.638 I print_info: rope type        = 2
0.00.039.639 I print_info: rope scaling     = linear
0.00.039.639 I print_info: freq_base_train  = 10000.0
0.00.039.640 I print_info: freq_scale_train = 1
0.00.039.640 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.640 I print_info: rope_finetuned   = unknown
0.00.039.640 I print_info: ssm_d_conv       = 0
0.00.039.640 I print_info: ssm_d_inner      = 0
0.00.039.640 I print_info: ssm_d_state      = 0
0.00.039.641 I print_info: ssm_dt_rank      = 0
0.00.039.643 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.643 I print_info: model type       = 1.4B
0.00.039.643 I print_info: model params     = 1.41 B
0.00.039.644 I print_info: general.name     = 1.4B
0.00.039.644 I print_info: vocab type       = BPE
0.00.039.644 I print_info: n_vocab          = 50304
0.00.039.644 I print_info: n_merges         = 50009
0.00.039.645 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.645 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.645 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.645 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.646 I print_info: LF token         = 128 'Ä'
0.00.039.646 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.648 I print_info: max token length = 1024
0.00.681.527 I load_tensors: offloading 24 repeating layers to GPU
0.00.681.540 I load_tensors: offloading output layer to GPU
0.00.681.540 I load_tensors: offloaded 25/25 layers to GPU
0.00.681.573 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.681.574 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.682.887 I llama_init_from_model: n_seq_max     = 1
0.00.682.893 I llama_init_from_model: n_ctx         = 2048
0.00.682.893 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.682.893 I llama_init_from_model: n_batch       = 2048
0.00.682.894 I llama_init_from_model: n_ubatch      = 512
0.00.682.894 I llama_init_from_model: flash_attn    = 0
0.00.682.895 I llama_init_from_model: freq_base     = 10000.0
0.00.682.896 I llama_init_from_model: freq_scale    = 1
0.00.682.898 I ggml_metal_init: allocating
0.00.682.943 I ggml_metal_init: found device: Apple M4
0.00.682.953 I ggml_metal_init: picking default device: Apple M4
0.00.685.087 I ggml_metal_init: using embedded metal library
0.00.692.193 I ggml_metal_init: GPU name:   Apple M4
0.00.692.198 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.692.199 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.692.200 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.692.201 I ggml_metal_init: simdgroup reduction   = true
0.00.692.201 I ggml_metal_init: simdgroup matrix mul. = true
0.00.692.201 I ggml_metal_init: has residency sets    = true
0.00.692.202 I ggml_metal_init: has bfloat            = true
0.00.692.202 I ggml_metal_init: use bfloat            = true
0.00.692.203 I ggml_metal_init: hasUnifiedMemory      = true
0.00.692.205 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.710.359 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.797.475 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.797.517 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.797.538 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.801.954 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.801.956 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.801.956 I llama_init_from_model: graph nodes  = 967
0.00.801.956 I llama_init_from_model: graph splits = 2
0.00.801.961 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.802.086 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.802.086 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.861.891 I main: llama threadpool init, n_threads = 4
0.00.861.937 I 
0.00.861.963 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.861.967 I 
0.00.862.120 I sampler seed: 1234
0.00.862.124 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.862.135 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.862.136 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.862.136 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.652.397 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51598.84 tokens per second)
0.01.652.397 I llama_perf_context_print:        load time =     851.16 ms
0.01.652.398 I llama_perf_context_print: prompt eval time =      53.66 ms /     7 tokens (    7.67 ms per token,   130.46 tokens per second)
0.01.652.399 I llama_perf_context_print:        eval time =     733.72 ms /    63 runs   (   11.65 ms per token,    85.86 tokens per second)
0.01.652.399 I llama_perf_context_print:       total time =     791.39 ms /    70 tokens
0.01.652.669 I ggml_metal_free: deallocating

real	0m1.672s
user	0m0.110s
sys	0m0.229s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4590 (e0449763) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.008.956 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.430 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.434 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.436 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.436 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.436 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.436 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.436 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.437 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.438 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.438 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.438 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.439 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.441 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.441 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.443 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.443 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.443 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.053 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.070 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.786 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.787 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.787 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.787 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.788 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.788 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.788 I llama_model_loader: - type  f32:  194 tensors
0.00.024.789 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.789 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.790 I print_info: file format = GGUF V3 (latest)
0.00.024.790 I print_info: file type   = Q5_1
0.00.024.791 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.275 I load: special tokens cache size = 25
0.00.038.197 I load: token to piece cache size = 0.2984 MB
0.00.038.200 I print_info: arch             = gptneox
0.00.038.200 I print_info: vocab_only       = 0
0.00.038.201 I print_info: n_ctx_train      = 2048
0.00.038.201 I print_info: n_embd           = 2048
0.00.038.201 I print_info: n_layer          = 24
0.00.038.205 I print_info: n_head           = 16
0.00.038.205 I print_info: n_head_kv        = 16
0.00.038.206 I print_info: n_rot            = 32
0.00.038.206 I print_info: n_swa            = 0
0.00.038.206 I print_info: n_embd_head_k    = 128
0.00.038.206 I print_info: n_embd_head_v    = 128
0.00.038.207 I print_info: n_gqa            = 1
0.00.038.210 I print_info: n_embd_k_gqa     = 2048
0.00.038.211 I print_info: n_embd_v_gqa     = 2048
0.00.038.211 I print_info: f_norm_eps       = 1.0e-05
0.00.038.212 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.212 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.212 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.213 I print_info: f_logit_scale    = 0.0e+00
0.00.038.214 I print_info: n_ff             = 8192
0.00.038.214 I print_info: n_expert         = 0
0.00.038.214 I print_info: n_expert_used    = 0
0.00.038.214 I print_info: causal attn      = 1
0.00.038.214 I print_info: pooling type     = 0
0.00.038.214 I print_info: rope type        = 2
0.00.038.215 I print_info: rope scaling     = linear
0.00.038.215 I print_info: freq_base_train  = 10000.0
0.00.038.215 I print_info: freq_scale_train = 1
0.00.038.216 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.217 I print_info: rope_finetuned   = unknown
0.00.038.217 I print_info: ssm_d_conv       = 0
0.00.038.217 I print_info: ssm_d_inner      = 0
0.00.038.217 I print_info: ssm_d_state      = 0
0.00.038.217 I print_info: ssm_dt_rank      = 0
0.00.038.217 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.218 I print_info: model type       = 1.4B
0.00.038.218 I print_info: model params     = 1.41 B
0.00.038.218 I print_info: general.name     = 1.4B
0.00.038.219 I print_info: vocab type       = BPE
0.00.038.219 I print_info: n_vocab          = 50304
0.00.038.219 I print_info: n_merges         = 50009
0.00.038.219 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.223 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.223 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.223 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.224 I print_info: LF token         = 128 'Ä'
0.00.038.224 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.224 I print_info: max token length = 1024
0.00.604.446 I load_tensors: offloading 24 repeating layers to GPU
0.00.604.458 I load_tensors: offloading output layer to GPU
0.00.604.458 I load_tensors: offloaded 25/25 layers to GPU
0.00.604.489 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.604.490 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.605.666 I llama_init_from_model: n_seq_max     = 1
0.00.605.677 I llama_init_from_model: n_ctx         = 2048
0.00.605.678 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.605.678 I llama_init_from_model: n_batch       = 2048
0.00.605.678 I llama_init_from_model: n_ubatch      = 512
0.00.605.679 I llama_init_from_model: flash_attn    = 0
0.00.605.680 I llama_init_from_model: freq_base     = 10000.0
0.00.605.680 I llama_init_from_model: freq_scale    = 1
0.00.605.686 I ggml_metal_init: allocating
0.00.605.746 I ggml_metal_init: found device: Apple M4
0.00.605.760 I ggml_metal_init: picking default device: Apple M4
0.00.607.470 I ggml_metal_init: using embedded metal library
0.00.614.010 I ggml_metal_init: GPU name:   Apple M4
0.00.614.015 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.614.016 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.614.017 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.614.018 I ggml_metal_init: simdgroup reduction   = true
0.00.614.018 I ggml_metal_init: simdgroup matrix mul. = true
0.00.614.018 I ggml_metal_init: has residency sets    = true
0.00.614.018 I ggml_metal_init: has bfloat            = true
0.00.614.019 I ggml_metal_init: use bfloat            = true
0.00.614.020 I ggml_metal_init: hasUnifiedMemory      = true
0.00.614.021 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.631.993 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.688.746 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.688.751 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.688.773 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.693.532 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.693.534 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.693.534 I llama_init_from_model: graph nodes  = 967
0.00.693.534 I llama_init_from_model: graph splits = 2
0.00.693.544 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.693.667 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.693.668 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.754.562 I main: llama threadpool init, n_threads = 4
0.00.754.604 I 
0.00.754.628 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.754.628 I 
0.00.754.809 I sampler seed: 1234
0.00.754.814 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.754.860 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.754.863 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.754.864 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.589.305 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54364.47 tokens per second)
0.01.589.306 I llama_perf_context_print:        load time =     744.71 ms
0.01.589.306 I llama_perf_context_print: prompt eval time =      50.47 ms /     7 tokens (    7.21 ms per token,   138.69 tokens per second)
0.01.589.307 I llama_perf_context_print:        eval time =     781.18 ms /    63 runs   (   12.40 ms per token,    80.65 tokens per second)
0.01.589.307 I llama_perf_context_print:       total time =     835.63 ms /    70 tokens
0.01.589.632 I ggml_metal_free: deallocating

real	0m1.608s
user	0m0.109s
sys	0m0.215s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4590 (e0449763) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.009.780 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.088 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.093 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.094 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.095 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.095 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.095 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.096 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.096 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.097 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.097 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.097 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.098 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.098 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.098 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.100 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.100 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.100 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.773 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.740 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.376 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.377 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.377 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.378 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.378 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.378 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.379 I llama_model_loader: - type  f32:  194 tensors
0.00.024.379 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.379 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.380 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.380 I print_info: file format = GGUF V3 (latest)
0.00.024.381 I print_info: file type   = Q2_K - Medium
0.00.024.382 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.031.896 I load: special tokens cache size = 25
0.00.037.752 I load: token to piece cache size = 0.2984 MB
0.00.037.755 I print_info: arch             = gptneox
0.00.037.755 I print_info: vocab_only       = 0
0.00.037.756 I print_info: n_ctx_train      = 2048
0.00.037.756 I print_info: n_embd           = 2048
0.00.037.756 I print_info: n_layer          = 24
0.00.037.759 I print_info: n_head           = 16
0.00.037.759 I print_info: n_head_kv        = 16
0.00.037.760 I print_info: n_rot            = 32
0.00.037.760 I print_info: n_swa            = 0
0.00.037.760 I print_info: n_embd_head_k    = 128
0.00.037.760 I print_info: n_embd_head_v    = 128
0.00.037.761 I print_info: n_gqa            = 1
0.00.037.762 I print_info: n_embd_k_gqa     = 2048
0.00.037.762 I print_info: n_embd_v_gqa     = 2048
0.00.037.763 I print_info: f_norm_eps       = 1.0e-05
0.00.037.763 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.764 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.764 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.764 I print_info: f_logit_scale    = 0.0e+00
0.00.037.765 I print_info: n_ff             = 8192
0.00.037.765 I print_info: n_expert         = 0
0.00.037.765 I print_info: n_expert_used    = 0
0.00.037.765 I print_info: causal attn      = 1
0.00.037.765 I print_info: pooling type     = 0
0.00.037.765 I print_info: rope type        = 2
0.00.037.766 I print_info: rope scaling     = linear
0.00.037.766 I print_info: freq_base_train  = 10000.0
0.00.037.766 I print_info: freq_scale_train = 1
0.00.037.766 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.767 I print_info: rope_finetuned   = unknown
0.00.037.767 I print_info: ssm_d_conv       = 0
0.00.037.767 I print_info: ssm_d_inner      = 0
0.00.037.767 I print_info: ssm_d_state      = 0
0.00.037.767 I print_info: ssm_dt_rank      = 0
0.00.037.767 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.768 I print_info: model type       = 1.4B
0.00.037.768 I print_info: model params     = 1.41 B
0.00.037.768 I print_info: general.name     = 1.4B
0.00.037.769 I print_info: vocab type       = BPE
0.00.037.769 I print_info: n_vocab          = 50304
0.00.037.769 I print_info: n_merges         = 50009
0.00.037.769 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.769 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.770 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.770 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.770 I print_info: LF token         = 128 'Ä'
0.00.037.770 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.771 I print_info: max token length = 1024
0.00.343.649 I load_tensors: offloading 24 repeating layers to GPU
0.00.343.663 I load_tensors: offloading output layer to GPU
0.00.343.664 I load_tensors: offloaded 25/25 layers to GPU
0.00.343.695 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.343.696 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.345.155 I llama_init_from_model: n_seq_max     = 1
0.00.345.162 I llama_init_from_model: n_ctx         = 2048
0.00.345.162 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.345.163 I llama_init_from_model: n_batch       = 2048
0.00.345.163 I llama_init_from_model: n_ubatch      = 512
0.00.345.164 I llama_init_from_model: flash_attn    = 0
0.00.345.166 I llama_init_from_model: freq_base     = 10000.0
0.00.345.170 I llama_init_from_model: freq_scale    = 1
0.00.345.177 I ggml_metal_init: allocating
0.00.345.261 I ggml_metal_init: found device: Apple M4
0.00.345.275 I ggml_metal_init: picking default device: Apple M4
0.00.347.078 I ggml_metal_init: using embedded metal library
0.00.352.682 I ggml_metal_init: GPU name:   Apple M4
0.00.352.698 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.352.699 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.352.702 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.352.702 I ggml_metal_init: simdgroup reduction   = true
0.00.352.703 I ggml_metal_init: simdgroup matrix mul. = true
0.00.352.703 I ggml_metal_init: has residency sets    = true
0.00.352.703 I ggml_metal_init: has bfloat            = true
0.00.352.703 I ggml_metal_init: use bfloat            = true
0.00.352.705 I ggml_metal_init: hasUnifiedMemory      = true
0.00.352.709 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.373.930 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.434.965 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.434.976 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.435.007 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.439.630 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.439.632 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.439.632 I llama_init_from_model: graph nodes  = 967
0.00.439.632 I llama_init_from_model: graph splits = 2
0.00.439.639 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.439.763 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.439.763 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.499.242 I main: llama threadpool init, n_threads = 4
0.00.499.283 I 
0.00.499.308 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.499.310 I 
0.00.499.488 I sampler seed: 1234
0.00.499.493 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.499.530 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.499.533 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.499.533 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.171.175 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52906.11 tokens per second)
0.01.171.175 I llama_perf_context_print:        load time =     488.58 ms
0.01.171.176 I llama_perf_context_print: prompt eval time =      35.83 ms /     7 tokens (    5.12 ms per token,   195.35 tokens per second)
0.01.171.176 I llama_perf_context_print:        eval time =     632.95 ms /    63 runs   (   10.05 ms per token,    99.53 tokens per second)
0.01.171.177 I llama_perf_context_print:       total time =     672.81 ms /    70 tokens
0.01.171.396 I ggml_metal_free: deallocating

real	0m1.190s
user	0m0.109s
sys	0m0.175s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4590 (e0449763) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.152 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.828 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.833 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.838 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.839 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.840 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.840 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.841 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.841 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.842 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.842 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.843 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.843 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.843 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.844 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.845 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.845 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.846 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.495 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.469 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.103 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.104 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.105 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.105 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.105 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.106 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.106 I llama_model_loader: - type  f32:  194 tensors
0.00.025.106 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.107 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.107 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.107 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.108 I print_info: file format = GGUF V3 (latest)
0.00.025.108 I print_info: file type   = Q3_K - Medium
0.00.025.109 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.662 I load: special tokens cache size = 25
0.00.038.465 I load: token to piece cache size = 0.2984 MB
0.00.038.467 I print_info: arch             = gptneox
0.00.038.467 I print_info: vocab_only       = 0
0.00.038.468 I print_info: n_ctx_train      = 2048
0.00.038.468 I print_info: n_embd           = 2048
0.00.038.468 I print_info: n_layer          = 24
0.00.038.471 I print_info: n_head           = 16
0.00.038.471 I print_info: n_head_kv        = 16
0.00.038.471 I print_info: n_rot            = 32
0.00.038.472 I print_info: n_swa            = 0
0.00.038.472 I print_info: n_embd_head_k    = 128
0.00.038.472 I print_info: n_embd_head_v    = 128
0.00.038.473 I print_info: n_gqa            = 1
0.00.038.474 I print_info: n_embd_k_gqa     = 2048
0.00.038.474 I print_info: n_embd_v_gqa     = 2048
0.00.038.475 I print_info: f_norm_eps       = 1.0e-05
0.00.038.475 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.475 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.475 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.476 I print_info: f_logit_scale    = 0.0e+00
0.00.038.478 I print_info: n_ff             = 8192
0.00.038.478 I print_info: n_expert         = 0
0.00.038.478 I print_info: n_expert_used    = 0
0.00.038.479 I print_info: causal attn      = 1
0.00.038.479 I print_info: pooling type     = 0
0.00.038.479 I print_info: rope type        = 2
0.00.038.479 I print_info: rope scaling     = linear
0.00.038.480 I print_info: freq_base_train  = 10000.0
0.00.038.481 I print_info: freq_scale_train = 1
0.00.038.481 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.481 I print_info: rope_finetuned   = unknown
0.00.038.481 I print_info: ssm_d_conv       = 0
0.00.038.482 I print_info: ssm_d_inner      = 0
0.00.038.482 I print_info: ssm_d_state      = 0
0.00.038.482 I print_info: ssm_dt_rank      = 0
0.00.038.483 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.483 I print_info: model type       = 1.4B
0.00.038.483 I print_info: model params     = 1.41 B
0.00.038.483 I print_info: general.name     = 1.4B
0.00.038.484 I print_info: vocab type       = BPE
0.00.038.484 I print_info: n_vocab          = 50304
0.00.038.484 I print_info: n_merges         = 50009
0.00.038.485 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.485 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.485 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.485 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.486 I print_info: LF token         = 128 'Ä'
0.00.038.486 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.486 I print_info: max token length = 1024
0.00.440.597 I load_tensors: offloading 24 repeating layers to GPU
0.00.440.615 I load_tensors: offloading output layer to GPU
0.00.440.616 I load_tensors: offloaded 25/25 layers to GPU
0.00.440.651 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.440.652 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.441.989 I llama_init_from_model: n_seq_max     = 1
0.00.441.994 I llama_init_from_model: n_ctx         = 2048
0.00.441.995 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.441.995 I llama_init_from_model: n_batch       = 2048
0.00.441.995 I llama_init_from_model: n_ubatch      = 512
0.00.441.996 I llama_init_from_model: flash_attn    = 0
0.00.441.998 I llama_init_from_model: freq_base     = 10000.0
0.00.441.999 I llama_init_from_model: freq_scale    = 1
0.00.442.001 I ggml_metal_init: allocating
0.00.442.073 I ggml_metal_init: found device: Apple M4
0.00.442.087 I ggml_metal_init: picking default device: Apple M4
0.00.443.906 I ggml_metal_init: using embedded metal library
0.00.450.129 I ggml_metal_init: GPU name:   Apple M4
0.00.450.134 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.450.135 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.450.136 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.450.137 I ggml_metal_init: simdgroup reduction   = true
0.00.450.137 I ggml_metal_init: simdgroup matrix mul. = true
0.00.450.137 I ggml_metal_init: has residency sets    = true
0.00.450.138 I ggml_metal_init: has bfloat            = true
0.00.450.138 I ggml_metal_init: use bfloat            = true
0.00.450.139 I ggml_metal_init: hasUnifiedMemory      = true
0.00.450.140 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.468.838 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.526.100 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.526.111 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.526.134 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.530.910 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.530.912 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.530.912 I llama_init_from_model: graph nodes  = 967
0.00.530.913 I llama_init_from_model: graph splits = 2
0.00.530.918 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.531.038 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.531.038 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.588.383 I main: llama threadpool init, n_threads = 4
0.00.588.432 I 
0.00.588.460 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.588.462 I 
0.00.588.609 I sampler seed: 1234
0.00.588.613 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.588.659 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.588.663 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.588.663 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.341.032 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50678.09 tokens per second)
0.01.341.033 I llama_perf_context_print:        load time =     578.33 ms
0.01.341.033 I llama_perf_context_print: prompt eval time =      50.07 ms /     7 tokens (    7.15 ms per token,   139.81 tokens per second)
0.01.341.037 I llama_perf_context_print:        eval time =     699.35 ms /    63 runs   (   11.10 ms per token,    90.08 tokens per second)
0.01.341.038 I llama_perf_context_print:       total time =     753.55 ms /    70 tokens
0.01.341.282 I ggml_metal_free: deallocating

real	0m1.357s
user	0m0.109s
sys	0m0.187s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4590 (e0449763) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.771 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.008 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.013 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.015 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.017 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.017 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.017 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.018 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.019 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.019 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.019 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.020 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.022 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.022 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.023 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.028 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.028 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.029 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.662 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.644 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.254 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.256 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.256 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.256 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.256 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.257 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.257 I llama_model_loader: - type  f32:  194 tensors
0.00.024.257 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.258 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.258 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.258 I print_info: file format = GGUF V3 (latest)
0.00.024.258 I print_info: file type   = Q4_K - Medium
0.00.024.259 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.031.774 I load: special tokens cache size = 25
0.00.037.354 I load: token to piece cache size = 0.2984 MB
0.00.037.357 I print_info: arch             = gptneox
0.00.037.357 I print_info: vocab_only       = 0
0.00.037.357 I print_info: n_ctx_train      = 2048
0.00.037.357 I print_info: n_embd           = 2048
0.00.037.358 I print_info: n_layer          = 24
0.00.037.360 I print_info: n_head           = 16
0.00.037.361 I print_info: n_head_kv        = 16
0.00.037.361 I print_info: n_rot            = 32
0.00.037.361 I print_info: n_swa            = 0
0.00.037.361 I print_info: n_embd_head_k    = 128
0.00.037.362 I print_info: n_embd_head_v    = 128
0.00.037.364 I print_info: n_gqa            = 1
0.00.037.365 I print_info: n_embd_k_gqa     = 2048
0.00.037.365 I print_info: n_embd_v_gqa     = 2048
0.00.037.366 I print_info: f_norm_eps       = 1.0e-05
0.00.037.367 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.367 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.367 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.367 I print_info: f_logit_scale    = 0.0e+00
0.00.037.368 I print_info: n_ff             = 8192
0.00.037.368 I print_info: n_expert         = 0
0.00.037.368 I print_info: n_expert_used    = 0
0.00.037.368 I print_info: causal attn      = 1
0.00.037.370 I print_info: pooling type     = 0
0.00.037.371 I print_info: rope type        = 2
0.00.037.371 I print_info: rope scaling     = linear
0.00.037.371 I print_info: freq_base_train  = 10000.0
0.00.037.372 I print_info: freq_scale_train = 1
0.00.037.372 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.372 I print_info: rope_finetuned   = unknown
0.00.037.372 I print_info: ssm_d_conv       = 0
0.00.037.372 I print_info: ssm_d_inner      = 0
0.00.037.372 I print_info: ssm_d_state      = 0
0.00.037.373 I print_info: ssm_dt_rank      = 0
0.00.037.373 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.373 I print_info: model type       = 1.4B
0.00.037.373 I print_info: model params     = 1.41 B
0.00.037.373 I print_info: general.name     = 1.4B
0.00.037.374 I print_info: vocab type       = BPE
0.00.037.374 I print_info: n_vocab          = 50304
0.00.037.378 I print_info: n_merges         = 50009
0.00.037.378 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.378 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.379 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.379 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.379 I print_info: LF token         = 128 'Ä'
0.00.037.379 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.379 I print_info: max token length = 1024
0.00.512.889 I load_tensors: offloading 24 repeating layers to GPU
0.00.512.905 I load_tensors: offloading output layer to GPU
0.00.512.906 I load_tensors: offloaded 25/25 layers to GPU
0.00.512.943 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.512.945 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.514.455 I llama_init_from_model: n_seq_max     = 1
0.00.514.465 I llama_init_from_model: n_ctx         = 2048
0.00.514.465 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.514.466 I llama_init_from_model: n_batch       = 2048
0.00.514.466 I llama_init_from_model: n_ubatch      = 512
0.00.514.466 I llama_init_from_model: flash_attn    = 0
0.00.514.468 I llama_init_from_model: freq_base     = 10000.0
0.00.514.473 I llama_init_from_model: freq_scale    = 1
0.00.514.476 I ggml_metal_init: allocating
0.00.514.548 I ggml_metal_init: found device: Apple M4
0.00.514.563 I ggml_metal_init: picking default device: Apple M4
0.00.516.345 I ggml_metal_init: using embedded metal library
0.00.523.027 I ggml_metal_init: GPU name:   Apple M4
0.00.523.032 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.523.033 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.523.034 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.523.035 I ggml_metal_init: simdgroup reduction   = true
0.00.523.035 I ggml_metal_init: simdgroup matrix mul. = true
0.00.523.035 I ggml_metal_init: has residency sets    = true
0.00.523.035 I ggml_metal_init: has bfloat            = true
0.00.523.036 I ggml_metal_init: use bfloat            = true
0.00.523.036 I ggml_metal_init: hasUnifiedMemory      = true
0.00.523.038 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.540.528 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.603.769 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.603.774 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.603.797 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.607.913 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.607.914 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.607.914 I llama_init_from_model: graph nodes  = 967
0.00.607.915 I llama_init_from_model: graph splits = 2
0.00.607.920 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.608.049 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.608.049 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.665.517 I main: llama threadpool init, n_threads = 4
0.00.665.573 I 
0.00.665.597 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.665.598 I 
0.00.665.757 I sampler seed: 1234
0.00.665.761 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.665.781 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.665.781 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.665.781 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.436.375 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50283.29 tokens per second)
0.01.436.376 I llama_perf_context_print:        load time =     655.86 ms
0.01.436.377 I llama_perf_context_print: prompt eval time =      57.49 ms /     7 tokens (    8.21 ms per token,   121.76 tokens per second)
0.01.436.377 I llama_perf_context_print:        eval time =     710.09 ms /    63 runs   (   11.27 ms per token,    88.72 tokens per second)
0.01.436.378 I llama_perf_context_print:       total time =     771.75 ms /    70 tokens
0.01.436.596 I ggml_metal_free: deallocating

real	0m1.454s
user	0m0.108s
sys	0m0.196s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4590 (e0449763) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.010.713 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.848 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.852 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.858 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.859 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.859 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.859 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.860 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.861 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.861 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.861 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.862 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.862 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.863 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.863 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.865 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.865 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.865 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.482 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.447 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.078 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.079 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.080 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.080 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.080 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.081 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.081 I llama_model_loader: - type  f32:  194 tensors
0.00.026.082 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.082 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.082 I print_info: file format = GGUF V3 (latest)
0.00.026.083 I print_info: file type   = Q5_K - Medium
0.00.026.084 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.601 I load: special tokens cache size = 25
0.00.039.423 I load: token to piece cache size = 0.2984 MB
0.00.039.426 I print_info: arch             = gptneox
0.00.039.426 I print_info: vocab_only       = 0
0.00.039.426 I print_info: n_ctx_train      = 2048
0.00.039.426 I print_info: n_embd           = 2048
0.00.039.426 I print_info: n_layer          = 24
0.00.039.429 I print_info: n_head           = 16
0.00.039.430 I print_info: n_head_kv        = 16
0.00.039.430 I print_info: n_rot            = 32
0.00.039.430 I print_info: n_swa            = 0
0.00.039.432 I print_info: n_embd_head_k    = 128
0.00.039.432 I print_info: n_embd_head_v    = 128
0.00.039.433 I print_info: n_gqa            = 1
0.00.039.434 I print_info: n_embd_k_gqa     = 2048
0.00.039.434 I print_info: n_embd_v_gqa     = 2048
0.00.039.440 I print_info: f_norm_eps       = 1.0e-05
0.00.039.443 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.443 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.443 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.443 I print_info: f_logit_scale    = 0.0e+00
0.00.039.445 I print_info: n_ff             = 8192
0.00.039.446 I print_info: n_expert         = 0
0.00.039.446 I print_info: n_expert_used    = 0
0.00.039.446 I print_info: causal attn      = 1
0.00.039.446 I print_info: pooling type     = 0
0.00.039.446 I print_info: rope type        = 2
0.00.039.447 I print_info: rope scaling     = linear
0.00.039.447 I print_info: freq_base_train  = 10000.0
0.00.039.447 I print_info: freq_scale_train = 1
0.00.039.447 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.448 I print_info: rope_finetuned   = unknown
0.00.039.448 I print_info: ssm_d_conv       = 0
0.00.039.448 I print_info: ssm_d_inner      = 0
0.00.039.448 I print_info: ssm_d_state      = 0
0.00.039.448 I print_info: ssm_dt_rank      = 0
0.00.039.448 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.449 I print_info: model type       = 1.4B
0.00.039.449 I print_info: model params     = 1.41 B
0.00.039.449 I print_info: general.name     = 1.4B
0.00.039.450 I print_info: vocab type       = BPE
0.00.039.450 I print_info: n_vocab          = 50304
0.00.039.450 I print_info: n_merges         = 50009
0.00.039.453 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.453 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.453 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.453 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.453 I print_info: LF token         = 128 'Ä'
0.00.039.456 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.456 I print_info: max token length = 1024
0.00.599.148 I load_tensors: offloading 24 repeating layers to GPU
0.00.599.152 I load_tensors: offloading output layer to GPU
0.00.599.153 I load_tensors: offloaded 25/25 layers to GPU
0.00.599.174 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.599.178 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.600.581 I llama_init_from_model: n_seq_max     = 1
0.00.600.583 I llama_init_from_model: n_ctx         = 2048
0.00.600.584 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.600.584 I llama_init_from_model: n_batch       = 2048
0.00.600.585 I llama_init_from_model: n_ubatch      = 512
0.00.600.585 I llama_init_from_model: flash_attn    = 0
0.00.600.586 I llama_init_from_model: freq_base     = 10000.0
0.00.600.587 I llama_init_from_model: freq_scale    = 1
0.00.600.588 I ggml_metal_init: allocating
0.00.600.607 I ggml_metal_init: found device: Apple M4
0.00.600.616 I ggml_metal_init: picking default device: Apple M4
0.00.602.046 I ggml_metal_init: using embedded metal library
0.00.608.067 I ggml_metal_init: GPU name:   Apple M4
0.00.608.071 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.608.072 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.608.072 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.608.073 I ggml_metal_init: simdgroup reduction   = true
0.00.608.073 I ggml_metal_init: simdgroup matrix mul. = true
0.00.608.073 I ggml_metal_init: has residency sets    = true
0.00.608.074 I ggml_metal_init: has bfloat            = true
0.00.608.074 I ggml_metal_init: use bfloat            = true
0.00.608.075 I ggml_metal_init: hasUnifiedMemory      = true
0.00.608.077 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.624.539 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.679.682 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.679.689 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.679.757 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.684.060 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.684.062 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.684.062 I llama_init_from_model: graph nodes  = 967
0.00.684.062 I llama_init_from_model: graph splits = 2
0.00.684.068 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.684.192 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.684.192 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.750.567 I main: llama threadpool init, n_threads = 4
0.00.750.610 I 
0.00.750.637 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.750.638 I 
0.00.750.838 I sampler seed: 1234
0.00.750.843 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.750.855 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.750.857 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.750.857 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.599.725 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51412.02 tokens per second)
0.01.599.726 I llama_perf_context_print:        load time =     738.86 ms
0.01.599.731 I llama_perf_context_print: prompt eval time =      51.26 ms /     7 tokens (    7.32 ms per token,   136.56 tokens per second)
0.01.599.731 I llama_perf_context_print:        eval time =     794.79 ms /    63 runs   (   12.62 ms per token,    79.27 tokens per second)
0.01.599.732 I llama_perf_context_print:       total time =     850.15 ms /    70 tokens
0.01.600.025 I ggml_metal_free: deallocating

real	0m1.618s
user	0m0.107s
sys	0m0.213s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4590 (e0449763) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.009.320 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.323 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.328 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.330 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.331 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.331 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.331 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.332 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.332 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.333 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.333 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.333 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.334 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.334 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.337 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.339 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.339 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.340 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.119 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.100 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.806 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.807 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.808 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.808 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.809 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.809 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.810 I llama_model_loader: - type  f32:  194 tensors
0.00.025.810 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.811 I print_info: file format = GGUF V3 (latest)
0.00.025.811 I print_info: file type   = Q6_K
0.00.025.812 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.695 I load: special tokens cache size = 25
0.00.039.634 I load: token to piece cache size = 0.2984 MB
0.00.039.638 I print_info: arch             = gptneox
0.00.039.638 I print_info: vocab_only       = 0
0.00.039.638 I print_info: n_ctx_train      = 2048
0.00.039.638 I print_info: n_embd           = 2048
0.00.039.638 I print_info: n_layer          = 24
0.00.039.642 I print_info: n_head           = 16
0.00.039.643 I print_info: n_head_kv        = 16
0.00.039.644 I print_info: n_rot            = 32
0.00.039.644 I print_info: n_swa            = 0
0.00.039.645 I print_info: n_embd_head_k    = 128
0.00.039.646 I print_info: n_embd_head_v    = 128
0.00.039.646 I print_info: n_gqa            = 1
0.00.039.647 I print_info: n_embd_k_gqa     = 2048
0.00.039.648 I print_info: n_embd_v_gqa     = 2048
0.00.039.648 I print_info: f_norm_eps       = 1.0e-05
0.00.039.648 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.649 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.649 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.649 I print_info: f_logit_scale    = 0.0e+00
0.00.039.650 I print_info: n_ff             = 8192
0.00.039.650 I print_info: n_expert         = 0
0.00.039.650 I print_info: n_expert_used    = 0
0.00.039.650 I print_info: causal attn      = 1
0.00.039.650 I print_info: pooling type     = 0
0.00.039.650 I print_info: rope type        = 2
0.00.039.651 I print_info: rope scaling     = linear
0.00.039.651 I print_info: freq_base_train  = 10000.0
0.00.039.651 I print_info: freq_scale_train = 1
0.00.039.651 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.652 I print_info: rope_finetuned   = unknown
0.00.039.652 I print_info: ssm_d_conv       = 0
0.00.039.653 I print_info: ssm_d_inner      = 0
0.00.039.654 I print_info: ssm_d_state      = 0
0.00.039.654 I print_info: ssm_dt_rank      = 0
0.00.039.654 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.654 I print_info: model type       = 1.4B
0.00.039.654 I print_info: model params     = 1.41 B
0.00.039.654 I print_info: general.name     = 1.4B
0.00.039.655 I print_info: vocab type       = BPE
0.00.039.655 I print_info: n_vocab          = 50304
0.00.039.655 I print_info: n_merges         = 50009
0.00.039.656 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.656 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.656 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.656 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.656 I print_info: LF token         = 128 'Ä'
0.00.039.656 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.657 I print_info: max token length = 1024
0.00.640.308 I load_tensors: offloading 24 repeating layers to GPU
0.00.640.317 I load_tensors: offloading output layer to GPU
0.00.640.318 I load_tensors: offloaded 25/25 layers to GPU
0.00.640.349 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.640.352 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.641.578 I llama_init_from_model: n_seq_max     = 1
0.00.641.580 I llama_init_from_model: n_ctx         = 2048
0.00.641.581 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.641.581 I llama_init_from_model: n_batch       = 2048
0.00.641.582 I llama_init_from_model: n_ubatch      = 512
0.00.641.582 I llama_init_from_model: flash_attn    = 0
0.00.641.583 I llama_init_from_model: freq_base     = 10000.0
0.00.641.584 I llama_init_from_model: freq_scale    = 1
0.00.641.586 I ggml_metal_init: allocating
0.00.641.634 I ggml_metal_init: found device: Apple M4
0.00.641.647 I ggml_metal_init: picking default device: Apple M4
0.00.643.119 I ggml_metal_init: using embedded metal library
0.00.649.186 I ggml_metal_init: GPU name:   Apple M4
0.00.649.190 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.649.192 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.649.192 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.649.193 I ggml_metal_init: simdgroup reduction   = true
0.00.649.193 I ggml_metal_init: simdgroup matrix mul. = true
0.00.649.193 I ggml_metal_init: has residency sets    = true
0.00.649.193 I ggml_metal_init: has bfloat            = true
0.00.649.194 I ggml_metal_init: use bfloat            = true
0.00.649.195 I ggml_metal_init: hasUnifiedMemory      = true
0.00.649.198 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.666.370 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.721.750 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.721.758 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.721.789 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.726.212 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.726.214 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.726.214 I llama_init_from_model: graph nodes  = 967
0.00.726.215 I llama_init_from_model: graph splits = 2
0.00.726.220 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.726.333 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.726.333 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.795.407 I main: llama threadpool init, n_threads = 4
0.00.795.452 I 
0.00.795.476 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.795.476 I 
0.00.795.647 I sampler seed: 1234
0.00.795.652 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.795.663 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.795.664 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.795.664 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.675.577 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51598.84 tokens per second)
0.01.675.578 I llama_perf_context_print:        load time =     785.12 ms
0.01.675.580 I llama_perf_context_print: prompt eval time =      54.09 ms /     7 tokens (    7.73 ms per token,   129.40 tokens per second)
0.01.675.580 I llama_perf_context_print:        eval time =     822.78 ms /    63 runs   (   13.06 ms per token,    76.57 tokens per second)
0.01.675.581 I llama_perf_context_print:       total time =     881.13 ms /    70 tokens
0.01.675.802 I ggml_metal_free: deallocating

real	0m1.696s
user	0m0.109s
sys	0m0.216s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.715 I build: 4590 (e0449763) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.457 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.039.210 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.218 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.227 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.229 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.229 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.230 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.231 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.236 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.237 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.238 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.238 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.239 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.239 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.240 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.243 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.244 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.244 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.427 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.500 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.630 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.632 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.632 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.633 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.633 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.634 I llama_model_loader: - type  f32:  194 tensors
0.00.056.634 I llama_model_loader: - type  f16:   98 tensors
0.00.056.635 I print_info: file format = GGUF V3 (latest)
0.00.056.636 I print_info: file type   = all F32 (guessed)
0.00.056.637 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.069.267 I load: special tokens cache size = 25
0.00.077.173 I load: token to piece cache size = 0.2984 MB
0.00.077.176 I print_info: arch             = gptneox
0.00.077.176 I print_info: vocab_only       = 0
0.00.077.177 I print_info: n_ctx_train      = 2048
0.00.077.177 I print_info: n_embd           = 2048
0.00.077.177 I print_info: n_layer          = 24
0.00.077.180 I print_info: n_head           = 16
0.00.077.181 I print_info: n_head_kv        = 16
0.00.077.181 I print_info: n_rot            = 32
0.00.077.182 I print_info: n_swa            = 0
0.00.077.182 I print_info: n_embd_head_k    = 128
0.00.077.182 I print_info: n_embd_head_v    = 128
0.00.077.183 I print_info: n_gqa            = 1
0.00.077.184 I print_info: n_embd_k_gqa     = 2048
0.00.077.184 I print_info: n_embd_v_gqa     = 2048
0.00.077.185 I print_info: f_norm_eps       = 1.0e-05
0.00.077.185 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.077.185 I print_info: f_clamp_kqv      = 0.0e+00
0.00.077.185 I print_info: f_max_alibi_bias = 0.0e+00
0.00.077.186 I print_info: f_logit_scale    = 0.0e+00
0.00.077.186 I print_info: n_ff             = 8192
0.00.077.187 I print_info: n_expert         = 0
0.00.077.187 I print_info: n_expert_used    = 0
0.00.077.188 I print_info: causal attn      = 1
0.00.077.189 I print_info: pooling type     = 0
0.00.077.189 I print_info: rope type        = 2
0.00.077.189 I print_info: rope scaling     = linear
0.00.077.190 I print_info: freq_base_train  = 10000.0
0.00.077.190 I print_info: freq_scale_train = 1
0.00.077.190 I print_info: n_ctx_orig_yarn  = 2048
0.00.077.190 I print_info: rope_finetuned   = unknown
0.00.077.191 I print_info: ssm_d_conv       = 0
0.00.077.191 I print_info: ssm_d_inner      = 0
0.00.077.191 I print_info: ssm_d_state      = 0
0.00.077.191 I print_info: ssm_dt_rank      = 0
0.00.077.191 I print_info: ssm_dt_b_c_rms   = 0
0.00.077.192 I print_info: model type       = 1.4B
0.00.077.192 I print_info: model params     = 1.41 B
0.00.077.192 I print_info: general.name     = 1.4B
0.00.077.193 I print_info: vocab type       = BPE
0.00.077.193 I print_info: n_vocab          = 50304
0.00.077.193 I print_info: n_merges         = 50009
0.00.077.193 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.077.194 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.077.194 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.077.194 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.077.196 I print_info: LF token         = 128 'Ä'
0.00.077.196 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.077.196 I print_info: max token length = 1024
0.01.448.236 I load_tensors: offloading 24 repeating layers to GPU
0.01.448.240 I load_tensors: offloading output layer to GPU
0.01.448.240 I load_tensors: offloaded 25/25 layers to GPU
0.01.448.264 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.448.266 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.449.043 I llama_init_from_model: n_seq_max     = 1
0.01.449.044 I llama_init_from_model: n_ctx         = 128
0.01.449.044 I llama_init_from_model: n_ctx_per_seq = 128
0.01.449.044 I llama_init_from_model: n_batch       = 128
0.01.449.044 I llama_init_from_model: n_ubatch      = 128
0.01.449.048 I llama_init_from_model: flash_attn    = 0
0.01.449.050 I llama_init_from_model: freq_base     = 10000.0
0.01.449.050 I llama_init_from_model: freq_scale    = 1
0.01.449.051 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.449.052 I ggml_metal_init: allocating
0.01.449.090 I ggml_metal_init: found device: Apple M4
0.01.449.096 I ggml_metal_init: picking default device: Apple M4
0.01.450.113 I ggml_metal_init: using embedded metal library
0.01.454.063 I ggml_metal_init: GPU name:   Apple M4
0.01.454.065 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.454.065 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.454.066 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.454.066 I ggml_metal_init: simdgroup reduction   = true
0.01.454.066 I ggml_metal_init: simdgroup matrix mul. = true
0.01.454.066 I ggml_metal_init: has residency sets    = true
0.01.454.067 I ggml_metal_init: has bfloat            = true
0.01.454.067 I ggml_metal_init: use bfloat            = true
0.01.454.067 I ggml_metal_init: hasUnifiedMemory      = true
0.01.454.068 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.465.005 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.466.834 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.466.836 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.466.849 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.468.492 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.468.493 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.468.493 I llama_init_from_model: graph nodes  = 967
0.01.468.493 I llama_init_from_model: graph splits = 2
0.01.468.495 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.468.495 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.503.879 I 
0.01.503.912 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.503.930 I perplexity: tokenizing the input ..
0.01.509.138 I perplexity: tokenization took 5.206 ms
0.01.509.160 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.627.551 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.628.900 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.628.916 I llama_perf_context_print:        load time =    1479.41 ms
0.01.628.917 I llama_perf_context_print: prompt eval time =     118.13 ms /   128 tokens (    0.92 ms per token,  1083.57 tokens per second)
0.01.628.918 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.628.918 I llama_perf_context_print:       total time =     125.04 ms /   129 tokens
0.01.629.322 I ggml_metal_free: deallocating

real	0m1.848s
user	0m0.098s
sys	0m0.272s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.270 I build: 4590 (e0449763) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.901 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.730 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.736 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.737 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.738 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.738 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.738 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.739 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.740 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.740 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.741 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.741 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.741 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.742 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.742 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.744 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.745 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.745 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.384 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.454 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.148 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.149 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.149 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.149 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.150 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.150 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.151 I llama_model_loader: - type  f32:  194 tensors
0.00.025.151 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.152 I print_info: file format = GGUF V3 (latest)
0.00.025.152 I print_info: file type   = Q8_0
0.00.025.155 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.032.777 I load: special tokens cache size = 25
0.00.040.560 I load: token to piece cache size = 0.2984 MB
0.00.040.565 I print_info: arch             = gptneox
0.00.040.565 I print_info: vocab_only       = 0
0.00.040.565 I print_info: n_ctx_train      = 2048
0.00.040.566 I print_info: n_embd           = 2048
0.00.040.566 I print_info: n_layer          = 24
0.00.040.570 I print_info: n_head           = 16
0.00.040.571 I print_info: n_head_kv        = 16
0.00.040.575 I print_info: n_rot            = 32
0.00.040.575 I print_info: n_swa            = 0
0.00.040.575 I print_info: n_embd_head_k    = 128
0.00.040.575 I print_info: n_embd_head_v    = 128
0.00.040.576 I print_info: n_gqa            = 1
0.00.040.578 I print_info: n_embd_k_gqa     = 2048
0.00.040.579 I print_info: n_embd_v_gqa     = 2048
0.00.040.580 I print_info: f_norm_eps       = 1.0e-05
0.00.040.580 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.581 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.581 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.581 I print_info: f_logit_scale    = 0.0e+00
0.00.040.582 I print_info: n_ff             = 8192
0.00.040.584 I print_info: n_expert         = 0
0.00.040.586 I print_info: n_expert_used    = 0
0.00.040.586 I print_info: causal attn      = 1
0.00.040.586 I print_info: pooling type     = 0
0.00.040.586 I print_info: rope type        = 2
0.00.040.587 I print_info: rope scaling     = linear
0.00.040.587 I print_info: freq_base_train  = 10000.0
0.00.040.587 I print_info: freq_scale_train = 1
0.00.040.588 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.593 I print_info: rope_finetuned   = unknown
0.00.040.593 I print_info: ssm_d_conv       = 0
0.00.040.593 I print_info: ssm_d_inner      = 0
0.00.040.594 I print_info: ssm_d_state      = 0
0.00.040.594 I print_info: ssm_dt_rank      = 0
0.00.040.594 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.594 I print_info: model type       = 1.4B
0.00.040.595 I print_info: model params     = 1.41 B
0.00.040.595 I print_info: general.name     = 1.4B
0.00.040.595 I print_info: vocab type       = BPE
0.00.040.596 I print_info: n_vocab          = 50304
0.00.040.596 I print_info: n_merges         = 50009
0.00.040.597 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.598 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.599 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.599 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.599 I print_info: LF token         = 128 'Ä'
0.00.040.600 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.600 I print_info: max token length = 1024
0.00.897.342 I load_tensors: offloading 24 repeating layers to GPU
0.00.897.348 I load_tensors: offloading output layer to GPU
0.00.897.350 I load_tensors: offloaded 25/25 layers to GPU
0.00.897.373 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.897.376 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.898.649 I llama_init_from_model: n_seq_max     = 1
0.00.898.651 I llama_init_from_model: n_ctx         = 128
0.00.898.651 I llama_init_from_model: n_ctx_per_seq = 128
0.00.898.652 I llama_init_from_model: n_batch       = 128
0.00.898.656 I llama_init_from_model: n_ubatch      = 128
0.00.898.656 I llama_init_from_model: flash_attn    = 0
0.00.898.657 I llama_init_from_model: freq_base     = 10000.0
0.00.898.659 I llama_init_from_model: freq_scale    = 1
0.00.898.666 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.898.667 I ggml_metal_init: allocating
0.00.898.686 I ggml_metal_init: found device: Apple M4
0.00.898.696 I ggml_metal_init: picking default device: Apple M4
0.00.899.879 I ggml_metal_init: using embedded metal library
0.00.905.396 I ggml_metal_init: GPU name:   Apple M4
0.00.905.399 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.905.400 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.905.401 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.905.401 I ggml_metal_init: simdgroup reduction   = true
0.00.905.401 I ggml_metal_init: simdgroup matrix mul. = true
0.00.905.402 I ggml_metal_init: has residency sets    = true
0.00.905.402 I ggml_metal_init: has bfloat            = true
0.00.905.402 I ggml_metal_init: use bfloat            = true
0.00.905.403 I ggml_metal_init: hasUnifiedMemory      = true
0.00.905.404 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.920.738 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.924.058 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.924.061 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.924.085 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.927.239 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.927.240 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.927.241 I llama_init_from_model: graph nodes  = 967
0.00.927.241 I llama_init_from_model: graph splits = 2
0.00.927.244 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.927.245 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.956.348 I 
0.00.956.419 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.956.437 I perplexity: tokenizing the input ..
0.00.963.846 I perplexity: tokenization took 7.405 ms
0.00.963.869 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.103.184 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.104.522 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.104.533 I llama_perf_context_print:        load time =     946.44 ms
0.01.104.534 I llama_perf_context_print: prompt eval time =     138.38 ms /   128 tokens (    1.08 ms per token,   925.01 tokens per second)
0.01.104.535 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.104.535 I llama_perf_context_print:       total time =     148.19 ms /   129 tokens
0.01.104.903 I ggml_metal_free: deallocating

real	0m1.121s
user	0m0.078s
sys	0m0.168s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.270 I build: 4590 (e0449763) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.669 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.413 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.418 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.419 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.420 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.420 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.421 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.421 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.422 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.422 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.422 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.423 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.423 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.423 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.424 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.426 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.426 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.426 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.105 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.135 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.848 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.849 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.849 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.849 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.850 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.850 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.850 I llama_model_loader: - type  f32:  194 tensors
0.00.024.851 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.851 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.852 I print_info: file format = GGUF V3 (latest)
0.00.024.852 I print_info: file type   = Q4_0
0.00.024.854 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.032.374 I load: special tokens cache size = 25
0.00.037.917 I load: token to piece cache size = 0.2984 MB
0.00.037.920 I print_info: arch             = gptneox
0.00.037.920 I print_info: vocab_only       = 0
0.00.037.920 I print_info: n_ctx_train      = 2048
0.00.037.920 I print_info: n_embd           = 2048
0.00.037.921 I print_info: n_layer          = 24
0.00.037.924 I print_info: n_head           = 16
0.00.037.925 I print_info: n_head_kv        = 16
0.00.037.925 I print_info: n_rot            = 32
0.00.037.925 I print_info: n_swa            = 0
0.00.037.925 I print_info: n_embd_head_k    = 128
0.00.037.926 I print_info: n_embd_head_v    = 128
0.00.037.927 I print_info: n_gqa            = 1
0.00.037.928 I print_info: n_embd_k_gqa     = 2048
0.00.037.928 I print_info: n_embd_v_gqa     = 2048
0.00.037.929 I print_info: f_norm_eps       = 1.0e-05
0.00.037.929 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.929 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.930 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.930 I print_info: f_logit_scale    = 0.0e+00
0.00.037.932 I print_info: n_ff             = 8192
0.00.037.932 I print_info: n_expert         = 0
0.00.037.933 I print_info: n_expert_used    = 0
0.00.037.933 I print_info: causal attn      = 1
0.00.037.933 I print_info: pooling type     = 0
0.00.037.933 I print_info: rope type        = 2
0.00.037.934 I print_info: rope scaling     = linear
0.00.037.935 I print_info: freq_base_train  = 10000.0
0.00.037.935 I print_info: freq_scale_train = 1
0.00.037.935 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.935 I print_info: rope_finetuned   = unknown
0.00.037.935 I print_info: ssm_d_conv       = 0
0.00.037.936 I print_info: ssm_d_inner      = 0
0.00.037.936 I print_info: ssm_d_state      = 0
0.00.037.936 I print_info: ssm_dt_rank      = 0
0.00.037.936 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.936 I print_info: model type       = 1.4B
0.00.037.937 I print_info: model params     = 1.41 B
0.00.037.937 I print_info: general.name     = 1.4B
0.00.037.941 I print_info: vocab type       = BPE
0.00.037.941 I print_info: n_vocab          = 50304
0.00.037.941 I print_info: n_merges         = 50009
0.00.037.942 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.942 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.942 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.942 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.942 I print_info: LF token         = 128 'Ä'
0.00.037.943 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.943 I print_info: max token length = 1024
0.00.583.796 I load_tensors: offloading 24 repeating layers to GPU
0.00.583.806 I load_tensors: offloading output layer to GPU
0.00.583.806 I load_tensors: offloaded 25/25 layers to GPU
0.00.583.841 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.583.843 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.585.414 I llama_init_from_model: n_seq_max     = 1
0.00.585.420 I llama_init_from_model: n_ctx         = 128
0.00.585.420 I llama_init_from_model: n_ctx_per_seq = 128
0.00.585.425 I llama_init_from_model: n_batch       = 128
0.00.585.425 I llama_init_from_model: n_ubatch      = 128
0.00.585.425 I llama_init_from_model: flash_attn    = 0
0.00.585.427 I llama_init_from_model: freq_base     = 10000.0
0.00.585.427 I llama_init_from_model: freq_scale    = 1
0.00.585.428 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.585.431 I ggml_metal_init: allocating
0.00.585.506 I ggml_metal_init: found device: Apple M4
0.00.585.520 I ggml_metal_init: picking default device: Apple M4
0.00.587.697 I ggml_metal_init: using embedded metal library
0.00.594.188 I ggml_metal_init: GPU name:   Apple M4
0.00.594.194 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.594.195 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.594.196 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.594.197 I ggml_metal_init: simdgroup reduction   = true
0.00.594.198 I ggml_metal_init: simdgroup matrix mul. = true
0.00.594.198 I ggml_metal_init: has residency sets    = true
0.00.594.198 I ggml_metal_init: has bfloat            = true
0.00.594.198 I ggml_metal_init: use bfloat            = true
0.00.594.200 I ggml_metal_init: hasUnifiedMemory      = true
0.00.594.201 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.613.651 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.617.206 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.617.213 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.617.250 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.620.478 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.620.480 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.620.480 I llama_init_from_model: graph nodes  = 967
0.00.620.480 I llama_init_from_model: graph splits = 2
0.00.620.484 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.620.484 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.650.574 I 
0.00.650.651 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.650.670 I perplexity: tokenizing the input ..
0.00.657.771 I perplexity: tokenization took 7.098 ms
0.00.657.793 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.794.421 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.795.831 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.795.846 I llama_perf_context_print:        load time =     640.89 ms
0.00.795.847 I llama_perf_context_print: prompt eval time =     135.64 ms /   128 tokens (    1.06 ms per token,   943.70 tokens per second)
0.00.795.848 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.795.848 I llama_perf_context_print:       total time =     145.28 ms /   129 tokens
0.00.796.248 I ggml_metal_free: deallocating

real	0m0.812s
user	0m0.078s
sys	0m0.123s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4590 (e0449763) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.973 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.094 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.100 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.106 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.107 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.107 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.107 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.108 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.109 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.109 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.109 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.110 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.110 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.111 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.111 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.113 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.113 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.113 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.785 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.777 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.505 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.506 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.506 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.506 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.507 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.507 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.507 I llama_model_loader: - type  f32:  194 tensors
0.00.024.508 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.508 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.509 I print_info: file format = GGUF V3 (latest)
0.00.024.509 I print_info: file type   = Q4_1
0.00.024.510 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.361 I load: special tokens cache size = 25
0.00.037.978 I load: token to piece cache size = 0.2984 MB
0.00.037.981 I print_info: arch             = gptneox
0.00.037.981 I print_info: vocab_only       = 0
0.00.037.981 I print_info: n_ctx_train      = 2048
0.00.037.981 I print_info: n_embd           = 2048
0.00.037.981 I print_info: n_layer          = 24
0.00.037.985 I print_info: n_head           = 16
0.00.037.985 I print_info: n_head_kv        = 16
0.00.037.986 I print_info: n_rot            = 32
0.00.037.986 I print_info: n_swa            = 0
0.00.037.986 I print_info: n_embd_head_k    = 128
0.00.037.986 I print_info: n_embd_head_v    = 128
0.00.037.987 I print_info: n_gqa            = 1
0.00.037.988 I print_info: n_embd_k_gqa     = 2048
0.00.037.988 I print_info: n_embd_v_gqa     = 2048
0.00.037.989 I print_info: f_norm_eps       = 1.0e-05
0.00.037.989 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.990 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.990 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.990 I print_info: f_logit_scale    = 0.0e+00
0.00.037.991 I print_info: n_ff             = 8192
0.00.037.991 I print_info: n_expert         = 0
0.00.037.991 I print_info: n_expert_used    = 0
0.00.037.991 I print_info: causal attn      = 1
0.00.037.991 I print_info: pooling type     = 0
0.00.037.993 I print_info: rope type        = 2
0.00.037.994 I print_info: rope scaling     = linear
0.00.037.994 I print_info: freq_base_train  = 10000.0
0.00.037.994 I print_info: freq_scale_train = 1
0.00.037.994 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.995 I print_info: rope_finetuned   = unknown
0.00.037.995 I print_info: ssm_d_conv       = 0
0.00.037.995 I print_info: ssm_d_inner      = 0
0.00.037.995 I print_info: ssm_d_state      = 0
0.00.037.995 I print_info: ssm_dt_rank      = 0
0.00.037.995 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.996 I print_info: model type       = 1.4B
0.00.037.996 I print_info: model params     = 1.41 B
0.00.037.996 I print_info: general.name     = 1.4B
0.00.037.997 I print_info: vocab type       = BPE
0.00.037.997 I print_info: n_vocab          = 50304
0.00.037.997 I print_info: n_merges         = 50009
0.00.037.997 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.997 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.998 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.998 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.999 I print_info: LF token         = 128 'Ä'
0.00.038.000 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.000 I print_info: max token length = 1024
0.00.636.383 I load_tensors: offloading 24 repeating layers to GPU
0.00.636.398 I load_tensors: offloading output layer to GPU
0.00.636.399 I load_tensors: offloaded 25/25 layers to GPU
0.00.636.433 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.636.435 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.637.841 I llama_init_from_model: n_seq_max     = 1
0.00.637.846 I llama_init_from_model: n_ctx         = 128
0.00.637.847 I llama_init_from_model: n_ctx_per_seq = 128
0.00.637.847 I llama_init_from_model: n_batch       = 128
0.00.637.848 I llama_init_from_model: n_ubatch      = 128
0.00.637.848 I llama_init_from_model: flash_attn    = 0
0.00.637.851 I llama_init_from_model: freq_base     = 10000.0
0.00.637.851 I llama_init_from_model: freq_scale    = 1
0.00.637.852 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.637.858 I ggml_metal_init: allocating
0.00.637.928 I ggml_metal_init: found device: Apple M4
0.00.637.942 I ggml_metal_init: picking default device: Apple M4
0.00.639.652 I ggml_metal_init: using embedded metal library
0.00.645.684 I ggml_metal_init: GPU name:   Apple M4
0.00.645.690 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.645.690 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.645.691 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.645.692 I ggml_metal_init: simdgroup reduction   = true
0.00.645.692 I ggml_metal_init: simdgroup matrix mul. = true
0.00.645.693 I ggml_metal_init: has residency sets    = true
0.00.645.693 I ggml_metal_init: has bfloat            = true
0.00.645.693 I ggml_metal_init: use bfloat            = true
0.00.645.694 I ggml_metal_init: hasUnifiedMemory      = true
0.00.645.697 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.664.303 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.667.821 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.667.830 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.667.864 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.671.191 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.671.193 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.671.193 I llama_init_from_model: graph nodes  = 967
0.00.671.194 I llama_init_from_model: graph splits = 2
0.00.671.196 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.671.196 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.696.600 I 
0.00.696.679 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.696.700 I perplexity: tokenizing the input ..
0.00.703.974 I perplexity: tokenization took 7.27 ms
0.00.703.994 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.837.975 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.839.314 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.839.328 I llama_perf_context_print:        load time =     687.62 ms
0.00.839.329 I llama_perf_context_print: prompt eval time =     133.12 ms /   128 tokens (    1.04 ms per token,   961.52 tokens per second)
0.00.839.330 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.839.330 I llama_perf_context_print:       total time =     142.73 ms /   129 tokens
0.00.839.721 I ggml_metal_free: deallocating

real	0m0.853s
user	0m0.078s
sys	0m0.122s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4590 (e0449763) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.874 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.036 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.040 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.042 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.042 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.044 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.044 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.045 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.046 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.046 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.046 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.048 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.048 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.049 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.049 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.053 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.053 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.054 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.670 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.668 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.339 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.340 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.340 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.340 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.341 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.341 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.342 I llama_model_loader: - type  f32:  194 tensors
0.00.025.342 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.342 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.343 I print_info: file format = GGUF V3 (latest)
0.00.025.343 I print_info: file type   = Q5_0
0.00.025.344 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.262 I load: special tokens cache size = 25
0.00.039.068 I load: token to piece cache size = 0.2984 MB
0.00.039.071 I print_info: arch             = gptneox
0.00.039.071 I print_info: vocab_only       = 0
0.00.039.072 I print_info: n_ctx_train      = 2048
0.00.039.072 I print_info: n_embd           = 2048
0.00.039.072 I print_info: n_layer          = 24
0.00.039.075 I print_info: n_head           = 16
0.00.039.076 I print_info: n_head_kv        = 16
0.00.039.076 I print_info: n_rot            = 32
0.00.039.076 I print_info: n_swa            = 0
0.00.039.077 I print_info: n_embd_head_k    = 128
0.00.039.079 I print_info: n_embd_head_v    = 128
0.00.039.079 I print_info: n_gqa            = 1
0.00.039.080 I print_info: n_embd_k_gqa     = 2048
0.00.039.081 I print_info: n_embd_v_gqa     = 2048
0.00.039.082 I print_info: f_norm_eps       = 1.0e-05
0.00.039.082 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.082 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.083 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.083 I print_info: f_logit_scale    = 0.0e+00
0.00.039.083 I print_info: n_ff             = 8192
0.00.039.084 I print_info: n_expert         = 0
0.00.039.084 I print_info: n_expert_used    = 0
0.00.039.084 I print_info: causal attn      = 1
0.00.039.084 I print_info: pooling type     = 0
0.00.039.084 I print_info: rope type        = 2
0.00.039.085 I print_info: rope scaling     = linear
0.00.039.086 I print_info: freq_base_train  = 10000.0
0.00.039.087 I print_info: freq_scale_train = 1
0.00.039.087 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.087 I print_info: rope_finetuned   = unknown
0.00.039.087 I print_info: ssm_d_conv       = 0
0.00.039.087 I print_info: ssm_d_inner      = 0
0.00.039.087 I print_info: ssm_d_state      = 0
0.00.039.087 I print_info: ssm_dt_rank      = 0
0.00.039.088 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.088 I print_info: model type       = 1.4B
0.00.039.088 I print_info: model params     = 1.41 B
0.00.039.088 I print_info: general.name     = 1.4B
0.00.039.089 I print_info: vocab type       = BPE
0.00.039.089 I print_info: n_vocab          = 50304
0.00.039.093 I print_info: n_merges         = 50009
0.00.039.093 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.093 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.093 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.094 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.094 I print_info: LF token         = 128 'Ä'
0.00.039.094 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.094 I print_info: max token length = 1024
0.00.663.842 I load_tensors: offloading 24 repeating layers to GPU
0.00.663.855 I load_tensors: offloading output layer to GPU
0.00.663.855 I load_tensors: offloaded 25/25 layers to GPU
0.00.663.887 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.663.888 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.665.266 I llama_init_from_model: n_seq_max     = 1
0.00.665.269 I llama_init_from_model: n_ctx         = 128
0.00.665.270 I llama_init_from_model: n_ctx_per_seq = 128
0.00.665.270 I llama_init_from_model: n_batch       = 128
0.00.665.270 I llama_init_from_model: n_ubatch      = 128
0.00.665.271 I llama_init_from_model: flash_attn    = 0
0.00.665.272 I llama_init_from_model: freq_base     = 10000.0
0.00.665.273 I llama_init_from_model: freq_scale    = 1
0.00.665.274 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.665.275 I ggml_metal_init: allocating
0.00.665.289 I ggml_metal_init: found device: Apple M4
0.00.665.301 I ggml_metal_init: picking default device: Apple M4
0.00.666.623 I ggml_metal_init: using embedded metal library
0.00.672.830 I ggml_metal_init: GPU name:   Apple M4
0.00.672.834 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.672.835 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.672.836 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.672.836 I ggml_metal_init: simdgroup reduction   = true
0.00.672.837 I ggml_metal_init: simdgroup matrix mul. = true
0.00.672.837 I ggml_metal_init: has residency sets    = true
0.00.672.837 I ggml_metal_init: has bfloat            = true
0.00.672.837 I ggml_metal_init: use bfloat            = true
0.00.672.838 I ggml_metal_init: hasUnifiedMemory      = true
0.00.672.840 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.690.241 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.693.866 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.693.873 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.693.902 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.697.110 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.697.111 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.697.112 I llama_init_from_model: graph nodes  = 967
0.00.697.112 I llama_init_from_model: graph splits = 2
0.00.697.115 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.697.115 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.726.151 I 
0.00.726.239 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.726.258 I perplexity: tokenizing the input ..
0.00.733.646 I perplexity: tokenization took 7.383 ms
0.00.733.669 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.869.935 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.871.362 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.871.383 I llama_perf_context_print:        load time =     716.27 ms
0.00.871.384 I llama_perf_context_print: prompt eval time =     135.33 ms /   128 tokens (    1.06 ms per token,   945.81 tokens per second)
0.00.871.387 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.871.387 I llama_perf_context_print:       total time =     145.23 ms /   129 tokens
0.00.871.794 I ggml_metal_free: deallocating

real	0m0.888s
user	0m0.078s
sys	0m0.139s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4590 (e0449763) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.089 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.935 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.939 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.940 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.941 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.941 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.942 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.942 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.943 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.944 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.944 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.945 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.945 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.946 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.948 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.949 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.950 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.950 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.706 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.880 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.651 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.652 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.653 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.653 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.653 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.654 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.654 I llama_model_loader: - type  f32:  194 tensors
0.00.024.654 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.654 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.655 I print_info: file format = GGUF V3 (latest)
0.00.024.655 I print_info: file type   = Q5_1
0.00.024.656 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.545 I load: special tokens cache size = 25
0.00.038.623 I load: token to piece cache size = 0.2984 MB
0.00.038.628 I print_info: arch             = gptneox
0.00.038.628 I print_info: vocab_only       = 0
0.00.038.629 I print_info: n_ctx_train      = 2048
0.00.038.629 I print_info: n_embd           = 2048
0.00.038.629 I print_info: n_layer          = 24
0.00.038.637 I print_info: n_head           = 16
0.00.038.638 I print_info: n_head_kv        = 16
0.00.038.638 I print_info: n_rot            = 32
0.00.038.638 I print_info: n_swa            = 0
0.00.038.638 I print_info: n_embd_head_k    = 128
0.00.038.638 I print_info: n_embd_head_v    = 128
0.00.038.639 I print_info: n_gqa            = 1
0.00.038.639 I print_info: n_embd_k_gqa     = 2048
0.00.038.640 I print_info: n_embd_v_gqa     = 2048
0.00.038.640 I print_info: f_norm_eps       = 1.0e-05
0.00.038.641 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.641 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.641 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.641 I print_info: f_logit_scale    = 0.0e+00
0.00.038.644 I print_info: n_ff             = 8192
0.00.038.644 I print_info: n_expert         = 0
0.00.038.645 I print_info: n_expert_used    = 0
0.00.038.645 I print_info: causal attn      = 1
0.00.038.645 I print_info: pooling type     = 0
0.00.038.645 I print_info: rope type        = 2
0.00.038.645 I print_info: rope scaling     = linear
0.00.038.649 I print_info: freq_base_train  = 10000.0
0.00.038.650 I print_info: freq_scale_train = 1
0.00.038.650 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.650 I print_info: rope_finetuned   = unknown
0.00.038.650 I print_info: ssm_d_conv       = 0
0.00.038.650 I print_info: ssm_d_inner      = 0
0.00.038.650 I print_info: ssm_d_state      = 0
0.00.038.651 I print_info: ssm_dt_rank      = 0
0.00.038.651 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.651 I print_info: model type       = 1.4B
0.00.038.651 I print_info: model params     = 1.41 B
0.00.038.651 I print_info: general.name     = 1.4B
0.00.038.652 I print_info: vocab type       = BPE
0.00.038.652 I print_info: n_vocab          = 50304
0.00.038.653 I print_info: n_merges         = 50009
0.00.038.653 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.653 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.653 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.653 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.654 I print_info: LF token         = 128 'Ä'
0.00.038.655 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.655 I print_info: max token length = 1024
0.00.617.842 I load_tensors: offloading 24 repeating layers to GPU
0.00.617.854 I load_tensors: offloading output layer to GPU
0.00.617.854 I load_tensors: offloaded 25/25 layers to GPU
0.00.617.889 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.617.890 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.619.402 I llama_init_from_model: n_seq_max     = 1
0.00.619.406 I llama_init_from_model: n_ctx         = 128
0.00.619.407 I llama_init_from_model: n_ctx_per_seq = 128
0.00.619.413 I llama_init_from_model: n_batch       = 128
0.00.619.413 I llama_init_from_model: n_ubatch      = 128
0.00.619.414 I llama_init_from_model: flash_attn    = 0
0.00.619.416 I llama_init_from_model: freq_base     = 10000.0
0.00.619.417 I llama_init_from_model: freq_scale    = 1
0.00.619.417 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.619.419 I ggml_metal_init: allocating
0.00.619.503 I ggml_metal_init: found device: Apple M4
0.00.619.517 I ggml_metal_init: picking default device: Apple M4
0.00.621.090 I ggml_metal_init: using embedded metal library
0.00.627.475 I ggml_metal_init: GPU name:   Apple M4
0.00.627.479 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.627.480 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.627.481 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.627.481 I ggml_metal_init: simdgroup reduction   = true
0.00.627.481 I ggml_metal_init: simdgroup matrix mul. = true
0.00.627.482 I ggml_metal_init: has residency sets    = true
0.00.627.482 I ggml_metal_init: has bfloat            = true
0.00.627.482 I ggml_metal_init: use bfloat            = true
0.00.627.483 I ggml_metal_init: hasUnifiedMemory      = true
0.00.627.495 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.644.772 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.648.268 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.648.272 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.648.301 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.651.504 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.651.505 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.651.506 I llama_init_from_model: graph nodes  = 967
0.00.651.506 I llama_init_from_model: graph splits = 2
0.00.651.509 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.651.509 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.684.484 I 
0.00.684.574 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.684.594 I perplexity: tokenizing the input ..
0.00.691.529 I perplexity: tokenization took 6.932 ms
0.00.691.552 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.834.116 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.835.447 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.835.463 I llama_perf_context_print:        load time =     675.38 ms
0.00.835.464 I llama_perf_context_print: prompt eval time =     141.67 ms /   128 tokens (    1.11 ms per token,   903.48 tokens per second)
0.00.835.464 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.835.465 I llama_perf_context_print:       total time =     150.99 ms /   129 tokens
0.00.835.873 I ggml_metal_free: deallocating

real	0m0.850s
user	0m0.079s
sys	0m0.148s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4590 (e0449763) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.073 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.773 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.778 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.780 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.780 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.780 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.781 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.781 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.784 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.784 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.784 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.785 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.785 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.786 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.786 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.787 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.788 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.788 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.463 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.515 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.192 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.193 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.193 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.194 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.194 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.194 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.195 I llama_model_loader: - type  f32:  194 tensors
0.00.025.195 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.195 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.195 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.196 I print_info: file format = GGUF V3 (latest)
0.00.025.196 I print_info: file type   = Q2_K - Medium
0.00.025.197 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.084 I load: special tokens cache size = 25
0.00.038.773 I load: token to piece cache size = 0.2984 MB
0.00.038.775 I print_info: arch             = gptneox
0.00.038.776 I print_info: vocab_only       = 0
0.00.038.776 I print_info: n_ctx_train      = 2048
0.00.038.776 I print_info: n_embd           = 2048
0.00.038.776 I print_info: n_layer          = 24
0.00.038.779 I print_info: n_head           = 16
0.00.038.780 I print_info: n_head_kv        = 16
0.00.038.780 I print_info: n_rot            = 32
0.00.038.780 I print_info: n_swa            = 0
0.00.038.780 I print_info: n_embd_head_k    = 128
0.00.038.782 I print_info: n_embd_head_v    = 128
0.00.038.783 I print_info: n_gqa            = 1
0.00.038.784 I print_info: n_embd_k_gqa     = 2048
0.00.038.785 I print_info: n_embd_v_gqa     = 2048
0.00.038.785 I print_info: f_norm_eps       = 1.0e-05
0.00.038.786 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.786 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.786 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.786 I print_info: f_logit_scale    = 0.0e+00
0.00.038.787 I print_info: n_ff             = 8192
0.00.038.787 I print_info: n_expert         = 0
0.00.038.787 I print_info: n_expert_used    = 0
0.00.038.787 I print_info: causal attn      = 1
0.00.038.787 I print_info: pooling type     = 0
0.00.038.788 I print_info: rope type        = 2
0.00.038.788 I print_info: rope scaling     = linear
0.00.038.789 I print_info: freq_base_train  = 10000.0
0.00.038.790 I print_info: freq_scale_train = 1
0.00.038.790 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.790 I print_info: rope_finetuned   = unknown
0.00.038.790 I print_info: ssm_d_conv       = 0
0.00.038.790 I print_info: ssm_d_inner      = 0
0.00.038.790 I print_info: ssm_d_state      = 0
0.00.038.791 I print_info: ssm_dt_rank      = 0
0.00.038.791 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.791 I print_info: model type       = 1.4B
0.00.038.791 I print_info: model params     = 1.41 B
0.00.038.791 I print_info: general.name     = 1.4B
0.00.038.792 I print_info: vocab type       = BPE
0.00.038.792 I print_info: n_vocab          = 50304
0.00.038.794 I print_info: n_merges         = 50009
0.00.038.794 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.794 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.794 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.795 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.795 I print_info: LF token         = 128 'Ä'
0.00.038.796 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.796 I print_info: max token length = 1024
0.00.342.507 I load_tensors: offloading 24 repeating layers to GPU
0.00.342.520 I load_tensors: offloading output layer to GPU
0.00.342.521 I load_tensors: offloaded 25/25 layers to GPU
0.00.342.557 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.342.558 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.343.954 I llama_init_from_model: n_seq_max     = 1
0.00.343.959 I llama_init_from_model: n_ctx         = 128
0.00.343.960 I llama_init_from_model: n_ctx_per_seq = 128
0.00.343.960 I llama_init_from_model: n_batch       = 128
0.00.343.960 I llama_init_from_model: n_ubatch      = 128
0.00.343.961 I llama_init_from_model: flash_attn    = 0
0.00.343.963 I llama_init_from_model: freq_base     = 10000.0
0.00.343.963 I llama_init_from_model: freq_scale    = 1
0.00.343.964 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.343.976 I ggml_metal_init: allocating
0.00.344.052 I ggml_metal_init: found device: Apple M4
0.00.344.065 I ggml_metal_init: picking default device: Apple M4
0.00.345.798 I ggml_metal_init: using embedded metal library
0.00.351.385 I ggml_metal_init: GPU name:   Apple M4
0.00.351.403 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.351.404 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.351.405 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.351.405 I ggml_metal_init: simdgroup reduction   = true
0.00.351.406 I ggml_metal_init: simdgroup matrix mul. = true
0.00.351.406 I ggml_metal_init: has residency sets    = true
0.00.351.406 I ggml_metal_init: has bfloat            = true
0.00.351.407 I ggml_metal_init: use bfloat            = true
0.00.351.409 I ggml_metal_init: hasUnifiedMemory      = true
0.00.351.413 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.372.604 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.376.118 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.376.131 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.376.180 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.379.478 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.379.480 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.379.481 I llama_init_from_model: graph nodes  = 967
0.00.379.481 I llama_init_from_model: graph splits = 2
0.00.379.484 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.379.484 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.412.966 I 
0.00.413.059 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.413.090 I perplexity: tokenizing the input ..
0.00.419.972 I perplexity: tokenization took 6.879 ms
0.00.419.992 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.563.787 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.565.124 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.565.138 I llama_perf_context_print:        load time =     402.89 ms
0.00.565.139 I llama_perf_context_print: prompt eval time =     142.89 ms /   128 tokens (    1.12 ms per token,   895.79 tokens per second)
0.00.565.139 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.565.140 I llama_perf_context_print:       total time =     152.18 ms /   129 tokens
0.00.565.519 I ggml_metal_free: deallocating

real	0m0.582s
user	0m0.080s
sys	0m0.096s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4590 (e0449763) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.697 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.745 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.750 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.751 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.751 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.751 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.752 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.753 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.754 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.754 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.755 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.755 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.757 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.757 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.758 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.762 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.762 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.763 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.425 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.408 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.149 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.150 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.151 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.151 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.151 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.152 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.152 I llama_model_loader: - type  f32:  194 tensors
0.00.024.152 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.152 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.152 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.153 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.153 I print_info: file format = GGUF V3 (latest)
0.00.024.154 I print_info: file type   = Q3_K - Medium
0.00.024.154 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.031.668 I load: special tokens cache size = 25
0.00.037.418 I load: token to piece cache size = 0.2984 MB
0.00.037.421 I print_info: arch             = gptneox
0.00.037.421 I print_info: vocab_only       = 0
0.00.037.421 I print_info: n_ctx_train      = 2048
0.00.037.421 I print_info: n_embd           = 2048
0.00.037.421 I print_info: n_layer          = 24
0.00.037.424 I print_info: n_head           = 16
0.00.037.425 I print_info: n_head_kv        = 16
0.00.037.425 I print_info: n_rot            = 32
0.00.037.425 I print_info: n_swa            = 0
0.00.037.425 I print_info: n_embd_head_k    = 128
0.00.037.427 I print_info: n_embd_head_v    = 128
0.00.037.427 I print_info: n_gqa            = 1
0.00.037.428 I print_info: n_embd_k_gqa     = 2048
0.00.037.429 I print_info: n_embd_v_gqa     = 2048
0.00.037.434 I print_info: f_norm_eps       = 1.0e-05
0.00.037.435 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.435 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.435 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.435 I print_info: f_logit_scale    = 0.0e+00
0.00.037.436 I print_info: n_ff             = 8192
0.00.037.436 I print_info: n_expert         = 0
0.00.037.436 I print_info: n_expert_used    = 0
0.00.037.436 I print_info: causal attn      = 1
0.00.037.437 I print_info: pooling type     = 0
0.00.037.438 I print_info: rope type        = 2
0.00.037.440 I print_info: rope scaling     = linear
0.00.037.440 I print_info: freq_base_train  = 10000.0
0.00.037.440 I print_info: freq_scale_train = 1
0.00.037.441 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.441 I print_info: rope_finetuned   = unknown
0.00.037.441 I print_info: ssm_d_conv       = 0
0.00.037.442 I print_info: ssm_d_inner      = 0
0.00.037.443 I print_info: ssm_d_state      = 0
0.00.037.443 I print_info: ssm_dt_rank      = 0
0.00.037.443 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.443 I print_info: model type       = 1.4B
0.00.037.443 I print_info: model params     = 1.41 B
0.00.037.444 I print_info: general.name     = 1.4B
0.00.037.444 I print_info: vocab type       = BPE
0.00.037.444 I print_info: n_vocab          = 50304
0.00.037.444 I print_info: n_merges         = 50009
0.00.037.446 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.446 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.446 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.446 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.447 I print_info: LF token         = 128 'Ä'
0.00.037.447 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.449 I print_info: max token length = 1024
0.00.443.712 I load_tensors: offloading 24 repeating layers to GPU
0.00.443.724 I load_tensors: offloading output layer to GPU
0.00.443.725 I load_tensors: offloaded 25/25 layers to GPU
0.00.443.758 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.443.760 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.445.221 I llama_init_from_model: n_seq_max     = 1
0.00.445.226 I llama_init_from_model: n_ctx         = 128
0.00.445.227 I llama_init_from_model: n_ctx_per_seq = 128
0.00.445.227 I llama_init_from_model: n_batch       = 128
0.00.445.227 I llama_init_from_model: n_ubatch      = 128
0.00.445.228 I llama_init_from_model: flash_attn    = 0
0.00.445.230 I llama_init_from_model: freq_base     = 10000.0
0.00.445.230 I llama_init_from_model: freq_scale    = 1
0.00.445.235 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.445.241 I ggml_metal_init: allocating
0.00.445.315 I ggml_metal_init: found device: Apple M4
0.00.445.329 I ggml_metal_init: picking default device: Apple M4
0.00.447.155 I ggml_metal_init: using embedded metal library
0.00.452.658 I ggml_metal_init: GPU name:   Apple M4
0.00.452.677 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.452.678 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.452.679 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.452.679 I ggml_metal_init: simdgroup reduction   = true
0.00.452.680 I ggml_metal_init: simdgroup matrix mul. = true
0.00.452.680 I ggml_metal_init: has residency sets    = true
0.00.452.680 I ggml_metal_init: has bfloat            = true
0.00.452.681 I ggml_metal_init: use bfloat            = true
0.00.452.683 I ggml_metal_init: hasUnifiedMemory      = true
0.00.452.686 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.472.970 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.476.501 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.476.511 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.476.558 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.479.779 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.479.781 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.479.781 I llama_init_from_model: graph nodes  = 967
0.00.479.782 I llama_init_from_model: graph splits = 2
0.00.479.785 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.479.785 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.505.475 I 
0.00.505.550 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.505.569 I perplexity: tokenizing the input ..
0.00.512.610 I perplexity: tokenization took 7.037 ms
0.00.512.630 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.646.287 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.647.859 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.647.875 I llama_perf_context_print:        load time =     496.77 ms
0.00.647.876 I llama_perf_context_print: prompt eval time =     132.74 ms /   128 tokens (    1.04 ms per token,   964.28 tokens per second)
0.00.647.877 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.647.877 I llama_perf_context_print:       total time =     142.41 ms /   129 tokens
0.00.648.306 I ggml_metal_free: deallocating

real	0m0.662s
user	0m0.078s
sys	0m0.114s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4590 (e0449763) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.795 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.910 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.916 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.918 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.919 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.919 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.919 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.919 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.920 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.921 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.921 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.922 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.922 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.922 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.923 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.924 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.925 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.925 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.647 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.636 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.324 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.325 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.325 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.325 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.326 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.326 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.327 I llama_model_loader: - type  f32:  194 tensors
0.00.024.327 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.327 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.327 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.328 I print_info: file format = GGUF V3 (latest)
0.00.024.328 I print_info: file type   = Q4_K - Medium
0.00.024.329 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.411 I load: special tokens cache size = 25
0.00.038.383 I load: token to piece cache size = 0.2984 MB
0.00.038.386 I print_info: arch             = gptneox
0.00.038.386 I print_info: vocab_only       = 0
0.00.038.386 I print_info: n_ctx_train      = 2048
0.00.038.387 I print_info: n_embd           = 2048
0.00.038.387 I print_info: n_layer          = 24
0.00.038.390 I print_info: n_head           = 16
0.00.038.391 I print_info: n_head_kv        = 16
0.00.038.391 I print_info: n_rot            = 32
0.00.038.391 I print_info: n_swa            = 0
0.00.038.391 I print_info: n_embd_head_k    = 128
0.00.038.391 I print_info: n_embd_head_v    = 128
0.00.038.393 I print_info: n_gqa            = 1
0.00.038.394 I print_info: n_embd_k_gqa     = 2048
0.00.038.395 I print_info: n_embd_v_gqa     = 2048
0.00.038.395 I print_info: f_norm_eps       = 1.0e-05
0.00.038.396 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.396 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.396 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.396 I print_info: f_logit_scale    = 0.0e+00
0.00.038.397 I print_info: n_ff             = 8192
0.00.038.397 I print_info: n_expert         = 0
0.00.038.397 I print_info: n_expert_used    = 0
0.00.038.398 I print_info: causal attn      = 1
0.00.038.398 I print_info: pooling type     = 0
0.00.038.398 I print_info: rope type        = 2
0.00.038.398 I print_info: rope scaling     = linear
0.00.038.398 I print_info: freq_base_train  = 10000.0
0.00.038.399 I print_info: freq_scale_train = 1
0.00.038.399 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.399 I print_info: rope_finetuned   = unknown
0.00.038.399 I print_info: ssm_d_conv       = 0
0.00.038.399 I print_info: ssm_d_inner      = 0
0.00.038.400 I print_info: ssm_d_state      = 0
0.00.038.400 I print_info: ssm_dt_rank      = 0
0.00.038.400 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.400 I print_info: model type       = 1.4B
0.00.038.401 I print_info: model params     = 1.41 B
0.00.038.401 I print_info: general.name     = 1.4B
0.00.038.401 I print_info: vocab type       = BPE
0.00.038.401 I print_info: n_vocab          = 50304
0.00.038.402 I print_info: n_merges         = 50009
0.00.038.402 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.402 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.404 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.405 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.405 I print_info: LF token         = 128 'Ä'
0.00.038.405 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.405 I print_info: max token length = 1024
0.00.528.207 I load_tensors: offloading 24 repeating layers to GPU
0.00.528.222 I load_tensors: offloading output layer to GPU
0.00.528.222 I load_tensors: offloaded 25/25 layers to GPU
0.00.528.258 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.528.259 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.529.859 I llama_init_from_model: n_seq_max     = 1
0.00.529.864 I llama_init_from_model: n_ctx         = 128
0.00.529.865 I llama_init_from_model: n_ctx_per_seq = 128
0.00.529.865 I llama_init_from_model: n_batch       = 128
0.00.529.865 I llama_init_from_model: n_ubatch      = 128
0.00.529.866 I llama_init_from_model: flash_attn    = 0
0.00.529.868 I llama_init_from_model: freq_base     = 10000.0
0.00.529.868 I llama_init_from_model: freq_scale    = 1
0.00.529.869 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.529.878 I ggml_metal_init: allocating
0.00.529.998 I ggml_metal_init: found device: Apple M4
0.00.530.016 I ggml_metal_init: picking default device: Apple M4
0.00.531.890 I ggml_metal_init: using embedded metal library
0.00.538.405 I ggml_metal_init: GPU name:   Apple M4
0.00.538.410 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.538.411 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.538.412 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.538.413 I ggml_metal_init: simdgroup reduction   = true
0.00.538.413 I ggml_metal_init: simdgroup matrix mul. = true
0.00.538.413 I ggml_metal_init: has residency sets    = true
0.00.538.413 I ggml_metal_init: has bfloat            = true
0.00.538.414 I ggml_metal_init: use bfloat            = true
0.00.538.415 I ggml_metal_init: hasUnifiedMemory      = true
0.00.538.417 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.556.763 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.560.349 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.560.353 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.560.385 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.563.530 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.563.532 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.563.532 I llama_init_from_model: graph nodes  = 967
0.00.563.532 I llama_init_from_model: graph splits = 2
0.00.563.536 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.563.536 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.592.076 I 
0.00.592.163 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.592.182 I perplexity: tokenizing the input ..
0.00.599.454 I perplexity: tokenization took 7.268 ms
0.00.599.483 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.743.132 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.744.471 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.744.487 I llama_perf_context_print:        load time =     583.27 ms
0.00.744.488 I llama_perf_context_print: prompt eval time =     142.71 ms /   128 tokens (    1.11 ms per token,   896.90 tokens per second)
0.00.744.489 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.744.489 I llama_perf_context_print:       total time =     152.42 ms /   129 tokens
0.00.744.870 I ggml_metal_free: deallocating

real	0m0.759s
user	0m0.079s
sys	0m0.126s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4590 (e0449763) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.844 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.746 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.750 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.751 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.752 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.752 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.752 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.752 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.753 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.754 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.754 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.754 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.755 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.755 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.756 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.757 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.758 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.758 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.452 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.472 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.152 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.153 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.154 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.154 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.154 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.155 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.155 I llama_model_loader: - type  f32:  194 tensors
0.00.025.156 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.156 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.156 I print_info: file format = GGUF V3 (latest)
0.00.025.157 I print_info: file type   = Q5_K - Medium
0.00.025.159 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.085 I load: special tokens cache size = 25
0.00.039.097 I load: token to piece cache size = 0.2984 MB
0.00.039.100 I print_info: arch             = gptneox
0.00.039.100 I print_info: vocab_only       = 0
0.00.039.101 I print_info: n_ctx_train      = 2048
0.00.039.101 I print_info: n_embd           = 2048
0.00.039.101 I print_info: n_layer          = 24
0.00.039.104 I print_info: n_head           = 16
0.00.039.105 I print_info: n_head_kv        = 16
0.00.039.105 I print_info: n_rot            = 32
0.00.039.105 I print_info: n_swa            = 0
0.00.039.106 I print_info: n_embd_head_k    = 128
0.00.039.106 I print_info: n_embd_head_v    = 128
0.00.039.107 I print_info: n_gqa            = 1
0.00.039.107 I print_info: n_embd_k_gqa     = 2048
0.00.039.108 I print_info: n_embd_v_gqa     = 2048
0.00.039.109 I print_info: f_norm_eps       = 1.0e-05
0.00.039.109 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.109 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.110 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.110 I print_info: f_logit_scale    = 0.0e+00
0.00.039.111 I print_info: n_ff             = 8192
0.00.039.111 I print_info: n_expert         = 0
0.00.039.111 I print_info: n_expert_used    = 0
0.00.039.112 I print_info: causal attn      = 1
0.00.039.112 I print_info: pooling type     = 0
0.00.039.112 I print_info: rope type        = 2
0.00.039.112 I print_info: rope scaling     = linear
0.00.039.113 I print_info: freq_base_train  = 10000.0
0.00.039.113 I print_info: freq_scale_train = 1
0.00.039.113 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.113 I print_info: rope_finetuned   = unknown
0.00.039.113 I print_info: ssm_d_conv       = 0
0.00.039.114 I print_info: ssm_d_inner      = 0
0.00.039.114 I print_info: ssm_d_state      = 0
0.00.039.114 I print_info: ssm_dt_rank      = 0
0.00.039.114 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.114 I print_info: model type       = 1.4B
0.00.039.115 I print_info: model params     = 1.41 B
0.00.039.115 I print_info: general.name     = 1.4B
0.00.039.115 I print_info: vocab type       = BPE
0.00.039.116 I print_info: n_vocab          = 50304
0.00.039.116 I print_info: n_merges         = 50009
0.00.039.116 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.116 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.116 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.117 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.117 I print_info: LF token         = 128 'Ä'
0.00.039.117 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.117 I print_info: max token length = 1024
0.00.587.213 I load_tensors: offloading 24 repeating layers to GPU
0.00.587.217 I load_tensors: offloading output layer to GPU
0.00.587.217 I load_tensors: offloaded 25/25 layers to GPU
0.00.587.238 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.587.240 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.588.649 I llama_init_from_model: n_seq_max     = 1
0.00.588.651 I llama_init_from_model: n_ctx         = 128
0.00.588.651 I llama_init_from_model: n_ctx_per_seq = 128
0.00.588.655 I llama_init_from_model: n_batch       = 128
0.00.588.656 I llama_init_from_model: n_ubatch      = 128
0.00.588.656 I llama_init_from_model: flash_attn    = 0
0.00.588.657 I llama_init_from_model: freq_base     = 10000.0
0.00.588.658 I llama_init_from_model: freq_scale    = 1
0.00.588.659 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.588.660 I ggml_metal_init: allocating
0.00.588.700 I ggml_metal_init: found device: Apple M4
0.00.588.711 I ggml_metal_init: picking default device: Apple M4
0.00.590.175 I ggml_metal_init: using embedded metal library
0.00.596.209 I ggml_metal_init: GPU name:   Apple M4
0.00.596.213 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.596.214 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.596.215 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.596.215 I ggml_metal_init: simdgroup reduction   = true
0.00.596.215 I ggml_metal_init: simdgroup matrix mul. = true
0.00.596.216 I ggml_metal_init: has residency sets    = true
0.00.596.216 I ggml_metal_init: has bfloat            = true
0.00.596.216 I ggml_metal_init: use bfloat            = true
0.00.596.217 I ggml_metal_init: hasUnifiedMemory      = true
0.00.596.220 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.612.583 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.616.132 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.616.136 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.616.161 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.619.259 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.619.261 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.619.261 I llama_init_from_model: graph nodes  = 967
0.00.619.261 I llama_init_from_model: graph splits = 2
0.00.619.264 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.619.264 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.651.498 I 
0.00.651.586 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.651.605 I perplexity: tokenizing the input ..
0.00.658.597 I perplexity: tokenization took 6.989 ms
0.00.658.615 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.800.667 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.802.027 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.802.048 I llama_perf_context_print:        load time =     641.64 ms
0.00.802.049 I llama_perf_context_print: prompt eval time =     141.16 ms /   128 tokens (    1.10 ms per token,   906.80 tokens per second)
0.00.802.050 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.802.050 I llama_perf_context_print:       total time =     150.55 ms /   129 tokens
0.00.802.436 I ggml_metal_free: deallocating

real	0m0.818s
user	0m0.077s
sys	0m0.138s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4590 (e0449763) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.369 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.164 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.168 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.170 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.175 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.176 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.176 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.177 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.178 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.178 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.178 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.179 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.179 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.179 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.180 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.182 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.182 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.182 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.748 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.713 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.335 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.336 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.336 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.336 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.337 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.337 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.337 I llama_model_loader: - type  f32:  194 tensors
0.00.024.338 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.338 I print_info: file format = GGUF V3 (latest)
0.00.024.338 I print_info: file type   = Q6_K
0.00.024.339 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.031.829 I load: special tokens cache size = 25
0.00.037.589 I load: token to piece cache size = 0.2984 MB
0.00.037.591 I print_info: arch             = gptneox
0.00.037.592 I print_info: vocab_only       = 0
0.00.037.592 I print_info: n_ctx_train      = 2048
0.00.037.592 I print_info: n_embd           = 2048
0.00.037.592 I print_info: n_layer          = 24
0.00.037.595 I print_info: n_head           = 16
0.00.037.596 I print_info: n_head_kv        = 16
0.00.037.597 I print_info: n_rot            = 32
0.00.037.597 I print_info: n_swa            = 0
0.00.037.599 I print_info: n_embd_head_k    = 128
0.00.037.599 I print_info: n_embd_head_v    = 128
0.00.037.600 I print_info: n_gqa            = 1
0.00.037.601 I print_info: n_embd_k_gqa     = 2048
0.00.037.601 I print_info: n_embd_v_gqa     = 2048
0.00.037.602 I print_info: f_norm_eps       = 1.0e-05
0.00.037.602 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.602 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.603 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.603 I print_info: f_logit_scale    = 0.0e+00
0.00.037.603 I print_info: n_ff             = 8192
0.00.037.604 I print_info: n_expert         = 0
0.00.037.604 I print_info: n_expert_used    = 0
0.00.037.604 I print_info: causal attn      = 1
0.00.037.604 I print_info: pooling type     = 0
0.00.037.604 I print_info: rope type        = 2
0.00.037.605 I print_info: rope scaling     = linear
0.00.037.605 I print_info: freq_base_train  = 10000.0
0.00.037.605 I print_info: freq_scale_train = 1
0.00.037.605 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.606 I print_info: rope_finetuned   = unknown
0.00.037.607 I print_info: ssm_d_conv       = 0
0.00.037.607 I print_info: ssm_d_inner      = 0
0.00.037.608 I print_info: ssm_d_state      = 0
0.00.037.608 I print_info: ssm_dt_rank      = 0
0.00.037.608 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.608 I print_info: model type       = 1.4B
0.00.037.609 I print_info: model params     = 1.41 B
0.00.037.609 I print_info: general.name     = 1.4B
0.00.037.609 I print_info: vocab type       = BPE
0.00.037.610 I print_info: n_vocab          = 50304
0.00.037.610 I print_info: n_merges         = 50009
0.00.037.610 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.610 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.611 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.611 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.611 I print_info: LF token         = 128 'Ä'
0.00.037.613 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.613 I print_info: max token length = 1024
0.00.627.902 I load_tensors: offloading 24 repeating layers to GPU
0.00.627.917 I load_tensors: offloading output layer to GPU
0.00.627.918 I load_tensors: offloaded 25/25 layers to GPU
0.00.627.948 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.627.950 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.629.391 I llama_init_from_model: n_seq_max     = 1
0.00.629.400 I llama_init_from_model: n_ctx         = 128
0.00.629.401 I llama_init_from_model: n_ctx_per_seq = 128
0.00.629.401 I llama_init_from_model: n_batch       = 128
0.00.629.401 I llama_init_from_model: n_ubatch      = 128
0.00.629.402 I llama_init_from_model: flash_attn    = 0
0.00.629.406 I llama_init_from_model: freq_base     = 10000.0
0.00.629.407 I llama_init_from_model: freq_scale    = 1
0.00.629.407 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.629.410 I ggml_metal_init: allocating
0.00.629.456 I ggml_metal_init: found device: Apple M4
0.00.629.466 I ggml_metal_init: picking default device: Apple M4
0.00.631.160 I ggml_metal_init: using embedded metal library
0.00.637.651 I ggml_metal_init: GPU name:   Apple M4
0.00.637.654 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.637.655 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.637.656 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.637.657 I ggml_metal_init: simdgroup reduction   = true
0.00.637.657 I ggml_metal_init: simdgroup matrix mul. = true
0.00.637.657 I ggml_metal_init: has residency sets    = true
0.00.637.658 I ggml_metal_init: has bfloat            = true
0.00.637.658 I ggml_metal_init: use bfloat            = true
0.00.637.659 I ggml_metal_init: hasUnifiedMemory      = true
0.00.637.661 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.654.537 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.657.913 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.657.916 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.657.947 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.661.367 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.661.369 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.661.370 I llama_init_from_model: graph nodes  = 967
0.00.661.370 I llama_init_from_model: graph splits = 2
0.00.661.374 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.661.374 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.699.205 I 
0.00.699.318 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.699.357 I perplexity: tokenizing the input ..
0.00.706.408 I perplexity: tokenization took 7.048 ms
0.00.706.428 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.847.283 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.848.558 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.848.575 I llama_perf_context_print:        load time =     689.83 ms
0.00.848.576 I llama_perf_context_print: prompt eval time =     140.00 ms /   128 tokens (    1.09 ms per token,   914.28 tokens per second)
0.00.848.576 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.848.577 I llama_perf_context_print:       total time =     149.38 ms /   129 tokens
0.00.848.985 I ggml_metal_free: deallocating

real	0m0.864s
user	0m0.078s
sys	0m0.156s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.262 I build: 4590 (e0449763) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.614 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.733 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.740 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.743 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.744 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.744 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.745 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.746 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.747 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.748 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.749 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.749 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.750 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.751 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.751 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.754 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.755 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.755 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.654 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.400 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.930 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.932 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.933 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.933 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.934 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.934 I llama_model_loader: - type  f32:  194 tensors
0.00.053.935 I llama_model_loader: - type  f16:   98 tensors
0.00.053.936 I print_info: file format = GGUF V3 (latest)
0.00.053.936 I print_info: file type   = all F32 (guessed)
0.00.053.938 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.065.643 I load: special tokens cache size = 25
0.00.073.522 I load: token to piece cache size = 0.2984 MB
0.00.073.525 I print_info: arch             = gptneox
0.00.073.525 I print_info: vocab_only       = 0
0.00.073.526 I print_info: n_ctx_train      = 2048
0.00.073.526 I print_info: n_embd           = 2048
0.00.073.526 I print_info: n_layer          = 24
0.00.073.529 I print_info: n_head           = 16
0.00.073.530 I print_info: n_head_kv        = 16
0.00.073.530 I print_info: n_rot            = 32
0.00.073.530 I print_info: n_swa            = 0
0.00.073.531 I print_info: n_embd_head_k    = 128
0.00.073.531 I print_info: n_embd_head_v    = 128
0.00.073.532 I print_info: n_gqa            = 1
0.00.073.533 I print_info: n_embd_k_gqa     = 2048
0.00.073.533 I print_info: n_embd_v_gqa     = 2048
0.00.073.534 I print_info: f_norm_eps       = 1.0e-05
0.00.073.534 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.073.535 I print_info: f_clamp_kqv      = 0.0e+00
0.00.073.535 I print_info: f_max_alibi_bias = 0.0e+00
0.00.073.535 I print_info: f_logit_scale    = 0.0e+00
0.00.073.536 I print_info: n_ff             = 8192
0.00.073.536 I print_info: n_expert         = 0
0.00.073.536 I print_info: n_expert_used    = 0
0.00.073.536 I print_info: causal attn      = 1
0.00.073.536 I print_info: pooling type     = 0
0.00.073.538 I print_info: rope type        = 2
0.00.073.539 I print_info: rope scaling     = linear
0.00.073.539 I print_info: freq_base_train  = 10000.0
0.00.073.540 I print_info: freq_scale_train = 1
0.00.073.540 I print_info: n_ctx_orig_yarn  = 2048
0.00.073.540 I print_info: rope_finetuned   = unknown
0.00.073.540 I print_info: ssm_d_conv       = 0
0.00.073.540 I print_info: ssm_d_inner      = 0
0.00.073.540 I print_info: ssm_d_state      = 0
0.00.073.541 I print_info: ssm_dt_rank      = 0
0.00.073.542 I print_info: ssm_dt_b_c_rms   = 0
0.00.073.542 I print_info: model type       = 1.4B
0.00.073.543 I print_info: model params     = 1.41 B
0.00.073.543 I print_info: general.name     = 1.4B
0.00.073.543 I print_info: vocab type       = BPE
0.00.073.543 I print_info: n_vocab          = 50304
0.00.073.544 I print_info: n_merges         = 50009
0.00.073.544 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.073.544 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.073.544 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.073.546 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.073.546 I print_info: LF token         = 128 'Ä'
0.00.073.547 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.073.547 I print_info: max token length = 1024
0.01.334.919 I load_tensors: offloading 24 repeating layers to GPU
0.01.334.926 I load_tensors: offloading output layer to GPU
0.01.334.927 I load_tensors: offloaded 25/25 layers to GPU
0.01.334.954 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.334.956 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.335.933 I llama_init_from_model: n_seq_max     = 1
0.01.335.934 I llama_init_from_model: n_ctx         = 128
0.01.335.934 I llama_init_from_model: n_ctx_per_seq = 128
0.01.335.934 I llama_init_from_model: n_batch       = 128
0.01.335.935 I llama_init_from_model: n_ubatch      = 128
0.01.335.935 I llama_init_from_model: flash_attn    = 0
0.01.335.935 I llama_init_from_model: freq_base     = 10000.0
0.01.335.936 I llama_init_from_model: freq_scale    = 1
0.01.335.936 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.335.937 I ggml_metal_init: allocating
0.01.335.968 I ggml_metal_init: found device: Apple M4
0.01.335.973 I ggml_metal_init: picking default device: Apple M4
0.01.336.938 I ggml_metal_init: using embedded metal library
0.01.340.689 I ggml_metal_init: GPU name:   Apple M4
0.01.340.692 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.340.693 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.340.693 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.340.694 I ggml_metal_init: simdgroup reduction   = true
0.01.340.694 I ggml_metal_init: simdgroup matrix mul. = true
0.01.340.694 I ggml_metal_init: has residency sets    = true
0.01.340.694 I ggml_metal_init: has bfloat            = true
0.01.340.695 I ggml_metal_init: use bfloat            = true
0.01.340.695 I ggml_metal_init: hasUnifiedMemory      = true
0.01.340.696 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.351.085 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.352.763 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.352.765 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.352.780 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.354.406 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.354.407 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.354.407 I llama_init_from_model: graph nodes  = 967
0.01.354.408 I llama_init_from_model: graph splits = 2
0.01.354.409 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.354.409 I 
0.01.354.446 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.354.447 I compute_imatrix: tokenizing the input ..
0.01.358.486 I compute_imatrix: tokenization took 4.038 ms
0.01.358.488 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.625.317 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.627.982 I llama_perf_context_print:        load time =    1603.70 ms
0.01.627.983 I llama_perf_context_print: prompt eval time =     265.09 ms /   128 tokens (    2.07 ms per token,   482.85 tokens per second)
0.01.627.984 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.627.984 I llama_perf_context_print:       total time =    1606.36 ms /   129 tokens
0.01.628.497 I ggml_metal_free: deallocating

real	0m1.837s
user	0m0.125s
sys	0m0.250s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4590 (e0449763)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x123604a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x123605160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x123605710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x123605cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x123606270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x123606820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x123606dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x123607380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x123607930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x123607e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x123608330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x123608830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x123609350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x123609b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12360a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12360aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12360b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12360b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12360bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12360c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12360ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12360d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12360dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12360e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12360ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12360ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12360f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1236101c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x123610700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1236109c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x123610e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x123611120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1236119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x123611ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1236121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x123612650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x123612af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x123612f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x123613430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1236138d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x123613d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x123614210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1236146b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x123614b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x123614e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x123615420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x123615a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x123616350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x123616960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x123616f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x123617580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x123617b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1236181a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1236187b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x123618fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x123619440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1236198e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x123619ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12361a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12361a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12361ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12361b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12361b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12361ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12361bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12361c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12361c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12361ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12361d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12361d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12361daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12361df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12361e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12361e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12361ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12361f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12361f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12361fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1236203c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x123620910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x123620e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1236213b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x123621900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x123621e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1236223a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1236228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x123622e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x123623390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1236238e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x123623e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x123624380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1236248d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x123624e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x123625370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1236258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x123625e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x123626360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x123616040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1236267d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x123626f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1236274d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x123627a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x123627f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1236284c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x123628a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x123628f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1236294b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x123629a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x123629f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12362a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12362a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12362af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12362b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12362b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12362bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12362c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12362c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12362cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12362d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12362d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12362d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12362de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12362e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12362e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12362ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12362f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12362f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12362f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12362fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x123630330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1236307d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x123630c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x123631110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1236315b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x123631a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x123631ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x123632390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x123632830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x123632cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x123633170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x123633610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x123633ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x123633f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1236343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x123634890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x123634d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1236351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x123635670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x123635b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x123635fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x123636450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1236368f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x123636d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x123637230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1236376d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x123637b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x123638010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1236384b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x123638950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x123638df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x123639290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x123639730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x123639bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12363a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12363a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12363a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12363ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12363b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12363b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12363bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12363c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12363c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12363ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12363ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12363d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12363d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12363dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12363e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12363e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12363ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12363ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12363f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12363f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12363fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x123640190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x123640630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x123640ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x123640f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x123641410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1236418b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x123641d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1236421f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x123642690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x123642be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x123643130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x123643680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x123643bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x123643e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1236444a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x123644ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1236450c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1236458b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x123645d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x123646010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x123646620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x123646c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x123647420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1236478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x123647d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x123648200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1236489b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x123648f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x123649450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1236499a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x123649ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12364a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12364a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12364aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12364b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12364b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12364bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12364c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12364c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12364cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12364d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12364d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12364deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12364e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12364e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12364eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12364f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12364f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12364fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1236503e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x123650930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x123650e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1236513d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x123651920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x123651e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1236523c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x123652910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x123652e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1236533b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x123653900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x123653e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1236543a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1236548f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x123654e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x123655390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1236558e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x123655e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x123656380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1236568d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x123656e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x123657370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1236578c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x123657e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x123658360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1236588b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x123658e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x123659350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1236598a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x123659df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12365a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12365a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12365ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12365b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12365b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12365bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12365c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12365c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12365ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12365cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12365d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12365d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12365dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12365e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12365e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12365eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12365ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12365f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12365f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12365fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x123660500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x123660c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x123661340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x123661a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x123661d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x123662510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1236627d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x123662de0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.718.763 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.718.766 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x123662a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x123644760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x123644150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x123644d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x123617e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x123617840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x123619e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1236468e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12360f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x123615cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x123616610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x123616c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1236150d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x123617230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12360e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x123604080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x123618a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12361a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x123626a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x123661fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1236113e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1236116a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x123646ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x123645380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12360f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12360fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12360fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x123663240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x123663500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1236637c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x123663a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x123663d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x123664000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1236642c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x123664580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x123664840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x123664b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x123664dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x123665080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x123665340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x123665600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1236658c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x123665b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x123665e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x123666100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1236663c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x123666680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x123666940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x123666c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x123666ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x123667180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x123667440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x123667700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1236679c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x123667c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x123667f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x123668200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1236684c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x123668780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x123668a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x123668d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x123668fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x123669280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x123669540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x123669800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x123669ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x123669d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12366a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12366a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12366a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12366a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12366ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12366ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12366b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12366b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12366b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12366b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12366bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12366be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12366c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12366c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12366c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12366c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12366cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12366cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12366d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12366d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12366d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12366da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12366dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12366df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12366e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12366e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12366e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12366ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12366ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12366f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12366f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12366f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12366f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12366fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12366fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x123670080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x123670340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x123670600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1236708c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x123670b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x123670e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x123671100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1236713c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x123671680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x123671940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x123671c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x123671ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x123672180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x123672440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x123672700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1236729c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x123672c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x123672f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x123673200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1236734c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x123673780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x123673a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x123673d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x123673fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x123674280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x123674540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x123674800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x123674ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x123674d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x123675040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x123675300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1236755c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x123675880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x123675b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x123675e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1236760c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x123676380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x123676640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x123676900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x123676bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x123676e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x123677140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x123677400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1236776c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x123677980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x123677c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x123677f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1236781c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x123678480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x123678740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x123678a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x123678cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x123678f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x123679240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x123679500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1236797c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x123679a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x123679d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12367a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12367a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12367a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12367a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12367ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12367adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12367b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12367b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12367b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12367b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12367bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12367be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12367c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12367c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12367c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12367c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12367cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12367cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12367d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12367d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12367d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12367d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12367dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12367df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12367e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12367e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12367e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12367ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12367ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12367efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12367f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12367f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12367f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12367fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12367fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x123680040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x123680300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1236805c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x123680880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x123680b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x123680e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1236810c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x123681380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x123681640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x123681900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x123681bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x123681e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x123682140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x123682400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1236826c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x123682c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x123682f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x123683210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1236834d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x123683790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x123683a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x123683d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x123683fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x123684290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x123684550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x123684810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x123684ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x123684d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x123685050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x123685310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1236855d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x123685890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x123685b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x123685e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1236860d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x123686390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x123686650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x123686910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x123686bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x123686e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x123687150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x123687410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1236876d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x123687990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x123687c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x123687f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1236881d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x123688490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1236889e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x123688f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x123689480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1236899d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x123689f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12368a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12368a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12368af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12368b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12368b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12368bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12368c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12368c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12368cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12368d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12368d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12368dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12368e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12368e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12368eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12368f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12368f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12368fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x123690410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1236906d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x123690990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x123690e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x123691390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x123691890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x123691d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x123692290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x123692790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x123692c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x123693190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x123693690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x123693b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x123694090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x123694590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x123694a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x123694f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1236959a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1236960c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1236967e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x123696f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1236971c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1236979b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x123697c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x123698280 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x123697f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1236462d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x123697480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1236986e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1236989a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x123698c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x123698f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1236991e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1236994a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x123699760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x123699a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x123699ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12369a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12369a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12369aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12369b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12369b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12369b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12369b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12369bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12369bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12369c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12369c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12369c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12369ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12369ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12369cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12369d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12369d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12369d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12369dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12369dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12369e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12369e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12369e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12369e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12369eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12369edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12369f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12369f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12369f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12369f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12369fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12369fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1236a0130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1236a03f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1236a06b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1236a0970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1236a0c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1236a0ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1236a11b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1236a1470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1236a1730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1236a19f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1236a1cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1236a1f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1236a2230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1236a24f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1236a27b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1236a2a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1236a2d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1236a2ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1236a32b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1236a3570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1236a3830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1236a3af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1236a3db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1236a4070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1236a4330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1236a45f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1236a48b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1236a4b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1236a4e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1236a50f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1236a53b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1236a5670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1236a5930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1236a5bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x133604c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x133605070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1336054e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x133607dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x133608240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1336086b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x133608b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x133608f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x133609400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x133609870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x133609ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13360a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13360a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13360aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13360aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13360b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13360b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13360bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13360c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13360c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13360c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13360cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13360d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13360d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13360db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13360e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13360e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13360eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13360f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13360f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13360f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13360fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1336101d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x133610640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x133610ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x133610f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x133611390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x133611800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x133611c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1336120e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x133612550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1336129c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x133612e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1336132a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x133613710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x133613b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x133613ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x133614460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1336148d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x133614d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1336151b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x133615620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x133615a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x133615f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x133616370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1336167e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x133616c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1336170c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x133617530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1336179a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x133617e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x133618280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1336186f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x133618b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x133618fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x133619440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1336198b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x133619d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13361a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13361a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13361aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13361aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13361b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13361b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13361bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13361c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13361c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13361c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13361cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13361d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13361d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13361db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13361dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13361e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13361e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13361ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13361f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13361f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13361fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13361fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x133620330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1336207a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x133620c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x133621080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1336214f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x133621960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x133621dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x133622240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1336226b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x133622b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x133622f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x133623400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x133623870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x133623ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x133624150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1336245c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x133624a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x133624ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x133625310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x133625780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x133625bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x133626060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1336264d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x133626940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x133626db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x133627220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x133627690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x133627b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x133627f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1336283e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x133628850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x133628cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x133629130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1336295a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x133629a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x133629e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13362a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13362a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13362abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13362b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13362b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13362b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13362c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13362c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13362ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13362cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13362d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13362d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13362dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13362e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13362e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13362e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13362edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13362f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13362f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13362fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13362ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1336303e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x133630850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x133630cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x133631130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1336315a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x133631a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x133631e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1336322f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x133632760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x133632bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x133633040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1336334b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x133633920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x133633d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x133634200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x133634670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x133634ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x133634f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1336353c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x133635830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x133635ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x133636110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x133636580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1336369f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x133636e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1336372d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x133637740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x133637bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x133638020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x133638490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x133638900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x133638d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1336391e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x133639650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x133639ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x133639f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13363a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13363a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13363ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13363b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13363b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13363b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13363be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13363c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13363c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13363cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13363d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13363d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13363d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13363dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13363e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13363e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13363eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13363ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13363f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13363f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13363fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1336400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x133640b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x133641260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x133641980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1336420a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x133642360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1336427d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x133642dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1336433e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.798s
user	0m0.280s
sys	0m0.316s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4590 (e0449763)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14770d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14770d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14770df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14770e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14770eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14770f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14770f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14770fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x147710160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x147710660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x147710b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x147711060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x147711b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x147712330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x147712b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x147713260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x147713980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1477140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1477147c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x147714f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1477156b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x147715dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1477164f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x147716d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1477174b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x147717770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x147717d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1477189f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x147718f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1477191f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x147719690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x147719950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14771a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14771a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14771a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14771ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14771b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14771b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14771bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14771c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14771c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14771ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14771cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14771d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14771d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14771dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14771e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14771eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14771f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14771f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14771fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1477203c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1477209d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x147720fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1477217d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x147721c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x147722110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1477223d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1477229e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1477231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x147723490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x147723930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x147723dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x147724270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x147724710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x147724bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x147725050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1477254f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x147725990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x147725e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1477262d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x147726770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x147726c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x147727160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1477276b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x147727c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x147728150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1477286a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x147728bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x147729140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x147729690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x147729be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14772a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14772a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14772abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14772b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14772b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14772bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14772c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14772c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14772cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14772d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14772d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14772dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14772e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14772e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14772eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14771e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14772f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14772f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14772fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x147730250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1477307a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x147730cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x147731240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x147731790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x147731ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x147732230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x147732780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x147732cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x147733220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x147733770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x147733cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x147734160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x147734600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x147734aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x147734f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1477353e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x147735880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x147735d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1477361c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x147736660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x147736b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x147736fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x147737440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1477378e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x147737d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x147738220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1477386c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x147738b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x147739000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1477394a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x147739940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x147739de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14773a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14773a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14773abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14773b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14773b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14773b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14773be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14773c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14773c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14773cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14773d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14773d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14773da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14773dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14773e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14773e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14773ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14773f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14773f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14773fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14773ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1477403a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x147740840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x147740ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x147741180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x147741620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x147741ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x147741f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x147742400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1477428a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x147742d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1477431e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x147743680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x147743b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x147743fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x147744460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x147744900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x147744da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x147745240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1477456e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x147745b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x147746020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1477464c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x147746960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x147746e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1477472a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x147747740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x147747be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x147748080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x147748520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1477489c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x147748e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x147749300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1477497a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x147749c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14774a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14774a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14774aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14774aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14774b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14774b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14774beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14774c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14774c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14774ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14774d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14774d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14774e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14774e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14774e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14774ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14774f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14774fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1477500f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x147750590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x147750a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1477511e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x147751730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x147751c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1477521d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x147752720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x147752c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1477531c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x147753710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x147753c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1477541b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x147754700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x147754c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1477551a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1477556f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x147755c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x147756190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1477566e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x147756c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x147757180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1477576d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x147757c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x147758170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1477586c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x147758c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x147759160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1477596b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x147759c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14775a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14775a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14775abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14775b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14775b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14775bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14775c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14775c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14775cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14775d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14775d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14775dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14775e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14775e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14775ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14775f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14775f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14775fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1477600f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x147760640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x147760b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1477610e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x147761630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x147761b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1477620d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x147762620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x147762b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1477630c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x147763610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x147763b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x147764000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1477644a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x147764940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x147764de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x147765280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x147765720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x147765bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x147766060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x147766500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1477669a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x147766e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1477672e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x147767780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x147767c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1477680c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x147768610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x147768d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x147769450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x147769b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14776a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14776a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14776ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14776b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14776b610 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.103.444 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.103.447 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x148809040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1488094b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x148809920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x148809d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14880a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14880a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14880aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14880af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14880b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14880b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14880bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14880c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14880ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14880d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14880de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14880e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14880ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14880f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14880faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x148810220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x148810940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x148811060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x148811780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x148811ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1488125c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x148812880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x148812b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x148812fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x148813420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x148813890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x148813d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1488142a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x148814710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1488149d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x148814e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1488152b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x148815810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x148815d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x148816210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x148816710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x148816c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x148817110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x148817610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x148817b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x148818010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x148818480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1488188f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x148818d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1488191d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x148819640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x148819ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x148819f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14881a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14881a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14881ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14881b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14881b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14881bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14881c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14881c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14881ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14881d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14881d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14881dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14881e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14881e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14881ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14881eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14881f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14881f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14881fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x148820120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1488205c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x148820b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x148821060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1488215b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x148821b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x148822050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1488225a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x148822af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x148823040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x148823590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x148823ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x148824030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x148824580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x148824ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x148825020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x148825570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x148825ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x148826010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x148826560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x148826ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x148827000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x148827550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x148827aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x148827ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x148828540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x148828a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x148828fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x148829530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x148829a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x148829fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14882a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14882aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14882afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14882b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14882ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14882bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14882c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14882ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14882cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14882d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14882da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14882dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14882e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14882e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14882ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14882f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14882f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14882faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14882ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1488303e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x148830880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x148830d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1488311c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x148831660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x148831b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x148831fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x148832440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1488328e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x148832d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x148833220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1488336c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x148833b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x148834000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1488344a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x148834940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x148834de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x148835280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x148835720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x148835bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x148836060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x148836500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1488369a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x148836e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1488372e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x148837780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x148837c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1488380c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x148838560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x148838a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x148838ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x148839340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1488397e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x148839c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14883a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14883a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14883aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14883af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14883b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14883b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14883bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14883c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14883c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14883cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14883cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14883d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14883d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14883dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14883e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14883e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14883eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14883efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14883f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14883f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14883fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x148840240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1488406e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x148840b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x148841020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1488414c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x148841960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x148841e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1488422a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x148842740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x148842be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x148843080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x148843520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1488439c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x148843e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x148844300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1488447a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x148844c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x148845190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1488456e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x148845c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x148846180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x148846440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x148846a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x148847060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x148847670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x148847e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x148848300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1488485c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x148848bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1488491e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1488499d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x148849e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14884a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14884a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14884af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14884b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14884ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14884bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14884c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14884c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14884cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14884d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14884d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14884df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14884e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14884e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14884ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14884f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14884f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14884ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x148850460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1488509b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x148850f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x148851450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1488519a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x148851ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x148852440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x148852990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x148852ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x148853430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x148853980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x148853ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x148854420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x148854970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x148854ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x148855410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x148855960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x148855eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x148856400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x148856950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x148856ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1488573f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x148857940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x148857e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1488583e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x148858930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x148858e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1488593d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x148859920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x148859e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14885a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14885a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14885ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14885b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14885b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14885be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14885c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14885c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14885ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14885d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14885d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14885dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14885e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14885e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14885eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14885f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14885f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14885f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14885fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x148860280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x148860720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x148860bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x148861060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x148861500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1488619a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x148861e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x148862390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x148862ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1488631d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1488638f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x148864010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1488642d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x148864ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x148864d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x148865390 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x144f044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x144f04950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x144f04dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x144f05230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x144f056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x144f05b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x144f05f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x144f063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x144f06860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x144f06cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x144f07140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x144f07860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x144f08380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x144f08b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x144f09340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x144f09a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x144f0a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x144f0a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x144f0afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x144f0b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x144f0be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x144f0c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x144f0cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x144f0d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x144f0da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x144f0dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x144f0e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x144f0e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x144f0e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x144f0ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x144f0f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x144f0f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x144f0fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x144f0fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x144f102a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x144f10710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x144f10b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x144f10ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x144f11460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x144f118d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x144f11d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x144f121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x144f12620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x144f12a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x144f12f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x144f13370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x144f137e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x144f13c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x144f140c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x144f14530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x144f149a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x144f14e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x144f15280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x144f156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x144f15b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x144f15fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x144f16540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x144f16a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x144f16eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x144f17320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x144f17790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x144f17c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x144f18070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x144f184e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x144f18950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x144f18dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x144f19230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x144f196a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x144f19b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x144f19f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x144f1a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x144f1a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x144f1acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x144f1b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x144f1b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x144f1ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x144f1be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x144f1c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x144f1c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x144f1cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x144f1d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x144f1d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x144f1d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x144f1dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x144f1e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x144f1e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x144f1eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x144f1ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x144f1f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x144f1f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x144f1fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x144f20120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x144f20590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x144f20a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x144f20e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x144f212e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x144f21750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x144f21bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x144f22030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x144f224a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x144f22910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x144f22d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x144f231f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x144f23a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x144f23d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x144f241b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x144f24620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x144f24a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x144f24f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x144f25370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x144f257e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x144f25c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x144f260c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x144f26530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x144f269a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x144f26e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x144f27280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x144f276f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x144f27b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x144f27fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x144f28440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x144f288b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x144f28d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x144f29190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x144f29600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x144f29a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x144f29ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x144f2a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x144f2a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x144f2ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x144f2b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x144f2b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x144f2b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x144f2bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x144f2c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x144f2c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x144f2cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x144f2cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x144f2d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x144f2d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x144f2dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x144f2e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x144f2e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x144f2ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x144f2eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x144f2f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x144f2f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x144f2fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x144f30080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x144f304f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x144f30960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x144f30dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x144f31240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x144f316b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x144f31b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x144f31f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x144f32400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x144f32870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x144f32ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x144f33150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x144f335c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x144f33a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x144f33ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x144f34310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x144f34780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x144f34bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x144f35060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x144f354d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x144f35940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x144f35db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x144f36220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x144f36690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x144f36b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x144f36f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x144f373e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x144f37850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x144f37cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x144f38130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x144f385a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x144f38a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x144f38e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x144f392f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x144f39760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x144f39bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x144f3a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x144f3a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x144f3a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x144f3ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x144f3b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x144f3b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x144f3bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x144f3bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x144f3c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x144f3c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x144f3cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x144f3d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x144f3d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x144f3d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x144f3de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x147608510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x147608980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x147608df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x147609590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x147609850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x147609e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14760a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14760ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14760b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14760b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14760ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14760c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14760c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14760cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14760d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14760d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14760dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14760e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14760e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14760ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14760f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14760f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14760fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1476101b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x147610700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x147610c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1476111a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1476116f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x147611c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x147612190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1476126e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x147612c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x147613180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1476136d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x147613c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x147614170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1476146c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x147614c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x147615160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1476156b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x147615c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x147616150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1476166a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x147616bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x147617140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x147617690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x147617be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x147618130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x147618680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x147618bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x147619120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x147619670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x147619bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14761a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14761a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14761abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14761b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14761b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14761bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14761c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14761c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14761cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14761d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14761d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14761db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14761e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14761e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14761eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14761f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14761f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14761f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14761fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x147620290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x147620730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x147620bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x147621070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x147621510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1476219b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x147621e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1476222f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x147622790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x147622c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1476230d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x147623620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x147623d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x147624460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x147624b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1476252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x147625560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x147625d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x147626010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x147626620 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.963s
user	0m0.234s
sys	0m0.190s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
