Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.3s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.606s
user	0m0.915s
sys	0m1.271s
++ nproc
+ make -j10
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  3%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Built target build_info
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target xxhash
[  5%] Built target sha1
[  5%] Built target sha256
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library libggml.dylib
[ 14%] Built target ggml
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 20%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Linking CXX executable ../../bin/llama-gguf-hash
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 26%] Linking CXX shared library libllama.dylib
[ 26%] Built target llama-gguf
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 28%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 29%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 30%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 31%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 32%] Linking C executable ../bin/test-c
[ 33%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-simple
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Built target llava
[ 35%] Linking CXX executable ../../bin/llama-quantize-stats
[ 36%] Linking CXX static library libcommon.a
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Linking CXX shared library libllava_shared.dylib
[ 37%] Built target llama-simple
[ 37%] Built target llama-simple-chat
[ 37%] Built target test-c
[ 37%] Built target llama-quantize-stats
[ 37%] Built target llava_static
[ 37%] Built target common
[ 37%] Built target llava_shared
[ 38%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 44%] Linking CXX executable ../bin/test-tokenizer-0
[ 45%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-sampling
[ 47%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 48%] Linking CXX executable ../bin/test-log
[ 49%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 49%] Linking CXX executable ../bin/test-llama-grammar
[ 49%] Linking CXX executable ../bin/test-grammar-integration
[ 49%] Linking CXX executable ../bin/test-arg-parser
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Built target test-tokenizer-0
[ 49%] Built target test-grammar-parser
[ 49%] Built target test-sampling
[ 49%] Built target test-log
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Built target test-grammar-integration
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Built target test-llama-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 50%] Built target test-arg-parser
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-gguf
[ 57%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 58%] Linking CXX executable ../bin/test-backend-ops
[ 59%] Linking CXX executable ../bin/test-barrier
[ 59%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 60%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 60%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 60%] Linking CXX executable ../bin/test-chat-template
[ 60%] Linking CXX executable ../bin/test-model-load-cancel
[ 60%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 61%] Linking CXX executable ../bin/test-autorelease
[ 61%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 61%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Built target test-gguf
[ 62%] Built target test-barrier
[ 63%] Linking CXX executable ../../bin/llama-batched-bench
[ 63%] Built target test-backend-ops
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Built target test-chat-template
[ 64%] Built target test-model-load-cancel
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Built target test-autorelease
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Built target test-quantize-perf
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Built target test-quantize-fns
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 67%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 69%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-embedding
[ 69%] Built target llama-batched-bench
[ 69%] Linking CXX executable ../../bin/llama-eval-callback
[ 70%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Built target test-rope
[ 70%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Built target llama-eval-callback
[ 72%] Built target llama-embedding
[ 72%] Built target llama-batched
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-gguf-split
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Built target llama-gritlm
[ 73%] Built target llama-imatrix
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Built target llama-infill
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 77%] Built target llama-bench
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 77%] Built target llama-lookahead
[ 78%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 79%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 81%] Built target llama-lookup-create
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 82%] Built target llama-lookup-merge
[ 82%] Built target llama-lookup-stats
[ 82%] Built target llama-lookup
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Generating loading.html.hpp
[ 83%] Built target llama-parallel
[ 83%] Built target llama-passkey
[ 83%] Built target llama-cli
[ 84%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 84%] Built target llama-perplexity
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Generating index.html.gz.hpp
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 87%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 87%] Built target llama-quantize
[ 87%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-save-load-state
[ 87%] Built target llama-retrieval
[ 88%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-speculative
[ 89%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-speculative-simple
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Linking CXX executable ../../bin/llama-run
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Built target llama-speculative
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-save-load-state
[ 93%] Built target llama-speculative-simple
[ 93%] Built target llama-tokenize
[ 93%] Built target llama-tts
[ 93%] Built target llama-run
[ 93%] Built target llama-gen-docs
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Built target llama-convert-llama2c-to-ggml
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Built target llama-cvector-generator
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Linking CXX executable ../../bin/llama-q8dot
[ 98%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-vdot
[ 99%] Built target llama-qwen2vl-cli
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.990s
user	0m5.988s
sys	0m9.338s

main: quantize time =  9355.22 ms
main:    total time =  9355.22 ms

main: quantize time =  3614.94 ms
main:    total time =  3614.94 ms

main: quantize time =  1354.19 ms
main:    total time =  1354.19 ms

main: quantize time =  3217.22 ms
main:    total time =  3217.22 ms

main: quantize time =  2406.39 ms
main:    total time =  2406.39 ms

main: quantize time =  5002.13 ms
main:    total time =  5002.13 ms

main: quantize time =  5690.36 ms
main:    total time =  5690.36 ms

main: quantize time =  6727.58 ms
main:    total time =  6727.58 ms

main: quantize time =  5700.54 ms
main:    total time =  5700.54 ms

main: quantize time =  4438.58 ms
main:    total time =  4438.58 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.173 I build: 4531 (a47d389c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.281 I main: llama backend init
0.00.000.288 I main: load the model and apply lora adapter, if any
0.00.029.875 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.042.465 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.042.477 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.042.481 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.042.482 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.042.483 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.042.484 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.042.484 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.042.487 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.042.488 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.042.489 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.042.489 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.042.490 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.042.491 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.042.492 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.042.496 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.042.497 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.042.497 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.051.693 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.054.129 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.061.561 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.061.564 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.061.564 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.061.564 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.061.565 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.061.566 I llama_model_loader: - type  f32:  194 tensors
0.00.061.566 I llama_model_loader: - type  f16:   98 tensors
0.00.061.568 I print_info: file format = GGUF V3 (latest)
0.00.061.569 I print_info: file type   = all F32 (guessed)
0.00.061.571 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.089.781 I load: special tokens cache size = 25
0.00.096.550 I load: token to piece cache size = 0.2984 MB
0.00.096.553 I print_info: arch             = gptneox
0.00.096.553 I print_info: vocab_only       = 0
0.00.096.553 I print_info: n_ctx_train      = 2048
0.00.096.554 I print_info: n_embd           = 2048
0.00.096.554 I print_info: n_layer          = 24
0.00.096.557 I print_info: n_head           = 16
0.00.096.558 I print_info: n_head_kv        = 16
0.00.096.558 I print_info: n_rot            = 32
0.00.096.558 I print_info: n_swa            = 0
0.00.096.558 I print_info: n_embd_head_k    = 128
0.00.096.559 I print_info: n_embd_head_v    = 128
0.00.096.559 I print_info: n_gqa            = 1
0.00.096.562 I print_info: n_embd_k_gqa     = 2048
0.00.096.563 I print_info: n_embd_v_gqa     = 2048
0.00.096.564 I print_info: f_norm_eps       = 1.0e-05
0.00.096.564 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.096.564 I print_info: f_clamp_kqv      = 0.0e+00
0.00.096.565 I print_info: f_max_alibi_bias = 0.0e+00
0.00.096.566 I print_info: f_logit_scale    = 0.0e+00
0.00.096.566 I print_info: n_ff             = 8192
0.00.096.566 I print_info: n_expert         = 0
0.00.096.567 I print_info: n_expert_used    = 0
0.00.096.567 I print_info: causal attn      = 1
0.00.096.567 I print_info: pooling type     = 0
0.00.096.567 I print_info: rope type        = 2
0.00.096.567 I print_info: rope scaling     = linear
0.00.096.567 I print_info: freq_base_train  = 10000.0
0.00.096.568 I print_info: freq_scale_train = 1
0.00.096.568 I print_info: n_ctx_orig_yarn  = 2048
0.00.096.568 I print_info: rope_finetuned   = unknown
0.00.096.568 I print_info: ssm_d_conv       = 0
0.00.096.568 I print_info: ssm_d_inner      = 0
0.00.096.568 I print_info: ssm_d_state      = 0
0.00.096.568 I print_info: ssm_dt_rank      = 0
0.00.096.569 I print_info: ssm_dt_b_c_rms   = 0
0.00.096.569 I print_info: model type       = 1.4B
0.00.096.569 I print_info: model params     = 1.41 B
0.00.096.569 I print_info: general.name     = 1.4B
0.00.096.570 I print_info: vocab type       = BPE
0.00.096.570 I print_info: n_vocab          = 50304
0.00.096.570 I print_info: n_merges         = 50009
0.00.096.570 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.096.570 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.096.571 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.096.571 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.096.571 I print_info: LF token         = 128 'Ä'
0.00.096.571 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.096.571 I print_info: max token length = 1024
0.00.099.093 I load_tensors: offloading 24 repeating layers to GPU
0.00.099.093 I load_tensors: offloading output layer to GPU
0.00.099.093 I load_tensors: offloaded 25/25 layers to GPU
0.00.099.112 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.099.113 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.099.404 I llama_context: n_seq_max     = 1
0.00.099.405 I llama_context: n_ctx         = 2048
0.00.099.405 I llama_context: n_ctx_per_seq = 2048
0.00.099.405 I llama_context: n_batch       = 2048
0.00.099.405 I llama_context: n_ubatch      = 512
0.00.099.405 I llama_context: flash_attn    = 0
0.00.099.406 I llama_context: freq_base     = 10000.0
0.00.099.406 I llama_context: freq_scale    = 1
0.00.099.406 I ggml_metal_init: allocating
0.00.099.409 I ggml_metal_init: found device: Apple M4
0.00.099.411 I ggml_metal_init: picking default device: Apple M4
0.00.100.097 I ggml_metal_init: using embedded metal library
0.00.109.654 I ggml_metal_init: GPU name:   Apple M4
0.00.109.656 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.109.656 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.109.656 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.109.657 I ggml_metal_init: simdgroup reduction   = true
0.00.109.657 I ggml_metal_init: simdgroup matrix mul. = true
0.00.109.657 I ggml_metal_init: has bfloat            = true
0.00.109.657 I ggml_metal_init: use bfloat            = true
0.00.109.657 I ggml_metal_init: hasUnifiedMemory      = true
0.00.109.658 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.133.034 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.152.831 I init:      Metal KV buffer size =   384.00 MiB
0.00.152.838 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.152.870 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.153.796 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.153.798 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.153.798 I llama_context: graph nodes  = 967
0.00.153.798 I llama_context: graph splits = 2
0.00.153.802 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.153.932 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.153.933 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.236.039 I main: llama threadpool init, n_threads = 4
0.00.236.081 I 
0.00.236.119 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.236.121 I 
0.00.236.187 I sampler seed: 1234
0.00.236.192 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.236.217 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.236.219 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.236.219 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.076.042 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57629.87 tokens per second)
0.02.076.043 I llama_perf_context_print:        load time =     206.15 ms
0.02.076.044 I llama_perf_context_print: prompt eval time =      43.51 ms /     7 tokens (    6.22 ms per token,   160.88 tokens per second)
0.02.076.044 I llama_perf_context_print:        eval time =    1793.39 ms /    63 runs   (   28.47 ms per token,    35.13 tokens per second)
0.02.076.045 I llama_perf_context_print:       total time =    1840.01 ms /    70 tokens
0.02.080.037 I ggml_metal_free: deallocating

real	0m2.366s
user	0m0.141s
sys	0m0.106s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4531 (a47d389c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.785 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.023.494 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.023.499 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.506 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.507 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.507 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.507 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.508 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.509 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.509 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.509 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.510 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.512 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.512 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.513 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.515 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.515 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.515 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.630 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.756 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.935 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.936 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.936 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.937 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.937 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.938 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.032.938 I llama_model_loader: - type  f32:  194 tensors
0.00.032.939 I llama_model_loader: - type q8_0:   98 tensors
0.00.032.939 I print_info: file format = GGUF V3 (latest)
0.00.032.940 I print_info: file type   = Q8_0
0.00.032.941 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.052.449 I load: special tokens cache size = 25
0.00.058.464 I load: token to piece cache size = 0.2984 MB
0.00.058.468 I print_info: arch             = gptneox
0.00.058.468 I print_info: vocab_only       = 0
0.00.058.468 I print_info: n_ctx_train      = 2048
0.00.058.469 I print_info: n_embd           = 2048
0.00.058.469 I print_info: n_layer          = 24
0.00.058.474 I print_info: n_head           = 16
0.00.058.475 I print_info: n_head_kv        = 16
0.00.058.475 I print_info: n_rot            = 32
0.00.058.475 I print_info: n_swa            = 0
0.00.058.476 I print_info: n_embd_head_k    = 128
0.00.058.476 I print_info: n_embd_head_v    = 128
0.00.058.479 I print_info: n_gqa            = 1
0.00.058.479 I print_info: n_embd_k_gqa     = 2048
0.00.058.482 I print_info: n_embd_v_gqa     = 2048
0.00.058.482 I print_info: f_norm_eps       = 1.0e-05
0.00.058.483 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.058.483 I print_info: f_clamp_kqv      = 0.0e+00
0.00.058.483 I print_info: f_max_alibi_bias = 0.0e+00
0.00.058.483 I print_info: f_logit_scale    = 0.0e+00
0.00.058.484 I print_info: n_ff             = 8192
0.00.058.484 I print_info: n_expert         = 0
0.00.058.485 I print_info: n_expert_used    = 0
0.00.058.485 I print_info: causal attn      = 1
0.00.058.485 I print_info: pooling type     = 0
0.00.058.485 I print_info: rope type        = 2
0.00.058.485 I print_info: rope scaling     = linear
0.00.058.486 I print_info: freq_base_train  = 10000.0
0.00.058.486 I print_info: freq_scale_train = 1
0.00.058.486 I print_info: n_ctx_orig_yarn  = 2048
0.00.058.487 I print_info: rope_finetuned   = unknown
0.00.058.487 I print_info: ssm_d_conv       = 0
0.00.058.487 I print_info: ssm_d_inner      = 0
0.00.058.487 I print_info: ssm_d_state      = 0
0.00.058.487 I print_info: ssm_dt_rank      = 0
0.00.058.487 I print_info: ssm_dt_b_c_rms   = 0
0.00.058.488 I print_info: model type       = 1.4B
0.00.058.489 I print_info: model params     = 1.41 B
0.00.058.489 I print_info: general.name     = 1.4B
0.00.058.490 I print_info: vocab type       = BPE
0.00.058.490 I print_info: n_vocab          = 50304
0.00.058.491 I print_info: n_merges         = 50009
0.00.058.491 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.058.492 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.058.492 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.058.492 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.058.492 I print_info: LF token         = 128 'Ä'
0.00.058.492 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.058.493 I print_info: max token length = 1024
0.00.060.923 I load_tensors: offloading 24 repeating layers to GPU
0.00.060.923 I load_tensors: offloading output layer to GPU
0.00.060.923 I load_tensors: offloaded 25/25 layers to GPU
0.00.060.935 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.060.936 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.061.271 I llama_context: n_seq_max     = 1
0.00.061.272 I llama_context: n_ctx         = 2048
0.00.061.272 I llama_context: n_ctx_per_seq = 2048
0.00.061.272 I llama_context: n_batch       = 2048
0.00.061.273 I llama_context: n_ubatch      = 512
0.00.061.273 I llama_context: flash_attn    = 0
0.00.061.273 I llama_context: freq_base     = 10000.0
0.00.061.273 I llama_context: freq_scale    = 1
0.00.061.274 I ggml_metal_init: allocating
0.00.061.277 I ggml_metal_init: found device: Apple M4
0.00.061.279 I ggml_metal_init: picking default device: Apple M4
0.00.062.033 I ggml_metal_init: using embedded metal library
0.00.064.595 I ggml_metal_init: GPU name:   Apple M4
0.00.064.596 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.064.597 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.064.597 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.064.597 I ggml_metal_init: simdgroup reduction   = true
0.00.064.597 I ggml_metal_init: simdgroup matrix mul. = true
0.00.064.598 I ggml_metal_init: has bfloat            = true
0.00.064.598 I ggml_metal_init: use bfloat            = true
0.00.064.598 I ggml_metal_init: hasUnifiedMemory      = true
0.00.064.599 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.074.979 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.099.280 I init:      Metal KV buffer size =   384.00 MiB
0.00.099.288 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.099.327 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.100.573 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.100.575 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.100.576 I llama_context: graph nodes  = 967
0.00.100.576 I llama_context: graph splits = 2
0.00.100.581 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.100.710 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.100.710 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.261.696 I main: llama threadpool init, n_threads = 4
0.01.261.733 I 
0.01.261.767 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.261.767 I 
0.01.261.981 I sampler seed: 1234
0.01.261.986 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.262.036 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.262.040 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.262.040 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.347.433 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61154.18 tokens per second)
0.02.347.434 I llama_perf_context_print:        load time =    1251.90 ms
0.02.347.435 I llama_perf_context_print: prompt eval time =      39.87 ms /     7 tokens (    5.70 ms per token,   175.57 tokens per second)
0.02.347.436 I llama_perf_context_print:        eval time =    1042.58 ms /    63 runs   (   16.55 ms per token,    60.43 tokens per second)
0.02.347.436 I llama_perf_context_print:       total time =    1085.74 ms /    70 tokens
0.02.350.336 I ggml_metal_free: deallocating

real	0m2.368s
user	0m0.111s
sys	0m0.226s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4531 (a47d389c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.017.799 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.014 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.036.019 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.026 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.026 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.028 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.029 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.029 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.030 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.031 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.031 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.031 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.032 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.032 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.033 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.035 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.035 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.035 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.040.288 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.041.512 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.046.029 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.046.031 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.046.031 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.046.031 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.046.032 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.046.032 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.046.033 I llama_model_loader: - type  f32:  194 tensors
0.00.046.033 I llama_model_loader: - type q4_0:   97 tensors
0.00.046.033 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.034 I print_info: file format = GGUF V3 (latest)
0.00.046.034 I print_info: file type   = Q4_0
0.00.046.035 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.070.799 I load: special tokens cache size = 25
0.00.079.380 I load: token to piece cache size = 0.2984 MB
0.00.079.385 I print_info: arch             = gptneox
0.00.079.385 I print_info: vocab_only       = 0
0.00.079.386 I print_info: n_ctx_train      = 2048
0.00.079.386 I print_info: n_embd           = 2048
0.00.079.386 I print_info: n_layer          = 24
0.00.079.391 I print_info: n_head           = 16
0.00.079.392 I print_info: n_head_kv        = 16
0.00.079.392 I print_info: n_rot            = 32
0.00.079.392 I print_info: n_swa            = 0
0.00.079.396 I print_info: n_embd_head_k    = 128
0.00.079.396 I print_info: n_embd_head_v    = 128
0.00.079.397 I print_info: n_gqa            = 1
0.00.079.398 I print_info: n_embd_k_gqa     = 2048
0.00.079.399 I print_info: n_embd_v_gqa     = 2048
0.00.079.400 I print_info: f_norm_eps       = 1.0e-05
0.00.079.401 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.079.401 I print_info: f_clamp_kqv      = 0.0e+00
0.00.079.401 I print_info: f_max_alibi_bias = 0.0e+00
0.00.079.402 I print_info: f_logit_scale    = 0.0e+00
0.00.079.403 I print_info: n_ff             = 8192
0.00.079.403 I print_info: n_expert         = 0
0.00.079.403 I print_info: n_expert_used    = 0
0.00.079.403 I print_info: causal attn      = 1
0.00.079.403 I print_info: pooling type     = 0
0.00.079.404 I print_info: rope type        = 2
0.00.079.404 I print_info: rope scaling     = linear
0.00.079.405 I print_info: freq_base_train  = 10000.0
0.00.079.405 I print_info: freq_scale_train = 1
0.00.079.406 I print_info: n_ctx_orig_yarn  = 2048
0.00.079.406 I print_info: rope_finetuned   = unknown
0.00.079.406 I print_info: ssm_d_conv       = 0
0.00.079.408 I print_info: ssm_d_inner      = 0
0.00.079.408 I print_info: ssm_d_state      = 0
0.00.079.408 I print_info: ssm_dt_rank      = 0
0.00.079.409 I print_info: ssm_dt_b_c_rms   = 0
0.00.079.409 I print_info: model type       = 1.4B
0.00.079.409 I print_info: model params     = 1.41 B
0.00.079.410 I print_info: general.name     = 1.4B
0.00.079.410 I print_info: vocab type       = BPE
0.00.079.411 I print_info: n_vocab          = 50304
0.00.079.411 I print_info: n_merges         = 50009
0.00.079.411 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.079.412 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.079.412 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.079.412 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.079.413 I print_info: LF token         = 128 'Ä'
0.00.079.413 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.079.413 I print_info: max token length = 1024
0.00.082.474 I load_tensors: offloading 24 repeating layers to GPU
0.00.082.474 I load_tensors: offloading output layer to GPU
0.00.082.474 I load_tensors: offloaded 25/25 layers to GPU
0.00.082.487 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.082.489 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.082.973 I llama_context: n_seq_max     = 1
0.00.082.975 I llama_context: n_ctx         = 2048
0.00.082.975 I llama_context: n_ctx_per_seq = 2048
0.00.082.975 I llama_context: n_batch       = 2048
0.00.082.975 I llama_context: n_ubatch      = 512
0.00.082.976 I llama_context: flash_attn    = 0
0.00.082.976 I llama_context: freq_base     = 10000.0
0.00.082.977 I llama_context: freq_scale    = 1
0.00.082.977 I ggml_metal_init: allocating
0.00.082.982 I ggml_metal_init: found device: Apple M4
0.00.082.985 I ggml_metal_init: picking default device: Apple M4
0.00.084.022 I ggml_metal_init: using embedded metal library
0.00.088.208 I ggml_metal_init: GPU name:   Apple M4
0.00.088.210 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.088.211 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.088.211 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.088.211 I ggml_metal_init: simdgroup reduction   = true
0.00.088.212 I ggml_metal_init: simdgroup matrix mul. = true
0.00.088.212 I ggml_metal_init: has bfloat            = true
0.00.088.212 I ggml_metal_init: use bfloat            = true
0.00.088.213 I ggml_metal_init: hasUnifiedMemory      = true
0.00.088.213 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.100.905 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.128.955 I init:      Metal KV buffer size =   384.00 MiB
0.00.128.972 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.129.016 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.130.051 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.130.054 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.130.054 I llama_context: graph nodes  = 967
0.00.130.054 I llama_context: graph splits = 2
0.00.130.059 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.130.186 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.130.186 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.758.881 I main: llama threadpool init, n_threads = 4
0.00.758.935 I 
0.00.758.977 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.758.979 I 
0.00.759.305 I sampler seed: 1234
0.00.759.311 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.759.376 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.759.376 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.759.377 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.439.383 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60220.53 tokens per second)
0.01.439.384 I llama_perf_context_print:        load time =     741.08 ms
0.01.439.385 I llama_perf_context_print: prompt eval time =      46.88 ms /     7 tokens (    6.70 ms per token,   149.33 tokens per second)
0.01.439.385 I llama_perf_context_print:        eval time =     630.22 ms /    63 runs   (   10.00 ms per token,    99.96 tokens per second)
0.01.439.389 I llama_perf_context_print:       total time =     680.50 ms /    70 tokens
0.01.442.249 I ggml_metal_free: deallocating

real	0m1.460s
user	0m0.127s
sys	0m0.182s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4531 (a47d389c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.617 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.819 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.022.824 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.825 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.826 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.826 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.826 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.827 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.831 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.832 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.833 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.833 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.837 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.838 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.839 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.843 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.843 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.843 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.888 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.999 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.994 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.995 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.995 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.995 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.996 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.996 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.031.996 I llama_model_loader: - type  f32:  194 tensors
0.00.031.997 I llama_model_loader: - type q4_1:   97 tensors
0.00.031.997 I llama_model_loader: - type q6_K:    1 tensors
0.00.031.997 I print_info: file format = GGUF V3 (latest)
0.00.031.998 I print_info: file type   = Q4_1
0.00.031.999 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.051.009 I load: special tokens cache size = 25
0.00.057.098 I load: token to piece cache size = 0.2984 MB
0.00.057.101 I print_info: arch             = gptneox
0.00.057.101 I print_info: vocab_only       = 0
0.00.057.102 I print_info: n_ctx_train      = 2048
0.00.057.102 I print_info: n_embd           = 2048
0.00.057.102 I print_info: n_layer          = 24
0.00.057.105 I print_info: n_head           = 16
0.00.057.106 I print_info: n_head_kv        = 16
0.00.057.106 I print_info: n_rot            = 32
0.00.057.106 I print_info: n_swa            = 0
0.00.057.107 I print_info: n_embd_head_k    = 128
0.00.057.107 I print_info: n_embd_head_v    = 128
0.00.057.107 I print_info: n_gqa            = 1
0.00.057.110 I print_info: n_embd_k_gqa     = 2048
0.00.057.111 I print_info: n_embd_v_gqa     = 2048
0.00.057.112 I print_info: f_norm_eps       = 1.0e-05
0.00.057.112 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.057.112 I print_info: f_clamp_kqv      = 0.0e+00
0.00.057.113 I print_info: f_max_alibi_bias = 0.0e+00
0.00.057.113 I print_info: f_logit_scale    = 0.0e+00
0.00.057.113 I print_info: n_ff             = 8192
0.00.057.114 I print_info: n_expert         = 0
0.00.057.114 I print_info: n_expert_used    = 0
0.00.057.114 I print_info: causal attn      = 1
0.00.057.114 I print_info: pooling type     = 0
0.00.057.116 I print_info: rope type        = 2
0.00.057.117 I print_info: rope scaling     = linear
0.00.057.117 I print_info: freq_base_train  = 10000.0
0.00.057.117 I print_info: freq_scale_train = 1
0.00.057.117 I print_info: n_ctx_orig_yarn  = 2048
0.00.057.118 I print_info: rope_finetuned   = unknown
0.00.057.118 I print_info: ssm_d_conv       = 0
0.00.057.118 I print_info: ssm_d_inner      = 0
0.00.057.118 I print_info: ssm_d_state      = 0
0.00.057.118 I print_info: ssm_dt_rank      = 0
0.00.057.118 I print_info: ssm_dt_b_c_rms   = 0
0.00.057.119 I print_info: model type       = 1.4B
0.00.057.119 I print_info: model params     = 1.41 B
0.00.057.119 I print_info: general.name     = 1.4B
0.00.057.120 I print_info: vocab type       = BPE
0.00.057.120 I print_info: n_vocab          = 50304
0.00.057.120 I print_info: n_merges         = 50009
0.00.057.120 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.057.124 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.057.124 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.057.128 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.057.129 I print_info: LF token         = 128 'Ä'
0.00.057.129 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.057.130 I print_info: max token length = 1024
0.00.059.075 I load_tensors: offloading 24 repeating layers to GPU
0.00.059.075 I load_tensors: offloading output layer to GPU
0.00.059.076 I load_tensors: offloaded 25/25 layers to GPU
0.00.059.086 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.059.087 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.059.364 I llama_context: n_seq_max     = 1
0.00.059.364 I llama_context: n_ctx         = 2048
0.00.059.364 I llama_context: n_ctx_per_seq = 2048
0.00.059.364 I llama_context: n_batch       = 2048
0.00.059.365 I llama_context: n_ubatch      = 512
0.00.059.365 I llama_context: flash_attn    = 0
0.00.059.365 I llama_context: freq_base     = 10000.0
0.00.059.365 I llama_context: freq_scale    = 1
0.00.059.366 I ggml_metal_init: allocating
0.00.059.369 I ggml_metal_init: found device: Apple M4
0.00.059.371 I ggml_metal_init: picking default device: Apple M4
0.00.059.971 I ggml_metal_init: using embedded metal library
0.00.062.298 I ggml_metal_init: GPU name:   Apple M4
0.00.062.299 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.062.300 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.062.300 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.062.300 I ggml_metal_init: simdgroup reduction   = true
0.00.062.301 I ggml_metal_init: simdgroup matrix mul. = true
0.00.062.301 I ggml_metal_init: has bfloat            = true
0.00.062.301 I ggml_metal_init: use bfloat            = true
0.00.062.301 I ggml_metal_init: hasUnifiedMemory      = true
0.00.062.302 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.072.027 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.091.087 I init:      Metal KV buffer size =   384.00 MiB
0.00.091.094 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.091.124 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.092.172 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.092.173 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.092.173 I llama_context: graph nodes  = 967
0.00.092.174 I llama_context: graph splits = 2
0.00.092.177 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.092.312 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.092.312 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.830.880 I main: llama threadpool init, n_threads = 4
0.00.830.925 I 
0.00.830.959 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.830.960 I 
0.00.831.201 I sampler seed: 1234
0.00.831.205 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.831.247 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.831.248 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.831.248 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.548.021 I llama_perf_sampler_print:    sampling time =       1.08 ms /    71 runs   (    0.02 ms per token, 65558.63 tokens per second)
0.01.548.021 I llama_perf_context_print:        load time =     822.26 ms
0.01.548.022 I llama_perf_context_print: prompt eval time =      39.65 ms /     7 tokens (    5.66 ms per token,   176.56 tokens per second)
0.01.548.023 I llama_perf_context_print:        eval time =     674.33 ms /    63 runs   (   10.70 ms per token,    93.43 tokens per second)
0.01.548.023 I llama_perf_context_print:       total time =     717.14 ms /    70 tokens
0.01.550.982 I ggml_metal_free: deallocating

real	0m1.567s
user	0m0.109s
sys	0m0.151s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4531 (a47d389c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.928 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.742 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.747 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.749 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.749 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.750 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.750 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.752 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.753 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.754 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.754 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.754 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.755 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.755 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.756 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.758 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.759 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.759 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.780 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.853 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.789 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.790 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.790 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.791 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.791 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.791 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.792 I llama_model_loader: - type  f32:  194 tensors
0.00.026.792 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.793 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.793 I print_info: file format = GGUF V3 (latest)
0.00.026.794 I print_info: file type   = Q5_0
0.00.026.795 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.046.309 I load: special tokens cache size = 25
0.00.052.397 I load: token to piece cache size = 0.2984 MB
0.00.052.400 I print_info: arch             = gptneox
0.00.052.400 I print_info: vocab_only       = 0
0.00.052.400 I print_info: n_ctx_train      = 2048
0.00.052.400 I print_info: n_embd           = 2048
0.00.052.401 I print_info: n_layer          = 24
0.00.052.403 I print_info: n_head           = 16
0.00.052.404 I print_info: n_head_kv        = 16
0.00.052.404 I print_info: n_rot            = 32
0.00.052.406 I print_info: n_swa            = 0
0.00.052.407 I print_info: n_embd_head_k    = 128
0.00.052.407 I print_info: n_embd_head_v    = 128
0.00.052.407 I print_info: n_gqa            = 1
0.00.052.408 I print_info: n_embd_k_gqa     = 2048
0.00.052.414 I print_info: n_embd_v_gqa     = 2048
0.00.052.415 I print_info: f_norm_eps       = 1.0e-05
0.00.052.415 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.415 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.419 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.420 I print_info: f_logit_scale    = 0.0e+00
0.00.052.421 I print_info: n_ff             = 8192
0.00.052.421 I print_info: n_expert         = 0
0.00.052.421 I print_info: n_expert_used    = 0
0.00.052.421 I print_info: causal attn      = 1
0.00.052.422 I print_info: pooling type     = 0
0.00.052.424 I print_info: rope type        = 2
0.00.052.425 I print_info: rope scaling     = linear
0.00.052.426 I print_info: freq_base_train  = 10000.0
0.00.052.426 I print_info: freq_scale_train = 1
0.00.052.426 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.426 I print_info: rope_finetuned   = unknown
0.00.052.426 I print_info: ssm_d_conv       = 0
0.00.052.426 I print_info: ssm_d_inner      = 0
0.00.052.426 I print_info: ssm_d_state      = 0
0.00.052.429 I print_info: ssm_dt_rank      = 0
0.00.052.430 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.430 I print_info: model type       = 1.4B
0.00.052.430 I print_info: model params     = 1.41 B
0.00.052.431 I print_info: general.name     = 1.4B
0.00.052.433 I print_info: vocab type       = BPE
0.00.052.433 I print_info: n_vocab          = 50304
0.00.052.433 I print_info: n_merges         = 50009
0.00.052.433 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.433 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.433 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.433 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.434 I print_info: LF token         = 128 'Ä'
0.00.052.434 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.435 I print_info: max token length = 1024
0.00.054.484 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.484 I load_tensors: offloading output layer to GPU
0.00.054.484 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.495 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.496 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.054.763 I llama_context: n_seq_max     = 1
0.00.054.763 I llama_context: n_ctx         = 2048
0.00.054.764 I llama_context: n_ctx_per_seq = 2048
0.00.054.764 I llama_context: n_batch       = 2048
0.00.054.764 I llama_context: n_ubatch      = 512
0.00.054.764 I llama_context: flash_attn    = 0
0.00.054.764 I llama_context: freq_base     = 10000.0
0.00.054.765 I llama_context: freq_scale    = 1
0.00.054.765 I ggml_metal_init: allocating
0.00.054.768 I ggml_metal_init: found device: Apple M4
0.00.054.770 I ggml_metal_init: picking default device: Apple M4
0.00.055.393 I ggml_metal_init: using embedded metal library
0.00.057.745 I ggml_metal_init: GPU name:   Apple M4
0.00.057.746 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.746 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.747 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.747 I ggml_metal_init: simdgroup reduction   = true
0.00.057.747 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.747 I ggml_metal_init: has bfloat            = true
0.00.057.747 I ggml_metal_init: use bfloat            = true
0.00.057.748 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.748 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.704 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.089.507 I init:      Metal KV buffer size =   384.00 MiB
0.00.089.514 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.546 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.090.559 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.090.560 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.090.560 I llama_context: graph nodes  = 967
0.00.090.561 I llama_context: graph splits = 2
0.00.090.564 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.693 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.693 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.745.735 I main: llama threadpool init, n_threads = 4
0.00.745.778 I 
0.00.745.808 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.745.811 I 
0.00.746.043 I sampler seed: 1234
0.00.746.049 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.746.082 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.746.083 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.746.083 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.528.011 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59814.66 tokens per second)
0.01.528.011 I llama_perf_context_print:        load time =     735.80 ms
0.01.528.012 I llama_perf_context_print: prompt eval time =      46.28 ms /     7 tokens (    6.61 ms per token,   151.25 tokens per second)
0.01.528.013 I llama_perf_context_print:        eval time =     732.70 ms /    63 runs   (   11.63 ms per token,    85.98 tokens per second)
0.01.528.013 I llama_perf_context_print:       total time =     782.28 ms /    70 tokens
0.01.530.396 I ggml_metal_free: deallocating

real	0m1.547s
user	0m0.109s
sys	0m0.168s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4531 (a47d389c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.644 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.197 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.202 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.204 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.204 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.205 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.205 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.205 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.207 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.207 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.208 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.208 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.208 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.209 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.209 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.211 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.211 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.212 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.274 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.311 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.283 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.284 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.284 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.285 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.285 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.285 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.286 I llama_model_loader: - type  f32:  194 tensors
0.00.025.286 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.286 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.287 I print_info: file format = GGUF V3 (latest)
0.00.025.287 I print_info: file type   = Q5_1
0.00.025.288 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.044.291 I load: special tokens cache size = 25
0.00.050.284 I load: token to piece cache size = 0.2984 MB
0.00.050.287 I print_info: arch             = gptneox
0.00.050.287 I print_info: vocab_only       = 0
0.00.050.287 I print_info: n_ctx_train      = 2048
0.00.050.288 I print_info: n_embd           = 2048
0.00.050.288 I print_info: n_layer          = 24
0.00.050.290 I print_info: n_head           = 16
0.00.050.291 I print_info: n_head_kv        = 16
0.00.050.291 I print_info: n_rot            = 32
0.00.050.291 I print_info: n_swa            = 0
0.00.050.291 I print_info: n_embd_head_k    = 128
0.00.050.292 I print_info: n_embd_head_v    = 128
0.00.050.292 I print_info: n_gqa            = 1
0.00.050.293 I print_info: n_embd_k_gqa     = 2048
0.00.050.294 I print_info: n_embd_v_gqa     = 2048
0.00.050.294 I print_info: f_norm_eps       = 1.0e-05
0.00.050.295 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.295 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.295 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.295 I print_info: f_logit_scale    = 0.0e+00
0.00.050.296 I print_info: n_ff             = 8192
0.00.050.296 I print_info: n_expert         = 0
0.00.050.297 I print_info: n_expert_used    = 0
0.00.050.297 I print_info: causal attn      = 1
0.00.050.298 I print_info: pooling type     = 0
0.00.050.299 I print_info: rope type        = 2
0.00.050.301 I print_info: rope scaling     = linear
0.00.050.301 I print_info: freq_base_train  = 10000.0
0.00.050.301 I print_info: freq_scale_train = 1
0.00.050.301 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.302 I print_info: rope_finetuned   = unknown
0.00.050.302 I print_info: ssm_d_conv       = 0
0.00.050.302 I print_info: ssm_d_inner      = 0
0.00.050.302 I print_info: ssm_d_state      = 0
0.00.050.302 I print_info: ssm_dt_rank      = 0
0.00.050.302 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.302 I print_info: model type       = 1.4B
0.00.050.303 I print_info: model params     = 1.41 B
0.00.050.303 I print_info: general.name     = 1.4B
0.00.050.304 I print_info: vocab type       = BPE
0.00.050.304 I print_info: n_vocab          = 50304
0.00.050.304 I print_info: n_merges         = 50009
0.00.050.304 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.304 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.305 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.305 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.306 I print_info: LF token         = 128 'Ä'
0.00.050.310 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.310 I print_info: max token length = 1024
0.00.052.324 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.324 I load_tensors: offloading output layer to GPU
0.00.052.324 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.335 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.336 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.052.612 I llama_context: n_seq_max     = 1
0.00.052.613 I llama_context: n_ctx         = 2048
0.00.052.613 I llama_context: n_ctx_per_seq = 2048
0.00.052.613 I llama_context: n_batch       = 2048
0.00.052.613 I llama_context: n_ubatch      = 512
0.00.052.613 I llama_context: flash_attn    = 0
0.00.052.614 I llama_context: freq_base     = 10000.0
0.00.052.614 I llama_context: freq_scale    = 1
0.00.052.614 I ggml_metal_init: allocating
0.00.052.617 I ggml_metal_init: found device: Apple M4
0.00.052.619 I ggml_metal_init: picking default device: Apple M4
0.00.053.216 I ggml_metal_init: using embedded metal library
0.00.055.558 I ggml_metal_init: GPU name:   Apple M4
0.00.055.559 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.559 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.560 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.560 I ggml_metal_init: simdgroup reduction   = true
0.00.055.560 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.560 I ggml_metal_init: has bfloat            = true
0.00.055.561 I ggml_metal_init: use bfloat            = true
0.00.055.561 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.562 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.473 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.626 I init:      Metal KV buffer size =   384.00 MiB
0.00.085.632 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.662 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.086.601 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.086.603 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.086.603 I llama_context: graph nodes  = 967
0.00.086.603 I llama_context: graph splits = 2
0.00.086.606 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.727 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.728 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.693.894 I main: llama threadpool init, n_threads = 4
0.00.693.958 I 
0.00.693.986 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.693.987 I 
0.00.694.220 I sampler seed: 1234
0.00.694.225 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.694.265 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.694.269 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.694.270 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.526.818 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59513.83 tokens per second)
0.01.526.819 I llama_perf_context_print:        load time =     685.24 ms
0.01.526.820 I llama_perf_context_print: prompt eval time =      42.22 ms /     7 tokens (    6.03 ms per token,   165.82 tokens per second)
0.01.526.821 I llama_perf_context_print:        eval time =     787.42 ms /    63 runs   (   12.50 ms per token,    80.01 tokens per second)
0.01.526.821 I llama_perf_context_print:       total time =     832.93 ms /    70 tokens
0.01.529.649 I ggml_metal_free: deallocating

real	0m1.547s
user	0m0.109s
sys	0m0.158s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4531 (a47d389c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.581 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.238 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.242 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.244 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.244 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.245 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.245 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.245 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.246 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.247 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.247 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.248 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.248 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.248 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.249 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.250 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.250 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.251 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.302 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.329 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.333 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.334 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.334 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.335 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.335 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.335 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.336 I llama_model_loader: - type  f32:  194 tensors
0.00.025.336 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.337 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.337 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.337 I print_info: file format = GGUF V3 (latest)
0.00.025.338 I print_info: file type   = Q2_K - Medium
0.00.025.339 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.044.488 I load: special tokens cache size = 25
0.00.050.519 I load: token to piece cache size = 0.2984 MB
0.00.050.522 I print_info: arch             = gptneox
0.00.050.522 I print_info: vocab_only       = 0
0.00.050.523 I print_info: n_ctx_train      = 2048
0.00.050.523 I print_info: n_embd           = 2048
0.00.050.523 I print_info: n_layer          = 24
0.00.050.526 I print_info: n_head           = 16
0.00.050.527 I print_info: n_head_kv        = 16
0.00.050.527 I print_info: n_rot            = 32
0.00.050.527 I print_info: n_swa            = 0
0.00.050.527 I print_info: n_embd_head_k    = 128
0.00.050.527 I print_info: n_embd_head_v    = 128
0.00.050.528 I print_info: n_gqa            = 1
0.00.050.529 I print_info: n_embd_k_gqa     = 2048
0.00.050.530 I print_info: n_embd_v_gqa     = 2048
0.00.050.530 I print_info: f_norm_eps       = 1.0e-05
0.00.050.530 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.530 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.531 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.533 I print_info: f_logit_scale    = 0.0e+00
0.00.050.534 I print_info: n_ff             = 8192
0.00.050.534 I print_info: n_expert         = 0
0.00.050.534 I print_info: n_expert_used    = 0
0.00.050.534 I print_info: causal attn      = 1
0.00.050.534 I print_info: pooling type     = 0
0.00.050.535 I print_info: rope type        = 2
0.00.050.535 I print_info: rope scaling     = linear
0.00.050.535 I print_info: freq_base_train  = 10000.0
0.00.050.535 I print_info: freq_scale_train = 1
0.00.050.536 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.536 I print_info: rope_finetuned   = unknown
0.00.050.537 I print_info: ssm_d_conv       = 0
0.00.050.538 I print_info: ssm_d_inner      = 0
0.00.050.538 I print_info: ssm_d_state      = 0
0.00.050.538 I print_info: ssm_dt_rank      = 0
0.00.050.538 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.538 I print_info: model type       = 1.4B
0.00.050.539 I print_info: model params     = 1.41 B
0.00.050.539 I print_info: general.name     = 1.4B
0.00.050.539 I print_info: vocab type       = BPE
0.00.050.539 I print_info: n_vocab          = 50304
0.00.050.540 I print_info: n_merges         = 50009
0.00.050.540 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.540 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.540 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.540 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.541 I print_info: LF token         = 128 'Ä'
0.00.050.545 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.545 I print_info: max token length = 1024
0.00.052.454 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.454 I load_tensors: offloading output layer to GPU
0.00.052.455 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.465 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.466 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.052.751 I llama_context: n_seq_max     = 1
0.00.052.751 I llama_context: n_ctx         = 2048
0.00.052.752 I llama_context: n_ctx_per_seq = 2048
0.00.052.752 I llama_context: n_batch       = 2048
0.00.052.752 I llama_context: n_ubatch      = 512
0.00.052.752 I llama_context: flash_attn    = 0
0.00.052.752 I llama_context: freq_base     = 10000.0
0.00.052.753 I llama_context: freq_scale    = 1
0.00.052.753 I ggml_metal_init: allocating
0.00.052.756 I ggml_metal_init: found device: Apple M4
0.00.052.757 I ggml_metal_init: picking default device: Apple M4
0.00.053.353 I ggml_metal_init: using embedded metal library
0.00.055.706 I ggml_metal_init: GPU name:   Apple M4
0.00.055.708 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.708 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.709 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.709 I ggml_metal_init: simdgroup reduction   = true
0.00.055.709 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.709 I ggml_metal_init: has bfloat            = true
0.00.055.709 I ggml_metal_init: use bfloat            = true
0.00.055.710 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.710 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.531 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.779 I init:      Metal KV buffer size =   384.00 MiB
0.00.084.787 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.828 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.085.856 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.085.857 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.085.857 I llama_context: graph nodes  = 967
0.00.085.857 I llama_context: graph splits = 2
0.00.085.860 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.987 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.988 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.437.337 I main: llama threadpool init, n_threads = 4
0.00.437.386 I 
0.00.437.418 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.437.419 I 
0.00.437.655 I sampler seed: 1234
0.00.437.673 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.437.704 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.437.707 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.437.707 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.113.837 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58053.97 tokens per second)
0.01.113.838 I llama_perf_context_print:        load time =     427.75 ms
0.01.113.838 I llama_perf_context_print: prompt eval time =      35.79 ms /     7 tokens (    5.11 ms per token,   195.57 tokens per second)
0.01.113.840 I llama_perf_context_print:        eval time =     637.33 ms /    63 runs   (   10.12 ms per token,    98.85 tokens per second)
0.01.113.842 I llama_perf_context_print:       total time =     676.50 ms /    70 tokens
0.01.116.968 I ggml_metal_free: deallocating

real	0m1.137s
user	0m0.109s
sys	0m0.112s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4531 (a47d389c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.206 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.955 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.960 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.962 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.967 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.967 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.968 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.968 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.969 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.969 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.970 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.970 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.970 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.971 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.974 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.978 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.978 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.978 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.301 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.389 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.430 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.431 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.431 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.432 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.432 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.432 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.433 I llama_model_loader: - type  f32:  194 tensors
0.00.026.433 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.433 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.434 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.434 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.434 I print_info: file format = GGUF V3 (latest)
0.00.026.435 I print_info: file type   = Q3_K - Medium
0.00.026.436 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.046.182 I load: special tokens cache size = 25
0.00.052.177 I load: token to piece cache size = 0.2984 MB
0.00.052.180 I print_info: arch             = gptneox
0.00.052.181 I print_info: vocab_only       = 0
0.00.052.181 I print_info: n_ctx_train      = 2048
0.00.052.181 I print_info: n_embd           = 2048
0.00.052.181 I print_info: n_layer          = 24
0.00.052.184 I print_info: n_head           = 16
0.00.052.185 I print_info: n_head_kv        = 16
0.00.052.185 I print_info: n_rot            = 32
0.00.052.186 I print_info: n_swa            = 0
0.00.052.186 I print_info: n_embd_head_k    = 128
0.00.052.188 I print_info: n_embd_head_v    = 128
0.00.052.189 I print_info: n_gqa            = 1
0.00.052.190 I print_info: n_embd_k_gqa     = 2048
0.00.052.191 I print_info: n_embd_v_gqa     = 2048
0.00.052.191 I print_info: f_norm_eps       = 1.0e-05
0.00.052.193 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.194 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.194 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.194 I print_info: f_logit_scale    = 0.0e+00
0.00.052.195 I print_info: n_ff             = 8192
0.00.052.195 I print_info: n_expert         = 0
0.00.052.195 I print_info: n_expert_used    = 0
0.00.052.196 I print_info: causal attn      = 1
0.00.052.196 I print_info: pooling type     = 0
0.00.052.196 I print_info: rope type        = 2
0.00.052.196 I print_info: rope scaling     = linear
0.00.052.196 I print_info: freq_base_train  = 10000.0
0.00.052.197 I print_info: freq_scale_train = 1
0.00.052.197 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.197 I print_info: rope_finetuned   = unknown
0.00.052.197 I print_info: ssm_d_conv       = 0
0.00.052.197 I print_info: ssm_d_inner      = 0
0.00.052.198 I print_info: ssm_d_state      = 0
0.00.052.198 I print_info: ssm_dt_rank      = 0
0.00.052.198 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.198 I print_info: model type       = 1.4B
0.00.052.198 I print_info: model params     = 1.41 B
0.00.052.199 I print_info: general.name     = 1.4B
0.00.052.205 I print_info: vocab type       = BPE
0.00.052.205 I print_info: n_vocab          = 50304
0.00.052.206 I print_info: n_merges         = 50009
0.00.052.206 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.206 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.206 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.206 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.207 I print_info: LF token         = 128 'Ä'
0.00.052.207 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.207 I print_info: max token length = 1024
0.00.054.266 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.266 I load_tensors: offloading output layer to GPU
0.00.054.266 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.277 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.054.278 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.054.630 I llama_context: n_seq_max     = 1
0.00.054.630 I llama_context: n_ctx         = 2048
0.00.054.630 I llama_context: n_ctx_per_seq = 2048
0.00.054.631 I llama_context: n_batch       = 2048
0.00.054.631 I llama_context: n_ubatch      = 512
0.00.054.631 I llama_context: flash_attn    = 0
0.00.054.631 I llama_context: freq_base     = 10000.0
0.00.054.632 I llama_context: freq_scale    = 1
0.00.054.632 I ggml_metal_init: allocating
0.00.054.636 I ggml_metal_init: found device: Apple M4
0.00.054.638 I ggml_metal_init: picking default device: Apple M4
0.00.055.261 I ggml_metal_init: using embedded metal library
0.00.057.645 I ggml_metal_init: GPU name:   Apple M4
0.00.057.647 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.647 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.648 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.648 I ggml_metal_init: simdgroup reduction   = true
0.00.057.648 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.648 I ggml_metal_init: has bfloat            = true
0.00.057.648 I ggml_metal_init: use bfloat            = true
0.00.057.649 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.649 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.626 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.709 I init:      Metal KV buffer size =   384.00 MiB
0.00.086.717 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.760 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.087.674 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.087.675 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.087.675 I llama_context: graph nodes  = 967
0.00.087.676 I llama_context: graph splits = 2
0.00.087.679 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.809 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.810 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.529.538 I main: llama threadpool init, n_threads = 4
0.00.529.580 I 
0.00.529.613 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.529.613 I 
0.00.529.831 I sampler seed: 1234
0.00.529.835 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.529.877 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.529.878 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.529.878 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.279.627 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60067.68 tokens per second)
0.01.279.627 I llama_perf_context_print:        load time =     520.33 ms
0.01.279.628 I llama_perf_context_print: prompt eval time =      47.88 ms /     7 tokens (    6.84 ms per token,   146.20 tokens per second)
0.01.279.629 I llama_perf_context_print:        eval time =     698.89 ms /    63 runs   (   11.09 ms per token,    90.14 tokens per second)
0.01.279.629 I llama_perf_context_print:       total time =     750.09 ms /    70 tokens
0.01.282.802 I ggml_metal_free: deallocating

real	0m1.300s
user	0m0.109s
sys	0m0.122s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4531 (a47d389c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.703 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.274 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.279 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.280 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.281 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.281 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.283 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.283 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.286 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.286 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.286 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.287 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.287 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.287 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.288 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.290 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.291 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.291 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.294 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.390 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.358 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.359 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.360 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.360 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.360 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.360 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.361 I llama_model_loader: - type  f32:  194 tensors
0.00.025.361 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.362 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.362 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.362 I print_info: file format = GGUF V3 (latest)
0.00.025.363 I print_info: file type   = Q4_K - Medium
0.00.025.364 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.547 I load: special tokens cache size = 25
0.00.050.614 I load: token to piece cache size = 0.2984 MB
0.00.050.617 I print_info: arch             = gptneox
0.00.050.617 I print_info: vocab_only       = 0
0.00.050.618 I print_info: n_ctx_train      = 2048
0.00.050.618 I print_info: n_embd           = 2048
0.00.050.618 I print_info: n_layer          = 24
0.00.050.621 I print_info: n_head           = 16
0.00.050.622 I print_info: n_head_kv        = 16
0.00.050.623 I print_info: n_rot            = 32
0.00.050.623 I print_info: n_swa            = 0
0.00.050.623 I print_info: n_embd_head_k    = 128
0.00.050.623 I print_info: n_embd_head_v    = 128
0.00.050.626 I print_info: n_gqa            = 1
0.00.050.627 I print_info: n_embd_k_gqa     = 2048
0.00.050.627 I print_info: n_embd_v_gqa     = 2048
0.00.050.628 I print_info: f_norm_eps       = 1.0e-05
0.00.050.628 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.630 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.630 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.630 I print_info: f_logit_scale    = 0.0e+00
0.00.050.631 I print_info: n_ff             = 8192
0.00.050.631 I print_info: n_expert         = 0
0.00.050.631 I print_info: n_expert_used    = 0
0.00.050.631 I print_info: causal attn      = 1
0.00.050.633 I print_info: pooling type     = 0
0.00.050.634 I print_info: rope type        = 2
0.00.050.634 I print_info: rope scaling     = linear
0.00.050.634 I print_info: freq_base_train  = 10000.0
0.00.050.635 I print_info: freq_scale_train = 1
0.00.050.635 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.635 I print_info: rope_finetuned   = unknown
0.00.050.635 I print_info: ssm_d_conv       = 0
0.00.050.635 I print_info: ssm_d_inner      = 0
0.00.050.635 I print_info: ssm_d_state      = 0
0.00.050.635 I print_info: ssm_dt_rank      = 0
0.00.050.636 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.636 I print_info: model type       = 1.4B
0.00.050.636 I print_info: model params     = 1.41 B
0.00.050.636 I print_info: general.name     = 1.4B
0.00.050.637 I print_info: vocab type       = BPE
0.00.050.637 I print_info: n_vocab          = 50304
0.00.050.637 I print_info: n_merges         = 50009
0.00.050.637 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.638 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.638 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.638 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.638 I print_info: LF token         = 128 'Ä'
0.00.050.639 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.639 I print_info: max token length = 1024
0.00.052.637 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.637 I load_tensors: offloading output layer to GPU
0.00.052.637 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.648 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.649 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.942 I llama_context: n_seq_max     = 1
0.00.052.942 I llama_context: n_ctx         = 2048
0.00.052.943 I llama_context: n_ctx_per_seq = 2048
0.00.052.943 I llama_context: n_batch       = 2048
0.00.052.943 I llama_context: n_ubatch      = 512
0.00.052.943 I llama_context: flash_attn    = 0
0.00.052.943 I llama_context: freq_base     = 10000.0
0.00.052.944 I llama_context: freq_scale    = 1
0.00.052.944 I ggml_metal_init: allocating
0.00.052.947 I ggml_metal_init: found device: Apple M4
0.00.052.949 I ggml_metal_init: picking default device: Apple M4
0.00.053.570 I ggml_metal_init: using embedded metal library
0.00.055.922 I ggml_metal_init: GPU name:   Apple M4
0.00.055.923 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.924 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.924 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.924 I ggml_metal_init: simdgroup reduction   = true
0.00.055.924 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.925 I ggml_metal_init: has bfloat            = true
0.00.055.925 I ggml_metal_init: use bfloat            = true
0.00.055.925 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.926 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.783 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.148 I init:      Metal KV buffer size =   384.00 MiB
0.00.086.153 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.184 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.087.255 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.087.256 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.087.257 I llama_context: graph nodes  = 967
0.00.087.257 I llama_context: graph splits = 2
0.00.087.260 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.388 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.389 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.606.351 I main: llama threadpool init, n_threads = 4
0.00.606.392 I 
0.00.606.445 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.606.447 I 
0.00.606.666 I sampler seed: 1234
0.00.606.672 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.606.694 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.606.695 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.606.695 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.363.389 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57959.18 tokens per second)
0.01.363.390 I llama_perf_context_print:        load time =     597.64 ms
0.01.363.391 I llama_perf_context_print: prompt eval time =      47.12 ms /     7 tokens (    6.73 ms per token,   148.57 tokens per second)
0.01.363.391 I llama_perf_context_print:        eval time =     706.58 ms /    63 runs   (   11.22 ms per token,    89.16 tokens per second)
0.01.363.392 I llama_perf_context_print:       total time =     757.04 ms /    70 tokens
0.01.366.300 I ggml_metal_free: deallocating

real	0m1.383s
user	0m0.109s
sys	0m0.140s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4531 (a47d389c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.009.729 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.287 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.292 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.293 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.294 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.294 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.294 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.295 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.296 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.296 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.297 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.297 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.297 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.298 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.298 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.299 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.300 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.300 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.218 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.295 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.224 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.225 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.226 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.226 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.226 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.227 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.227 I llama_model_loader: - type  f32:  194 tensors
0.00.025.228 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.228 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.228 I print_info: file format = GGUF V3 (latest)
0.00.025.229 I print_info: file type   = Q5_K - Medium
0.00.025.230 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.044.119 I load: special tokens cache size = 25
0.00.050.186 I load: token to piece cache size = 0.2984 MB
0.00.050.189 I print_info: arch             = gptneox
0.00.050.190 I print_info: vocab_only       = 0
0.00.050.190 I print_info: n_ctx_train      = 2048
0.00.050.190 I print_info: n_embd           = 2048
0.00.050.190 I print_info: n_layer          = 24
0.00.050.193 I print_info: n_head           = 16
0.00.050.194 I print_info: n_head_kv        = 16
0.00.050.194 I print_info: n_rot            = 32
0.00.050.194 I print_info: n_swa            = 0
0.00.050.194 I print_info: n_embd_head_k    = 128
0.00.050.195 I print_info: n_embd_head_v    = 128
0.00.050.195 I print_info: n_gqa            = 1
0.00.050.196 I print_info: n_embd_k_gqa     = 2048
0.00.050.199 I print_info: n_embd_v_gqa     = 2048
0.00.050.199 I print_info: f_norm_eps       = 1.0e-05
0.00.050.199 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.200 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.200 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.200 I print_info: f_logit_scale    = 0.0e+00
0.00.050.201 I print_info: n_ff             = 8192
0.00.050.201 I print_info: n_expert         = 0
0.00.050.201 I print_info: n_expert_used    = 0
0.00.050.201 I print_info: causal attn      = 1
0.00.050.208 I print_info: pooling type     = 0
0.00.050.210 I print_info: rope type        = 2
0.00.050.210 I print_info: rope scaling     = linear
0.00.050.211 I print_info: freq_base_train  = 10000.0
0.00.050.211 I print_info: freq_scale_train = 1
0.00.050.211 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.212 I print_info: rope_finetuned   = unknown
0.00.050.212 I print_info: ssm_d_conv       = 0
0.00.050.212 I print_info: ssm_d_inner      = 0
0.00.050.213 I print_info: ssm_d_state      = 0
0.00.050.213 I print_info: ssm_dt_rank      = 0
0.00.050.213 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.214 I print_info: model type       = 1.4B
0.00.050.214 I print_info: model params     = 1.41 B
0.00.050.214 I print_info: general.name     = 1.4B
0.00.050.215 I print_info: vocab type       = BPE
0.00.050.215 I print_info: n_vocab          = 50304
0.00.050.215 I print_info: n_merges         = 50009
0.00.050.215 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.215 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.216 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.216 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.216 I print_info: LF token         = 128 'Ä'
0.00.050.216 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.216 I print_info: max token length = 1024
0.00.052.247 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.247 I load_tensors: offloading output layer to GPU
0.00.052.247 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.258 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.259 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.532 I llama_context: n_seq_max     = 1
0.00.052.533 I llama_context: n_ctx         = 2048
0.00.052.533 I llama_context: n_ctx_per_seq = 2048
0.00.052.533 I llama_context: n_batch       = 2048
0.00.052.533 I llama_context: n_ubatch      = 512
0.00.052.533 I llama_context: flash_attn    = 0
0.00.052.534 I llama_context: freq_base     = 10000.0
0.00.052.534 I llama_context: freq_scale    = 1
0.00.052.534 I ggml_metal_init: allocating
0.00.052.538 I ggml_metal_init: found device: Apple M4
0.00.052.539 I ggml_metal_init: picking default device: Apple M4
0.00.053.127 I ggml_metal_init: using embedded metal library
0.00.055.509 I ggml_metal_init: GPU name:   Apple M4
0.00.055.511 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.511 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.512 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.512 I ggml_metal_init: simdgroup reduction   = true
0.00.055.512 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.512 I ggml_metal_init: has bfloat            = true
0.00.055.512 I ggml_metal_init: use bfloat            = true
0.00.055.513 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.513 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.255 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.272 I init:      Metal KV buffer size =   384.00 MiB
0.00.085.278 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.306 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.086.302 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.086.303 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.086.303 I llama_context: graph nodes  = 967
0.00.086.304 I llama_context: graph splits = 2
0.00.086.307 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.437 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.437 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.686.415 I main: llama threadpool init, n_threads = 4
0.00.686.455 I 
0.00.686.490 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.686.491 I 
0.00.686.725 I sampler seed: 1234
0.00.686.731 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.686.771 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.686.774 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.686.775 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.535.899 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57119.87 tokens per second)
0.01.535.899 I llama_perf_context_print:        load time =     676.68 ms
0.01.535.902 I llama_perf_context_print: prompt eval time =      51.51 ms /     7 tokens (    7.36 ms per token,   135.91 tokens per second)
0.01.535.903 I llama_perf_context_print:        eval time =     794.54 ms /    63 runs   (   12.61 ms per token,    79.29 tokens per second)
0.01.535.903 I llama_perf_context_print:       total time =     849.49 ms /    70 tokens
0.01.538.728 I ggml_metal_free: deallocating

real	0m1.557s
user	0m0.108s
sys	0m0.155s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4531 (a47d389c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.670 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.517 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.521 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.523 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.523 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.524 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.524 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.524 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.525 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.526 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.526 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.527 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.527 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.527 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.528 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.529 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.530 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.530 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.652 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.748 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.759 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.760 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.760 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.761 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.761 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.761 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.762 I llama_model_loader: - type  f32:  194 tensors
0.00.024.762 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.763 I print_info: file format = GGUF V3 (latest)
0.00.024.763 I print_info: file type   = Q6_K
0.00.024.764 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.044.554 I load: special tokens cache size = 25
0.00.050.643 I load: token to piece cache size = 0.2984 MB
0.00.050.646 I print_info: arch             = gptneox
0.00.050.646 I print_info: vocab_only       = 0
0.00.050.646 I print_info: n_ctx_train      = 2048
0.00.050.647 I print_info: n_embd           = 2048
0.00.050.647 I print_info: n_layer          = 24
0.00.050.649 I print_info: n_head           = 16
0.00.050.650 I print_info: n_head_kv        = 16
0.00.050.651 I print_info: n_rot            = 32
0.00.050.651 I print_info: n_swa            = 0
0.00.050.651 I print_info: n_embd_head_k    = 128
0.00.050.651 I print_info: n_embd_head_v    = 128
0.00.050.652 I print_info: n_gqa            = 1
0.00.050.652 I print_info: n_embd_k_gqa     = 2048
0.00.050.653 I print_info: n_embd_v_gqa     = 2048
0.00.050.654 I print_info: f_norm_eps       = 1.0e-05
0.00.050.654 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.654 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.656 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.656 I print_info: f_logit_scale    = 0.0e+00
0.00.050.656 I print_info: n_ff             = 8192
0.00.050.657 I print_info: n_expert         = 0
0.00.050.657 I print_info: n_expert_used    = 0
0.00.050.657 I print_info: causal attn      = 1
0.00.050.657 I print_info: pooling type     = 0
0.00.050.657 I print_info: rope type        = 2
0.00.050.658 I print_info: rope scaling     = linear
0.00.050.658 I print_info: freq_base_train  = 10000.0
0.00.050.658 I print_info: freq_scale_train = 1
0.00.050.658 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.659 I print_info: rope_finetuned   = unknown
0.00.050.661 I print_info: ssm_d_conv       = 0
0.00.050.661 I print_info: ssm_d_inner      = 0
0.00.050.661 I print_info: ssm_d_state      = 0
0.00.050.661 I print_info: ssm_dt_rank      = 0
0.00.050.661 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.662 I print_info: model type       = 1.4B
0.00.050.662 I print_info: model params     = 1.41 B
0.00.050.662 I print_info: general.name     = 1.4B
0.00.050.663 I print_info: vocab type       = BPE
0.00.050.663 I print_info: n_vocab          = 50304
0.00.050.663 I print_info: n_merges         = 50009
0.00.050.663 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.664 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.664 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.668 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.668 I print_info: LF token         = 128 'Ä'
0.00.050.669 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.669 I print_info: max token length = 1024
0.00.052.685 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.685 I load_tensors: offloading output layer to GPU
0.00.052.686 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.696 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.697 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.052.965 I llama_context: n_seq_max     = 1
0.00.052.966 I llama_context: n_ctx         = 2048
0.00.052.966 I llama_context: n_ctx_per_seq = 2048
0.00.052.966 I llama_context: n_batch       = 2048
0.00.052.967 I llama_context: n_ubatch      = 512
0.00.052.967 I llama_context: flash_attn    = 0
0.00.052.967 I llama_context: freq_base     = 10000.0
0.00.052.967 I llama_context: freq_scale    = 1
0.00.052.968 I ggml_metal_init: allocating
0.00.052.971 I ggml_metal_init: found device: Apple M4
0.00.052.972 I ggml_metal_init: picking default device: Apple M4
0.00.053.550 I ggml_metal_init: using embedded metal library
0.00.055.887 I ggml_metal_init: GPU name:   Apple M4
0.00.055.888 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.888 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.889 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.889 I ggml_metal_init: simdgroup reduction   = true
0.00.055.889 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.889 I ggml_metal_init: has bfloat            = true
0.00.055.889 I ggml_metal_init: use bfloat            = true
0.00.055.890 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.890 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.555 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.310 I init:      Metal KV buffer size =   384.00 MiB
0.00.085.316 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.346 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.086.275 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.086.277 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.086.277 I llama_context: graph nodes  = 967
0.00.086.277 I llama_context: graph splits = 2
0.00.086.280 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.408 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.409 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.738.206 I main: llama threadpool init, n_threads = 4
0.00.738.253 I 
0.00.738.307 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.738.309 I 
0.00.738.539 I sampler seed: 1234
0.00.738.543 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.738.587 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.738.606 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.738.607 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.620.697 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59314.95 tokens per second)
0.01.620.697 I llama_perf_context_print:        load time =     729.53 ms
0.01.620.698 I llama_perf_context_print: prompt eval time =      54.47 ms /     7 tokens (    7.78 ms per token,   128.52 tokens per second)
0.01.620.699 I llama_perf_context_print:        eval time =     824.60 ms /    63 runs   (   13.09 ms per token,    76.40 tokens per second)
0.01.620.699 I llama_perf_context_print:       total time =     882.50 ms /    70 tokens
0.01.623.640 I ggml_metal_free: deallocating

real	0m1.640s
user	0m0.109s
sys	0m0.163s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.503 I build: 4531 (a47d389c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.314 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.894 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.899 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.902 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.903 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.903 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.903 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.904 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.905 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.905 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.906 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.906 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.907 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.907 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.908 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.909 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.910 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.910 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.353 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.246 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.748 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.750 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.750 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.751 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.751 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.752 I llama_model_loader: - type  f32:  194 tensors
0.00.054.752 I llama_model_loader: - type  f16:   98 tensors
0.00.054.753 I print_info: file format = GGUF V3 (latest)
0.00.054.754 I print_info: file type   = all F32 (guessed)
0.00.054.755 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.080.747 I load: special tokens cache size = 25
0.00.087.145 I load: token to piece cache size = 0.2984 MB
0.00.087.148 I print_info: arch             = gptneox
0.00.087.148 I print_info: vocab_only       = 0
0.00.087.148 I print_info: n_ctx_train      = 2048
0.00.087.148 I print_info: n_embd           = 2048
0.00.087.148 I print_info: n_layer          = 24
0.00.087.151 I print_info: n_head           = 16
0.00.087.152 I print_info: n_head_kv        = 16
0.00.087.152 I print_info: n_rot            = 32
0.00.087.152 I print_info: n_swa            = 0
0.00.087.153 I print_info: n_embd_head_k    = 128
0.00.087.153 I print_info: n_embd_head_v    = 128
0.00.087.153 I print_info: n_gqa            = 1
0.00.087.154 I print_info: n_embd_k_gqa     = 2048
0.00.087.154 I print_info: n_embd_v_gqa     = 2048
0.00.087.155 I print_info: f_norm_eps       = 1.0e-05
0.00.087.164 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.087.165 I print_info: f_clamp_kqv      = 0.0e+00
0.00.087.165 I print_info: f_max_alibi_bias = 0.0e+00
0.00.087.165 I print_info: f_logit_scale    = 0.0e+00
0.00.087.171 I print_info: n_ff             = 8192
0.00.087.171 I print_info: n_expert         = 0
0.00.087.172 I print_info: n_expert_used    = 0
0.00.087.173 I print_info: causal attn      = 1
0.00.087.173 I print_info: pooling type     = 0
0.00.087.173 I print_info: rope type        = 2
0.00.087.173 I print_info: rope scaling     = linear
0.00.087.174 I print_info: freq_base_train  = 10000.0
0.00.087.174 I print_info: freq_scale_train = 1
0.00.087.174 I print_info: n_ctx_orig_yarn  = 2048
0.00.087.176 I print_info: rope_finetuned   = unknown
0.00.087.176 I print_info: ssm_d_conv       = 0
0.00.087.176 I print_info: ssm_d_inner      = 0
0.00.087.176 I print_info: ssm_d_state      = 0
0.00.087.176 I print_info: ssm_dt_rank      = 0
0.00.087.176 I print_info: ssm_dt_b_c_rms   = 0
0.00.087.177 I print_info: model type       = 1.4B
0.00.087.177 I print_info: model params     = 1.41 B
0.00.087.177 I print_info: general.name     = 1.4B
0.00.087.178 I print_info: vocab type       = BPE
0.00.087.178 I print_info: n_vocab          = 50304
0.00.087.178 I print_info: n_merges         = 50009
0.00.087.178 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.087.179 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.087.179 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.087.179 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.087.179 I print_info: LF token         = 128 'Ä'
0.00.087.180 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.087.180 I print_info: max token length = 1024
0.00.089.677 I load_tensors: offloading 24 repeating layers to GPU
0.00.089.678 I load_tensors: offloading output layer to GPU
0.00.089.678 I load_tensors: offloaded 25/25 layers to GPU
0.00.089.688 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.089.689 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.089.989 I llama_context: n_seq_max     = 1
0.00.089.989 I llama_context: n_ctx         = 128
0.00.089.990 I llama_context: n_ctx_per_seq = 128
0.00.089.990 I llama_context: n_batch       = 128
0.00.089.990 I llama_context: n_ubatch      = 128
0.00.089.990 I llama_context: flash_attn    = 0
0.00.089.991 I llama_context: freq_base     = 10000.0
0.00.089.991 I llama_context: freq_scale    = 1
0.00.089.991 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.089.992 I ggml_metal_init: allocating
0.00.089.994 I ggml_metal_init: found device: Apple M4
0.00.089.996 I ggml_metal_init: picking default device: Apple M4
0.00.090.610 I ggml_metal_init: using embedded metal library
0.00.093.147 I ggml_metal_init: GPU name:   Apple M4
0.00.093.148 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.093.148 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.093.149 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.093.149 I ggml_metal_init: simdgroup reduction   = true
0.00.093.149 I ggml_metal_init: simdgroup matrix mul. = true
0.00.093.149 I ggml_metal_init: has bfloat            = true
0.00.093.150 I ggml_metal_init: use bfloat            = true
0.00.093.150 I ggml_metal_init: hasUnifiedMemory      = true
0.00.093.151 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.102.724 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.103.974 I init:      Metal KV buffer size =    24.00 MiB
0.00.103.977 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.104.002 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.104.910 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.104.911 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.104.911 I llama_context: graph nodes  = 967
0.00.104.912 I llama_context: graph splits = 2
0.00.104.913 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.104.913 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.159.110 I 
0.01.159.163 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.159.170 I perplexity: tokenizing the input ..
0.01.172.313 I perplexity: tokenization took 13.139 ms
0.01.172.319 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.306.907 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.308.637 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.308.694 I llama_perf_context_print:        load time =    1135.78 ms
0.01.308.695 I llama_perf_context_print: prompt eval time =     133.72 ms /   128 tokens (    1.04 ms per token,   957.22 tokens per second)
0.01.308.697 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.308.697 I llama_perf_context_print:       total time =     149.59 ms /   129 tokens
0.01.309.606 I ggml_metal_free: deallocating

real	0m1.498s
user	0m0.122s
sys	0m0.218s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.121 I build: 4531 (a47d389c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.893 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.727 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.022.733 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.741 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.742 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.742 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.743 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.743 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.744 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.745 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.745 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.746 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.746 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.748 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.750 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.752 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.752 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.753 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.866 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.450 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.513 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.515 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.516 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.516 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.517 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.517 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.036.518 I llama_model_loader: - type  f32:  194 tensors
0.00.036.518 I llama_model_loader: - type q8_0:   98 tensors
0.00.036.519 I print_info: file format = GGUF V3 (latest)
0.00.036.520 I print_info: file type   = Q8_0
0.00.036.521 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.061.484 I load: special tokens cache size = 25
0.00.068.015 I load: token to piece cache size = 0.2984 MB
0.00.068.019 I print_info: arch             = gptneox
0.00.068.019 I print_info: vocab_only       = 0
0.00.068.019 I print_info: n_ctx_train      = 2048
0.00.068.019 I print_info: n_embd           = 2048
0.00.068.020 I print_info: n_layer          = 24
0.00.068.023 I print_info: n_head           = 16
0.00.068.024 I print_info: n_head_kv        = 16
0.00.068.024 I print_info: n_rot            = 32
0.00.068.024 I print_info: n_swa            = 0
0.00.068.024 I print_info: n_embd_head_k    = 128
0.00.068.024 I print_info: n_embd_head_v    = 128
0.00.068.028 I print_info: n_gqa            = 1
0.00.068.028 I print_info: n_embd_k_gqa     = 2048
0.00.068.029 I print_info: n_embd_v_gqa     = 2048
0.00.068.030 I print_info: f_norm_eps       = 1.0e-05
0.00.068.030 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.068.030 I print_info: f_clamp_kqv      = 0.0e+00
0.00.068.030 I print_info: f_max_alibi_bias = 0.0e+00
0.00.068.031 I print_info: f_logit_scale    = 0.0e+00
0.00.068.032 I print_info: n_ff             = 8192
0.00.068.033 I print_info: n_expert         = 0
0.00.068.033 I print_info: n_expert_used    = 0
0.00.068.033 I print_info: causal attn      = 1
0.00.068.033 I print_info: pooling type     = 0
0.00.068.033 I print_info: rope type        = 2
0.00.068.033 I print_info: rope scaling     = linear
0.00.068.034 I print_info: freq_base_train  = 10000.0
0.00.068.034 I print_info: freq_scale_train = 1
0.00.068.034 I print_info: n_ctx_orig_yarn  = 2048
0.00.068.038 I print_info: rope_finetuned   = unknown
0.00.068.038 I print_info: ssm_d_conv       = 0
0.00.068.038 I print_info: ssm_d_inner      = 0
0.00.068.038 I print_info: ssm_d_state      = 0
0.00.068.038 I print_info: ssm_dt_rank      = 0
0.00.068.038 I print_info: ssm_dt_b_c_rms   = 0
0.00.068.039 I print_info: model type       = 1.4B
0.00.068.041 I print_info: model params     = 1.41 B
0.00.068.041 I print_info: general.name     = 1.4B
0.00.068.042 I print_info: vocab type       = BPE
0.00.068.042 I print_info: n_vocab          = 50304
0.00.068.042 I print_info: n_merges         = 50009
0.00.068.043 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.068.043 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.068.044 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.068.044 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.068.044 I print_info: LF token         = 128 'Ä'
0.00.068.044 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.068.044 I print_info: max token length = 1024
0.00.070.541 I load_tensors: offloading 24 repeating layers to GPU
0.00.070.541 I load_tensors: offloading output layer to GPU
0.00.070.542 I load_tensors: offloaded 25/25 layers to GPU
0.00.070.553 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.070.554 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.070.958 I llama_context: n_seq_max     = 1
0.00.070.959 I llama_context: n_ctx         = 128
0.00.070.960 I llama_context: n_ctx_per_seq = 128
0.00.070.960 I llama_context: n_batch       = 128
0.00.070.960 I llama_context: n_ubatch      = 128
0.00.070.960 I llama_context: flash_attn    = 0
0.00.070.960 I llama_context: freq_base     = 10000.0
0.00.070.961 I llama_context: freq_scale    = 1
0.00.070.961 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.070.961 I ggml_metal_init: allocating
0.00.070.964 I ggml_metal_init: found device: Apple M4
0.00.070.967 I ggml_metal_init: picking default device: Apple M4
0.00.071.689 I ggml_metal_init: using embedded metal library
0.00.074.394 I ggml_metal_init: GPU name:   Apple M4
0.00.074.395 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.074.396 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.074.396 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.074.397 I ggml_metal_init: simdgroup reduction   = true
0.00.074.397 I ggml_metal_init: simdgroup matrix mul. = true
0.00.074.397 I ggml_metal_init: has bfloat            = true
0.00.074.397 I ggml_metal_init: use bfloat            = true
0.00.074.397 I ggml_metal_init: hasUnifiedMemory      = true
0.00.074.398 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.174 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.654 I init:      Metal KV buffer size =    24.00 MiB
0.00.085.660 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.085.688 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.086.777 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.086.778 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.086.778 I llama_context: graph nodes  = 967
0.00.086.778 I llama_context: graph splits = 2
0.00.086.781 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.086.781 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.902.113 I 
0.00.902.143 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.902.162 I perplexity: tokenizing the input ..
0.00.910.012 I perplexity: tokenization took 7.849 ms
0.00.910.015 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.034.357 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.035.536 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.035.566 I llama_perf_context_print:        load time =     890.22 ms
0.01.035.567 I llama_perf_context_print: prompt eval time =     124.12 ms /   128 tokens (    0.97 ms per token,  1031.28 tokens per second)
0.01.035.568 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.035.569 I llama_perf_context_print:       total time =     133.45 ms /   129 tokens
0.01.036.267 I ggml_metal_free: deallocating

real	0m1.054s
user	0m0.095s
sys	0m0.154s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4531 (a47d389c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.952 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.056 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.060 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.063 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.064 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.064 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.065 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.065 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.068 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.068 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.068 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.069 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.069 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.069 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.073 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.076 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.077 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.077 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.047 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.075 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.949 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.951 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.951 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.951 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.952 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.952 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.953 I llama_model_loader: - type  f32:  194 tensors
0.00.025.953 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.953 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.954 I print_info: file format = GGUF V3 (latest)
0.00.025.954 I print_info: file type   = Q4_0
0.00.025.955 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.044.738 I load: special tokens cache size = 25
0.00.050.570 I load: token to piece cache size = 0.2984 MB
0.00.050.574 I print_info: arch             = gptneox
0.00.050.574 I print_info: vocab_only       = 0
0.00.050.574 I print_info: n_ctx_train      = 2048
0.00.050.574 I print_info: n_embd           = 2048
0.00.050.574 I print_info: n_layer          = 24
0.00.050.577 I print_info: n_head           = 16
0.00.050.578 I print_info: n_head_kv        = 16
0.00.050.578 I print_info: n_rot            = 32
0.00.050.578 I print_info: n_swa            = 0
0.00.050.579 I print_info: n_embd_head_k    = 128
0.00.050.579 I print_info: n_embd_head_v    = 128
0.00.050.580 I print_info: n_gqa            = 1
0.00.050.581 I print_info: n_embd_k_gqa     = 2048
0.00.050.581 I print_info: n_embd_v_gqa     = 2048
0.00.050.582 I print_info: f_norm_eps       = 1.0e-05
0.00.050.582 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.582 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.583 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.583 I print_info: f_logit_scale    = 0.0e+00
0.00.050.583 I print_info: n_ff             = 8192
0.00.050.584 I print_info: n_expert         = 0
0.00.050.584 I print_info: n_expert_used    = 0
0.00.050.584 I print_info: causal attn      = 1
0.00.050.584 I print_info: pooling type     = 0
0.00.050.584 I print_info: rope type        = 2
0.00.050.584 I print_info: rope scaling     = linear
0.00.050.585 I print_info: freq_base_train  = 10000.0
0.00.050.585 I print_info: freq_scale_train = 1
0.00.050.585 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.586 I print_info: rope_finetuned   = unknown
0.00.050.586 I print_info: ssm_d_conv       = 0
0.00.050.587 I print_info: ssm_d_inner      = 0
0.00.050.587 I print_info: ssm_d_state      = 0
0.00.050.587 I print_info: ssm_dt_rank      = 0
0.00.050.587 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.587 I print_info: model type       = 1.4B
0.00.050.588 I print_info: model params     = 1.41 B
0.00.050.588 I print_info: general.name     = 1.4B
0.00.050.588 I print_info: vocab type       = BPE
0.00.050.590 I print_info: n_vocab          = 50304
0.00.050.590 I print_info: n_merges         = 50009
0.00.050.590 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.591 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.591 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.591 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.591 I print_info: LF token         = 128 'Ä'
0.00.050.592 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.592 I print_info: max token length = 1024
0.00.052.444 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.444 I load_tensors: offloading output layer to GPU
0.00.052.444 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.455 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.456 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.736 I llama_context: n_seq_max     = 1
0.00.052.737 I llama_context: n_ctx         = 128
0.00.052.737 I llama_context: n_ctx_per_seq = 128
0.00.052.737 I llama_context: n_batch       = 128
0.00.052.738 I llama_context: n_ubatch      = 128
0.00.052.738 I llama_context: flash_attn    = 0
0.00.052.738 I llama_context: freq_base     = 10000.0
0.00.052.738 I llama_context: freq_scale    = 1
0.00.052.739 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.739 I ggml_metal_init: allocating
0.00.052.742 I ggml_metal_init: found device: Apple M4
0.00.052.744 I ggml_metal_init: picking default device: Apple M4
0.00.053.293 I ggml_metal_init: using embedded metal library
0.00.055.625 I ggml_metal_init: GPU name:   Apple M4
0.00.055.627 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.627 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.627 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.628 I ggml_metal_init: simdgroup reduction   = true
0.00.055.628 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.628 I ggml_metal_init: has bfloat            = true
0.00.055.628 I ggml_metal_init: use bfloat            = true
0.00.055.629 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.629 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.238 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.540 I init:      Metal KV buffer size =    24.00 MiB
0.00.066.545 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.571 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.067.443 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.067.444 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.067.445 I llama_context: graph nodes  = 967
0.00.067.445 I llama_context: graph splits = 2
0.00.067.446 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.446 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.625.579 I 
0.00.625.622 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.625.630 I perplexity: tokenizing the input ..
0.00.633.460 I perplexity: tokenization took 7.829 ms
0.00.633.465 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.756.099 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.757.243 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.757.279 I llama_perf_context_print:        load time =     615.62 ms
0.00.757.280 I llama_perf_context_print: prompt eval time =     122.41 ms /   128 tokens (    0.96 ms per token,  1045.69 tokens per second)
0.00.757.281 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.757.281 I llama_perf_context_print:       total time =     131.70 ms /   129 tokens
0.00.758.011 I ggml_metal_free: deallocating

real	0m0.773s
user	0m0.077s
sys	0m0.104s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4531 (a47d389c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.772 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.195 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.200 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.202 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.202 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.203 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.203 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.203 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.204 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.205 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.208 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.208 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.209 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.209 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.210 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.213 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.213 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.213 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.084 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.124 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.991 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.992 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.993 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.993 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.993 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.994 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.994 I llama_model_loader: - type  f32:  194 tensors
0.00.024.994 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.994 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.995 I print_info: file format = GGUF V3 (latest)
0.00.024.995 I print_info: file type   = Q4_1
0.00.024.996 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.043.910 I load: special tokens cache size = 25
0.00.049.941 I load: token to piece cache size = 0.2984 MB
0.00.049.944 I print_info: arch             = gptneox
0.00.049.945 I print_info: vocab_only       = 0
0.00.049.945 I print_info: n_ctx_train      = 2048
0.00.049.945 I print_info: n_embd           = 2048
0.00.049.945 I print_info: n_layer          = 24
0.00.049.948 I print_info: n_head           = 16
0.00.049.949 I print_info: n_head_kv        = 16
0.00.049.949 I print_info: n_rot            = 32
0.00.049.949 I print_info: n_swa            = 0
0.00.049.951 I print_info: n_embd_head_k    = 128
0.00.049.951 I print_info: n_embd_head_v    = 128
0.00.049.952 I print_info: n_gqa            = 1
0.00.049.953 I print_info: n_embd_k_gqa     = 2048
0.00.049.954 I print_info: n_embd_v_gqa     = 2048
0.00.049.954 I print_info: f_norm_eps       = 1.0e-05
0.00.049.954 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.955 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.955 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.955 I print_info: f_logit_scale    = 0.0e+00
0.00.049.955 I print_info: n_ff             = 8192
0.00.049.956 I print_info: n_expert         = 0
0.00.049.956 I print_info: n_expert_used    = 0
0.00.049.956 I print_info: causal attn      = 1
0.00.049.956 I print_info: pooling type     = 0
0.00.049.956 I print_info: rope type        = 2
0.00.049.956 I print_info: rope scaling     = linear
0.00.049.958 I print_info: freq_base_train  = 10000.0
0.00.049.960 I print_info: freq_scale_train = 1
0.00.049.960 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.960 I print_info: rope_finetuned   = unknown
0.00.049.960 I print_info: ssm_d_conv       = 0
0.00.049.960 I print_info: ssm_d_inner      = 0
0.00.049.961 I print_info: ssm_d_state      = 0
0.00.049.961 I print_info: ssm_dt_rank      = 0
0.00.049.962 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.962 I print_info: model type       = 1.4B
0.00.049.962 I print_info: model params     = 1.41 B
0.00.049.962 I print_info: general.name     = 1.4B
0.00.049.963 I print_info: vocab type       = BPE
0.00.049.963 I print_info: n_vocab          = 50304
0.00.049.963 I print_info: n_merges         = 50009
0.00.049.968 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.968 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.968 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.968 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.968 I print_info: LF token         = 128 'Ä'
0.00.049.969 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.970 I print_info: max token length = 1024
0.00.051.941 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.941 I load_tensors: offloading output layer to GPU
0.00.051.941 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.952 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.953 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.052.226 I llama_context: n_seq_max     = 1
0.00.052.227 I llama_context: n_ctx         = 128
0.00.052.227 I llama_context: n_ctx_per_seq = 128
0.00.052.227 I llama_context: n_batch       = 128
0.00.052.227 I llama_context: n_ubatch      = 128
0.00.052.228 I llama_context: flash_attn    = 0
0.00.052.228 I llama_context: freq_base     = 10000.0
0.00.052.228 I llama_context: freq_scale    = 1
0.00.052.229 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.229 I ggml_metal_init: allocating
0.00.052.232 I ggml_metal_init: found device: Apple M4
0.00.052.234 I ggml_metal_init: picking default device: Apple M4
0.00.052.797 I ggml_metal_init: using embedded metal library
0.00.055.126 I ggml_metal_init: GPU name:   Apple M4
0.00.055.127 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.127 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.128 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.128 I ggml_metal_init: simdgroup reduction   = true
0.00.055.128 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.128 I ggml_metal_init: has bfloat            = true
0.00.055.128 I ggml_metal_init: use bfloat            = true
0.00.055.129 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.129 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.822 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.089 I init:      Metal KV buffer size =    24.00 MiB
0.00.066.091 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.117 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.067.079 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.067.080 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.067.080 I llama_context: graph nodes  = 967
0.00.067.081 I llama_context: graph splits = 2
0.00.067.082 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.082 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.650.244 I 
0.00.650.293 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.650.304 I perplexity: tokenizing the input ..
0.00.658.242 I perplexity: tokenization took 7.937 ms
0.00.658.246 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.780.501 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.781.698 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.781.723 I llama_perf_context_print:        load time =     641.47 ms
0.00.781.724 I llama_perf_context_print: prompt eval time =     122.03 ms /   128 tokens (    0.95 ms per token,  1048.92 tokens per second)
0.00.781.725 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.781.725 I llama_perf_context_print:       total time =     131.48 ms /   129 tokens
0.00.782.350 I ggml_metal_free: deallocating

real	0m0.795s
user	0m0.077s
sys	0m0.092s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4531 (a47d389c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.584 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.509 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.513 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.515 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.517 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.517 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.517 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.518 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.519 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.519 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.521 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.521 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.522 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.522 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.522 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.524 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.524 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.525 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.585 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.694 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.772 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.773 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.773 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.774 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.774 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.774 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.775 I llama_model_loader: - type  f32:  194 tensors
0.00.025.775 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.775 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.776 I print_info: file format = GGUF V3 (latest)
0.00.025.776 I print_info: file type   = Q5_0
0.00.025.781 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.045.593 I load: special tokens cache size = 25
0.00.051.591 I load: token to piece cache size = 0.2984 MB
0.00.051.595 I print_info: arch             = gptneox
0.00.051.596 I print_info: vocab_only       = 0
0.00.051.596 I print_info: n_ctx_train      = 2048
0.00.051.596 I print_info: n_embd           = 2048
0.00.051.596 I print_info: n_layer          = 24
0.00.051.599 I print_info: n_head           = 16
0.00.051.600 I print_info: n_head_kv        = 16
0.00.051.600 I print_info: n_rot            = 32
0.00.051.600 I print_info: n_swa            = 0
0.00.051.600 I print_info: n_embd_head_k    = 128
0.00.051.600 I print_info: n_embd_head_v    = 128
0.00.051.602 I print_info: n_gqa            = 1
0.00.051.603 I print_info: n_embd_k_gqa     = 2048
0.00.051.605 I print_info: n_embd_v_gqa     = 2048
0.00.051.606 I print_info: f_norm_eps       = 1.0e-05
0.00.051.612 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.613 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.613 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.613 I print_info: f_logit_scale    = 0.0e+00
0.00.051.625 I print_info: n_ff             = 8192
0.00.051.625 I print_info: n_expert         = 0
0.00.051.625 I print_info: n_expert_used    = 0
0.00.051.625 I print_info: causal attn      = 1
0.00.051.625 I print_info: pooling type     = 0
0.00.051.625 I print_info: rope type        = 2
0.00.051.626 I print_info: rope scaling     = linear
0.00.051.626 I print_info: freq_base_train  = 10000.0
0.00.051.626 I print_info: freq_scale_train = 1
0.00.051.626 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.627 I print_info: rope_finetuned   = unknown
0.00.051.627 I print_info: ssm_d_conv       = 0
0.00.051.627 I print_info: ssm_d_inner      = 0
0.00.051.627 I print_info: ssm_d_state      = 0
0.00.051.627 I print_info: ssm_dt_rank      = 0
0.00.051.627 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.628 I print_info: model type       = 1.4B
0.00.051.628 I print_info: model params     = 1.41 B
0.00.051.628 I print_info: general.name     = 1.4B
0.00.051.628 I print_info: vocab type       = BPE
0.00.051.629 I print_info: n_vocab          = 50304
0.00.051.629 I print_info: n_merges         = 50009
0.00.051.629 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.629 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.629 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.629 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.630 I print_info: LF token         = 128 'Ä'
0.00.051.630 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.632 I print_info: max token length = 1024
0.00.053.669 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.670 I load_tensors: offloading output layer to GPU
0.00.053.670 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.680 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.681 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.974 I llama_context: n_seq_max     = 1
0.00.053.975 I llama_context: n_ctx         = 128
0.00.053.975 I llama_context: n_ctx_per_seq = 128
0.00.053.975 I llama_context: n_batch       = 128
0.00.053.975 I llama_context: n_ubatch      = 128
0.00.053.976 I llama_context: flash_attn    = 0
0.00.053.976 I llama_context: freq_base     = 10000.0
0.00.053.976 I llama_context: freq_scale    = 1
0.00.053.976 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.977 I ggml_metal_init: allocating
0.00.053.980 I ggml_metal_init: found device: Apple M4
0.00.053.981 I ggml_metal_init: picking default device: Apple M4
0.00.054.540 I ggml_metal_init: using embedded metal library
0.00.056.836 I ggml_metal_init: GPU name:   Apple M4
0.00.056.837 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.837 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.838 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.838 I ggml_metal_init: simdgroup reduction   = true
0.00.056.838 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.838 I ggml_metal_init: has bfloat            = true
0.00.056.838 I ggml_metal_init: use bfloat            = true
0.00.056.839 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.839 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.257 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.718 I init:      Metal KV buffer size =    24.00 MiB
0.00.067.722 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.748 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.068.642 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.068.643 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.068.643 I llama_context: graph nodes  = 967
0.00.068.644 I llama_context: graph splits = 2
0.00.068.645 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.645 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.674.331 I 
0.00.674.372 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.674.377 I perplexity: tokenizing the input ..
0.00.682.260 I perplexity: tokenization took 7.881 ms
0.00.682.263 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.817.303 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.818.469 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.818.500 I llama_perf_context_print:        load time =     664.74 ms
0.00.818.502 I llama_perf_context_print: prompt eval time =     134.80 ms /   128 tokens (    1.05 ms per token,   949.55 tokens per second)
0.00.818.502 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.818.503 I llama_perf_context_print:       total time =     144.17 ms /   129 tokens
0.00.819.092 I ggml_metal_free: deallocating

real	0m0.834s
user	0m0.079s
sys	0m0.118s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4531 (a47d389c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.847 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.642 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.652 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.654 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.655 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.655 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.656 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.656 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.657 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.657 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.657 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.659 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.660 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.660 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.661 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.662 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.663 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.663 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.744 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.836 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.821 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.822 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.822 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.822 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.823 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.823 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.824 I llama_model_loader: - type  f32:  194 tensors
0.00.024.824 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.824 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.825 I print_info: file format = GGUF V3 (latest)
0.00.024.825 I print_info: file type   = Q5_1
0.00.024.829 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.043.669 I load: special tokens cache size = 25
0.00.049.517 I load: token to piece cache size = 0.2984 MB
0.00.049.519 I print_info: arch             = gptneox
0.00.049.520 I print_info: vocab_only       = 0
0.00.049.520 I print_info: n_ctx_train      = 2048
0.00.049.520 I print_info: n_embd           = 2048
0.00.049.520 I print_info: n_layer          = 24
0.00.049.523 I print_info: n_head           = 16
0.00.049.524 I print_info: n_head_kv        = 16
0.00.049.524 I print_info: n_rot            = 32
0.00.049.526 I print_info: n_swa            = 0
0.00.049.526 I print_info: n_embd_head_k    = 128
0.00.049.526 I print_info: n_embd_head_v    = 128
0.00.049.527 I print_info: n_gqa            = 1
0.00.049.528 I print_info: n_embd_k_gqa     = 2048
0.00.049.528 I print_info: n_embd_v_gqa     = 2048
0.00.049.529 I print_info: f_norm_eps       = 1.0e-05
0.00.049.529 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.529 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.530 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.530 I print_info: f_logit_scale    = 0.0e+00
0.00.049.531 I print_info: n_ff             = 8192
0.00.049.531 I print_info: n_expert         = 0
0.00.049.531 I print_info: n_expert_used    = 0
0.00.049.531 I print_info: causal attn      = 1
0.00.049.531 I print_info: pooling type     = 0
0.00.049.531 I print_info: rope type        = 2
0.00.049.531 I print_info: rope scaling     = linear
0.00.049.532 I print_info: freq_base_train  = 10000.0
0.00.049.532 I print_info: freq_scale_train = 1
0.00.049.532 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.533 I print_info: rope_finetuned   = unknown
0.00.049.533 I print_info: ssm_d_conv       = 0
0.00.049.533 I print_info: ssm_d_inner      = 0
0.00.049.533 I print_info: ssm_d_state      = 0
0.00.049.533 I print_info: ssm_dt_rank      = 0
0.00.049.533 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.534 I print_info: model type       = 1.4B
0.00.049.536 I print_info: model params     = 1.41 B
0.00.049.536 I print_info: general.name     = 1.4B
0.00.049.536 I print_info: vocab type       = BPE
0.00.049.537 I print_info: n_vocab          = 50304
0.00.049.537 I print_info: n_merges         = 50009
0.00.049.537 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.537 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.537 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.538 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.538 I print_info: LF token         = 128 'Ä'
0.00.049.542 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.546 I print_info: max token length = 1024
0.00.051.553 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.553 I load_tensors: offloading output layer to GPU
0.00.051.553 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.564 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.565 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.051.833 I llama_context: n_seq_max     = 1
0.00.051.833 I llama_context: n_ctx         = 128
0.00.051.834 I llama_context: n_ctx_per_seq = 128
0.00.051.834 I llama_context: n_batch       = 128
0.00.051.834 I llama_context: n_ubatch      = 128
0.00.051.834 I llama_context: flash_attn    = 0
0.00.051.834 I llama_context: freq_base     = 10000.0
0.00.051.835 I llama_context: freq_scale    = 1
0.00.051.835 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.835 I ggml_metal_init: allocating
0.00.051.838 I ggml_metal_init: found device: Apple M4
0.00.051.840 I ggml_metal_init: picking default device: Apple M4
0.00.052.392 I ggml_metal_init: using embedded metal library
0.00.054.669 I ggml_metal_init: GPU name:   Apple M4
0.00.054.670 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.670 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.671 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.671 I ggml_metal_init: simdgroup reduction   = true
0.00.054.671 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.671 I ggml_metal_init: has bfloat            = true
0.00.054.671 I ggml_metal_init: use bfloat            = true
0.00.054.672 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.672 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.182 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.425 I init:      Metal KV buffer size =    24.00 MiB
0.00.065.431 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.456 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.066.326 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.066.327 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.066.327 I llama_context: graph nodes  = 967
0.00.066.328 I llama_context: graph splits = 2
0.00.066.329 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.329 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.622.717 I 
0.00.622.790 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.622.801 I perplexity: tokenizing the input ..
0.00.630.851 I perplexity: tokenization took 8.053 ms
0.00.630.865 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.765.875 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.767.154 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.767.183 I llama_perf_context_print:        load time =     613.87 ms
0.00.767.184 I llama_perf_context_print: prompt eval time =     134.78 ms /   128 tokens (    1.05 ms per token,   949.67 tokens per second)
0.00.767.185 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.767.186 I llama_perf_context_print:       total time =     144.47 ms /   129 tokens
0.00.767.930 I ggml_metal_free: deallocating

real	0m0.782s
user	0m0.077s
sys	0m0.105s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4531 (a47d389c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.058 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.819 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.824 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.825 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.826 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.826 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.827 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.827 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.828 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.828 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.828 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.829 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.829 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.830 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.830 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.832 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.832 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.833 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.891 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.001 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.010 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.011 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.011 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.012 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.012 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.012 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.013 I llama_model_loader: - type  f32:  194 tensors
0.00.026.013 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.013 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.013 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.014 I print_info: file format = GGUF V3 (latest)
0.00.026.014 I print_info: file type   = Q2_K - Medium
0.00.026.015 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.044.840 I load: special tokens cache size = 25
0.00.050.883 I load: token to piece cache size = 0.2984 MB
0.00.050.886 I print_info: arch             = gptneox
0.00.050.886 I print_info: vocab_only       = 0
0.00.050.887 I print_info: n_ctx_train      = 2048
0.00.050.887 I print_info: n_embd           = 2048
0.00.050.887 I print_info: n_layer          = 24
0.00.050.890 I print_info: n_head           = 16
0.00.050.895 I print_info: n_head_kv        = 16
0.00.050.895 I print_info: n_rot            = 32
0.00.050.895 I print_info: n_swa            = 0
0.00.050.896 I print_info: n_embd_head_k    = 128
0.00.050.896 I print_info: n_embd_head_v    = 128
0.00.050.897 I print_info: n_gqa            = 1
0.00.050.897 I print_info: n_embd_k_gqa     = 2048
0.00.050.899 I print_info: n_embd_v_gqa     = 2048
0.00.050.900 I print_info: f_norm_eps       = 1.0e-05
0.00.050.901 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.901 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.901 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.901 I print_info: f_logit_scale    = 0.0e+00
0.00.050.902 I print_info: n_ff             = 8192
0.00.050.902 I print_info: n_expert         = 0
0.00.050.904 I print_info: n_expert_used    = 0
0.00.050.904 I print_info: causal attn      = 1
0.00.050.904 I print_info: pooling type     = 0
0.00.050.904 I print_info: rope type        = 2
0.00.050.906 I print_info: rope scaling     = linear
0.00.050.906 I print_info: freq_base_train  = 10000.0
0.00.050.906 I print_info: freq_scale_train = 1
0.00.050.907 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.907 I print_info: rope_finetuned   = unknown
0.00.050.907 I print_info: ssm_d_conv       = 0
0.00.050.907 I print_info: ssm_d_inner      = 0
0.00.050.907 I print_info: ssm_d_state      = 0
0.00.050.910 I print_info: ssm_dt_rank      = 0
0.00.050.910 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.910 I print_info: model type       = 1.4B
0.00.050.911 I print_info: model params     = 1.41 B
0.00.050.911 I print_info: general.name     = 1.4B
0.00.050.911 I print_info: vocab type       = BPE
0.00.050.911 I print_info: n_vocab          = 50304
0.00.050.912 I print_info: n_merges         = 50009
0.00.050.912 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.912 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.912 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.912 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.913 I print_info: LF token         = 128 'Ä'
0.00.050.913 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.913 I print_info: max token length = 1024
0.00.052.685 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.686 I load_tensors: offloading output layer to GPU
0.00.052.686 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.692 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.692 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.053.142 I llama_context: n_seq_max     = 1
0.00.053.142 I llama_context: n_ctx         = 128
0.00.053.143 I llama_context: n_ctx_per_seq = 128
0.00.053.143 I llama_context: n_batch       = 128
0.00.053.143 I llama_context: n_ubatch      = 128
0.00.053.143 I llama_context: flash_attn    = 0
0.00.053.143 I llama_context: freq_base     = 10000.0
0.00.053.144 I llama_context: freq_scale    = 1
0.00.053.144 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.144 I ggml_metal_init: allocating
0.00.053.147 I ggml_metal_init: found device: Apple M4
0.00.053.149 I ggml_metal_init: picking default device: Apple M4
0.00.053.723 I ggml_metal_init: using embedded metal library
0.00.056.065 I ggml_metal_init: GPU name:   Apple M4
0.00.056.067 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.067 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.067 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.068 I ggml_metal_init: simdgroup reduction   = true
0.00.056.068 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.068 I ggml_metal_init: has bfloat            = true
0.00.056.068 I ggml_metal_init: use bfloat            = true
0.00.056.068 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.069 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.728 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.192 I init:      Metal KV buffer size =    24.00 MiB
0.00.067.194 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.219 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.068.147 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.068.148 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.068.148 I llama_context: graph nodes  = 967
0.00.068.148 I llama_context: graph splits = 2
0.00.068.150 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.150 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.381.594 I 
0.00.381.640 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.381.644 I perplexity: tokenizing the input ..
0.00.389.109 I perplexity: tokenization took 7.463 ms
0.00.389.113 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.521.690 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.522.885 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.522.927 I llama_perf_context_print:        load time =     371.53 ms
0.00.522.928 I llama_perf_context_print: prompt eval time =     132.34 ms /   128 tokens (    1.03 ms per token,   967.20 tokens per second)
0.00.522.929 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.522.929 I llama_perf_context_print:       total time =     141.33 ms /   129 tokens
0.00.523.729 I ggml_metal_free: deallocating

real	0m0.539s
user	0m0.078s
sys	0m0.067s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4531 (a47d389c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.592 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.721 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.726 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.727 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.728 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.728 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.729 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.729 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.730 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.730 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.731 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.731 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.732 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.734 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.735 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.736 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.736 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.737 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.773 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.879 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.842 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.844 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.844 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.844 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.845 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.845 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.846 I llama_model_loader: - type  f32:  194 tensors
0.00.024.846 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.846 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.846 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.847 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.847 I print_info: file format = GGUF V3 (latest)
0.00.024.848 I print_info: file type   = Q3_K - Medium
0.00.024.848 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.044.422 I load: special tokens cache size = 25
0.00.050.400 I load: token to piece cache size = 0.2984 MB
0.00.050.403 I print_info: arch             = gptneox
0.00.050.404 I print_info: vocab_only       = 0
0.00.050.404 I print_info: n_ctx_train      = 2048
0.00.050.404 I print_info: n_embd           = 2048
0.00.050.404 I print_info: n_layer          = 24
0.00.050.407 I print_info: n_head           = 16
0.00.050.408 I print_info: n_head_kv        = 16
0.00.050.408 I print_info: n_rot            = 32
0.00.050.408 I print_info: n_swa            = 0
0.00.050.408 I print_info: n_embd_head_k    = 128
0.00.050.408 I print_info: n_embd_head_v    = 128
0.00.050.409 I print_info: n_gqa            = 1
0.00.050.410 I print_info: n_embd_k_gqa     = 2048
0.00.050.411 I print_info: n_embd_v_gqa     = 2048
0.00.050.411 I print_info: f_norm_eps       = 1.0e-05
0.00.050.412 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.412 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.412 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.412 I print_info: f_logit_scale    = 0.0e+00
0.00.050.413 I print_info: n_ff             = 8192
0.00.050.413 I print_info: n_expert         = 0
0.00.050.413 I print_info: n_expert_used    = 0
0.00.050.413 I print_info: causal attn      = 1
0.00.050.413 I print_info: pooling type     = 0
0.00.050.413 I print_info: rope type        = 2
0.00.050.414 I print_info: rope scaling     = linear
0.00.050.414 I print_info: freq_base_train  = 10000.0
0.00.050.414 I print_info: freq_scale_train = 1
0.00.050.415 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.415 I print_info: rope_finetuned   = unknown
0.00.050.415 I print_info: ssm_d_conv       = 0
0.00.050.415 I print_info: ssm_d_inner      = 0
0.00.050.415 I print_info: ssm_d_state      = 0
0.00.050.415 I print_info: ssm_dt_rank      = 0
0.00.050.417 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.417 I print_info: model type       = 1.4B
0.00.050.418 I print_info: model params     = 1.41 B
0.00.050.418 I print_info: general.name     = 1.4B
0.00.050.418 I print_info: vocab type       = BPE
0.00.050.419 I print_info: n_vocab          = 50304
0.00.050.419 I print_info: n_merges         = 50009
0.00.050.419 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.419 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.419 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.419 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.420 I print_info: LF token         = 128 'Ä'
0.00.050.420 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.420 I print_info: max token length = 1024
0.00.052.349 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.349 I load_tensors: offloading output layer to GPU
0.00.052.349 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.360 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.361 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.052.629 I llama_context: n_seq_max     = 1
0.00.052.630 I llama_context: n_ctx         = 128
0.00.052.630 I llama_context: n_ctx_per_seq = 128
0.00.052.630 I llama_context: n_batch       = 128
0.00.052.630 I llama_context: n_ubatch      = 128
0.00.052.630 I llama_context: flash_attn    = 0
0.00.052.631 I llama_context: freq_base     = 10000.0
0.00.052.631 I llama_context: freq_scale    = 1
0.00.052.631 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.632 I ggml_metal_init: allocating
0.00.052.635 I ggml_metal_init: found device: Apple M4
0.00.052.637 I ggml_metal_init: picking default device: Apple M4
0.00.053.201 I ggml_metal_init: using embedded metal library
0.00.055.525 I ggml_metal_init: GPU name:   Apple M4
0.00.055.526 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.527 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.527 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.527 I ggml_metal_init: simdgroup reduction   = true
0.00.055.527 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.527 I ggml_metal_init: has bfloat            = true
0.00.055.528 I ggml_metal_init: use bfloat            = true
0.00.055.528 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.529 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.350 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.586 I init:      Metal KV buffer size =    24.00 MiB
0.00.066.590 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.615 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.067.454 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.067.455 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.067.455 I llama_context: graph nodes  = 967
0.00.067.455 I llama_context: graph splits = 2
0.00.067.456 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.457 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.467.138 I 
0.00.467.193 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.467.204 I perplexity: tokenizing the input ..
0.00.474.932 I perplexity: tokenization took 7.726 ms
0.00.474.935 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.606.925 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.608.101 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.608.133 I llama_perf_context_print:        load time =     458.54 ms
0.00.608.137 I llama_perf_context_print: prompt eval time =     131.76 ms /   128 tokens (    1.03 ms per token,   971.45 tokens per second)
0.00.608.138 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.608.138 I llama_perf_context_print:       total time =     141.00 ms /   129 tokens
0.00.608.896 I ggml_metal_free: deallocating

real	0m0.622s
user	0m0.079s
sys	0m0.078s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4531 (a47d389c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.856 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.091 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.097 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.098 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.099 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.099 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.099 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.100 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.100 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.101 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.101 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.102 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.102 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.104 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.105 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.108 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.108 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.108 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.193 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.219 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.281 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.282 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.283 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.283 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.283 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.284 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.284 I llama_model_loader: - type  f32:  194 tensors
0.00.025.284 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.285 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.285 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.285 I print_info: file format = GGUF V3 (latest)
0.00.025.286 I print_info: file type   = Q4_K - Medium
0.00.025.291 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.892 I load: special tokens cache size = 25
0.00.050.801 I load: token to piece cache size = 0.2984 MB
0.00.050.804 I print_info: arch             = gptneox
0.00.050.804 I print_info: vocab_only       = 0
0.00.050.805 I print_info: n_ctx_train      = 2048
0.00.050.805 I print_info: n_embd           = 2048
0.00.050.805 I print_info: n_layer          = 24
0.00.050.808 I print_info: n_head           = 16
0.00.050.808 I print_info: n_head_kv        = 16
0.00.050.809 I print_info: n_rot            = 32
0.00.050.809 I print_info: n_swa            = 0
0.00.050.809 I print_info: n_embd_head_k    = 128
0.00.050.809 I print_info: n_embd_head_v    = 128
0.00.050.810 I print_info: n_gqa            = 1
0.00.050.812 I print_info: n_embd_k_gqa     = 2048
0.00.050.813 I print_info: n_embd_v_gqa     = 2048
0.00.050.814 I print_info: f_norm_eps       = 1.0e-05
0.00.050.814 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.814 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.818 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.819 I print_info: f_logit_scale    = 0.0e+00
0.00.050.819 I print_info: n_ff             = 8192
0.00.050.820 I print_info: n_expert         = 0
0.00.050.820 I print_info: n_expert_used    = 0
0.00.050.820 I print_info: causal attn      = 1
0.00.050.820 I print_info: pooling type     = 0
0.00.050.820 I print_info: rope type        = 2
0.00.050.820 I print_info: rope scaling     = linear
0.00.050.821 I print_info: freq_base_train  = 10000.0
0.00.050.821 I print_info: freq_scale_train = 1
0.00.050.821 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.822 I print_info: rope_finetuned   = unknown
0.00.050.822 I print_info: ssm_d_conv       = 0
0.00.050.822 I print_info: ssm_d_inner      = 0
0.00.050.822 I print_info: ssm_d_state      = 0
0.00.050.822 I print_info: ssm_dt_rank      = 0
0.00.050.822 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.823 I print_info: model type       = 1.4B
0.00.050.823 I print_info: model params     = 1.41 B
0.00.050.823 I print_info: general.name     = 1.4B
0.00.050.824 I print_info: vocab type       = BPE
0.00.050.824 I print_info: n_vocab          = 50304
0.00.050.824 I print_info: n_merges         = 50009
0.00.050.824 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.824 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.825 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.825 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.826 I print_info: LF token         = 128 'Ä'
0.00.050.826 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.827 I print_info: max token length = 1024
0.00.052.359 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.359 I load_tensors: offloading output layer to GPU
0.00.052.359 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.369 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.370 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.631 I llama_context: n_seq_max     = 1
0.00.052.632 I llama_context: n_ctx         = 128
0.00.052.632 I llama_context: n_ctx_per_seq = 128
0.00.052.632 I llama_context: n_batch       = 128
0.00.052.632 I llama_context: n_ubatch      = 128
0.00.052.633 I llama_context: flash_attn    = 0
0.00.052.633 I llama_context: freq_base     = 10000.0
0.00.052.633 I llama_context: freq_scale    = 1
0.00.052.633 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.634 I ggml_metal_init: allocating
0.00.052.637 I ggml_metal_init: found device: Apple M4
0.00.052.639 I ggml_metal_init: picking default device: Apple M4
0.00.053.185 I ggml_metal_init: using embedded metal library
0.00.055.494 I ggml_metal_init: GPU name:   Apple M4
0.00.055.495 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.496 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.496 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.496 I ggml_metal_init: simdgroup reduction   = true
0.00.055.497 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.497 I ggml_metal_init: has bfloat            = true
0.00.055.497 I ggml_metal_init: use bfloat            = true
0.00.055.497 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.499 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.207 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.460 I init:      Metal KV buffer size =    24.00 MiB
0.00.066.462 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.488 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.067.373 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.067.374 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.067.374 I llama_context: graph nodes  = 967
0.00.067.374 I llama_context: graph splits = 2
0.00.067.376 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.376 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.561.250 I 
0.00.561.295 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.561.305 I perplexity: tokenizing the input ..
0.00.569.598 I perplexity: tokenization took 8.292 ms
0.00.569.602 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.703.615 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.704.851 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.704.882 I llama_perf_context_print:        load time =     552.39 ms
0.00.704.882 I llama_perf_context_print: prompt eval time =     133.79 ms /   128 tokens (    1.05 ms per token,   956.74 tokens per second)
0.00.704.883 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.704.884 I llama_perf_context_print:       total time =     143.63 ms /   129 tokens
0.00.705.631 I ggml_metal_free: deallocating

real	0m0.719s
user	0m0.079s
sys	0m0.100s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4531 (a47d389c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.049 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.721 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.726 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.727 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.728 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.728 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.729 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.729 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.730 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.730 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.730 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.731 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.731 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.732 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.733 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.735 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.735 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.735 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.751 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.774 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.740 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.741 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.741 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.741 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.741 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.742 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.742 I llama_model_loader: - type  f32:  194 tensors
0.00.025.745 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.745 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.745 I print_info: file format = GGUF V3 (latest)
0.00.025.746 I print_info: file type   = Q5_K - Medium
0.00.025.747 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.044.602 I load: special tokens cache size = 25
0.00.050.577 I load: token to piece cache size = 0.2984 MB
0.00.050.580 I print_info: arch             = gptneox
0.00.050.580 I print_info: vocab_only       = 0
0.00.050.580 I print_info: n_ctx_train      = 2048
0.00.050.580 I print_info: n_embd           = 2048
0.00.050.580 I print_info: n_layer          = 24
0.00.050.583 I print_info: n_head           = 16
0.00.050.584 I print_info: n_head_kv        = 16
0.00.050.584 I print_info: n_rot            = 32
0.00.050.585 I print_info: n_swa            = 0
0.00.050.586 I print_info: n_embd_head_k    = 128
0.00.050.588 I print_info: n_embd_head_v    = 128
0.00.050.588 I print_info: n_gqa            = 1
0.00.050.589 I print_info: n_embd_k_gqa     = 2048
0.00.050.590 I print_info: n_embd_v_gqa     = 2048
0.00.050.591 I print_info: f_norm_eps       = 1.0e-05
0.00.050.591 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.591 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.592 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.592 I print_info: f_logit_scale    = 0.0e+00
0.00.050.592 I print_info: n_ff             = 8192
0.00.050.593 I print_info: n_expert         = 0
0.00.050.593 I print_info: n_expert_used    = 0
0.00.050.593 I print_info: causal attn      = 1
0.00.050.593 I print_info: pooling type     = 0
0.00.050.593 I print_info: rope type        = 2
0.00.050.593 I print_info: rope scaling     = linear
0.00.050.594 I print_info: freq_base_train  = 10000.0
0.00.050.594 I print_info: freq_scale_train = 1
0.00.050.594 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.595 I print_info: rope_finetuned   = unknown
0.00.050.595 I print_info: ssm_d_conv       = 0
0.00.050.595 I print_info: ssm_d_inner      = 0
0.00.050.595 I print_info: ssm_d_state      = 0
0.00.050.595 I print_info: ssm_dt_rank      = 0
0.00.050.596 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.597 I print_info: model type       = 1.4B
0.00.050.597 I print_info: model params     = 1.41 B
0.00.050.597 I print_info: general.name     = 1.4B
0.00.050.598 I print_info: vocab type       = BPE
0.00.050.598 I print_info: n_vocab          = 50304
0.00.050.598 I print_info: n_merges         = 50009
0.00.050.599 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.600 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.600 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.600 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.600 I print_info: LF token         = 128 'Ä'
0.00.050.600 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.601 I print_info: max token length = 1024
0.00.052.568 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.568 I load_tensors: offloading output layer to GPU
0.00.052.569 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.580 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.581 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.855 I llama_context: n_seq_max     = 1
0.00.052.856 I llama_context: n_ctx         = 128
0.00.052.856 I llama_context: n_ctx_per_seq = 128
0.00.052.856 I llama_context: n_batch       = 128
0.00.052.856 I llama_context: n_ubatch      = 128
0.00.052.856 I llama_context: flash_attn    = 0
0.00.052.856 I llama_context: freq_base     = 10000.0
0.00.052.857 I llama_context: freq_scale    = 1
0.00.052.857 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.858 I ggml_metal_init: allocating
0.00.052.860 I ggml_metal_init: found device: Apple M4
0.00.052.862 I ggml_metal_init: picking default device: Apple M4
0.00.053.424 I ggml_metal_init: using embedded metal library
0.00.055.755 I ggml_metal_init: GPU name:   Apple M4
0.00.055.757 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.757 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.757 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.758 I ggml_metal_init: simdgroup reduction   = true
0.00.055.758 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.758 I ggml_metal_init: has bfloat            = true
0.00.055.758 I ggml_metal_init: use bfloat            = true
0.00.055.759 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.759 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.362 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.685 I init:      Metal KV buffer size =    24.00 MiB
0.00.066.688 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.724 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.067.638 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.067.639 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.067.640 I llama_context: graph nodes  = 967
0.00.067.640 I llama_context: graph splits = 2
0.00.067.641 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.641 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.612.466 I 
0.00.612.525 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.612.535 I perplexity: tokenizing the input ..
0.00.620.684 I perplexity: tokenization took 8.147 ms
0.00.620.692 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.761.356 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.762.537 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.762.568 I llama_perf_context_print:        load time =     602.41 ms
0.00.762.569 I llama_perf_context_print: prompt eval time =     140.44 ms /   128 tokens (    1.10 ms per token,   911.43 tokens per second)
0.00.762.570 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.762.570 I llama_perf_context_print:       total time =     150.11 ms /   129 tokens
0.00.763.284 I ggml_metal_free: deallocating

real	0m0.779s
user	0m0.078s
sys	0m0.106s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4531 (a47d389c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.800 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.431 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.435 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.437 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.438 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.440 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.440 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.441 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.441 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.442 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.442 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.443 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.443 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.443 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.444 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.445 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.445 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.446 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.386 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.360 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.219 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.220 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.220 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.221 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.221 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.221 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.222 I llama_model_loader: - type  f32:  194 tensors
0.00.024.222 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.223 I print_info: file format = GGUF V3 (latest)
0.00.024.223 I print_info: file type   = Q6_K
0.00.024.224 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.043.045 I load: special tokens cache size = 25
0.00.049.068 I load: token to piece cache size = 0.2984 MB
0.00.049.071 I print_info: arch             = gptneox
0.00.049.071 I print_info: vocab_only       = 0
0.00.049.071 I print_info: n_ctx_train      = 2048
0.00.049.071 I print_info: n_embd           = 2048
0.00.049.072 I print_info: n_layer          = 24
0.00.049.074 I print_info: n_head           = 16
0.00.049.075 I print_info: n_head_kv        = 16
0.00.049.075 I print_info: n_rot            = 32
0.00.049.075 I print_info: n_swa            = 0
0.00.049.077 I print_info: n_embd_head_k    = 128
0.00.049.077 I print_info: n_embd_head_v    = 128
0.00.049.080 I print_info: n_gqa            = 1
0.00.049.081 I print_info: n_embd_k_gqa     = 2048
0.00.049.083 I print_info: n_embd_v_gqa     = 2048
0.00.049.083 I print_info: f_norm_eps       = 1.0e-05
0.00.049.084 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.084 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.084 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.084 I print_info: f_logit_scale    = 0.0e+00
0.00.049.085 I print_info: n_ff             = 8192
0.00.049.085 I print_info: n_expert         = 0
0.00.049.085 I print_info: n_expert_used    = 0
0.00.049.085 I print_info: causal attn      = 1
0.00.049.086 I print_info: pooling type     = 0
0.00.049.086 I print_info: rope type        = 2
0.00.049.086 I print_info: rope scaling     = linear
0.00.049.087 I print_info: freq_base_train  = 10000.0
0.00.049.087 I print_info: freq_scale_train = 1
0.00.049.087 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.087 I print_info: rope_finetuned   = unknown
0.00.049.087 I print_info: ssm_d_conv       = 0
0.00.049.087 I print_info: ssm_d_inner      = 0
0.00.049.092 I print_info: ssm_d_state      = 0
0.00.049.092 I print_info: ssm_dt_rank      = 0
0.00.049.092 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.094 I print_info: model type       = 1.4B
0.00.049.094 I print_info: model params     = 1.41 B
0.00.049.094 I print_info: general.name     = 1.4B
0.00.049.094 I print_info: vocab type       = BPE
0.00.049.095 I print_info: n_vocab          = 50304
0.00.049.095 I print_info: n_merges         = 50009
0.00.049.095 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.095 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.095 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.095 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.096 I print_info: LF token         = 128 'Ä'
0.00.049.096 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.096 I print_info: max token length = 1024
0.00.050.680 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.680 I load_tensors: offloading output layer to GPU
0.00.050.681 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.691 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.050.692 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.050.971 I llama_context: n_seq_max     = 1
0.00.050.972 I llama_context: n_ctx         = 128
0.00.050.972 I llama_context: n_ctx_per_seq = 128
0.00.050.973 I llama_context: n_batch       = 128
0.00.050.973 I llama_context: n_ubatch      = 128
0.00.050.973 I llama_context: flash_attn    = 0
0.00.050.973 I llama_context: freq_base     = 10000.0
0.00.050.973 I llama_context: freq_scale    = 1
0.00.050.974 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.974 I ggml_metal_init: allocating
0.00.050.977 I ggml_metal_init: found device: Apple M4
0.00.050.979 I ggml_metal_init: picking default device: Apple M4
0.00.051.544 I ggml_metal_init: using embedded metal library
0.00.053.915 I ggml_metal_init: GPU name:   Apple M4
0.00.053.917 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.917 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.917 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.918 I ggml_metal_init: simdgroup reduction   = true
0.00.053.918 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.918 I ggml_metal_init: has bfloat            = true
0.00.053.918 I ggml_metal_init: use bfloat            = true
0.00.053.919 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.919 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.563 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.830 I init:      Metal KV buffer size =    24.00 MiB
0.00.064.834 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.863 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.065.717 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.065.718 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.065.718 I llama_context: graph nodes  = 967
0.00.065.719 I llama_context: graph splits = 2
0.00.065.720 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.720 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.398.043 I 
0.00.398.090 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.398.094 I perplexity: tokenizing the input ..
0.00.405.557 I perplexity: tokenization took 7.46 ms
0.00.405.560 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.546.032 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.547.199 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.547.225 I llama_perf_context_print:        load time =     389.23 ms
0.00.547.226 I llama_perf_context_print: prompt eval time =     140.23 ms /   128 tokens (    1.10 ms per token,   912.78 tokens per second)
0.00.547.227 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.547.227 I llama_perf_context_print:       total time =     149.19 ms /   129 tokens
0.00.547.980 I ggml_metal_free: deallocating

real	0m0.561s
user	0m0.077s
sys	0m0.085s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.245 I build: 4531 (a47d389c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.386 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.719 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.728 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.733 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.734 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.735 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.735 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.736 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.738 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.738 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.739 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.740 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.740 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.741 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.742 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.745 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.745 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.746 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.325 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.345 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.419 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.421 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.421 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.422 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.422 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.423 I llama_model_loader: - type  f32:  194 tensors
0.00.055.423 I llama_model_loader: - type  f16:   98 tensors
0.00.055.424 I print_info: file format = GGUF V3 (latest)
0.00.055.425 I print_info: file type   = all F32 (guessed)
0.00.055.426 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.081.602 I load: special tokens cache size = 25
0.00.088.069 I load: token to piece cache size = 0.2984 MB
0.00.088.072 I print_info: arch             = gptneox
0.00.088.072 I print_info: vocab_only       = 0
0.00.088.072 I print_info: n_ctx_train      = 2048
0.00.088.073 I print_info: n_embd           = 2048
0.00.088.073 I print_info: n_layer          = 24
0.00.088.076 I print_info: n_head           = 16
0.00.088.079 I print_info: n_head_kv        = 16
0.00.088.079 I print_info: n_rot            = 32
0.00.088.079 I print_info: n_swa            = 0
0.00.088.079 I print_info: n_embd_head_k    = 128
0.00.088.079 I print_info: n_embd_head_v    = 128
0.00.088.080 I print_info: n_gqa            = 1
0.00.088.085 I print_info: n_embd_k_gqa     = 2048
0.00.088.085 I print_info: n_embd_v_gqa     = 2048
0.00.088.086 I print_info: f_norm_eps       = 1.0e-05
0.00.088.086 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.088.087 I print_info: f_clamp_kqv      = 0.0e+00
0.00.088.087 I print_info: f_max_alibi_bias = 0.0e+00
0.00.088.087 I print_info: f_logit_scale    = 0.0e+00
0.00.088.088 I print_info: n_ff             = 8192
0.00.088.088 I print_info: n_expert         = 0
0.00.088.088 I print_info: n_expert_used    = 0
0.00.088.088 I print_info: causal attn      = 1
0.00.088.088 I print_info: pooling type     = 0
0.00.088.088 I print_info: rope type        = 2
0.00.088.088 I print_info: rope scaling     = linear
0.00.088.089 I print_info: freq_base_train  = 10000.0
0.00.088.089 I print_info: freq_scale_train = 1
0.00.088.089 I print_info: n_ctx_orig_yarn  = 2048
0.00.088.089 I print_info: rope_finetuned   = unknown
0.00.088.091 I print_info: ssm_d_conv       = 0
0.00.088.091 I print_info: ssm_d_inner      = 0
0.00.088.091 I print_info: ssm_d_state      = 0
0.00.088.091 I print_info: ssm_dt_rank      = 0
0.00.088.091 I print_info: ssm_dt_b_c_rms   = 0
0.00.088.091 I print_info: model type       = 1.4B
0.00.088.092 I print_info: model params     = 1.41 B
0.00.088.092 I print_info: general.name     = 1.4B
0.00.088.092 I print_info: vocab type       = BPE
0.00.088.093 I print_info: n_vocab          = 50304
0.00.088.093 I print_info: n_merges         = 50009
0.00.088.093 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.088.093 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.088.093 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.088.093 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.088.094 I print_info: LF token         = 128 'Ä'
0.00.088.094 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.088.094 I print_info: max token length = 1024
0.00.090.643 I load_tensors: offloading 24 repeating layers to GPU
0.00.090.644 I load_tensors: offloading output layer to GPU
0.00.090.644 I load_tensors: offloaded 25/25 layers to GPU
0.00.090.654 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.090.655 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.090.932 I llama_context: n_seq_max     = 1
0.00.090.933 I llama_context: n_ctx         = 128
0.00.090.933 I llama_context: n_ctx_per_seq = 128
0.00.090.933 I llama_context: n_batch       = 128
0.00.090.934 I llama_context: n_ubatch      = 128
0.00.090.934 I llama_context: flash_attn    = 0
0.00.090.934 I llama_context: freq_base     = 10000.0
0.00.090.934 I llama_context: freq_scale    = 1
0.00.090.935 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.090.935 I ggml_metal_init: allocating
0.00.090.938 I ggml_metal_init: found device: Apple M4
0.00.090.940 I ggml_metal_init: picking default device: Apple M4
0.00.091.541 I ggml_metal_init: using embedded metal library
0.00.094.043 I ggml_metal_init: GPU name:   Apple M4
0.00.094.045 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.094.045 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.094.046 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.094.046 I ggml_metal_init: simdgroup reduction   = true
0.00.094.046 I ggml_metal_init: simdgroup matrix mul. = true
0.00.094.046 I ggml_metal_init: has bfloat            = true
0.00.094.047 I ggml_metal_init: use bfloat            = true
0.00.094.047 I ggml_metal_init: hasUnifiedMemory      = true
0.00.094.048 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.103.353 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.104.733 I init:      Metal KV buffer size =    24.00 MiB
0.00.104.737 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.104.763 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.105.639 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.105.640 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.105.641 I llama_context: graph nodes  = 967
0.00.105.641 I llama_context: graph splits = 2
0.00.105.642 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.105.642 I 
0.00.105.678 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.105.679 I compute_imatrix: tokenizing the input ..
0.00.112.252 I compute_imatrix: tokenization took 6.572 ms
0.00.112.254 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.607.430 I compute_imatrix: 1.50 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.610.140 I llama_perf_context_print:        load time =    1585.04 ms
0.01.610.140 I llama_perf_context_print: prompt eval time =    1494.53 ms /   128 tokens (   11.68 ms per token,    85.65 tokens per second)
0.01.610.141 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.610.141 I llama_perf_context_print:       total time =    1587.75 ms /   129 tokens
0.01.610.799 I ggml_metal_free: deallocating

real	0m1.793s
user	0m0.171s
sys	0m0.225s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4531 (a47d389c)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x103607590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x103607ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x103608250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x103608800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x103608db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x103609360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x103609910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x103609ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10360a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10360a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10360ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10360b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10360be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10360c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10360ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10360d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10360dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10360e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10360ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10360f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10360f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1036100e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x103610800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1036110a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1036117c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x103611a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x103612090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x103612d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x103613240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x103613500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1036139a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x103613c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1036144f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x103614a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x103614cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x103615190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x103615630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x103615ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x103615f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x103616410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1036168b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x103616d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1036171f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x103617690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x103617950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x103617f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x103618570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x103618e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1036194a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x103619ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10361a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10361a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10361ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10361b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10361bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10361bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10361c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10361c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10361ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10361d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10361d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10361dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10361e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10361e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10361ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10361eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10361f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10361f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10361fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x103620140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1036205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x103620a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x103620f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x103621470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1036219c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x103621f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x103622460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1036229b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x103622f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x103623450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1036239a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x103623ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x103624440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x103624990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x103624ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x103625430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x103625980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x103625ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x103626420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x103626970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x103626ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x103627410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x103627960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x103627eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x103628400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x103628950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x103628ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x103618b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x103629310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x103629ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10362a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10362a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10362aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10362b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10362b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10362baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10362bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10362c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10362ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10362cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10362d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10362da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10362dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10362e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10362e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10362edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10362f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10362f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10362fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x103630030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1036304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x103630970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x103630e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1036312b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x103631750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x103631bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x103632090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x103632530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1036329d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x103632e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x103633310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1036337b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x103633c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1036340f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x103634590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x103634a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x103634ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x103635370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x103635810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x103635cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x103636150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1036365f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x103636a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x103636f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1036373d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x103637870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x103637d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1036381b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x103638650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x103638af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x103638f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x103639430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1036398d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x103639d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10363a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10363a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10363ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10363aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10363b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10363b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10363bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10363c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10363c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10363cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10363d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10363d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10363d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10363de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10363e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10363e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10363ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10363f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10363f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10363f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10363fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x103640330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1036407d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x103640c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x103641110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1036415b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x103641a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x103641ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x103642390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x103642830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x103642cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x103643170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x103643610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x103643ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x103643f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1036443f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x103644890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x103644d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1036451d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x103645720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x103645c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1036461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x103646710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1036469d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x103646fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1036475f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x103647c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1036483f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x103648890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x103648b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x103649160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x103649770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x103649f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10364a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10364a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10364ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10364b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10364ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10364bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10364c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10364ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10364cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10364d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10364da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10364df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10364e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10364ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10364ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10364f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10364fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10364ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1036504a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1036509f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x103650f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x103651490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1036519e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x103651f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x103652480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1036529d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x103652f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x103653470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1036539c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x103653f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x103654460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1036549b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x103654f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x103655450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1036559a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x103655ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x103656440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x103656990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x103656ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x103657430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x103657980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x103657ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x103658420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x103658970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x103658ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x103659410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x103659960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x103659eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10365a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10365a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10365aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10365b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10365b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10365be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10365c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10365c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10365ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10365d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10365d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10365de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10365e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10365e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10365ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10365f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10365f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10365fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10365fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x103660370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x103660810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x103660cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x103661150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1036615f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x103661a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x103661f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1036623d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x103662920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x103663040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x103663760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x103663e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1036645a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x103664860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x103665050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x103665310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x103665920 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 967
llama_context: graph splits = 2
0.00.145.248 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.145.252 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x107f04b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x107f04f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x107f05400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x107f05870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x107f05ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x107f06150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x107f065c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x107f06a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x107f06ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x107f07310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x107f07780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x107f07e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x107f08990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x107f09140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x107f09950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x107f0a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x107f0a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x107f0aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x107f0b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x107f0bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x107f0c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x107f0cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x107f0d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x107f0d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x107f0e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x107f0e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x107f0e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x107f0ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x107f0ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x107f0f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x107f0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x107f0fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x107f10180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x107f10440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x107f108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x107f10d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x107f11190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x107f11600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x107f11a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x107f11ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x107f12350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x107f127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x107f12c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x107f130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x107f13510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x107f13980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x107f13df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x107f14260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x107f146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x107f14b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x107f14fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x107f15420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x107f15890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x107f15d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x107f16170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x107f165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x107f16b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x107f17050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x107f174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x107f17930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x107f17da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x107f18210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x107f18680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x107f18af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x107f18f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x107f193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x107f19840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x107f19cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x107f1a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x107f1a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x107f1aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x107f1ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x107f1b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x107f1b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x107f1bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x107f1c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x107f1c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x107f1c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x107f1cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x107f1d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x107f1d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x107f1dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x107f1df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x107f1e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x107f1e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x107f1ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x107f1f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x107f1f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x107f1f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x107f1fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x107f202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x107f20730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x107f20ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x107f21010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x107f21480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x107f218f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x107f21d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x107f221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x107f22640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x107f22ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x107f22f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x107f23390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x107f23800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x107f23c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x107f240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x107f24550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x107f249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x107f24e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x107f252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x107f25710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x107f25b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x107f25ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x107f26460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x107f268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x107f26d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x107f271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x107f27620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x107f27a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x107f27f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x107f28370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x107f287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x107f28c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x107f290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x107f29530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x107f299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x107f29e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x107f2a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x107f2a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x107f2ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x107f2afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x107f2b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x107f2b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x107f2bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x107f2c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x107f2c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x107f2ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x107f2cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x107f2d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x107f2d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x107f2dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x107f2e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x107f2e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x107f2e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x107f2edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x107f2f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x107f2f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x107f2fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x107f2ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x107f30420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x107f30890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x107f30d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x107f31170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x107f315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x107f31a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x107f31ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x107f32330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x107f327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x107f32c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x107f33080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x107f334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x107f33960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x107f33dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x107f34240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x107f346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x107f34b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x107f34f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x107f35bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x107f35e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x107f36140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x107f365b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x107f36a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x107f36e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x107f37300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x107f37770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x107f37be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x107f38050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x107f384c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x107f38930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x107f38da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x107f39210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x107f39680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x107f39af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x107f39f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x107f3a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x107f3a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x107f3acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x107f3b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x107f3b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x107f3ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x107f3be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x107f3c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x107f3c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x107f3cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x107f3d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x107f3d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x107f3d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x107f3dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x107f3e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x107f3e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x107f3ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x107f3ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x107f3f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x107f3f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x107f3fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x107f40290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x107f40700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x107f40b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x107f40fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x107f41500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x107f41a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x107f42580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x107f42840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x107f42e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x107f433c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x107f43980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x107f43f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x107f44500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x107f44ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x107f45080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x107f45640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x107f45c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x107f461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x107f46780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x107f46d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x107f47300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x107f478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x107f47e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x107f48440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x107f48a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x107f48fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x107f49580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x107f49b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x107f4a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x107f4a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x107f4ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x107f4b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x107f4b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x107f4bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x107f4c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x107f4c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x107f4cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x107f4d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x107f4da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x107f4e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x107f4e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x107f4ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x107f4f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x107f4f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x107f4fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x107f502c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x107f50880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x107f50e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x107f51400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x107f519c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x107f51f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x107f52540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x107f52b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x107f530c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x107f53680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x107f53c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x107f54200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x107f547c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x107f54d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x107f55340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x107f55900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x107f55ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x107f56480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x107f56a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x107f56f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x107f57440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x107f57940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x107f57e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x107f58340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x107f58840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x107f58d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x107f59240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x107f59740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x107f59c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x107f5a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x107f5a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x107f5ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x107f5b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x107f5b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x107f5bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x107f5c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x107f5cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x107f5d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x107f5d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x107f5df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x107f5e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x107f5e830 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 967
llama_context: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1036655d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x103648e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x103646c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1036478b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10361a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10361a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10361c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x103649420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x103611d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x103618830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x103619150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x103619760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x103617c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x103619d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x103610d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10361cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1036295d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x103664b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x103613f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1036141e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x103649a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x103647ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x103612350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x103612610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1036128d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x103665d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x103666040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x103666300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1036665c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x103666880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x103666b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x103666e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1036670c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x103667380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x103667640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x103667900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x103667bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x103667e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x103668140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x103668400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1036686c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x103668980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x103668c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x103668f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1036691c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x103669480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x103669740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x103669a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x103669cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x103669f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10366a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10366a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10366a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10366aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10366ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10366b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10366b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10366b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10366b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10366bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10366bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10366c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10366c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10366c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10366c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10366cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10366ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10366d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10366d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10366d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10366d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10366dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10366dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10366e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10366e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10366e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10366e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10366ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10366ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10366f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10366f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10366f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10366fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10366fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10366ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x103670280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x103670540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x103670800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x103670ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x103670d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x103671040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x103671300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1036715c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x103671880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x103671b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x103671e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1036720c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x103672380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x103672640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x103672900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x103672bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x103672e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x103673140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x103673400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1036736c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x103673980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x103673c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x103673f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1036741c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x103674480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x103674740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x103674a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x103674cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x103674f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x103675240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x103675500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1036757c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x103675a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x103675d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x103676000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1036762c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x103676580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x103676840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x103676b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x103676dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x103677080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x103677340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x103677600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1036778c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x103677b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x103677e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x103678100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1036783c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x103678680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x103678940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x103678c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x103678ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x103679180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x103679440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x103679700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1036799c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x103679c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x103679f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10367a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10367a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10367a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10367aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10367ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10367afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10367b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10367b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10367b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10367bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10367bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10367c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10367c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10367c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10367c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10367cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10367ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10367d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10367d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10367d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10367d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10367dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10367de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10367e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10367e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10367e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10367e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10367ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10367ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10367f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10367f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10367f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10367fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10367fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10367ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x103680240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x103680500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1036807c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x103680a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x103680d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x103681000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1036812c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x103681580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x103681840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x103681b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x103681dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x103682080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x103682340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x103682600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1036828c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x103682b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x103682e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x103683100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1036833c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x103683680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x103683940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x103683c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x103683ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x103684180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x103684440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x103684700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1036849c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x103684c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x103684f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x103685200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1036854c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x103685780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x103685d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1036862a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1036867f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x103686d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x103687290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1036877e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x103687d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x103688280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1036887d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x103688d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x103689270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1036897c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x103689d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10368a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10368a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10368ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10368b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10368b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10368bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10368c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10368c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10368cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10368d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10368d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10368dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10368e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10368e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10368ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10368f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10368f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10368fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x103690200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x103690750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x103690ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1036911f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x103691740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x103691c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1036921e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x103692730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x103692c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1036931d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x103693720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x103693c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1036941c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x103694710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x103694c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1036951b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x103695700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x103695c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1036961a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1036966f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x103696c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x103697190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1036976e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x103697c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x103698180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1036986d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x103698990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x103698c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x103698f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x103699380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1036997f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x103699c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10369a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10369a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10369a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10369ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10369b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10369b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10369bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10369bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10369c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10369c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10369cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10369da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10369e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10369e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10369eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10369ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10369f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10369fba0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 967
llama_context: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.804s
user	0m0.303s
sys	0m0.288s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4531 (a47d389c)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12380b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12380be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12380c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12380c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12380cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12380d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12380dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12380e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12380e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12380eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12380f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12380f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x123810050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x123810800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x123811010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x123811730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x123811e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x123812570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x123812c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x123813460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x123813b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1238142a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1238149c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x123815260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x123815980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x123815c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x123816250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x123816ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x123817400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1238176c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x123817b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x123817e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1238186b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x123818bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x123818eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x123819350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1238197f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x123819c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12381a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12381a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12381aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12381af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12381b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12381b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12381bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12381c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12381c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12381d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12381d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12381dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12381e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12381e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12381eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12381f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12381fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x123820140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1238205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1238208a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x123820eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1238216a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x123821960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x123821e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1238222a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x123822740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x123822be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x123823080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x123823520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1238239c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x123823e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x123824300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1238247a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x123824c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1238250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x123825630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x123825b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1238260d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x123826620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x123826b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1238270c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x123827610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x123827b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1238280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x123828600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x123828b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1238290a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1238295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x123829b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12382a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12382a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12382ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12382b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12382b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12382bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12382c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12382c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12382cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12382d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12381cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12382d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12382dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12382e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12382e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12382ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12382f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12382f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12382fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1238301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x123830700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x123830c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1238311a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1238316f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x123831c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x123832190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x123832630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x123832ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x123832f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x123833410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1238338b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x123833d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1238341f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x123834690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x123834b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x123834fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x123835470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x123835910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x123835db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x123836250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1238366f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x123836b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x123837030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1238374d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x123837970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x123837e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1238382b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x123838750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x123838bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x123839090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x123839530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1238399d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x123839e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12383a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12383a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12383ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12383b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12383b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12383ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12383bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12383c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12383c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12383ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12383d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12383d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12383da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12383df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12383e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12383e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12383ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12383f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12383f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12383faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12383ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x123840430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1238408d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x123840d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x123841210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1238416b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x123841b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x123841ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x123842490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x123842930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x123842dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x123843270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x123843710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x123843bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x123844050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1238444f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x123844990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x123844e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1238452d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x123845770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x123845c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1238460b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x123846550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1238469f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x123846e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x123847330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1238477d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x123847c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x123848110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1238485b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x123848a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x123848ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x123849390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1238498e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x123849e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12384a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12384a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12384ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12384b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12384b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12384bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12384c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12384ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12384cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12384d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12384d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12384e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12384e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12384ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12384ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12384f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12384fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x123850150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1238506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x123850bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x123851140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x123851690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x123851be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x123852130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x123852680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x123852bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x123853120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x123853670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x123853bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x123854110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x123854660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x123854bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x123855100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x123855650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x123855ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1238560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x123856640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x123856b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1238570e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x123857630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x123857b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1238580d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x123858620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x123858b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1238590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x123859610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x123859b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12385a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12385a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12385ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12385b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12385b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12385bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12385c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12385c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12385cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12385d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12385d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12385db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12385e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12385e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12385eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12385f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12385f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12385fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x123860050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1238605a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x123860af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x123861040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x123861590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x123861ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x123862030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1238624d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x123862970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x123862e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1238632b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x123863750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x123863bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x123864090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x123864530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1238649d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x123864e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x123865310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1238657b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x123865c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1238660f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x123866590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x123866ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x123867200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x123867920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x123868040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x123868760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x123868a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x123869210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1238694d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x123869ae0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 872
llama_context: graph splits = 2
0.00.088.162 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.167 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x123869790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12384cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12384ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12384ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12381eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12381e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x123820b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12384d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x123815f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12381c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12381d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12381d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12381bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12381df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x123814f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x123821170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12382d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x123868ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1238180e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1238183a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12384dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12384c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x123816510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1238167d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x123816a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x123869f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12386a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12386a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12386a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12386aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12386ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12386afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12386b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12386b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12386b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12386bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12386bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12386c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12386c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12386c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12386c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12386cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12386ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12386d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12386d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12386d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12386d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12386dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12386de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12386e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12386e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12386e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12386e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12386ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12386ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12386f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12386f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12386f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12386fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12386fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12386ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x123870240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x123870500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1238707c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x123870a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x123870d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x123871000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1238712c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x123871580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x123871840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x123871b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x123871dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x123872080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x123872340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x123872600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1238728c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x123872b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x123872e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x123873100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1238733c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x123873680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x123873940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x123873c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x123873ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x123874180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x123874440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x123874700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1238749c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x123874c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x123874f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x123875200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1238754c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x123875780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x123875a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x123875d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x123875fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x123876280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x123876540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x123876800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x123876ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x123876d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x123877040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x123877300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1238775c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x123877880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x123877b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x123877e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1238780c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x123878380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x123878640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x123878900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x123878bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x123878e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x123879140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x123879400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1238796c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x123879980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x123879c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x123879f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12387a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12387a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12387a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12387aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12387acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12387af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12387b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12387b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12387b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12387ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12387bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12387c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12387c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12387c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12387c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12387cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12387cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12387d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12387d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12387d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12387d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12387db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12387de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12387e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12387e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12387e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12387e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12387ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12387eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12387f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12387f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12387f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12387f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12387fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12387ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x123880200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1238804c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x123880780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x123880a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x123880d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x123880fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x123881280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x123881540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x123881800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x123881ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x123881d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x123882040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x123882300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1238825c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x123882880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x123882b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x123882e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1238830c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x123883380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x123883640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x123883900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x123883bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x123883e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x123884140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x123884400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1238846c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x123884980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x123884c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x123884f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1238851c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x123885480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x123885740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x123885a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x123885cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x123885f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x123886240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x123886500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1238867c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x123886a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x123886d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x123887000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1238872c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x123887580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x123887840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x123887b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x123887dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x123888080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x123888340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x123888600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1238888c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x123888b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x123888e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x123889100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1238893c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x123889680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x123889940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12388a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12388a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12388a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12388aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12388af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12388b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12388b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12388bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12388c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12388c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12388c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12388ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12388d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12388d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12388dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12388e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12388e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12388e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12388ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12388f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12388f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12388fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12388ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1238903a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x123890810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x123890c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1238910f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x123891560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1238919d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x123891e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1238922b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x123892720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x123892b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x123893000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x123893470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1238938e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x123893d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1238941c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x123894630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x123894aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x123894f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x123895380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1238957f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x123895c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1238960d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x123896540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1238969b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x123896e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x123897290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x123897700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x123897b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x123897fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x123898450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1238988c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x123898d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1238991a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x123899610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x123899a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x123899ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12389a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12389a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12389ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12389b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12389b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12389b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12389be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12389c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12389c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12389cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12389cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12389d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12389d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12389dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12389e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12389eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12389f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12389fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12389ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1238a0790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1238a0a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1238a1060 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 872
llama_context: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x121e076c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x121e07b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x121e07fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x121e08410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x121e08880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x121e08cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x121e09160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x121e095d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x121e09a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x121e09eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x121e0a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x121e0aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x121e0b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x121e0bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x121e0c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x121e0cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x121e0d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x121e0da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x121e0e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x121e0e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x121e0efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x121e0f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x121e0fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x121e10520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x121e10c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x121e10f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x121e111c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x121e11630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x121e11aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x121e11f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x121e12380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x121e128b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x121e12d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x121e12fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x121e13450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x121e138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x121e13d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x121e141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x121e14610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x121e14a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x121e14ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x121e15360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x121e157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x121e15c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x121e160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x121e16520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x121e16990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x121e16e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x121e17270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x121e176e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x121e17b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x121e17fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x121e18430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x121e188a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x121e18d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x121e19180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x121e196f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x121e19bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x121e1a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x121e1a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x121e1a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x121e1adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x121e1b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x121e1b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x121e1bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x121e1bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x121e1c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x121e1c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x121e1ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x121e1d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x121e1d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x121e1da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x121e1de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x121e1e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x121e1e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x121e1ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x121e1f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x121e1f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x121e1f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x121e1fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x121e20200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x121e20670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x121e20ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x121e20f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x121e213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x121e21830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x121e21ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x121e22110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x121e22580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x121e229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x121e22e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x121e232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x121e23740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x121e23bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x121e24020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x121e24490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x121e24900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x121e24d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x121e251e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x121e25650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x121e25ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x121e25f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x121e263a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x121e26c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x121e26ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x121e27360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x121e277d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x121e27c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x121e280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x121e28520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x121e28990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x121e28e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x121e29270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x121e296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x121e29b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x121e29fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x121e2a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x121e2a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x121e2ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x121e2b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x121e2b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x121e2ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x121e2bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x121e2c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x121e2c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x121e2cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x121e2d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x121e2d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x121e2d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x121e2dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x121e2e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x121e2e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x121e2eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x121e2efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x121e2f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x121e2f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x121e2fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x121e30160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x121e305d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x121e30a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x121e30eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x121e31320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x121e31790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x121e31c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x121e32070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x121e324e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x121e32950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x121e32dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x121e33230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x121e336a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x121e33b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x121e33f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x121e343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x121e34860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x121e34cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x121e35140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x121e355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x121e35a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x121e35e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x121e36300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x121e36770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x121e36be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x121e37050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x121e374c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x121e37930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x121e37da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x121e38210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x121e38680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x121e38af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x121e38f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x121e393d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x121e39840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x121e39cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x121e3a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x121e3a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x121e3aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x121e3ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x121e3b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x121e3b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x121e3bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x121e3c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x121e3c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x121e3c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x121e3cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x121e3d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x121e3d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x121e3dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x121e3df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x121e3e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x121e3e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x121e3ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x121e3f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x121e3f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x121e3f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x121e3fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x121e402c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x121e40730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x121e40ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x121e41010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x121e41480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x121e418f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x121e41d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x121e421d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x121e42640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x121e42ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x121e42f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x121e43390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x121e43800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x121e43c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x121e440e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x121e44c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x121e44f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x121e451e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x121e45650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x121e45ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x121e45f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x121e463a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x121e46810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x121e46c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x121e470f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x121e47560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x121e479d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x121e47e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x121e482b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x121e48720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x121e48b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x121e49000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x121e49470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x121e498e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x121e49d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x121e4a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x121e4a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x121e4aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x121e4af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x121e4b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x121e4b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x121e4bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x121e4c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x121e4c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x121e4c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x121e4ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x121e4d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x121e4d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x121e4db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x121e4dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x121e4e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x121e4e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x121e4ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x121e4f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x121e4f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x121e4fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x121e4fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x121e50360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x121e507d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x121e50c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x121e510b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x121e51520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x121e51990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x121e51e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x121e52270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x121e526e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x121e52b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x121e52fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x121e53430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x121e538a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x121e53d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x121e54180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x121e545f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x121e54a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x121e54ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x121e55340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x121e557b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x121e55c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x121e56090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x121e56500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x121e56970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x121e56de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x121e57250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x121e576c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x121e57b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x121e57fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x121e58410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x121e58880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x121e592f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x121e59a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x121e5a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x121e5a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x121e5ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x121e5af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x121e5b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x121e5bb90 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 872
llama_context: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.919s
user	0m0.242s
sys	0m0.138s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
