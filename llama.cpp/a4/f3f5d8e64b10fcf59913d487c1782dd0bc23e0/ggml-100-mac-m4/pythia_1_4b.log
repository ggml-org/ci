Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.554s
user	0m0.847s
sys	0m1.208s
++ nproc
+ make -j10
[  2%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  3%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target build_info
[  5%] Built target xxhash
[  5%] Built target sha256
[  5%] Built target sha1
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  7%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 14%] Linking C shared library libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 15%] Linking CXX shared library libggml.dylib
[ 15%] Built target ggml
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 16%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 16%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Linking CXX executable ../../bin/llama-gguf-hash
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 26%] Linking CXX shared library libllama.dylib
[ 26%] Built target llama-gguf
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama
[ 26%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 27%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 29%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 30%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 30%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 31%] Linking C executable ../bin/test-c
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-simple
[ 35%] Linking CXX executable ../../bin/llama-simple-chat
[ 35%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 35%] Linking CXX executable ../../bin/llama-quantize-stats
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llava
[ 36%] Built target llama-simple-chat
[ 36%] Built target llama-quantize-stats
[ 36%] Built target test-c
[ 36%] Built target llama-simple
[ 36%] Linking CXX shared library libllava_shared.dylib
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Built target common
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 40%] Built target llava_static
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 42%] Built target llava_shared
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-0
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 44%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 46%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-sampling
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Built target test-tokenizer-0
[ 50%] Linking CXX executable ../bin/test-log
[ 50%] Built target test-grammar-parser
[ 50%] Built target test-tokenizer-1-bpe
[ 50%] Linking CXX executable ../bin/test-arg-parser
[ 50%] Built target test-grammar-integration
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Built target test-sampling
[ 50%] Built target test-llama-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 55%] Built target test-log
[ 57%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 57%] Built target test-arg-parser
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-chat-template
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-gguf
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-model-load-cancel
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 59%] Linking CXX executable ../bin/test-backend-ops
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 61%] Linking CXX executable ../bin/test-barrier
[ 61%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-chat-template
[ 63%] Linking CXX executable ../../bin/llama-batched-bench
[ 63%] Built target test-gguf
[ 63%] Built target test-model-load-cancel
[ 63%] Built target test-backend-ops
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Built target test-autorelease
[ 64%] Built target test-quantize-fns
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Built target test-barrier
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Built target test-quantize-perf
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 64%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 65%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Built target llama-batched-bench
[ 66%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 67%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 69%] Linking CXX executable ../../bin/llama-embedding
[ 71%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Linking CXX executable ../../bin/llama-eval-callback
[ 71%] Built target test-rope
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-embedding
[ 73%] Built target llama-batched
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-gguf-split
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Built target llama-imatrix
[ 73%] Built target llama-gritlm
[ 73%] Built target llama-infill
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Built target llama-bench
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-lookup
[ 80%] Linking CXX executable ../../bin/llama-lookup-merge
[ 80%] Linking CXX executable ../../bin/llama-lookup-create
[ 81%] Linking CXX executable ../../bin/llama-cli
[ 81%] Built target llama-lookahead
[ 81%] Linking CXX executable ../../bin/llama-lookup-stats
[ 81%] Linking CXX executable ../../bin/llama-passkey
[ 81%] Linking CXX executable ../../bin/llama-parallel
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 82%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 83%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 83%] Built target llama-lookup-merge
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Built target llama-lookup
[ 83%] Built target llama-lookup-create
[ 84%] Generating loading.html.hpp
[ 84%] Built target llama-passkey
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Built target llama-parallel
[ 84%] Built target llama-cli
[ 84%] Built target llama-lookup-stats
[ 84%] Built target llama-perplexity
[ 84%] Generating index.html.gz.hpp
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 86%] Built target llama-quantize
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 88%] Linking CXX executable ../../bin/llama-tts
[ 88%] Linking CXX executable ../../bin/llama-run
[ 89%] Linking CXX executable ../../bin/llama-speculative-simple
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Built target llama-retrieval
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Built target llama-save-load-state
[ 92%] Built target llama-run
[ 92%] Built target llama-speculative
[ 92%] Built target llama-speculative-simple
[ 92%] Built target llama-tokenize
[ 92%] Built target llama-tts
[ 92%] Built target llama-gen-docs
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 97%] Built target llama-convert-llama2c-to-ggml
[ 97%] Linking CXX executable ../../bin/llama-llava-cli
[ 98%] Linking CXX executable ../../bin/llama-export-lora
[ 98%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 98%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-vdot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.023s
user	0m6.093s
sys	0m9.449s

main: quantize time =  5098.80 ms
main:    total time =  5098.80 ms

main: quantize time =  2197.21 ms
main:    total time =  2197.21 ms

main: quantize time =  1559.82 ms
main:    total time =  1559.82 ms

main: quantize time =  1964.40 ms
main:    total time =  1964.40 ms

main: quantize time =  2048.96 ms
main:    total time =  2048.96 ms

main: quantize time =  4957.81 ms
main:    total time =  4957.81 ms

main: quantize time =  5725.14 ms
main:    total time =  5725.14 ms

main: quantize time =  6797.20 ms
main:    total time =  6797.20 ms

main: quantize time =  5806.02 ms
main:    total time =  5806.02 ms

main: quantize time =  4544.94 ms
main:    total time =  4544.94 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.209 I build: 4479 (a4f3f5d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.323 I main: llama backend init
0.00.000.329 I main: load the model and apply lora adapter, if any
0.00.030.905 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.043.161 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.043.174 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.043.178 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.043.179 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.043.180 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.043.180 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.043.181 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.043.185 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.043.185 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.043.206 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.043.209 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.043.210 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.043.210 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.043.211 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.043.218 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.043.219 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.043.219 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.051.265 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.053.747 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.061.915 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.061.919 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.061.919 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.061.920 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.061.920 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.061.921 I llama_model_loader: - type  f32:  194 tensors
0.00.061.922 I llama_model_loader: - type  f16:   98 tensors
0.00.061.924 I print_info: file format = GGUF V3 (latest)
0.00.061.925 I print_info: file type   = all F32 (guessed)
0.00.061.927 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.092.308 I load: special tokens cache size = 25
0.00.099.235 I load: token to piece cache size = 0.2984 MB
0.00.099.238 I print_info: arch             = gptneox
0.00.099.239 I print_info: vocab_only       = 0
0.00.099.239 I print_info: n_ctx_train      = 2048
0.00.099.239 I print_info: n_embd           = 2048
0.00.099.239 I print_info: n_layer          = 24
0.00.099.242 I print_info: n_head           = 16
0.00.099.243 I print_info: n_head_kv        = 16
0.00.099.243 I print_info: n_rot            = 32
0.00.099.245 I print_info: n_swa            = 0
0.00.099.245 I print_info: n_embd_head_k    = 128
0.00.099.245 I print_info: n_embd_head_v    = 128
0.00.099.246 I print_info: n_gqa            = 1
0.00.099.247 I print_info: n_embd_k_gqa     = 2048
0.00.099.247 I print_info: n_embd_v_gqa     = 2048
0.00.099.248 I print_info: f_norm_eps       = 1.0e-05
0.00.099.249 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.099.249 I print_info: f_clamp_kqv      = 0.0e+00
0.00.099.250 I print_info: f_max_alibi_bias = 0.0e+00
0.00.099.250 I print_info: f_logit_scale    = 0.0e+00
0.00.099.251 I print_info: n_ff             = 8192
0.00.099.251 I print_info: n_expert         = 0
0.00.099.252 I print_info: n_expert_used    = 0
0.00.099.252 I print_info: causal attn      = 1
0.00.099.252 I print_info: pooling type     = 0
0.00.099.252 I print_info: rope type        = 2
0.00.099.252 I print_info: rope scaling     = linear
0.00.099.253 I print_info: freq_base_train  = 10000.0
0.00.099.253 I print_info: freq_scale_train = 1
0.00.099.253 I print_info: n_ctx_orig_yarn  = 2048
0.00.099.253 I print_info: rope_finetuned   = unknown
0.00.099.254 I print_info: ssm_d_conv       = 0
0.00.099.254 I print_info: ssm_d_inner      = 0
0.00.099.254 I print_info: ssm_d_state      = 0
0.00.099.254 I print_info: ssm_dt_rank      = 0
0.00.099.254 I print_info: ssm_dt_b_c_rms   = 0
0.00.099.258 I print_info: model type       = 1.4B
0.00.099.258 I print_info: model params     = 1.41 B
0.00.099.259 I print_info: general.name     = 1.4B
0.00.099.259 I print_info: vocab type       = BPE
0.00.099.259 I print_info: n_vocab          = 50304
0.00.099.260 I print_info: n_merges         = 50009
0.00.099.260 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.099.260 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.099.261 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.099.261 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.099.261 I print_info: LF token         = 128 'Ä'
0.00.099.261 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.099.262 I print_info: max token length = 1024
0.00.101.880 I load_tensors: offloading 24 repeating layers to GPU
0.00.101.880 I load_tensors: offloading output layer to GPU
0.00.101.880 I load_tensors: offloaded 25/25 layers to GPU
0.00.101.898 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.101.900 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.102.214 I llama_init_from_model: n_seq_max     = 1
0.00.102.215 I llama_init_from_model: n_ctx         = 2048
0.00.102.215 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.102.215 I llama_init_from_model: n_batch       = 2048
0.00.102.216 I llama_init_from_model: n_ubatch      = 512
0.00.102.216 I llama_init_from_model: flash_attn    = 0
0.00.102.216 I llama_init_from_model: freq_base     = 10000.0
0.00.102.216 I llama_init_from_model: freq_scale    = 1
0.00.102.217 I ggml_metal_init: allocating
0.00.102.220 I ggml_metal_init: found device: Apple M4
0.00.102.222 I ggml_metal_init: picking default device: Apple M4
0.00.102.912 I ggml_metal_init: using embedded metal library
0.00.106.601 I ggml_metal_init: GPU name:   Apple M4
0.00.106.603 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.106.603 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.106.604 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.106.604 I ggml_metal_init: simdgroup reduction   = true
0.00.106.604 I ggml_metal_init: simdgroup matrix mul. = true
0.00.106.604 I ggml_metal_init: has bfloat            = true
0.00.106.604 I ggml_metal_init: use bfloat            = true
0.00.106.605 I ggml_metal_init: hasUnifiedMemory      = true
0.00.106.605 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.130.658 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.151.233 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.151.241 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.151.265 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.152.156 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.152.158 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.152.158 I llama_init_from_model: graph nodes  = 967
0.00.152.159 I llama_init_from_model: graph splits = 2
0.00.152.162 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.152.272 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.152.272 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.232.607 I main: llama threadpool init, n_threads = 4
0.00.232.648 I 
0.00.232.672 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.232.672 I 
0.00.232.740 I sampler seed: 1234
0.00.232.744 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.232.770 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.232.771 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.232.771 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.073.577 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55081.46 tokens per second)
0.02.073.578 I llama_perf_context_print:        load time =     201.69 ms
0.02.073.579 I llama_perf_context_print: prompt eval time =      43.48 ms /     7 tokens (    6.21 ms per token,   160.99 tokens per second)
0.02.073.579 I llama_perf_context_print:        eval time =    1794.31 ms /    63 runs   (   28.48 ms per token,    35.11 tokens per second)
0.02.073.580 I llama_perf_context_print:       total time =    1840.97 ms /    70 tokens
0.02.073.797 I ggml_metal_free: deallocating

real	0m2.365s
user	0m0.144s
sys	0m0.101s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.058 I build: 4479 (a4f3f5d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.010.074 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.029.173 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.029.179 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.182 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.182 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.183 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.183 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.183 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.184 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.185 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.185 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.185 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.186 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.186 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.188 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.189 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.189 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.190 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.016 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.042 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.844 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.845 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.846 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.846 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.846 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.847 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.037.847 I llama_model_loader: - type  f32:  194 tensors
0.00.037.848 I llama_model_loader: - type q8_0:   98 tensors
0.00.037.849 I print_info: file format = GGUF V3 (latest)
0.00.037.849 I print_info: file type   = Q8_0
0.00.037.850 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.060.974 I load: special tokens cache size = 25
0.00.068.364 I load: token to piece cache size = 0.2984 MB
0.00.068.368 I print_info: arch             = gptneox
0.00.068.369 I print_info: vocab_only       = 0
0.00.068.369 I print_info: n_ctx_train      = 2048
0.00.068.369 I print_info: n_embd           = 2048
0.00.068.369 I print_info: n_layer          = 24
0.00.068.375 I print_info: n_head           = 16
0.00.068.376 I print_info: n_head_kv        = 16
0.00.068.376 I print_info: n_rot            = 32
0.00.068.376 I print_info: n_swa            = 0
0.00.068.376 I print_info: n_embd_head_k    = 128
0.00.068.376 I print_info: n_embd_head_v    = 128
0.00.068.377 I print_info: n_gqa            = 1
0.00.068.378 I print_info: n_embd_k_gqa     = 2048
0.00.068.378 I print_info: n_embd_v_gqa     = 2048
0.00.068.379 I print_info: f_norm_eps       = 1.0e-05
0.00.068.379 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.068.379 I print_info: f_clamp_kqv      = 0.0e+00
0.00.068.379 I print_info: f_max_alibi_bias = 0.0e+00
0.00.068.380 I print_info: f_logit_scale    = 0.0e+00
0.00.068.380 I print_info: n_ff             = 8192
0.00.068.380 I print_info: n_expert         = 0
0.00.068.381 I print_info: n_expert_used    = 0
0.00.068.381 I print_info: causal attn      = 1
0.00.068.381 I print_info: pooling type     = 0
0.00.068.383 I print_info: rope type        = 2
0.00.068.384 I print_info: rope scaling     = linear
0.00.068.384 I print_info: freq_base_train  = 10000.0
0.00.068.384 I print_info: freq_scale_train = 1
0.00.068.384 I print_info: n_ctx_orig_yarn  = 2048
0.00.068.385 I print_info: rope_finetuned   = unknown
0.00.068.385 I print_info: ssm_d_conv       = 0
0.00.068.385 I print_info: ssm_d_inner      = 0
0.00.068.385 I print_info: ssm_d_state      = 0
0.00.068.385 I print_info: ssm_dt_rank      = 0
0.00.068.385 I print_info: ssm_dt_b_c_rms   = 0
0.00.068.385 I print_info: model type       = 1.4B
0.00.068.386 I print_info: model params     = 1.41 B
0.00.068.386 I print_info: general.name     = 1.4B
0.00.068.387 I print_info: vocab type       = BPE
0.00.068.387 I print_info: n_vocab          = 50304
0.00.068.387 I print_info: n_merges         = 50009
0.00.068.387 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.068.387 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.068.388 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.068.388 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.068.388 I print_info: LF token         = 128 'Ä'
0.00.068.388 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.068.389 I print_info: max token length = 1024
0.00.070.946 I load_tensors: offloading 24 repeating layers to GPU
0.00.070.946 I load_tensors: offloading output layer to GPU
0.00.070.946 I load_tensors: offloaded 25/25 layers to GPU
0.00.070.958 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.070.959 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.071.324 I llama_init_from_model: n_seq_max     = 1
0.00.071.325 I llama_init_from_model: n_ctx         = 2048
0.00.071.325 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.071.325 I llama_init_from_model: n_batch       = 2048
0.00.071.326 I llama_init_from_model: n_ubatch      = 512
0.00.071.326 I llama_init_from_model: flash_attn    = 0
0.00.071.326 I llama_init_from_model: freq_base     = 10000.0
0.00.071.326 I llama_init_from_model: freq_scale    = 1
0.00.071.327 I ggml_metal_init: allocating
0.00.071.330 I ggml_metal_init: found device: Apple M4
0.00.071.333 I ggml_metal_init: picking default device: Apple M4
0.00.072.161 I ggml_metal_init: using embedded metal library
0.00.075.152 I ggml_metal_init: GPU name:   Apple M4
0.00.075.154 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.075.154 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.075.155 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.075.155 I ggml_metal_init: simdgroup reduction   = true
0.00.075.155 I ggml_metal_init: simdgroup matrix mul. = true
0.00.075.155 I ggml_metal_init: has bfloat            = true
0.00.075.155 I ggml_metal_init: use bfloat            = true
0.00.075.156 I ggml_metal_init: hasUnifiedMemory      = true
0.00.075.157 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.434 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.112.799 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.112.807 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.112.835 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.114.156 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.114.158 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.114.159 I llama_init_from_model: graph nodes  = 967
0.00.114.159 I llama_init_from_model: graph splits = 2
0.00.114.163 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.114.292 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.114.293 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.378.964 I main: llama threadpool init, n_threads = 4
0.01.379.055 I 
0.01.379.104 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.379.105 I 
0.01.379.518 I sampler seed: 1234
0.01.379.528 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.379.556 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.379.558 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.379.559 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.477.939 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52631.58 tokens per second)
0.02.477.940 I llama_perf_context_print:        load time =    1368.88 ms
0.02.477.941 I llama_perf_context_print: prompt eval time =      50.27 ms /     7 tokens (    7.18 ms per token,   139.25 tokens per second)
0.02.477.941 I llama_perf_context_print:        eval time =    1045.02 ms /    63 runs   (   16.59 ms per token,    60.29 tokens per second)
0.02.477.942 I llama_perf_context_print:       total time =    1098.98 ms /    70 tokens
0.02.478.184 I ggml_metal_free: deallocating

real	0m2.496s
user	0m0.127s
sys	0m0.254s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4479 (a4f3f5d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.023.211 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.032.296 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.032.302 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.310 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.310 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.310 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.311 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.311 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.314 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.314 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.314 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.315 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.315 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.316 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.316 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.320 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.322 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.322 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.036.312 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.037.482 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.041.656 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.041.657 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.041.657 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.041.657 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.041.658 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.041.658 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.041.658 I llama_model_loader: - type  f32:  194 tensors
0.00.041.659 I llama_model_loader: - type q4_0:   97 tensors
0.00.041.659 I llama_model_loader: - type q6_K:    1 tensors
0.00.041.659 I print_info: file format = GGUF V3 (latest)
0.00.041.660 I print_info: file type   = Q4_0
0.00.041.661 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.064.970 I load: special tokens cache size = 25
0.00.073.356 I load: token to piece cache size = 0.2984 MB
0.00.073.360 I print_info: arch             = gptneox
0.00.073.361 I print_info: vocab_only       = 0
0.00.073.361 I print_info: n_ctx_train      = 2048
0.00.073.361 I print_info: n_embd           = 2048
0.00.073.362 I print_info: n_layer          = 24
0.00.073.366 I print_info: n_head           = 16
0.00.073.368 I print_info: n_head_kv        = 16
0.00.073.368 I print_info: n_rot            = 32
0.00.073.368 I print_info: n_swa            = 0
0.00.073.368 I print_info: n_embd_head_k    = 128
0.00.073.368 I print_info: n_embd_head_v    = 128
0.00.073.369 I print_info: n_gqa            = 1
0.00.073.370 I print_info: n_embd_k_gqa     = 2048
0.00.073.371 I print_info: n_embd_v_gqa     = 2048
0.00.073.372 I print_info: f_norm_eps       = 1.0e-05
0.00.073.374 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.073.375 I print_info: f_clamp_kqv      = 0.0e+00
0.00.073.375 I print_info: f_max_alibi_bias = 0.0e+00
0.00.073.375 I print_info: f_logit_scale    = 0.0e+00
0.00.073.376 I print_info: n_ff             = 8192
0.00.073.376 I print_info: n_expert         = 0
0.00.073.376 I print_info: n_expert_used    = 0
0.00.073.377 I print_info: causal attn      = 1
0.00.073.377 I print_info: pooling type     = 0
0.00.073.377 I print_info: rope type        = 2
0.00.073.377 I print_info: rope scaling     = linear
0.00.073.378 I print_info: freq_base_train  = 10000.0
0.00.073.378 I print_info: freq_scale_train = 1
0.00.073.378 I print_info: n_ctx_orig_yarn  = 2048
0.00.073.379 I print_info: rope_finetuned   = unknown
0.00.073.379 I print_info: ssm_d_conv       = 0
0.00.073.379 I print_info: ssm_d_inner      = 0
0.00.073.379 I print_info: ssm_d_state      = 0
0.00.073.379 I print_info: ssm_dt_rank      = 0
0.00.073.379 I print_info: ssm_dt_b_c_rms   = 0
0.00.073.380 I print_info: model type       = 1.4B
0.00.073.380 I print_info: model params     = 1.41 B
0.00.073.381 I print_info: general.name     = 1.4B
0.00.073.386 I print_info: vocab type       = BPE
0.00.073.386 I print_info: n_vocab          = 50304
0.00.073.387 I print_info: n_merges         = 50009
0.00.073.387 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.073.387 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.073.387 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.073.388 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.073.388 I print_info: LF token         = 128 'Ä'
0.00.073.388 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.073.388 I print_info: max token length = 1024
0.00.076.244 I load_tensors: offloading 24 repeating layers to GPU
0.00.076.245 I load_tensors: offloading output layer to GPU
0.00.076.245 I load_tensors: offloaded 25/25 layers to GPU
0.00.076.257 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.076.258 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.076.683 I llama_init_from_model: n_seq_max     = 1
0.00.076.684 I llama_init_from_model: n_ctx         = 2048
0.00.076.684 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.076.684 I llama_init_from_model: n_batch       = 2048
0.00.076.685 I llama_init_from_model: n_ubatch      = 512
0.00.076.685 I llama_init_from_model: flash_attn    = 0
0.00.076.685 I llama_init_from_model: freq_base     = 10000.0
0.00.076.686 I llama_init_from_model: freq_scale    = 1
0.00.076.686 I ggml_metal_init: allocating
0.00.076.690 I ggml_metal_init: found device: Apple M4
0.00.076.693 I ggml_metal_init: picking default device: Apple M4
0.00.077.669 I ggml_metal_init: using embedded metal library
0.00.081.549 I ggml_metal_init: GPU name:   Apple M4
0.00.081.552 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.081.552 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.081.553 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.081.553 I ggml_metal_init: simdgroup reduction   = true
0.00.081.553 I ggml_metal_init: simdgroup matrix mul. = true
0.00.081.554 I ggml_metal_init: has bfloat            = true
0.00.081.554 I ggml_metal_init: use bfloat            = true
0.00.081.554 I ggml_metal_init: hasUnifiedMemory      = true
0.00.081.555 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.094.549 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.119.357 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.119.371 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.119.398 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.120.496 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.120.499 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.120.499 I llama_init_from_model: graph nodes  = 967
0.00.120.500 I llama_init_from_model: graph splits = 2
0.00.120.504 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.120.632 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.120.633 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.913.704 I main: llama threadpool init, n_threads = 4
0.00.913.748 I 
0.00.913.773 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.913.775 I 
0.00.914.003 I sampler seed: 1234
0.00.914.008 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.914.019 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.914.020 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.914.020 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.589.484 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55729.98 tokens per second)
0.01.589.485 I llama_perf_context_print:        load time =     890.49 ms
0.01.589.485 I llama_perf_context_print: prompt eval time =      44.14 ms /     7 tokens (    6.31 ms per token,   158.59 tokens per second)
0.01.589.486 I llama_perf_context_print:        eval time =     628.27 ms /    63 runs   (    9.97 ms per token,   100.28 tokens per second)
0.01.589.488 I llama_perf_context_print:       total time =     675.78 ms /    70 tokens
0.01.589.689 I ggml_metal_free: deallocating

real	0m1.609s
user	0m0.125s
sys	0m0.170s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4479 (a4f3f5d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.917 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.881 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.886 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.887 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.892 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.893 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.893 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.894 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.894 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.895 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.895 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.895 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.899 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.899 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.900 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.901 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.901 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.902 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.732 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.792 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.589 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.591 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.591 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.591 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.591 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.592 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.592 I llama_model_loader: - type  f32:  194 tensors
0.00.026.593 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.593 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.593 I print_info: file format = GGUF V3 (latest)
0.00.026.594 I print_info: file type   = Q4_1
0.00.026.595 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.046.408 I load: special tokens cache size = 25
0.00.052.321 I load: token to piece cache size = 0.2984 MB
0.00.052.324 I print_info: arch             = gptneox
0.00.052.324 I print_info: vocab_only       = 0
0.00.052.324 I print_info: n_ctx_train      = 2048
0.00.052.324 I print_info: n_embd           = 2048
0.00.052.325 I print_info: n_layer          = 24
0.00.052.327 I print_info: n_head           = 16
0.00.052.328 I print_info: n_head_kv        = 16
0.00.052.330 I print_info: n_rot            = 32
0.00.052.330 I print_info: n_swa            = 0
0.00.052.330 I print_info: n_embd_head_k    = 128
0.00.052.330 I print_info: n_embd_head_v    = 128
0.00.052.331 I print_info: n_gqa            = 1
0.00.052.332 I print_info: n_embd_k_gqa     = 2048
0.00.052.338 I print_info: n_embd_v_gqa     = 2048
0.00.052.338 I print_info: f_norm_eps       = 1.0e-05
0.00.052.340 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.341 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.341 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.341 I print_info: f_logit_scale    = 0.0e+00
0.00.052.342 I print_info: n_ff             = 8192
0.00.052.342 I print_info: n_expert         = 0
0.00.052.342 I print_info: n_expert_used    = 0
0.00.052.342 I print_info: causal attn      = 1
0.00.052.343 I print_info: pooling type     = 0
0.00.052.343 I print_info: rope type        = 2
0.00.052.343 I print_info: rope scaling     = linear
0.00.052.343 I print_info: freq_base_train  = 10000.0
0.00.052.344 I print_info: freq_scale_train = 1
0.00.052.344 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.344 I print_info: rope_finetuned   = unknown
0.00.052.344 I print_info: ssm_d_conv       = 0
0.00.052.345 I print_info: ssm_d_inner      = 0
0.00.052.345 I print_info: ssm_d_state      = 0
0.00.052.345 I print_info: ssm_dt_rank      = 0
0.00.052.345 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.345 I print_info: model type       = 1.4B
0.00.052.346 I print_info: model params     = 1.41 B
0.00.052.351 I print_info: general.name     = 1.4B
0.00.052.353 I print_info: vocab type       = BPE
0.00.052.353 I print_info: n_vocab          = 50304
0.00.052.354 I print_info: n_merges         = 50009
0.00.052.355 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.355 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.355 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.355 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.355 I print_info: LF token         = 128 'Ä'
0.00.052.356 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.356 I print_info: max token length = 1024
0.00.054.416 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.416 I load_tensors: offloading output layer to GPU
0.00.054.416 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.427 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.054.429 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.054.711 I llama_init_from_model: n_seq_max     = 1
0.00.054.711 I llama_init_from_model: n_ctx         = 2048
0.00.054.712 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.712 I llama_init_from_model: n_batch       = 2048
0.00.054.712 I llama_init_from_model: n_ubatch      = 512
0.00.054.712 I llama_init_from_model: flash_attn    = 0
0.00.054.713 I llama_init_from_model: freq_base     = 10000.0
0.00.054.713 I llama_init_from_model: freq_scale    = 1
0.00.054.713 I ggml_metal_init: allocating
0.00.054.716 I ggml_metal_init: found device: Apple M4
0.00.054.718 I ggml_metal_init: picking default device: Apple M4
0.00.055.306 I ggml_metal_init: using embedded metal library
0.00.057.662 I ggml_metal_init: GPU name:   Apple M4
0.00.057.664 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.664 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.665 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.665 I ggml_metal_init: simdgroup reduction   = true
0.00.057.665 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.665 I ggml_metal_init: has bfloat            = true
0.00.057.665 I ggml_metal_init: use bfloat            = true
0.00.057.666 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.666 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.769 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.239 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.244 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.270 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.089.386 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.089.387 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.089.388 I llama_init_from_model: graph nodes  = 967
0.00.089.388 I llama_init_from_model: graph splits = 2
0.00.089.390 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.530 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.530 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.734.753 I main: llama threadpool init, n_threads = 4
0.00.734.820 I 
0.00.734.842 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.734.844 I 
0.00.735.065 I sampler seed: 1234
0.00.735.070 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.735.082 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.735.082 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.735.082 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.459.212 I llama_perf_sampler_print:    sampling time =       1.05 ms /    71 runs   (    0.01 ms per token, 67426.40 tokens per second)
0.01.459.213 I llama_perf_context_print:        load time =     724.83 ms
0.01.459.214 I llama_perf_context_print: prompt eval time =      39.56 ms /     7 tokens (    5.65 ms per token,   176.96 tokens per second)
0.01.459.214 I llama_perf_context_print:        eval time =     681.69 ms /    63 runs   (   10.82 ms per token,    92.42 tokens per second)
0.01.459.215 I llama_perf_context_print:       total time =     724.46 ms /    70 tokens
0.01.459.447 I ggml_metal_free: deallocating

real	0m1.476s
user	0m0.111s
sys	0m0.154s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4479 (a4f3f5d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.010.415 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.574 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.579 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.581 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.583 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.583 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.583 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.584 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.585 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.586 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.586 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.586 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.587 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.587 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.588 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.590 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.591 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.591 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.404 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.416 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.211 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.212 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.212 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.213 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.213 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.213 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.214 I llama_model_loader: - type  f32:  194 tensors
0.00.027.214 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.214 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.215 I print_info: file format = GGUF V3 (latest)
0.00.027.215 I print_info: file type   = Q5_0
0.00.027.216 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.046.342 I load: special tokens cache size = 25
0.00.052.266 I load: token to piece cache size = 0.2984 MB
0.00.052.269 I print_info: arch             = gptneox
0.00.052.269 I print_info: vocab_only       = 0
0.00.052.269 I print_info: n_ctx_train      = 2048
0.00.052.269 I print_info: n_embd           = 2048
0.00.052.270 I print_info: n_layer          = 24
0.00.052.273 I print_info: n_head           = 16
0.00.052.273 I print_info: n_head_kv        = 16
0.00.052.274 I print_info: n_rot            = 32
0.00.052.274 I print_info: n_swa            = 0
0.00.052.274 I print_info: n_embd_head_k    = 128
0.00.052.274 I print_info: n_embd_head_v    = 128
0.00.052.275 I print_info: n_gqa            = 1
0.00.052.276 I print_info: n_embd_k_gqa     = 2048
0.00.052.276 I print_info: n_embd_v_gqa     = 2048
0.00.052.277 I print_info: f_norm_eps       = 1.0e-05
0.00.052.277 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.278 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.278 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.278 I print_info: f_logit_scale    = 0.0e+00
0.00.052.279 I print_info: n_ff             = 8192
0.00.052.279 I print_info: n_expert         = 0
0.00.052.279 I print_info: n_expert_used    = 0
0.00.052.280 I print_info: causal attn      = 1
0.00.052.282 I print_info: pooling type     = 0
0.00.052.282 I print_info: rope type        = 2
0.00.052.282 I print_info: rope scaling     = linear
0.00.052.282 I print_info: freq_base_train  = 10000.0
0.00.052.283 I print_info: freq_scale_train = 1
0.00.052.283 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.283 I print_info: rope_finetuned   = unknown
0.00.052.285 I print_info: ssm_d_conv       = 0
0.00.052.285 I print_info: ssm_d_inner      = 0
0.00.052.285 I print_info: ssm_d_state      = 0
0.00.052.285 I print_info: ssm_dt_rank      = 0
0.00.052.285 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.285 I print_info: model type       = 1.4B
0.00.052.286 I print_info: model params     = 1.41 B
0.00.052.286 I print_info: general.name     = 1.4B
0.00.052.287 I print_info: vocab type       = BPE
0.00.052.287 I print_info: n_vocab          = 50304
0.00.052.287 I print_info: n_merges         = 50009
0.00.052.287 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.287 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.288 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.288 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.292 I print_info: LF token         = 128 'Ä'
0.00.052.293 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.293 I print_info: max token length = 1024
0.00.053.932 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.932 I load_tensors: offloading output layer to GPU
0.00.053.932 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.942 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.943 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.054.232 I llama_init_from_model: n_seq_max     = 1
0.00.054.233 I llama_init_from_model: n_ctx         = 2048
0.00.054.233 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.233 I llama_init_from_model: n_batch       = 2048
0.00.054.233 I llama_init_from_model: n_ubatch      = 512
0.00.054.234 I llama_init_from_model: flash_attn    = 0
0.00.054.234 I llama_init_from_model: freq_base     = 10000.0
0.00.054.234 I llama_init_from_model: freq_scale    = 1
0.00.054.235 I ggml_metal_init: allocating
0.00.054.238 I ggml_metal_init: found device: Apple M4
0.00.054.240 I ggml_metal_init: picking default device: Apple M4
0.00.054.851 I ggml_metal_init: using embedded metal library
0.00.057.204 I ggml_metal_init: GPU name:   Apple M4
0.00.057.205 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.205 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.206 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.206 I ggml_metal_init: simdgroup reduction   = true
0.00.057.206 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.206 I ggml_metal_init: has bfloat            = true
0.00.057.207 I ggml_metal_init: use bfloat            = true
0.00.057.207 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.208 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.173 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.926 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.940 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.967 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.088.058 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.088.060 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.088.061 I llama_init_from_model: graph nodes  = 967
0.00.088.061 I llama_init_from_model: graph splits = 2
0.00.088.064 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.209 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.210 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.748.820 I main: llama threadpool init, n_threads = 4
0.00.748.863 I 
0.00.748.884 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.748.885 I 
0.00.749.050 I sampler seed: 1234
0.00.749.055 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.749.105 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.749.109 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.749.109 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.576.096 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59613.77 tokens per second)
0.01.576.097 I llama_perf_context_print:        load time =     738.40 ms
0.01.576.098 I llama_perf_context_print: prompt eval time =      43.23 ms /     7 tokens (    6.18 ms per token,   161.91 tokens per second)
0.01.576.098 I llama_perf_context_print:        eval time =     780.79 ms /    63 runs   (   12.39 ms per token,    80.69 tokens per second)
0.01.576.099 I llama_perf_context_print:       total time =     827.28 ms /    70 tokens
0.01.576.348 I ggml_metal_free: deallocating

real	0m1.593s
user	0m0.110s
sys	0m0.149s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4479 (a4f3f5d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.010.113 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.804 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.808 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.810 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.810 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.813 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.813 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.814 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.816 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.816 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.817 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.817 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.817 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.818 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.818 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.820 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.821 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.821 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.621 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.679 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.450 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.451 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.451 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.451 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.452 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.452 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.453 I llama_model_loader: - type  f32:  194 tensors
0.00.026.453 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.453 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.454 I print_info: file format = GGUF V3 (latest)
0.00.026.454 I print_info: file type   = Q5_1
0.00.026.455 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.045.486 I load: special tokens cache size = 25
0.00.051.658 I load: token to piece cache size = 0.2984 MB
0.00.051.661 I print_info: arch             = gptneox
0.00.051.661 I print_info: vocab_only       = 0
0.00.051.661 I print_info: n_ctx_train      = 2048
0.00.051.662 I print_info: n_embd           = 2048
0.00.051.662 I print_info: n_layer          = 24
0.00.051.665 I print_info: n_head           = 16
0.00.051.666 I print_info: n_head_kv        = 16
0.00.051.666 I print_info: n_rot            = 32
0.00.051.666 I print_info: n_swa            = 0
0.00.051.667 I print_info: n_embd_head_k    = 128
0.00.051.667 I print_info: n_embd_head_v    = 128
0.00.051.668 I print_info: n_gqa            = 1
0.00.051.669 I print_info: n_embd_k_gqa     = 2048
0.00.051.669 I print_info: n_embd_v_gqa     = 2048
0.00.051.670 I print_info: f_norm_eps       = 1.0e-05
0.00.051.670 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.670 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.671 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.671 I print_info: f_logit_scale    = 0.0e+00
0.00.051.671 I print_info: n_ff             = 8192
0.00.051.672 I print_info: n_expert         = 0
0.00.051.672 I print_info: n_expert_used    = 0
0.00.051.673 I print_info: causal attn      = 1
0.00.051.674 I print_info: pooling type     = 0
0.00.051.674 I print_info: rope type        = 2
0.00.051.674 I print_info: rope scaling     = linear
0.00.051.675 I print_info: freq_base_train  = 10000.0
0.00.051.675 I print_info: freq_scale_train = 1
0.00.051.675 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.675 I print_info: rope_finetuned   = unknown
0.00.051.676 I print_info: ssm_d_conv       = 0
0.00.051.676 I print_info: ssm_d_inner      = 0
0.00.051.676 I print_info: ssm_d_state      = 0
0.00.051.676 I print_info: ssm_dt_rank      = 0
0.00.051.676 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.676 I print_info: model type       = 1.4B
0.00.051.677 I print_info: model params     = 1.41 B
0.00.051.677 I print_info: general.name     = 1.4B
0.00.051.678 I print_info: vocab type       = BPE
0.00.051.678 I print_info: n_vocab          = 50304
0.00.051.678 I print_info: n_merges         = 50009
0.00.051.678 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.679 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.680 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.680 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.680 I print_info: LF token         = 128 'Ä'
0.00.051.680 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.681 I print_info: max token length = 1024
0.00.053.330 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.331 I load_tensors: offloading output layer to GPU
0.00.053.331 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.341 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.342 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.053.625 I llama_init_from_model: n_seq_max     = 1
0.00.053.625 I llama_init_from_model: n_ctx         = 2048
0.00.053.625 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.626 I llama_init_from_model: n_batch       = 2048
0.00.053.626 I llama_init_from_model: n_ubatch      = 512
0.00.053.626 I llama_init_from_model: flash_attn    = 0
0.00.053.626 I llama_init_from_model: freq_base     = 10000.0
0.00.053.627 I llama_init_from_model: freq_scale    = 1
0.00.053.627 I ggml_metal_init: allocating
0.00.053.630 I ggml_metal_init: found device: Apple M4
0.00.053.632 I ggml_metal_init: picking default device: Apple M4
0.00.054.238 I ggml_metal_init: using embedded metal library
0.00.056.598 I ggml_metal_init: GPU name:   Apple M4
0.00.056.600 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.600 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.600 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.601 I ggml_metal_init: simdgroup reduction   = true
0.00.056.601 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.601 I ggml_metal_init: has bfloat            = true
0.00.056.601 I ggml_metal_init: use bfloat            = true
0.00.056.601 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.602 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.615 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.091 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.101 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.122 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.088.254 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.088.256 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.088.256 I llama_init_from_model: graph nodes  = 967
0.00.088.256 I llama_init_from_model: graph splits = 2
0.00.088.260 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.406 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.407 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.704.636 I main: llama threadpool init, n_threads = 4
0.00.704.685 I 
0.00.704.709 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.704.709 I 
0.00.704.880 I sampler seed: 1234
0.00.704.885 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.704.895 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.704.897 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.704.898 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.585.227 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57583.13 tokens per second)
0.01.585.227 I llama_perf_context_print:        load time =     694.52 ms
0.01.585.228 I llama_perf_context_print: prompt eval time =      42.36 ms /     7 tokens (    6.05 ms per token,   165.23 tokens per second)
0.01.585.229 I llama_perf_context_print:        eval time =     834.88 ms /    63 runs   (   13.25 ms per token,    75.46 tokens per second)
0.01.585.229 I llama_perf_context_print:       total time =     880.59 ms /    70 tokens
0.01.585.492 I ggml_metal_free: deallocating

real	0m1.604s
user	0m0.111s
sys	0m0.158s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.060 I build: 4479 (a4f3f5d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.091 I main: llama backend init
0.00.000.094 I main: load the model and apply lora adapter, if any
0.00.010.543 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.530 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.536 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.537 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.538 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.538 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.538 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.539 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.540 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.540 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.540 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.541 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.541 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.542 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.542 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.544 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.544 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.545 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.336 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.340 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.082 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.083 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.083 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.084 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.084 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.084 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.085 I llama_model_loader: - type  f32:  194 tensors
0.00.026.085 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.085 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.085 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.086 I print_info: file format = GGUF V3 (latest)
0.00.026.086 I print_info: file type   = Q2_K - Medium
0.00.026.089 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.045.197 I load: special tokens cache size = 25
0.00.050.926 I load: token to piece cache size = 0.2984 MB
0.00.050.929 I print_info: arch             = gptneox
0.00.050.929 I print_info: vocab_only       = 0
0.00.050.929 I print_info: n_ctx_train      = 2048
0.00.050.929 I print_info: n_embd           = 2048
0.00.050.930 I print_info: n_layer          = 24
0.00.050.933 I print_info: n_head           = 16
0.00.050.934 I print_info: n_head_kv        = 16
0.00.050.934 I print_info: n_rot            = 32
0.00.050.934 I print_info: n_swa            = 0
0.00.050.934 I print_info: n_embd_head_k    = 128
0.00.050.934 I print_info: n_embd_head_v    = 128
0.00.050.937 I print_info: n_gqa            = 1
0.00.050.939 I print_info: n_embd_k_gqa     = 2048
0.00.050.939 I print_info: n_embd_v_gqa     = 2048
0.00.050.940 I print_info: f_norm_eps       = 1.0e-05
0.00.050.940 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.941 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.942 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.942 I print_info: f_logit_scale    = 0.0e+00
0.00.050.943 I print_info: n_ff             = 8192
0.00.050.943 I print_info: n_expert         = 0
0.00.050.945 I print_info: n_expert_used    = 0
0.00.050.945 I print_info: causal attn      = 1
0.00.050.945 I print_info: pooling type     = 0
0.00.050.945 I print_info: rope type        = 2
0.00.050.946 I print_info: rope scaling     = linear
0.00.050.946 I print_info: freq_base_train  = 10000.0
0.00.050.946 I print_info: freq_scale_train = 1
0.00.050.946 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.947 I print_info: rope_finetuned   = unknown
0.00.050.947 I print_info: ssm_d_conv       = 0
0.00.050.947 I print_info: ssm_d_inner      = 0
0.00.050.947 I print_info: ssm_d_state      = 0
0.00.050.947 I print_info: ssm_dt_rank      = 0
0.00.050.951 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.952 I print_info: model type       = 1.4B
0.00.050.952 I print_info: model params     = 1.41 B
0.00.050.952 I print_info: general.name     = 1.4B
0.00.050.953 I print_info: vocab type       = BPE
0.00.050.953 I print_info: n_vocab          = 50304
0.00.050.953 I print_info: n_merges         = 50009
0.00.050.955 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.955 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.955 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.955 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.955 I print_info: LF token         = 128 'Ä'
0.00.050.956 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.956 I print_info: max token length = 1024
0.00.052.583 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.583 I load_tensors: offloading output layer to GPU
0.00.052.583 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.593 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.595 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.052.867 I llama_init_from_model: n_seq_max     = 1
0.00.052.867 I llama_init_from_model: n_ctx         = 2048
0.00.052.868 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.868 I llama_init_from_model: n_batch       = 2048
0.00.052.868 I llama_init_from_model: n_ubatch      = 512
0.00.052.868 I llama_init_from_model: flash_attn    = 0
0.00.052.869 I llama_init_from_model: freq_base     = 10000.0
0.00.052.869 I llama_init_from_model: freq_scale    = 1
0.00.052.869 I ggml_metal_init: allocating
0.00.052.873 I ggml_metal_init: found device: Apple M4
0.00.052.875 I ggml_metal_init: picking default device: Apple M4
0.00.053.470 I ggml_metal_init: using embedded metal library
0.00.055.848 I ggml_metal_init: GPU name:   Apple M4
0.00.055.850 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.850 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.851 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.851 I ggml_metal_init: simdgroup reduction   = true
0.00.055.851 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.851 I ggml_metal_init: has bfloat            = true
0.00.055.851 I ggml_metal_init: use bfloat            = true
0.00.055.852 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.853 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.865 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.257 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.270 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.291 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.282 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.283 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.284 I llama_init_from_model: graph nodes  = 967
0.00.086.284 I llama_init_from_model: graph splits = 2
0.00.086.287 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.421 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.422 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.439.675 I main: llama threadpool init, n_threads = 4
0.00.439.722 I 
0.00.439.751 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.439.751 I 
0.00.439.917 I sampler seed: 1234
0.00.439.921 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.439.932 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.439.934 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.439.934 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.120.698 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61259.71 tokens per second)
0.01.120.698 I llama_perf_context_print:        load time =     429.13 ms
0.01.120.699 I llama_perf_context_print: prompt eval time =      35.88 ms /     7 tokens (    5.13 ms per token,   195.08 tokens per second)
0.01.120.700 I llama_perf_context_print:        eval time =     641.90 ms /    63 runs   (   10.19 ms per token,    98.15 tokens per second)
0.01.120.700 I llama_perf_context_print:       total time =     681.03 ms /    70 tokens
0.01.120.912 I ggml_metal_free: deallocating

real	0m1.141s
user	0m0.110s
sys	0m0.102s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4479 (a4f3f5d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.462 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.136 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.141 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.143 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.143 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.144 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.144 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.144 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.148 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.148 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.149 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.149 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.149 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.154 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.154 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.159 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.160 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.160 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.997 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.001 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.753 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.754 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.754 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.754 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.755 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.755 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.756 I llama_model_loader: - type  f32:  194 tensors
0.00.025.756 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.756 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.756 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.757 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.757 I print_info: file format = GGUF V3 (latest)
0.00.025.758 I print_info: file type   = Q3_K - Medium
0.00.025.759 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.044.942 I load: special tokens cache size = 25
0.00.051.014 I load: token to piece cache size = 0.2984 MB
0.00.051.017 I print_info: arch             = gptneox
0.00.051.017 I print_info: vocab_only       = 0
0.00.051.017 I print_info: n_ctx_train      = 2048
0.00.051.018 I print_info: n_embd           = 2048
0.00.051.018 I print_info: n_layer          = 24
0.00.051.020 I print_info: n_head           = 16
0.00.051.021 I print_info: n_head_kv        = 16
0.00.051.021 I print_info: n_rot            = 32
0.00.051.021 I print_info: n_swa            = 0
0.00.051.021 I print_info: n_embd_head_k    = 128
0.00.051.022 I print_info: n_embd_head_v    = 128
0.00.051.022 I print_info: n_gqa            = 1
0.00.051.023 I print_info: n_embd_k_gqa     = 2048
0.00.051.026 I print_info: n_embd_v_gqa     = 2048
0.00.051.027 I print_info: f_norm_eps       = 1.0e-05
0.00.051.027 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.028 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.028 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.028 I print_info: f_logit_scale    = 0.0e+00
0.00.051.029 I print_info: n_ff             = 8192
0.00.051.030 I print_info: n_expert         = 0
0.00.051.031 I print_info: n_expert_used    = 0
0.00.051.032 I print_info: causal attn      = 1
0.00.051.032 I print_info: pooling type     = 0
0.00.051.032 I print_info: rope type        = 2
0.00.051.032 I print_info: rope scaling     = linear
0.00.051.033 I print_info: freq_base_train  = 10000.0
0.00.051.033 I print_info: freq_scale_train = 1
0.00.051.033 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.033 I print_info: rope_finetuned   = unknown
0.00.051.033 I print_info: ssm_d_conv       = 0
0.00.051.034 I print_info: ssm_d_inner      = 0
0.00.051.034 I print_info: ssm_d_state      = 0
0.00.051.034 I print_info: ssm_dt_rank      = 0
0.00.051.034 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.034 I print_info: model type       = 1.4B
0.00.051.035 I print_info: model params     = 1.41 B
0.00.051.035 I print_info: general.name     = 1.4B
0.00.051.035 I print_info: vocab type       = BPE
0.00.051.035 I print_info: n_vocab          = 50304
0.00.051.036 I print_info: n_merges         = 50009
0.00.051.036 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.037 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.038 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.038 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.039 I print_info: LF token         = 128 'Ä'
0.00.051.039 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.039 I print_info: max token length = 1024
0.00.052.994 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.994 I load_tensors: offloading output layer to GPU
0.00.052.994 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.005 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.006 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.053.286 I llama_init_from_model: n_seq_max     = 1
0.00.053.287 I llama_init_from_model: n_ctx         = 2048
0.00.053.287 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.287 I llama_init_from_model: n_batch       = 2048
0.00.053.288 I llama_init_from_model: n_ubatch      = 512
0.00.053.288 I llama_init_from_model: flash_attn    = 0
0.00.053.288 I llama_init_from_model: freq_base     = 10000.0
0.00.053.288 I llama_init_from_model: freq_scale    = 1
0.00.053.289 I ggml_metal_init: allocating
0.00.053.292 I ggml_metal_init: found device: Apple M4
0.00.053.294 I ggml_metal_init: picking default device: Apple M4
0.00.053.894 I ggml_metal_init: using embedded metal library
0.00.056.257 I ggml_metal_init: GPU name:   Apple M4
0.00.056.259 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.259 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.260 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.260 I ggml_metal_init: simdgroup reduction   = true
0.00.056.260 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.260 I ggml_metal_init: has bfloat            = true
0.00.056.261 I ggml_metal_init: use bfloat            = true
0.00.056.261 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.262 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.260 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.554 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.559 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.576 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.608 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.609 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.610 I llama_init_from_model: graph nodes  = 967
0.00.086.610 I llama_init_from_model: graph splits = 2
0.00.086.613 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.734 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.735 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.527.771 I main: llama threadpool init, n_threads = 4
0.00.527.820 I 
0.00.527.843 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.527.843 I 
0.00.528.071 I sampler seed: 1234
0.00.528.076 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.528.129 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.528.130 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.528.130 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.279.331 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60374.15 tokens per second)
0.01.279.332 I llama_perf_context_print:        load time =     518.31 ms
0.01.279.333 I llama_perf_context_print: prompt eval time =      44.52 ms /     7 tokens (    6.36 ms per token,   157.22 tokens per second)
0.01.279.334 I llama_perf_context_print:        eval time =     703.70 ms /    63 runs   (   11.17 ms per token,    89.53 tokens per second)
0.01.279.335 I llama_perf_context_print:       total time =     751.56 ms /    70 tokens
0.01.279.547 I ggml_metal_free: deallocating

real	0m1.297s
user	0m0.109s
sys	0m0.126s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4479 (a4f3f5d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.717 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.293 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.298 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.304 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.305 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.305 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.305 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.306 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.308 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.309 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.309 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.309 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.310 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.311 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.311 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.313 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.313 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.313 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.154 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.158 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.968 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.969 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.969 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.970 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.970 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.970 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.971 I llama_model_loader: - type  f32:  194 tensors
0.00.024.971 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.971 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.971 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.972 I print_info: file format = GGUF V3 (latest)
0.00.024.972 I print_info: file type   = Q4_K - Medium
0.00.024.973 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.075 I load: special tokens cache size = 25
0.00.050.052 I load: token to piece cache size = 0.2984 MB
0.00.050.055 I print_info: arch             = gptneox
0.00.050.055 I print_info: vocab_only       = 0
0.00.050.055 I print_info: n_ctx_train      = 2048
0.00.050.056 I print_info: n_embd           = 2048
0.00.050.056 I print_info: n_layer          = 24
0.00.050.059 I print_info: n_head           = 16
0.00.050.060 I print_info: n_head_kv        = 16
0.00.050.060 I print_info: n_rot            = 32
0.00.050.060 I print_info: n_swa            = 0
0.00.050.060 I print_info: n_embd_head_k    = 128
0.00.050.060 I print_info: n_embd_head_v    = 128
0.00.050.061 I print_info: n_gqa            = 1
0.00.050.062 I print_info: n_embd_k_gqa     = 2048
0.00.050.063 I print_info: n_embd_v_gqa     = 2048
0.00.050.063 I print_info: f_norm_eps       = 1.0e-05
0.00.050.063 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.064 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.064 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.064 I print_info: f_logit_scale    = 0.0e+00
0.00.050.065 I print_info: n_ff             = 8192
0.00.050.065 I print_info: n_expert         = 0
0.00.050.065 I print_info: n_expert_used    = 0
0.00.050.065 I print_info: causal attn      = 1
0.00.050.065 I print_info: pooling type     = 0
0.00.050.066 I print_info: rope type        = 2
0.00.050.067 I print_info: rope scaling     = linear
0.00.050.067 I print_info: freq_base_train  = 10000.0
0.00.050.067 I print_info: freq_scale_train = 1
0.00.050.067 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.068 I print_info: rope_finetuned   = unknown
0.00.050.068 I print_info: ssm_d_conv       = 0
0.00.050.068 I print_info: ssm_d_inner      = 0
0.00.050.068 I print_info: ssm_d_state      = 0
0.00.050.068 I print_info: ssm_dt_rank      = 0
0.00.050.068 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.069 I print_info: model type       = 1.4B
0.00.050.070 I print_info: model params     = 1.41 B
0.00.050.071 I print_info: general.name     = 1.4B
0.00.050.071 I print_info: vocab type       = BPE
0.00.050.071 I print_info: n_vocab          = 50304
0.00.050.072 I print_info: n_merges         = 50009
0.00.050.072 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.072 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.072 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.072 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.073 I print_info: LF token         = 128 'Ä'
0.00.050.073 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.074 I print_info: max token length = 1024
0.00.052.063 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.063 I load_tensors: offloading output layer to GPU
0.00.052.063 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.074 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.075 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.418 I llama_init_from_model: n_seq_max     = 1
0.00.052.418 I llama_init_from_model: n_ctx         = 2048
0.00.052.419 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.419 I llama_init_from_model: n_batch       = 2048
0.00.052.419 I llama_init_from_model: n_ubatch      = 512
0.00.052.419 I llama_init_from_model: flash_attn    = 0
0.00.052.419 I llama_init_from_model: freq_base     = 10000.0
0.00.052.420 I llama_init_from_model: freq_scale    = 1
0.00.052.420 I ggml_metal_init: allocating
0.00.052.423 I ggml_metal_init: found device: Apple M4
0.00.052.425 I ggml_metal_init: picking default device: Apple M4
0.00.053.009 I ggml_metal_init: using embedded metal library
0.00.055.390 I ggml_metal_init: GPU name:   Apple M4
0.00.055.392 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.392 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.393 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.393 I ggml_metal_init: simdgroup reduction   = true
0.00.055.393 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.393 I ggml_metal_init: has bfloat            = true
0.00.055.393 I ggml_metal_init: use bfloat            = true
0.00.055.394 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.394 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.227 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.715 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.722 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.742 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.742 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.744 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.744 I llama_init_from_model: graph nodes  = 967
0.00.085.745 I llama_init_from_model: graph splits = 2
0.00.085.747 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.876 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.876 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.601.556 I main: llama threadpool init, n_threads = 4
0.00.601.608 I 
0.00.601.633 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.601.635 I 
0.00.601.889 I sampler seed: 1234
0.00.601.895 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.601.942 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.601.945 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.601.945 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.362.366 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55993.69 tokens per second)
0.01.362.366 I llama_perf_context_print:        load time =     592.83 ms
0.01.362.367 I llama_perf_context_print: prompt eval time =      47.10 ms /     7 tokens (    6.73 ms per token,   148.62 tokens per second)
0.01.362.368 I llama_perf_context_print:        eval time =     710.19 ms /    63 runs   (   11.27 ms per token,    88.71 tokens per second)
0.01.362.368 I llama_perf_context_print:       total time =     760.82 ms /    70 tokens
0.01.362.631 I ggml_metal_free: deallocating

real	0m1.379s
user	0m0.109s
sys	0m0.132s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4479 (a4f3f5d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.779 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.464 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.469 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.474 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.475 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.475 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.476 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.476 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.479 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.479 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.480 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.480 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.480 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.481 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.481 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.484 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.484 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.484 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.249 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.248 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.945 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.946 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.946 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.947 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.947 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.947 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.948 I llama_model_loader: - type  f32:  194 tensors
0.00.025.948 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.948 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.949 I print_info: file format = GGUF V3 (latest)
0.00.025.949 I print_info: file type   = Q5_K - Medium
0.00.025.950 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.045.771 I load: special tokens cache size = 25
0.00.051.576 I load: token to piece cache size = 0.2984 MB
0.00.051.579 I print_info: arch             = gptneox
0.00.051.579 I print_info: vocab_only       = 0
0.00.051.579 I print_info: n_ctx_train      = 2048
0.00.051.580 I print_info: n_embd           = 2048
0.00.051.580 I print_info: n_layer          = 24
0.00.051.583 I print_info: n_head           = 16
0.00.051.583 I print_info: n_head_kv        = 16
0.00.051.583 I print_info: n_rot            = 32
0.00.051.584 I print_info: n_swa            = 0
0.00.051.585 I print_info: n_embd_head_k    = 128
0.00.051.586 I print_info: n_embd_head_v    = 128
0.00.051.587 I print_info: n_gqa            = 1
0.00.051.587 I print_info: n_embd_k_gqa     = 2048
0.00.051.588 I print_info: n_embd_v_gqa     = 2048
0.00.051.589 I print_info: f_norm_eps       = 1.0e-05
0.00.051.591 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.591 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.591 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.592 I print_info: f_logit_scale    = 0.0e+00
0.00.051.592 I print_info: n_ff             = 8192
0.00.051.592 I print_info: n_expert         = 0
0.00.051.593 I print_info: n_expert_used    = 0
0.00.051.593 I print_info: causal attn      = 1
0.00.051.593 I print_info: pooling type     = 0
0.00.051.593 I print_info: rope type        = 2
0.00.051.593 I print_info: rope scaling     = linear
0.00.051.594 I print_info: freq_base_train  = 10000.0
0.00.051.594 I print_info: freq_scale_train = 1
0.00.051.594 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.594 I print_info: rope_finetuned   = unknown
0.00.051.596 I print_info: ssm_d_conv       = 0
0.00.051.596 I print_info: ssm_d_inner      = 0
0.00.051.596 I print_info: ssm_d_state      = 0
0.00.051.596 I print_info: ssm_dt_rank      = 0
0.00.051.596 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.597 I print_info: model type       = 1.4B
0.00.051.597 I print_info: model params     = 1.41 B
0.00.051.597 I print_info: general.name     = 1.4B
0.00.051.597 I print_info: vocab type       = BPE
0.00.051.598 I print_info: n_vocab          = 50304
0.00.051.598 I print_info: n_merges         = 50009
0.00.051.598 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.598 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.598 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.600 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.600 I print_info: LF token         = 128 'Ä'
0.00.051.600 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.600 I print_info: max token length = 1024
0.00.053.624 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.624 I load_tensors: offloading output layer to GPU
0.00.053.624 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.635 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.636 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.930 I llama_init_from_model: n_seq_max     = 1
0.00.053.931 I llama_init_from_model: n_ctx         = 2048
0.00.053.932 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.932 I llama_init_from_model: n_batch       = 2048
0.00.053.932 I llama_init_from_model: n_ubatch      = 512
0.00.053.932 I llama_init_from_model: flash_attn    = 0
0.00.053.932 I llama_init_from_model: freq_base     = 10000.0
0.00.053.933 I llama_init_from_model: freq_scale    = 1
0.00.053.933 I ggml_metal_init: allocating
0.00.053.936 I ggml_metal_init: found device: Apple M4
0.00.053.938 I ggml_metal_init: picking default device: Apple M4
0.00.054.568 I ggml_metal_init: using embedded metal library
0.00.056.988 I ggml_metal_init: GPU name:   Apple M4
0.00.056.989 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.990 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.990 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.990 I ggml_metal_init: simdgroup reduction   = true
0.00.056.990 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.991 I ggml_metal_init: has bfloat            = true
0.00.056.991 I ggml_metal_init: use bfloat            = true
0.00.056.991 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.992 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.155 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.579 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.589 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.612 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.089.642 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.089.643 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.089.644 I llama_init_from_model: graph nodes  = 967
0.00.089.644 I llama_init_from_model: graph splits = 2
0.00.089.647 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.782 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.782 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.701.057 I main: llama threadpool init, n_threads = 4
0.00.701.128 I 
0.00.701.150 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.701.150 I 
0.00.701.371 I sampler seed: 1234
0.00.701.376 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.701.387 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.701.387 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.701.387 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.550.316 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60735.67 tokens per second)
0.01.550.316 I llama_perf_context_print:        load time =     691.27 ms
0.01.550.318 I llama_perf_context_print: prompt eval time =      55.43 ms /     7 tokens (    7.92 ms per token,   126.30 tokens per second)
0.01.550.319 I llama_perf_context_print:        eval time =     790.54 ms /    63 runs   (   12.55 ms per token,    79.69 tokens per second)
0.01.550.319 I llama_perf_context_print:       total time =     849.26 ms /    70 tokens
0.01.550.576 I ggml_metal_free: deallocating

real	0m1.567s
user	0m0.110s
sys	0m0.160s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4479 (a4f3f5d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.937 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.089 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.018.093 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.095 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.096 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.096 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.096 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.097 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.099 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.099 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.100 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.100 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.103 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.103 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.104 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.109 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.109 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.109 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.884 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.890 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.647 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.648 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.649 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.649 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.649 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.650 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.650 I llama_model_loader: - type  f32:  194 tensors
0.00.026.650 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.651 I print_info: file format = GGUF V3 (latest)
0.00.026.651 I print_info: file type   = Q6_K
0.00.026.652 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.046.487 I load: special tokens cache size = 25
0.00.052.519 I load: token to piece cache size = 0.2984 MB
0.00.052.522 I print_info: arch             = gptneox
0.00.052.522 I print_info: vocab_only       = 0
0.00.052.523 I print_info: n_ctx_train      = 2048
0.00.052.523 I print_info: n_embd           = 2048
0.00.052.523 I print_info: n_layer          = 24
0.00.052.526 I print_info: n_head           = 16
0.00.052.527 I print_info: n_head_kv        = 16
0.00.052.527 I print_info: n_rot            = 32
0.00.052.527 I print_info: n_swa            = 0
0.00.052.527 I print_info: n_embd_head_k    = 128
0.00.052.528 I print_info: n_embd_head_v    = 128
0.00.052.528 I print_info: n_gqa            = 1
0.00.052.529 I print_info: n_embd_k_gqa     = 2048
0.00.052.530 I print_info: n_embd_v_gqa     = 2048
0.00.052.530 I print_info: f_norm_eps       = 1.0e-05
0.00.052.531 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.531 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.531 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.531 I print_info: f_logit_scale    = 0.0e+00
0.00.052.534 I print_info: n_ff             = 8192
0.00.052.534 I print_info: n_expert         = 0
0.00.052.534 I print_info: n_expert_used    = 0
0.00.052.534 I print_info: causal attn      = 1
0.00.052.536 I print_info: pooling type     = 0
0.00.052.538 I print_info: rope type        = 2
0.00.052.538 I print_info: rope scaling     = linear
0.00.052.538 I print_info: freq_base_train  = 10000.0
0.00.052.538 I print_info: freq_scale_train = 1
0.00.052.539 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.539 I print_info: rope_finetuned   = unknown
0.00.052.539 I print_info: ssm_d_conv       = 0
0.00.052.539 I print_info: ssm_d_inner      = 0
0.00.052.539 I print_info: ssm_d_state      = 0
0.00.052.539 I print_info: ssm_dt_rank      = 0
0.00.052.540 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.540 I print_info: model type       = 1.4B
0.00.052.540 I print_info: model params     = 1.41 B
0.00.052.540 I print_info: general.name     = 1.4B
0.00.052.542 I print_info: vocab type       = BPE
0.00.052.542 I print_info: n_vocab          = 50304
0.00.052.542 I print_info: n_merges         = 50009
0.00.052.542 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.542 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.543 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.543 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.543 I print_info: LF token         = 128 'Ä'
0.00.052.543 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.543 I print_info: max token length = 1024
0.00.054.678 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.678 I load_tensors: offloading output layer to GPU
0.00.054.678 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.689 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.690 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.055.088 I llama_init_from_model: n_seq_max     = 1
0.00.055.089 I llama_init_from_model: n_ctx         = 2048
0.00.055.089 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.055.089 I llama_init_from_model: n_batch       = 2048
0.00.055.089 I llama_init_from_model: n_ubatch      = 512
0.00.055.089 I llama_init_from_model: flash_attn    = 0
0.00.055.090 I llama_init_from_model: freq_base     = 10000.0
0.00.055.090 I llama_init_from_model: freq_scale    = 1
0.00.055.090 I ggml_metal_init: allocating
0.00.055.093 I ggml_metal_init: found device: Apple M4
0.00.055.095 I ggml_metal_init: picking default device: Apple M4
0.00.055.736 I ggml_metal_init: using embedded metal library
0.00.058.218 I ggml_metal_init: GPU name:   Apple M4
0.00.058.220 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.220 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.221 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.221 I ggml_metal_init: simdgroup reduction   = true
0.00.058.221 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.221 I ggml_metal_init: has bfloat            = true
0.00.058.221 I ggml_metal_init: use bfloat            = true
0.00.058.222 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.222 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.505 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.089.951 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.955 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.973 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.091.056 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.091.057 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.091.057 I llama_init_from_model: graph nodes  = 967
0.00.091.058 I llama_init_from_model: graph splits = 2
0.00.091.060 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.091.188 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.091.189 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.745.851 I main: llama threadpool init, n_threads = 4
0.00.745.886 I 
0.00.745.903 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.745.904 I 
0.00.746.132 I sampler seed: 1234
0.00.746.136 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.746.147 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.746.148 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.746.148 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.629.885 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62390.16 tokens per second)
0.01.629.886 I llama_perf_context_print:        load time =     735.91 ms
0.01.629.886 I llama_perf_context_print: prompt eval time =      54.46 ms /     7 tokens (    7.78 ms per token,   128.53 tokens per second)
0.01.629.887 I llama_perf_context_print:        eval time =     826.42 ms /    63 runs   (   13.12 ms per token,    76.23 tokens per second)
0.01.629.887 I llama_perf_context_print:       total time =     884.04 ms /    70 tokens
0.01.630.110 I ggml_metal_free: deallocating

real	0m1.648s
user	0m0.111s
sys	0m0.161s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.437 I build: 4479 (a4f3f5d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.969 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.036.145 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.151 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.156 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.157 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.157 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.158 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.158 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.160 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.160 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.160 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.162 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.163 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.163 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.164 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.165 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.166 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.166 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.341 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.640 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.337 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.339 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.340 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.340 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.341 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.342 I llama_model_loader: - type  f32:  194 tensors
0.00.055.342 I llama_model_loader: - type  f16:   98 tensors
0.00.055.343 I print_info: file format = GGUF V3 (latest)
0.00.055.344 I print_info: file type   = all F32 (guessed)
0.00.055.347 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.084.493 I load: special tokens cache size = 25
0.00.091.407 I load: token to piece cache size = 0.2984 MB
0.00.091.410 I print_info: arch             = gptneox
0.00.091.410 I print_info: vocab_only       = 0
0.00.091.410 I print_info: n_ctx_train      = 2048
0.00.091.411 I print_info: n_embd           = 2048
0.00.091.411 I print_info: n_layer          = 24
0.00.091.414 I print_info: n_head           = 16
0.00.091.415 I print_info: n_head_kv        = 16
0.00.091.415 I print_info: n_rot            = 32
0.00.091.415 I print_info: n_swa            = 0
0.00.091.415 I print_info: n_embd_head_k    = 128
0.00.091.417 I print_info: n_embd_head_v    = 128
0.00.091.418 I print_info: n_gqa            = 1
0.00.091.419 I print_info: n_embd_k_gqa     = 2048
0.00.091.419 I print_info: n_embd_v_gqa     = 2048
0.00.091.420 I print_info: f_norm_eps       = 1.0e-05
0.00.091.420 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.091.420 I print_info: f_clamp_kqv      = 0.0e+00
0.00.091.420 I print_info: f_max_alibi_bias = 0.0e+00
0.00.091.421 I print_info: f_logit_scale    = 0.0e+00
0.00.091.421 I print_info: n_ff             = 8192
0.00.091.421 I print_info: n_expert         = 0
0.00.091.422 I print_info: n_expert_used    = 0
0.00.091.422 I print_info: causal attn      = 1
0.00.091.422 I print_info: pooling type     = 0
0.00.091.422 I print_info: rope type        = 2
0.00.091.422 I print_info: rope scaling     = linear
0.00.091.423 I print_info: freq_base_train  = 10000.0
0.00.091.423 I print_info: freq_scale_train = 1
0.00.091.423 I print_info: n_ctx_orig_yarn  = 2048
0.00.091.423 I print_info: rope_finetuned   = unknown
0.00.091.423 I print_info: ssm_d_conv       = 0
0.00.091.424 I print_info: ssm_d_inner      = 0
0.00.091.424 I print_info: ssm_d_state      = 0
0.00.091.424 I print_info: ssm_dt_rank      = 0
0.00.091.424 I print_info: ssm_dt_b_c_rms   = 0
0.00.091.424 I print_info: model type       = 1.4B
0.00.091.424 I print_info: model params     = 1.41 B
0.00.091.425 I print_info: general.name     = 1.4B
0.00.091.425 I print_info: vocab type       = BPE
0.00.091.425 I print_info: n_vocab          = 50304
0.00.091.425 I print_info: n_merges         = 50009
0.00.091.426 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.091.426 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.091.426 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.091.426 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.091.427 I print_info: LF token         = 128 'Ä'
0.00.091.428 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.091.428 I print_info: max token length = 1024
0.00.094.009 I load_tensors: offloading 24 repeating layers to GPU
0.00.094.009 I load_tensors: offloading output layer to GPU
0.00.094.009 I load_tensors: offloaded 25/25 layers to GPU
0.00.094.020 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.094.021 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.094.318 I llama_init_from_model: n_seq_max     = 1
0.00.094.319 I llama_init_from_model: n_ctx         = 128
0.00.094.319 I llama_init_from_model: n_ctx_per_seq = 128
0.00.094.319 I llama_init_from_model: n_batch       = 128
0.00.094.319 I llama_init_from_model: n_ubatch      = 128
0.00.094.320 I llama_init_from_model: flash_attn    = 0
0.00.094.320 I llama_init_from_model: freq_base     = 10000.0
0.00.094.320 I llama_init_from_model: freq_scale    = 1
0.00.094.320 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.094.321 I ggml_metal_init: allocating
0.00.094.324 I ggml_metal_init: found device: Apple M4
0.00.094.326 I ggml_metal_init: picking default device: Apple M4
0.00.094.961 I ggml_metal_init: using embedded metal library
0.00.097.542 I ggml_metal_init: GPU name:   Apple M4
0.00.097.543 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.544 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.544 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.544 I ggml_metal_init: simdgroup reduction   = true
0.00.097.544 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.545 I ggml_metal_init: has bfloat            = true
0.00.097.545 I ggml_metal_init: use bfloat            = true
0.00.097.545 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.546 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.107.238 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.108.485 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.108.488 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.108.503 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.109.364 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.109.365 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.109.365 I llama_init_from_model: graph nodes  = 967
0.00.109.365 I llama_init_from_model: graph splits = 2
0.00.109.367 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.109.367 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.802.594 I 
0.00.802.628 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.802.638 I perplexity: tokenizing the input ..
0.00.815.404 I perplexity: tokenization took 12.763 ms
0.00.815.409 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.935.207 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.00.936.857 I Final estimate: PPL = 10.1498 +/- 3.22650

0.00.936.911 I llama_perf_context_print:        load time =     780.61 ms
0.00.936.913 I llama_perf_context_print: prompt eval time =     119.26 ms /   128 tokens (    0.93 ms per token,  1073.30 tokens per second)
0.00.936.914 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.936.914 I llama_perf_context_print:       total time =     134.32 ms /   129 tokens
0.00.937.516 I ggml_metal_free: deallocating

real	0m1.125s
user	0m0.123s
sys	0m0.158s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.112 I build: 4479 (a4f3f5d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.454 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.166 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.023.172 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.175 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.175 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.176 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.176 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.177 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.177 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.178 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.178 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.180 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.180 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.181 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.181 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.183 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.184 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.184 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.048 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.556 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.518 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.520 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.521 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.521 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.521 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.522 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.523 I llama_model_loader: - type  f32:  194 tensors
0.00.035.523 I llama_model_loader: - type q8_0:   98 tensors
0.00.035.524 I print_info: file format = GGUF V3 (latest)
0.00.035.524 I print_info: file type   = Q8_0
0.00.035.525 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.058.247 I load: special tokens cache size = 25
0.00.064.409 I load: token to piece cache size = 0.2984 MB
0.00.064.412 I print_info: arch             = gptneox
0.00.064.412 I print_info: vocab_only       = 0
0.00.064.413 I print_info: n_ctx_train      = 2048
0.00.064.413 I print_info: n_embd           = 2048
0.00.064.413 I print_info: n_layer          = 24
0.00.064.417 I print_info: n_head           = 16
0.00.064.418 I print_info: n_head_kv        = 16
0.00.064.418 I print_info: n_rot            = 32
0.00.064.418 I print_info: n_swa            = 0
0.00.064.418 I print_info: n_embd_head_k    = 128
0.00.064.419 I print_info: n_embd_head_v    = 128
0.00.064.420 I print_info: n_gqa            = 1
0.00.064.420 I print_info: n_embd_k_gqa     = 2048
0.00.064.421 I print_info: n_embd_v_gqa     = 2048
0.00.064.422 I print_info: f_norm_eps       = 1.0e-05
0.00.064.422 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.064.422 I print_info: f_clamp_kqv      = 0.0e+00
0.00.064.422 I print_info: f_max_alibi_bias = 0.0e+00
0.00.064.422 I print_info: f_logit_scale    = 0.0e+00
0.00.064.423 I print_info: n_ff             = 8192
0.00.064.424 I print_info: n_expert         = 0
0.00.064.425 I print_info: n_expert_used    = 0
0.00.064.425 I print_info: causal attn      = 1
0.00.064.425 I print_info: pooling type     = 0
0.00.064.425 I print_info: rope type        = 2
0.00.064.425 I print_info: rope scaling     = linear
0.00.064.426 I print_info: freq_base_train  = 10000.0
0.00.064.426 I print_info: freq_scale_train = 1
0.00.064.428 I print_info: n_ctx_orig_yarn  = 2048
0.00.064.428 I print_info: rope_finetuned   = unknown
0.00.064.428 I print_info: ssm_d_conv       = 0
0.00.064.428 I print_info: ssm_d_inner      = 0
0.00.064.428 I print_info: ssm_d_state      = 0
0.00.064.428 I print_info: ssm_dt_rank      = 0
0.00.064.429 I print_info: ssm_dt_b_c_rms   = 0
0.00.064.429 I print_info: model type       = 1.4B
0.00.064.429 I print_info: model params     = 1.41 B
0.00.064.429 I print_info: general.name     = 1.4B
0.00.064.430 I print_info: vocab type       = BPE
0.00.064.430 I print_info: n_vocab          = 50304
0.00.064.430 I print_info: n_merges         = 50009
0.00.064.430 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.064.434 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.064.434 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.064.435 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.064.435 I print_info: LF token         = 128 'Ä'
0.00.064.436 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.064.436 I print_info: max token length = 1024
0.00.066.840 I load_tensors: offloading 24 repeating layers to GPU
0.00.066.840 I load_tensors: offloading output layer to GPU
0.00.066.840 I load_tensors: offloaded 25/25 layers to GPU
0.00.066.851 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.066.853 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.067.224 I llama_init_from_model: n_seq_max     = 1
0.00.067.225 I llama_init_from_model: n_ctx         = 128
0.00.067.225 I llama_init_from_model: n_ctx_per_seq = 128
0.00.067.225 I llama_init_from_model: n_batch       = 128
0.00.067.225 I llama_init_from_model: n_ubatch      = 128
0.00.067.226 I llama_init_from_model: flash_attn    = 0
0.00.067.226 I llama_init_from_model: freq_base     = 10000.0
0.00.067.226 I llama_init_from_model: freq_scale    = 1
0.00.067.226 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.067.227 I ggml_metal_init: allocating
0.00.067.229 I ggml_metal_init: found device: Apple M4
0.00.067.231 I ggml_metal_init: picking default device: Apple M4
0.00.067.871 I ggml_metal_init: using embedded metal library
0.00.070.542 I ggml_metal_init: GPU name:   Apple M4
0.00.070.543 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.544 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.544 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.544 I ggml_metal_init: simdgroup reduction   = true
0.00.070.545 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.545 I ggml_metal_init: has bfloat            = true
0.00.070.545 I ggml_metal_init: use bfloat            = true
0.00.070.545 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.546 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.080.443 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.081.821 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.081.824 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.081.846 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.082.869 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.082.870 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.082.871 I llama_init_from_model: graph nodes  = 967
0.00.082.871 I llama_init_from_model: graph splits = 2
0.00.082.872 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.082.873 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.887.124 I 
0.00.887.152 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.887.154 I perplexity: tokenizing the input ..
0.00.894.840 I perplexity: tokenization took 7.684 ms
0.00.894.843 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.019.007 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.020.175 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.020.204 I llama_perf_context_print:        load time =     874.66 ms
0.01.020.205 I llama_perf_context_print: prompt eval time =     123.94 ms /   128 tokens (    0.97 ms per token,  1032.78 tokens per second)
0.01.020.206 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.020.207 I llama_perf_context_print:       total time =     133.08 ms /   129 tokens
0.01.020.701 I ggml_metal_free: deallocating

real	0m1.038s
user	0m0.092s
sys	0m0.153s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4479 (a4f3f5d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.090 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.777 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.782 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.789 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.789 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.790 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.790 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.791 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.791 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.792 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.792 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.792 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.793 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.793 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.794 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.795 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.796 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.796 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.673 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.698 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.569 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.570 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.570 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.571 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.571 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.571 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.572 I llama_model_loader: - type  f32:  194 tensors
0.00.027.572 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.572 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.573 I print_info: file format = GGUF V3 (latest)
0.00.027.573 I print_info: file type   = Q4_0
0.00.027.578 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.047.388 I load: special tokens cache size = 25
0.00.053.465 I load: token to piece cache size = 0.2984 MB
0.00.053.468 I print_info: arch             = gptneox
0.00.053.469 I print_info: vocab_only       = 0
0.00.053.469 I print_info: n_ctx_train      = 2048
0.00.053.469 I print_info: n_embd           = 2048
0.00.053.469 I print_info: n_layer          = 24
0.00.053.472 I print_info: n_head           = 16
0.00.053.473 I print_info: n_head_kv        = 16
0.00.053.473 I print_info: n_rot            = 32
0.00.053.473 I print_info: n_swa            = 0
0.00.053.474 I print_info: n_embd_head_k    = 128
0.00.053.474 I print_info: n_embd_head_v    = 128
0.00.053.474 I print_info: n_gqa            = 1
0.00.053.475 I print_info: n_embd_k_gqa     = 2048
0.00.053.476 I print_info: n_embd_v_gqa     = 2048
0.00.053.477 I print_info: f_norm_eps       = 1.0e-05
0.00.053.477 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.477 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.478 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.478 I print_info: f_logit_scale    = 0.0e+00
0.00.053.479 I print_info: n_ff             = 8192
0.00.053.479 I print_info: n_expert         = 0
0.00.053.479 I print_info: n_expert_used    = 0
0.00.053.479 I print_info: causal attn      = 1
0.00.053.479 I print_info: pooling type     = 0
0.00.053.480 I print_info: rope type        = 2
0.00.053.480 I print_info: rope scaling     = linear
0.00.053.480 I print_info: freq_base_train  = 10000.0
0.00.053.480 I print_info: freq_scale_train = 1
0.00.053.481 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.481 I print_info: rope_finetuned   = unknown
0.00.053.481 I print_info: ssm_d_conv       = 0
0.00.053.481 I print_info: ssm_d_inner      = 0
0.00.053.481 I print_info: ssm_d_state      = 0
0.00.053.482 I print_info: ssm_dt_rank      = 0
0.00.053.482 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.482 I print_info: model type       = 1.4B
0.00.053.482 I print_info: model params     = 1.41 B
0.00.053.482 I print_info: general.name     = 1.4B
0.00.053.483 I print_info: vocab type       = BPE
0.00.053.483 I print_info: n_vocab          = 50304
0.00.053.483 I print_info: n_merges         = 50009
0.00.053.484 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.484 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.484 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.484 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.484 I print_info: LF token         = 128 'Ä'
0.00.053.485 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.485 I print_info: max token length = 1024
0.00.055.537 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.537 I load_tensors: offloading output layer to GPU
0.00.055.538 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.548 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.055.549 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.055.868 I llama_init_from_model: n_seq_max     = 1
0.00.055.869 I llama_init_from_model: n_ctx         = 128
0.00.055.869 I llama_init_from_model: n_ctx_per_seq = 128
0.00.055.870 I llama_init_from_model: n_batch       = 128
0.00.055.870 I llama_init_from_model: n_ubatch      = 128
0.00.055.870 I llama_init_from_model: flash_attn    = 0
0.00.055.870 I llama_init_from_model: freq_base     = 10000.0
0.00.055.871 I llama_init_from_model: freq_scale    = 1
0.00.055.871 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.872 I ggml_metal_init: allocating
0.00.055.875 I ggml_metal_init: found device: Apple M4
0.00.055.877 I ggml_metal_init: picking default device: Apple M4
0.00.056.458 I ggml_metal_init: using embedded metal library
0.00.058.923 I ggml_metal_init: GPU name:   Apple M4
0.00.058.925 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.925 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.926 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.926 I ggml_metal_init: simdgroup reduction   = true
0.00.058.926 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.926 I ggml_metal_init: has bfloat            = true
0.00.058.927 I ggml_metal_init: use bfloat            = true
0.00.058.927 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.928 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.060 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.070.631 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.070.633 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.070.647 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.071.643 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.071.644 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.071.645 I llama_init_from_model: graph nodes  = 967
0.00.071.645 I llama_init_from_model: graph splits = 2
0.00.071.646 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.071.647 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.611.893 I 
0.00.611.917 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.611.920 I perplexity: tokenizing the input ..
0.00.619.527 I perplexity: tokenization took 7.606 ms
0.00.619.534 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.741.966 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.743.485 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.743.502 I llama_perf_context_print:        load time =     601.80 ms
0.00.743.504 I llama_perf_context_print: prompt eval time =     122.17 ms /   128 tokens (    0.95 ms per token,  1047.71 tokens per second)
0.00.743.506 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.743.506 I llama_perf_context_print:       total time =     131.61 ms /   129 tokens
0.00.743.904 I ggml_metal_free: deallocating

real	0m0.760s
user	0m0.080s
sys	0m0.092s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4479 (a4f3f5d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.255 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.458 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.462 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.464 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.465 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.465 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.465 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.466 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.466 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.467 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.467 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.468 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.468 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.469 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.470 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.471 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.472 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.472 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.281 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.295 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.233 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.234 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.234 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.234 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.235 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.235 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.236 I llama_model_loader: - type  f32:  194 tensors
0.00.026.236 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.236 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.237 I print_info: file format = GGUF V3 (latest)
0.00.026.238 I print_info: file type   = Q4_1
0.00.026.239 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.046.443 I load: special tokens cache size = 25
0.00.052.316 I load: token to piece cache size = 0.2984 MB
0.00.052.319 I print_info: arch             = gptneox
0.00.052.319 I print_info: vocab_only       = 0
0.00.052.320 I print_info: n_ctx_train      = 2048
0.00.052.320 I print_info: n_embd           = 2048
0.00.052.320 I print_info: n_layer          = 24
0.00.052.324 I print_info: n_head           = 16
0.00.052.325 I print_info: n_head_kv        = 16
0.00.052.325 I print_info: n_rot            = 32
0.00.052.325 I print_info: n_swa            = 0
0.00.052.325 I print_info: n_embd_head_k    = 128
0.00.052.325 I print_info: n_embd_head_v    = 128
0.00.052.326 I print_info: n_gqa            = 1
0.00.052.327 I print_info: n_embd_k_gqa     = 2048
0.00.052.330 I print_info: n_embd_v_gqa     = 2048
0.00.052.330 I print_info: f_norm_eps       = 1.0e-05
0.00.052.331 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.331 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.331 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.331 I print_info: f_logit_scale    = 0.0e+00
0.00.052.332 I print_info: n_ff             = 8192
0.00.052.332 I print_info: n_expert         = 0
0.00.052.332 I print_info: n_expert_used    = 0
0.00.052.332 I print_info: causal attn      = 1
0.00.052.333 I print_info: pooling type     = 0
0.00.052.333 I print_info: rope type        = 2
0.00.052.333 I print_info: rope scaling     = linear
0.00.052.334 I print_info: freq_base_train  = 10000.0
0.00.052.335 I print_info: freq_scale_train = 1
0.00.052.335 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.335 I print_info: rope_finetuned   = unknown
0.00.052.335 I print_info: ssm_d_conv       = 0
0.00.052.336 I print_info: ssm_d_inner      = 0
0.00.052.336 I print_info: ssm_d_state      = 0
0.00.052.336 I print_info: ssm_dt_rank      = 0
0.00.052.336 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.336 I print_info: model type       = 1.4B
0.00.052.336 I print_info: model params     = 1.41 B
0.00.052.337 I print_info: general.name     = 1.4B
0.00.052.337 I print_info: vocab type       = BPE
0.00.052.337 I print_info: n_vocab          = 50304
0.00.052.337 I print_info: n_merges         = 50009
0.00.052.338 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.338 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.338 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.338 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.338 I print_info: LF token         = 128 'Ä'
0.00.052.338 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.338 I print_info: max token length = 1024
0.00.054.314 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.314 I load_tensors: offloading output layer to GPU
0.00.054.314 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.325 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.054.326 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.054.602 I llama_init_from_model: n_seq_max     = 1
0.00.054.603 I llama_init_from_model: n_ctx         = 128
0.00.054.603 I llama_init_from_model: n_ctx_per_seq = 128
0.00.054.603 I llama_init_from_model: n_batch       = 128
0.00.054.604 I llama_init_from_model: n_ubatch      = 128
0.00.054.604 I llama_init_from_model: flash_attn    = 0
0.00.054.604 I llama_init_from_model: freq_base     = 10000.0
0.00.054.604 I llama_init_from_model: freq_scale    = 1
0.00.054.605 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.605 I ggml_metal_init: allocating
0.00.054.608 I ggml_metal_init: found device: Apple M4
0.00.054.611 I ggml_metal_init: picking default device: Apple M4
0.00.055.203 I ggml_metal_init: using embedded metal library
0.00.057.593 I ggml_metal_init: GPU name:   Apple M4
0.00.057.594 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.595 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.595 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.595 I ggml_metal_init: simdgroup reduction   = true
0.00.057.596 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.596 I ggml_metal_init: has bfloat            = true
0.00.057.596 I ggml_metal_init: use bfloat            = true
0.00.057.596 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.597 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.939 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.069.527 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.530 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.542 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.070.393 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.070.394 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.070.394 I llama_init_from_model: graph nodes  = 967
0.00.070.394 I llama_init_from_model: graph splits = 2
0.00.070.396 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.396 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.677.840 I 
0.00.677.875 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.677.879 I perplexity: tokenizing the input ..
0.00.685.854 I perplexity: tokenization took 7.971 ms
0.00.685.862 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.807.828 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.809.134 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.809.161 I llama_perf_context_print:        load time =     667.58 ms
0.00.809.162 I llama_perf_context_print: prompt eval time =     121.73 ms /   128 tokens (    0.95 ms per token,  1051.52 tokens per second)
0.00.809.163 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.809.163 I llama_perf_context_print:       total time =     131.32 ms /   129 tokens
0.00.809.557 I ggml_metal_free: deallocating

real	0m0.825s
user	0m0.080s
sys	0m0.107s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4479 (a4f3f5d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.825 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.386 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.391 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.393 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.398 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.399 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.399 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.399 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.400 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.401 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.402 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.403 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.403 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.403 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.404 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.406 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.406 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.406 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.215 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.314 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.158 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.159 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.159 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.160 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.160 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.160 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.161 I llama_model_loader: - type  f32:  194 tensors
0.00.025.161 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.161 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.162 I print_info: file format = GGUF V3 (latest)
0.00.025.163 I print_info: file type   = Q5_0
0.00.025.164 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.045.526 I load: special tokens cache size = 25
0.00.051.485 I load: token to piece cache size = 0.2984 MB
0.00.051.489 I print_info: arch             = gptneox
0.00.051.489 I print_info: vocab_only       = 0
0.00.051.490 I print_info: n_ctx_train      = 2048
0.00.051.490 I print_info: n_embd           = 2048
0.00.051.490 I print_info: n_layer          = 24
0.00.051.495 I print_info: n_head           = 16
0.00.051.495 I print_info: n_head_kv        = 16
0.00.051.497 I print_info: n_rot            = 32
0.00.051.497 I print_info: n_swa            = 0
0.00.051.497 I print_info: n_embd_head_k    = 128
0.00.051.498 I print_info: n_embd_head_v    = 128
0.00.051.498 I print_info: n_gqa            = 1
0.00.051.499 I print_info: n_embd_k_gqa     = 2048
0.00.051.500 I print_info: n_embd_v_gqa     = 2048
0.00.051.500 I print_info: f_norm_eps       = 1.0e-05
0.00.051.500 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.501 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.501 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.501 I print_info: f_logit_scale    = 0.0e+00
0.00.051.502 I print_info: n_ff             = 8192
0.00.051.502 I print_info: n_expert         = 0
0.00.051.502 I print_info: n_expert_used    = 0
0.00.051.502 I print_info: causal attn      = 1
0.00.051.503 I print_info: pooling type     = 0
0.00.051.504 I print_info: rope type        = 2
0.00.051.504 I print_info: rope scaling     = linear
0.00.051.504 I print_info: freq_base_train  = 10000.0
0.00.051.505 I print_info: freq_scale_train = 1
0.00.051.505 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.507 I print_info: rope_finetuned   = unknown
0.00.051.507 I print_info: ssm_d_conv       = 0
0.00.051.507 I print_info: ssm_d_inner      = 0
0.00.051.507 I print_info: ssm_d_state      = 0
0.00.051.507 I print_info: ssm_dt_rank      = 0
0.00.051.507 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.507 I print_info: model type       = 1.4B
0.00.051.508 I print_info: model params     = 1.41 B
0.00.051.508 I print_info: general.name     = 1.4B
0.00.051.509 I print_info: vocab type       = BPE
0.00.051.509 I print_info: n_vocab          = 50304
0.00.051.509 I print_info: n_merges         = 50009
0.00.051.509 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.509 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.510 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.510 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.510 I print_info: LF token         = 128 'Ä'
0.00.051.510 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.510 I print_info: max token length = 1024
0.00.053.629 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.629 I load_tensors: offloading output layer to GPU
0.00.053.630 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.640 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.641 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.947 I llama_init_from_model: n_seq_max     = 1
0.00.053.948 I llama_init_from_model: n_ctx         = 128
0.00.053.948 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.948 I llama_init_from_model: n_batch       = 128
0.00.053.948 I llama_init_from_model: n_ubatch      = 128
0.00.053.949 I llama_init_from_model: flash_attn    = 0
0.00.053.949 I llama_init_from_model: freq_base     = 10000.0
0.00.053.949 I llama_init_from_model: freq_scale    = 1
0.00.053.950 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.950 I ggml_metal_init: allocating
0.00.053.954 I ggml_metal_init: found device: Apple M4
0.00.053.956 I ggml_metal_init: picking default device: Apple M4
0.00.054.555 I ggml_metal_init: using embedded metal library
0.00.056.929 I ggml_metal_init: GPU name:   Apple M4
0.00.056.931 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.932 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.932 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.932 I ggml_metal_init: simdgroup reduction   = true
0.00.056.932 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.932 I ggml_metal_init: has bfloat            = true
0.00.056.933 I ggml_metal_init: use bfloat            = true
0.00.056.933 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.934 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.077 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.592 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.595 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.610 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.069.498 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.069.500 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.069.500 I llama_init_from_model: graph nodes  = 967
0.00.069.500 I llama_init_from_model: graph splits = 2
0.00.069.502 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.502 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.707.737 I 
0.00.707.765 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.707.768 I perplexity: tokenizing the input ..
0.00.715.168 I perplexity: tokenization took 7.398 ms
0.00.715.171 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.850.032 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.851.191 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.851.219 I llama_perf_context_print:        load time =     698.91 ms
0.00.851.220 I llama_perf_context_print: prompt eval time =     134.64 ms /   128 tokens (    1.05 ms per token,   950.71 tokens per second)
0.00.851.221 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.851.221 I llama_perf_context_print:       total time =     143.48 ms /   129 tokens
0.00.851.681 I ggml_metal_free: deallocating

real	0m0.867s
user	0m0.080s
sys	0m0.109s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4479 (a4f3f5d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.787 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.503 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.507 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.509 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.509 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.510 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.510 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.510 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.511 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.512 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.512 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.513 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.513 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.513 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.514 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.515 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.515 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.517 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.322 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.367 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.059 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.061 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.061 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.061 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.061 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.062 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.062 I llama_model_loader: - type  f32:  194 tensors
0.00.025.063 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.063 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.063 I print_info: file format = GGUF V3 (latest)
0.00.025.064 I print_info: file type   = Q5_1
0.00.025.065 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.044.206 I load: special tokens cache size = 25
0.00.050.199 I load: token to piece cache size = 0.2984 MB
0.00.050.202 I print_info: arch             = gptneox
0.00.050.202 I print_info: vocab_only       = 0
0.00.050.202 I print_info: n_ctx_train      = 2048
0.00.050.203 I print_info: n_embd           = 2048
0.00.050.203 I print_info: n_layer          = 24
0.00.050.206 I print_info: n_head           = 16
0.00.050.207 I print_info: n_head_kv        = 16
0.00.050.207 I print_info: n_rot            = 32
0.00.050.207 I print_info: n_swa            = 0
0.00.050.207 I print_info: n_embd_head_k    = 128
0.00.050.207 I print_info: n_embd_head_v    = 128
0.00.050.208 I print_info: n_gqa            = 1
0.00.050.209 I print_info: n_embd_k_gqa     = 2048
0.00.050.210 I print_info: n_embd_v_gqa     = 2048
0.00.050.210 I print_info: f_norm_eps       = 1.0e-05
0.00.050.211 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.211 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.211 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.214 I print_info: f_logit_scale    = 0.0e+00
0.00.050.215 I print_info: n_ff             = 8192
0.00.050.215 I print_info: n_expert         = 0
0.00.050.215 I print_info: n_expert_used    = 0
0.00.050.215 I print_info: causal attn      = 1
0.00.050.215 I print_info: pooling type     = 0
0.00.050.215 I print_info: rope type        = 2
0.00.050.216 I print_info: rope scaling     = linear
0.00.050.216 I print_info: freq_base_train  = 10000.0
0.00.050.217 I print_info: freq_scale_train = 1
0.00.050.217 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.217 I print_info: rope_finetuned   = unknown
0.00.050.218 I print_info: ssm_d_conv       = 0
0.00.050.218 I print_info: ssm_d_inner      = 0
0.00.050.218 I print_info: ssm_d_state      = 0
0.00.050.218 I print_info: ssm_dt_rank      = 0
0.00.050.218 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.218 I print_info: model type       = 1.4B
0.00.050.219 I print_info: model params     = 1.41 B
0.00.050.219 I print_info: general.name     = 1.4B
0.00.050.220 I print_info: vocab type       = BPE
0.00.050.220 I print_info: n_vocab          = 50304
0.00.050.220 I print_info: n_merges         = 50009
0.00.050.220 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.220 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.221 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.221 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.221 I print_info: LF token         = 128 'Ä'
0.00.050.221 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.222 I print_info: max token length = 1024
0.00.052.195 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.195 I load_tensors: offloading output layer to GPU
0.00.052.195 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.206 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.207 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.052.497 I llama_init_from_model: n_seq_max     = 1
0.00.052.498 I llama_init_from_model: n_ctx         = 128
0.00.052.498 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.498 I llama_init_from_model: n_batch       = 128
0.00.052.498 I llama_init_from_model: n_ubatch      = 128
0.00.052.498 I llama_init_from_model: flash_attn    = 0
0.00.052.499 I llama_init_from_model: freq_base     = 10000.0
0.00.052.499 I llama_init_from_model: freq_scale    = 1
0.00.052.499 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.500 I ggml_metal_init: allocating
0.00.052.503 I ggml_metal_init: found device: Apple M4
0.00.052.505 I ggml_metal_init: picking default device: Apple M4
0.00.053.070 I ggml_metal_init: using embedded metal library
0.00.055.418 I ggml_metal_init: GPU name:   Apple M4
0.00.055.420 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.420 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.420 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.421 I ggml_metal_init: simdgroup reduction   = true
0.00.055.421 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.421 I ggml_metal_init: has bfloat            = true
0.00.055.421 I ggml_metal_init: use bfloat            = true
0.00.055.422 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.422 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.264 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.541 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.546 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.564 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.456 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.457 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.458 I llama_init_from_model: graph nodes  = 967
0.00.067.458 I llama_init_from_model: graph splits = 2
0.00.067.459 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.459 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.604.898 I 
0.00.604.925 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.604.930 I perplexity: tokenizing the input ..
0.00.612.232 I perplexity: tokenization took 7.301 ms
0.00.612.236 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.747.416 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.748.695 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.748.729 I llama_perf_context_print:        load time =     595.10 ms
0.00.748.730 I llama_perf_context_print: prompt eval time =     134.95 ms /   128 tokens (    1.05 ms per token,   948.51 tokens per second)
0.00.748.732 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.748.733 I llama_perf_context_print:       total time =     143.83 ms /   129 tokens
0.00.749.240 I ggml_metal_free: deallocating

real	0m0.765s
user	0m0.078s
sys	0m0.104s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4479 (a4f3f5d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.940 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.731 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.736 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.737 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.738 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.738 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.738 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.739 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.740 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.740 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.740 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.741 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.741 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.742 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.742 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.743 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.744 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.744 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.530 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.577 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.342 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.343 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.344 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.344 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.344 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.345 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.345 I llama_model_loader: - type  f32:  194 tensors
0.00.024.346 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.346 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.346 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.347 I print_info: file format = GGUF V3 (latest)
0.00.024.347 I print_info: file type   = Q2_K - Medium
0.00.024.348 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.043.401 I load: special tokens cache size = 25
0.00.049.364 I load: token to piece cache size = 0.2984 MB
0.00.049.368 I print_info: arch             = gptneox
0.00.049.368 I print_info: vocab_only       = 0
0.00.049.368 I print_info: n_ctx_train      = 2048
0.00.049.369 I print_info: n_embd           = 2048
0.00.049.369 I print_info: n_layer          = 24
0.00.049.372 I print_info: n_head           = 16
0.00.049.372 I print_info: n_head_kv        = 16
0.00.049.373 I print_info: n_rot            = 32
0.00.049.373 I print_info: n_swa            = 0
0.00.049.373 I print_info: n_embd_head_k    = 128
0.00.049.373 I print_info: n_embd_head_v    = 128
0.00.049.374 I print_info: n_gqa            = 1
0.00.049.375 I print_info: n_embd_k_gqa     = 2048
0.00.049.377 I print_info: n_embd_v_gqa     = 2048
0.00.049.378 I print_info: f_norm_eps       = 1.0e-05
0.00.049.379 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.380 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.380 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.380 I print_info: f_logit_scale    = 0.0e+00
0.00.049.381 I print_info: n_ff             = 8192
0.00.049.381 I print_info: n_expert         = 0
0.00.049.383 I print_info: n_expert_used    = 0
0.00.049.383 I print_info: causal attn      = 1
0.00.049.383 I print_info: pooling type     = 0
0.00.049.383 I print_info: rope type        = 2
0.00.049.383 I print_info: rope scaling     = linear
0.00.049.384 I print_info: freq_base_train  = 10000.0
0.00.049.384 I print_info: freq_scale_train = 1
0.00.049.384 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.385 I print_info: rope_finetuned   = unknown
0.00.049.385 I print_info: ssm_d_conv       = 0
0.00.049.385 I print_info: ssm_d_inner      = 0
0.00.049.385 I print_info: ssm_d_state      = 0
0.00.049.385 I print_info: ssm_dt_rank      = 0
0.00.049.385 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.386 I print_info: model type       = 1.4B
0.00.049.386 I print_info: model params     = 1.41 B
0.00.049.387 I print_info: general.name     = 1.4B
0.00.049.388 I print_info: vocab type       = BPE
0.00.049.388 I print_info: n_vocab          = 50304
0.00.049.388 I print_info: n_merges         = 50009
0.00.049.388 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.389 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.389 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.389 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.389 I print_info: LF token         = 128 'Ä'
0.00.049.389 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.390 I print_info: max token length = 1024
0.00.051.333 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.334 I load_tensors: offloading output layer to GPU
0.00.051.334 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.345 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.346 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.051.696 I llama_init_from_model: n_seq_max     = 1
0.00.051.696 I llama_init_from_model: n_ctx         = 128
0.00.051.697 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.697 I llama_init_from_model: n_batch       = 128
0.00.051.697 I llama_init_from_model: n_ubatch      = 128
0.00.051.697 I llama_init_from_model: flash_attn    = 0
0.00.051.697 I llama_init_from_model: freq_base     = 10000.0
0.00.051.698 I llama_init_from_model: freq_scale    = 1
0.00.051.698 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.698 I ggml_metal_init: allocating
0.00.051.701 I ggml_metal_init: found device: Apple M4
0.00.051.703 I ggml_metal_init: picking default device: Apple M4
0.00.052.285 I ggml_metal_init: using embedded metal library
0.00.054.674 I ggml_metal_init: GPU name:   Apple M4
0.00.054.676 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.676 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.676 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.677 I ggml_metal_init: simdgroup reduction   = true
0.00.054.677 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.677 I ggml_metal_init: has bfloat            = true
0.00.054.677 I ggml_metal_init: use bfloat            = true
0.00.054.678 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.678 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.544 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.857 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.861 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.878 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.731 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.732 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.732 I llama_init_from_model: graph nodes  = 967
0.00.066.732 I llama_init_from_model: graph splits = 2
0.00.066.734 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.734 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.376.295 I 
0.00.376.324 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.376.336 I perplexity: tokenizing the input ..
0.00.383.869 I perplexity: tokenization took 7.53 ms
0.00.383.872 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.516.219 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.517.394 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.517.426 I llama_perf_context_print:        load time =     367.35 ms
0.00.517.427 I llama_perf_context_print: prompt eval time =     132.12 ms /   128 tokens (    1.03 ms per token,   968.80 tokens per second)
0.00.517.428 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.517.428 I llama_perf_context_print:       total time =     141.13 ms /   129 tokens
0.00.517.952 I ggml_metal_free: deallocating

real	0m0.532s
user	0m0.077s
sys	0m0.062s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4479 (a4f3f5d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.335 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.416 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.421 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.423 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.423 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.424 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.424 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.424 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.425 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.425 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.426 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.426 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.426 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.427 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.429 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.431 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.431 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.431 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.237 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.317 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.063 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.064 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.064 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.064 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.065 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.065 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.065 I llama_model_loader: - type  f32:  194 tensors
0.00.025.066 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.066 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.066 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.066 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.067 I print_info: file format = GGUF V3 (latest)
0.00.025.067 I print_info: file type   = Q3_K - Medium
0.00.025.068 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.044.068 I load: special tokens cache size = 25
0.00.050.144 I load: token to piece cache size = 0.2984 MB
0.00.050.147 I print_info: arch             = gptneox
0.00.050.147 I print_info: vocab_only       = 0
0.00.050.148 I print_info: n_ctx_train      = 2048
0.00.050.148 I print_info: n_embd           = 2048
0.00.050.148 I print_info: n_layer          = 24
0.00.050.151 I print_info: n_head           = 16
0.00.050.152 I print_info: n_head_kv        = 16
0.00.050.153 I print_info: n_rot            = 32
0.00.050.153 I print_info: n_swa            = 0
0.00.050.154 I print_info: n_embd_head_k    = 128
0.00.050.155 I print_info: n_embd_head_v    = 128
0.00.050.155 I print_info: n_gqa            = 1
0.00.050.156 I print_info: n_embd_k_gqa     = 2048
0.00.050.157 I print_info: n_embd_v_gqa     = 2048
0.00.050.157 I print_info: f_norm_eps       = 1.0e-05
0.00.050.158 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.158 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.158 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.158 I print_info: f_logit_scale    = 0.0e+00
0.00.050.159 I print_info: n_ff             = 8192
0.00.050.159 I print_info: n_expert         = 0
0.00.050.159 I print_info: n_expert_used    = 0
0.00.050.159 I print_info: causal attn      = 1
0.00.050.159 I print_info: pooling type     = 0
0.00.050.160 I print_info: rope type        = 2
0.00.050.164 I print_info: rope scaling     = linear
0.00.050.164 I print_info: freq_base_train  = 10000.0
0.00.050.165 I print_info: freq_scale_train = 1
0.00.050.165 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.165 I print_info: rope_finetuned   = unknown
0.00.050.165 I print_info: ssm_d_conv       = 0
0.00.050.165 I print_info: ssm_d_inner      = 0
0.00.050.165 I print_info: ssm_d_state      = 0
0.00.050.166 I print_info: ssm_dt_rank      = 0
0.00.050.167 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.167 I print_info: model type       = 1.4B
0.00.050.168 I print_info: model params     = 1.41 B
0.00.050.169 I print_info: general.name     = 1.4B
0.00.050.169 I print_info: vocab type       = BPE
0.00.050.169 I print_info: n_vocab          = 50304
0.00.050.169 I print_info: n_merges         = 50009
0.00.050.169 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.170 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.170 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.170 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.170 I print_info: LF token         = 128 'Ä'
0.00.050.170 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.171 I print_info: max token length = 1024
0.00.052.124 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.124 I load_tensors: offloading output layer to GPU
0.00.052.125 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.135 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.137 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.052.422 I llama_init_from_model: n_seq_max     = 1
0.00.052.422 I llama_init_from_model: n_ctx         = 128
0.00.052.423 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.423 I llama_init_from_model: n_batch       = 128
0.00.052.423 I llama_init_from_model: n_ubatch      = 128
0.00.052.423 I llama_init_from_model: flash_attn    = 0
0.00.052.423 I llama_init_from_model: freq_base     = 10000.0
0.00.052.424 I llama_init_from_model: freq_scale    = 1
0.00.052.424 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.424 I ggml_metal_init: allocating
0.00.052.427 I ggml_metal_init: found device: Apple M4
0.00.052.429 I ggml_metal_init: picking default device: Apple M4
0.00.052.994 I ggml_metal_init: using embedded metal library
0.00.055.408 I ggml_metal_init: GPU name:   Apple M4
0.00.055.410 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.410 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.411 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.411 I ggml_metal_init: simdgroup reduction   = true
0.00.055.411 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.411 I ggml_metal_init: has bfloat            = true
0.00.055.411 I ggml_metal_init: use bfloat            = true
0.00.055.412 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.412 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.234 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.505 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.510 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.526 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.420 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.421 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.421 I llama_init_from_model: graph nodes  = 967
0.00.067.421 I llama_init_from_model: graph splits = 2
0.00.067.423 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.423 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.469.999 I 
0.00.470.033 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.470.038 I perplexity: tokenizing the input ..
0.00.477.769 I perplexity: tokenization took 7.729 ms
0.00.477.773 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.610.157 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.611.465 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.611.494 I llama_perf_context_print:        load time =     460.66 ms
0.00.611.495 I llama_perf_context_print: prompt eval time =     132.15 ms /   128 tokens (    1.03 ms per token,   968.62 tokens per second)
0.00.611.496 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.611.497 I llama_perf_context_print:       total time =     141.50 ms /   129 tokens
0.00.612.013 I ggml_metal_free: deallocating

real	0m0.627s
user	0m0.077s
sys	0m0.084s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4479 (a4f3f5d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.918 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.138 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.144 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.145 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.146 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.146 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.146 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.147 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.149 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.150 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.150 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.150 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.151 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.151 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.152 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.154 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.155 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.155 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.894 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.894 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.577 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.579 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.579 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.579 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.580 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.580 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.581 I llama_model_loader: - type  f32:  194 tensors
0.00.024.581 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.581 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.581 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.582 I print_info: file format = GGUF V3 (latest)
0.00.024.582 I print_info: file type   = Q4_K - Medium
0.00.024.583 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.483 I load: special tokens cache size = 25
0.00.050.587 I load: token to piece cache size = 0.2984 MB
0.00.050.590 I print_info: arch             = gptneox
0.00.050.590 I print_info: vocab_only       = 0
0.00.050.591 I print_info: n_ctx_train      = 2048
0.00.050.591 I print_info: n_embd           = 2048
0.00.050.591 I print_info: n_layer          = 24
0.00.050.594 I print_info: n_head           = 16
0.00.050.595 I print_info: n_head_kv        = 16
0.00.050.595 I print_info: n_rot            = 32
0.00.050.595 I print_info: n_swa            = 0
0.00.050.595 I print_info: n_embd_head_k    = 128
0.00.050.595 I print_info: n_embd_head_v    = 128
0.00.050.596 I print_info: n_gqa            = 1
0.00.050.597 I print_info: n_embd_k_gqa     = 2048
0.00.050.598 I print_info: n_embd_v_gqa     = 2048
0.00.050.598 I print_info: f_norm_eps       = 1.0e-05
0.00.050.600 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.600 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.605 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.606 I print_info: f_logit_scale    = 0.0e+00
0.00.050.607 I print_info: n_ff             = 8192
0.00.050.607 I print_info: n_expert         = 0
0.00.050.607 I print_info: n_expert_used    = 0
0.00.050.607 I print_info: causal attn      = 1
0.00.050.607 I print_info: pooling type     = 0
0.00.050.607 I print_info: rope type        = 2
0.00.050.609 I print_info: rope scaling     = linear
0.00.050.610 I print_info: freq_base_train  = 10000.0
0.00.050.610 I print_info: freq_scale_train = 1
0.00.050.610 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.610 I print_info: rope_finetuned   = unknown
0.00.050.610 I print_info: ssm_d_conv       = 0
0.00.050.612 I print_info: ssm_d_inner      = 0
0.00.050.612 I print_info: ssm_d_state      = 0
0.00.050.612 I print_info: ssm_dt_rank      = 0
0.00.050.612 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.612 I print_info: model type       = 1.4B
0.00.050.612 I print_info: model params     = 1.41 B
0.00.050.612 I print_info: general.name     = 1.4B
0.00.050.613 I print_info: vocab type       = BPE
0.00.050.613 I print_info: n_vocab          = 50304
0.00.050.613 I print_info: n_merges         = 50009
0.00.050.613 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.616 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.616 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.616 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.617 I print_info: LF token         = 128 'Ä'
0.00.050.617 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.617 I print_info: max token length = 1024
0.00.052.681 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.681 I load_tensors: offloading output layer to GPU
0.00.052.682 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.692 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.694 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.987 I llama_init_from_model: n_seq_max     = 1
0.00.052.988 I llama_init_from_model: n_ctx         = 128
0.00.052.989 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.989 I llama_init_from_model: n_batch       = 128
0.00.052.989 I llama_init_from_model: n_ubatch      = 128
0.00.052.989 I llama_init_from_model: flash_attn    = 0
0.00.052.989 I llama_init_from_model: freq_base     = 10000.0
0.00.052.990 I llama_init_from_model: freq_scale    = 1
0.00.052.990 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.991 I ggml_metal_init: allocating
0.00.052.994 I ggml_metal_init: found device: Apple M4
0.00.052.996 I ggml_metal_init: picking default device: Apple M4
0.00.053.585 I ggml_metal_init: using embedded metal library
0.00.056.024 I ggml_metal_init: GPU name:   Apple M4
0.00.056.026 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.026 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.026 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.027 I ggml_metal_init: simdgroup reduction   = true
0.00.056.027 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.027 I ggml_metal_init: has bfloat            = true
0.00.056.027 I ggml_metal_init: use bfloat            = true
0.00.056.027 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.028 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.081 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.439 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.442 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.468 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.432 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.433 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.433 I llama_init_from_model: graph nodes  = 967
0.00.068.434 I llama_init_from_model: graph splits = 2
0.00.068.435 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.435 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.547.093 I 
0.00.547.136 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.547.141 I perplexity: tokenizing the input ..
0.00.554.784 I perplexity: tokenization took 7.64 ms
0.00.554.797 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.688.960 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.690.095 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.690.119 I llama_perf_context_print:        load time =     538.17 ms
0.00.690.119 I llama_perf_context_print: prompt eval time =     133.94 ms /   128 tokens (    1.05 ms per token,   955.67 tokens per second)
0.00.690.120 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.690.121 I llama_perf_context_print:       total time =     143.03 ms /   129 tokens
0.00.690.559 I ggml_metal_free: deallocating

real	0m0.705s
user	0m0.079s
sys	0m0.092s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4479 (a4f3f5d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.058 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.178 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.184 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.186 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.186 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.187 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.187 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.187 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.188 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.188 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.189 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.189 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.190 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.190 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.190 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.193 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.194 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.194 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.031 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.080 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.817 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.818 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.818 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.819 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.819 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.819 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.820 I llama_model_loader: - type  f32:  194 tensors
0.00.025.820 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.820 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.821 I print_info: file format = GGUF V3 (latest)
0.00.025.821 I print_info: file type   = Q5_K - Medium
0.00.025.824 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.045.542 I load: special tokens cache size = 25
0.00.051.417 I load: token to piece cache size = 0.2984 MB
0.00.051.420 I print_info: arch             = gptneox
0.00.051.420 I print_info: vocab_only       = 0
0.00.051.421 I print_info: n_ctx_train      = 2048
0.00.051.421 I print_info: n_embd           = 2048
0.00.051.421 I print_info: n_layer          = 24
0.00.051.424 I print_info: n_head           = 16
0.00.051.425 I print_info: n_head_kv        = 16
0.00.051.425 I print_info: n_rot            = 32
0.00.051.425 I print_info: n_swa            = 0
0.00.051.426 I print_info: n_embd_head_k    = 128
0.00.051.426 I print_info: n_embd_head_v    = 128
0.00.051.427 I print_info: n_gqa            = 1
0.00.051.427 I print_info: n_embd_k_gqa     = 2048
0.00.051.428 I print_info: n_embd_v_gqa     = 2048
0.00.051.429 I print_info: f_norm_eps       = 1.0e-05
0.00.051.429 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.429 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.429 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.430 I print_info: f_logit_scale    = 0.0e+00
0.00.051.430 I print_info: n_ff             = 8192
0.00.051.430 I print_info: n_expert         = 0
0.00.051.431 I print_info: n_expert_used    = 0
0.00.051.431 I print_info: causal attn      = 1
0.00.051.431 I print_info: pooling type     = 0
0.00.051.433 I print_info: rope type        = 2
0.00.051.434 I print_info: rope scaling     = linear
0.00.051.434 I print_info: freq_base_train  = 10000.0
0.00.051.434 I print_info: freq_scale_train = 1
0.00.051.435 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.435 I print_info: rope_finetuned   = unknown
0.00.051.435 I print_info: ssm_d_conv       = 0
0.00.051.435 I print_info: ssm_d_inner      = 0
0.00.051.435 I print_info: ssm_d_state      = 0
0.00.051.435 I print_info: ssm_dt_rank      = 0
0.00.051.436 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.436 I print_info: model type       = 1.4B
0.00.051.436 I print_info: model params     = 1.41 B
0.00.051.437 I print_info: general.name     = 1.4B
0.00.051.437 I print_info: vocab type       = BPE
0.00.051.438 I print_info: n_vocab          = 50304
0.00.051.438 I print_info: n_merges         = 50009
0.00.051.438 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.438 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.439 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.439 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.439 I print_info: LF token         = 128 'Ä'
0.00.051.439 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.439 I print_info: max token length = 1024
0.00.053.501 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.501 I load_tensors: offloading output layer to GPU
0.00.053.502 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.512 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.513 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.802 I llama_init_from_model: n_seq_max     = 1
0.00.053.802 I llama_init_from_model: n_ctx         = 128
0.00.053.803 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.803 I llama_init_from_model: n_batch       = 128
0.00.053.803 I llama_init_from_model: n_ubatch      = 128
0.00.053.803 I llama_init_from_model: flash_attn    = 0
0.00.053.803 I llama_init_from_model: freq_base     = 10000.0
0.00.053.804 I llama_init_from_model: freq_scale    = 1
0.00.053.804 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.805 I ggml_metal_init: allocating
0.00.053.808 I ggml_metal_init: found device: Apple M4
0.00.053.810 I ggml_metal_init: picking default device: Apple M4
0.00.054.388 I ggml_metal_init: using embedded metal library
0.00.056.779 I ggml_metal_init: GPU name:   Apple M4
0.00.056.781 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.781 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.781 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.782 I ggml_metal_init: simdgroup reduction   = true
0.00.056.782 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.782 I ggml_metal_init: has bfloat            = true
0.00.056.782 I ggml_metal_init: use bfloat            = true
0.00.056.783 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.783 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.692 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.993 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.995 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.009 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.975 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.976 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.976 I llama_init_from_model: graph nodes  = 967
0.00.068.976 I llama_init_from_model: graph splits = 2
0.00.068.977 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.977 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.615.580 I 
0.00.615.605 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.615.608 I perplexity: tokenizing the input ..
0.00.622.846 I perplexity: tokenization took 7.236 ms
0.00.622.849 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.763.741 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.765.022 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.765.054 I llama_perf_context_print:        load time =     605.52 ms
0.00.765.056 I llama_perf_context_print: prompt eval time =     140.65 ms /   128 tokens (    1.10 ms per token,   910.09 tokens per second)
0.00.765.057 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.765.058 I llama_perf_context_print:       total time =     149.47 ms /   129 tokens
0.00.765.593 I ggml_metal_free: deallocating

real	0m0.781s
user	0m0.078s
sys	0m0.103s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4479 (a4f3f5d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.912 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.857 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.861 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.868 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.868 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.870 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.870 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.870 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.871 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.871 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.872 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.872 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.873 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.873 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.873 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.875 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.875 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.875 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.622 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.619 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.404 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.405 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.405 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.406 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.406 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.406 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.407 I llama_model_loader: - type  f32:  194 tensors
0.00.025.407 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.408 I print_info: file format = GGUF V3 (latest)
0.00.025.408 I print_info: file type   = Q6_K
0.00.025.409 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.044.639 I load: special tokens cache size = 25
0.00.050.477 I load: token to piece cache size = 0.2984 MB
0.00.050.480 I print_info: arch             = gptneox
0.00.050.481 I print_info: vocab_only       = 0
0.00.050.481 I print_info: n_ctx_train      = 2048
0.00.050.481 I print_info: n_embd           = 2048
0.00.050.481 I print_info: n_layer          = 24
0.00.050.484 I print_info: n_head           = 16
0.00.050.485 I print_info: n_head_kv        = 16
0.00.050.485 I print_info: n_rot            = 32
0.00.050.485 I print_info: n_swa            = 0
0.00.050.485 I print_info: n_embd_head_k    = 128
0.00.050.486 I print_info: n_embd_head_v    = 128
0.00.050.486 I print_info: n_gqa            = 1
0.00.050.487 I print_info: n_embd_k_gqa     = 2048
0.00.050.488 I print_info: n_embd_v_gqa     = 2048
0.00.050.488 I print_info: f_norm_eps       = 1.0e-05
0.00.050.489 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.489 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.489 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.489 I print_info: f_logit_scale    = 0.0e+00
0.00.050.490 I print_info: n_ff             = 8192
0.00.050.490 I print_info: n_expert         = 0
0.00.050.490 I print_info: n_expert_used    = 0
0.00.050.491 I print_info: causal attn      = 1
0.00.050.491 I print_info: pooling type     = 0
0.00.050.491 I print_info: rope type        = 2
0.00.050.491 I print_info: rope scaling     = linear
0.00.050.494 I print_info: freq_base_train  = 10000.0
0.00.050.494 I print_info: freq_scale_train = 1
0.00.050.494 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.494 I print_info: rope_finetuned   = unknown
0.00.050.494 I print_info: ssm_d_conv       = 0
0.00.050.495 I print_info: ssm_d_inner      = 0
0.00.050.495 I print_info: ssm_d_state      = 0
0.00.050.495 I print_info: ssm_dt_rank      = 0
0.00.050.495 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.495 I print_info: model type       = 1.4B
0.00.050.495 I print_info: model params     = 1.41 B
0.00.050.496 I print_info: general.name     = 1.4B
0.00.050.496 I print_info: vocab type       = BPE
0.00.050.496 I print_info: n_vocab          = 50304
0.00.050.496 I print_info: n_merges         = 50009
0.00.050.497 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.497 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.497 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.497 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.497 I print_info: LF token         = 128 'Ä'
0.00.050.498 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.498 I print_info: max token length = 1024
0.00.052.569 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.569 I load_tensors: offloading output layer to GPU
0.00.052.569 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.580 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.581 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.052.932 I llama_init_from_model: n_seq_max     = 1
0.00.052.932 I llama_init_from_model: n_ctx         = 128
0.00.052.932 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.933 I llama_init_from_model: n_batch       = 128
0.00.052.933 I llama_init_from_model: n_ubatch      = 128
0.00.052.933 I llama_init_from_model: flash_attn    = 0
0.00.052.933 I llama_init_from_model: freq_base     = 10000.0
0.00.052.933 I llama_init_from_model: freq_scale    = 1
0.00.052.934 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.934 I ggml_metal_init: allocating
0.00.052.937 I ggml_metal_init: found device: Apple M4
0.00.052.939 I ggml_metal_init: picking default device: Apple M4
0.00.053.497 I ggml_metal_init: using embedded metal library
0.00.055.869 I ggml_metal_init: GPU name:   Apple M4
0.00.055.871 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.871 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.871 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.872 I ggml_metal_init: simdgroup reduction   = true
0.00.055.872 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.872 I ggml_metal_init: has bfloat            = true
0.00.055.872 I ggml_metal_init: use bfloat            = true
0.00.055.872 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.873 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.645 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.933 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.938 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.955 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.840 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.841 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.842 I llama_init_from_model: graph nodes  = 967
0.00.067.842 I llama_init_from_model: graph splits = 2
0.00.067.843 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.843 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.109.237 I 
0.00.109.267 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.109.271 I perplexity: tokenizing the input ..
0.00.117.195 I perplexity: tokenization took 7.923 ms
0.00.117.200 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.255.645 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.256.773 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.256.789 I llama_perf_context_print:        load time =      99.32 ms
0.00.256.790 I llama_perf_context_print: prompt eval time =     138.20 ms /   128 tokens (    1.08 ms per token,   926.21 tokens per second)
0.00.256.791 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.256.791 I llama_perf_context_print:       total time =     147.55 ms /   129 tokens
0.00.257.160 I ggml_metal_free: deallocating

real	0m0.273s
user	0m0.078s
sys	0m0.035s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.270 I build: 4479 (a4f3f5d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.786 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.038.886 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.893 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.898 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.898 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.899 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.900 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.900 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.902 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.903 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.903 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.904 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.904 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.905 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.909 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.911 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.912 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.912 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.763 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.551 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.788 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.790 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.790 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.791 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.791 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.792 I llama_model_loader: - type  f32:  194 tensors
0.00.053.792 I llama_model_loader: - type  f16:   98 tensors
0.00.053.793 I print_info: file format = GGUF V3 (latest)
0.00.053.794 I print_info: file type   = all F32 (guessed)
0.00.053.795 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.078.226 I load: special tokens cache size = 25
0.00.084.783 I load: token to piece cache size = 0.2984 MB
0.00.084.786 I print_info: arch             = gptneox
0.00.084.786 I print_info: vocab_only       = 0
0.00.084.786 I print_info: n_ctx_train      = 2048
0.00.084.786 I print_info: n_embd           = 2048
0.00.084.786 I print_info: n_layer          = 24
0.00.084.790 I print_info: n_head           = 16
0.00.084.790 I print_info: n_head_kv        = 16
0.00.084.791 I print_info: n_rot            = 32
0.00.084.792 I print_info: n_swa            = 0
0.00.084.793 I print_info: n_embd_head_k    = 128
0.00.084.793 I print_info: n_embd_head_v    = 128
0.00.084.793 I print_info: n_gqa            = 1
0.00.084.794 I print_info: n_embd_k_gqa     = 2048
0.00.084.795 I print_info: n_embd_v_gqa     = 2048
0.00.084.801 I print_info: f_norm_eps       = 1.0e-05
0.00.084.801 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.084.801 I print_info: f_clamp_kqv      = 0.0e+00
0.00.084.801 I print_info: f_max_alibi_bias = 0.0e+00
0.00.084.802 I print_info: f_logit_scale    = 0.0e+00
0.00.084.802 I print_info: n_ff             = 8192
0.00.084.803 I print_info: n_expert         = 0
0.00.084.803 I print_info: n_expert_used    = 0
0.00.084.803 I print_info: causal attn      = 1
0.00.084.803 I print_info: pooling type     = 0
0.00.084.803 I print_info: rope type        = 2
0.00.084.803 I print_info: rope scaling     = linear
0.00.084.804 I print_info: freq_base_train  = 10000.0
0.00.084.804 I print_info: freq_scale_train = 1
0.00.084.804 I print_info: n_ctx_orig_yarn  = 2048
0.00.084.804 I print_info: rope_finetuned   = unknown
0.00.084.804 I print_info: ssm_d_conv       = 0
0.00.084.804 I print_info: ssm_d_inner      = 0
0.00.084.804 I print_info: ssm_d_state      = 0
0.00.084.804 I print_info: ssm_dt_rank      = 0
0.00.084.805 I print_info: ssm_dt_b_c_rms   = 0
0.00.084.805 I print_info: model type       = 1.4B
0.00.084.805 I print_info: model params     = 1.41 B
0.00.084.805 I print_info: general.name     = 1.4B
0.00.084.806 I print_info: vocab type       = BPE
0.00.084.806 I print_info: n_vocab          = 50304
0.00.084.806 I print_info: n_merges         = 50009
0.00.084.806 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.084.806 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.084.807 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.084.808 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.084.808 I print_info: LF token         = 128 'Ä'
0.00.084.808 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.084.808 I print_info: max token length = 1024
0.00.087.172 I load_tensors: offloading 24 repeating layers to GPU
0.00.087.172 I load_tensors: offloading output layer to GPU
0.00.087.172 I load_tensors: offloaded 25/25 layers to GPU
0.00.087.183 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.087.185 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.087.559 I llama_init_from_model: n_seq_max     = 1
0.00.087.560 I llama_init_from_model: n_ctx         = 128
0.00.087.560 I llama_init_from_model: n_ctx_per_seq = 128
0.00.087.560 I llama_init_from_model: n_batch       = 128
0.00.087.560 I llama_init_from_model: n_ubatch      = 128
0.00.087.560 I llama_init_from_model: flash_attn    = 0
0.00.087.561 I llama_init_from_model: freq_base     = 10000.0
0.00.087.561 I llama_init_from_model: freq_scale    = 1
0.00.087.561 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.087.562 I ggml_metal_init: allocating
0.00.087.565 I ggml_metal_init: found device: Apple M4
0.00.087.567 I ggml_metal_init: picking default device: Apple M4
0.00.088.200 I ggml_metal_init: using embedded metal library
0.00.090.705 I ggml_metal_init: GPU name:   Apple M4
0.00.090.706 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.090.707 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.090.707 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.090.707 I ggml_metal_init: simdgroup reduction   = true
0.00.090.707 I ggml_metal_init: simdgroup matrix mul. = true
0.00.090.708 I ggml_metal_init: has bfloat            = true
0.00.090.708 I ggml_metal_init: use bfloat            = true
0.00.090.708 I ggml_metal_init: hasUnifiedMemory      = true
0.00.090.709 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.099.980 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.101.232 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.101.237 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.101.252 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.102.119 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.102.120 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.102.120 I llama_init_from_model: graph nodes  = 967
0.00.102.120 I llama_init_from_model: graph splits = 2
0.00.102.121 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.102.122 I 
0.00.102.148 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.102.149 I compute_imatrix: tokenizing the input ..
0.00.109.010 I compute_imatrix: tokenization took 6.86 ms
0.00.109.012 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.622.768 I compute_imatrix: 1.51 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.625.194 I llama_perf_context_print:        load time =    1598.98 ms
0.01.625.197 I llama_perf_context_print: prompt eval time =    1513.12 ms /   128 tokens (   11.82 ms per token,    84.59 tokens per second)
0.01.625.197 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.625.198 I llama_perf_context_print:       total time =    1601.40 ms /   129 tokens
0.01.625.882 I ggml_metal_free: deallocating

real	0m1.810s
user	0m0.163s
sys	0m0.234s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4479 (a4f3f5d8)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13760a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13760aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13760b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13760b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13760bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13760c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13760c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13760cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13760d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13760d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13760dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13760e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13760ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13760f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13760fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x137610390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x137610ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1376111d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1376118f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1376120c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1376127e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x137612f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x137613620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x137613ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1376145e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1376148a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x137614eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x137615b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x137616060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x137616320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1376167c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x137616a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x137617310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x137617850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x137617b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x137617fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x137618450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1376188f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x137618d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x137619230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1376196d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x137619b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13761a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13761a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13761a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13761ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13761b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13761bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13761c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13761c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13761cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13761d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13761db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13761e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13761e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13761eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13761f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13761f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13761fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x137620300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1376205c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x137620a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x137620f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1376213a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x137621840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x137621ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x137622180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x137622620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x137622ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x137622f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x137623400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1376238a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x137623d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x137624290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1376247e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x137624d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x137625280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1376257d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x137625d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x137626270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1376267c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x137626d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x137627260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1376277b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x137627d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x137628250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1376287a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x137628cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x137629240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x137629790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x137629ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13762a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13762a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13762acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13762b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13762b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13762bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13761b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13762c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13762c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13762ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13762d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13762d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13762de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13762e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13762e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13762ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13762f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13762f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13762fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x137630350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1376308a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x137630df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x137631290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x137631730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x137631bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x137632070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x137632510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1376329b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x137632e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1376332f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x137633790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x137633c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1376340d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x137634570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x137634a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x137634eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x137635350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1376357f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x137635c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x137636130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1376365d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x137636a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x137636f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1376373b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x137637850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x137637cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x137638190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x137638630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x137638ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x137638f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x137639410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1376398b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x137639d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13763a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13763a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13763ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13763afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13763b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13763b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13763bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13763c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13763c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13763cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13763d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13763d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13763d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13763de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13763e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13763e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13763ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13763f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13763f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13763f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13763fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x137640310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1376407b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x137640c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1376410f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x137641590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x137641a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x137641ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x137642370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x137642810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x137642cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x137643150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1376435f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x137643a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x137643f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1376443d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x137644870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x137644d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1376451b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x137645650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x137645af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x137645f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x137646430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1376468d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x137646d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x137647210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1376476b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x137647b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x137647ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x137648540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x137648a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x137648fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x137649530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1376497f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x137649e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13764a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13764aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13764b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13764b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13764b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13764bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13764c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13764cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13764d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13764d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13764db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13764e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13764e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13764edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13764f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13764f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13764fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1376502f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x137650840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x137650d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1376512e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x137651830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x137651d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1376522d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x137652820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x137652d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1376532c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x137653810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x137653d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1376542b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x137654800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x137654d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1376552a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1376557f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x137655d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x137656290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1376567e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x137656d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x137657280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1376577d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x137657d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x137658270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1376587c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x137658d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x137659260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1376597b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x137659d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13765a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13765a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13765acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13765b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13765b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13765bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13765c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13765c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13765ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13765d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13765d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13765dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13765e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13765e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13765ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13765f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13765f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13765fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1376601f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x137660740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x137660c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x137661130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1376615d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x137661a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x137661f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1376623b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x137662850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x137662cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x137663190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x137663630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x137663ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x137663f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x137664410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1376648b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x137664d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1376651f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x137665740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x137665e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x137666580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x137666ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1376673c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x137667680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x137667e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x137668130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x137668740 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.143.821 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.143.825 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x133f04b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x133f04f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x133f05400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x133f05870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x133f05ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x133f06150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x133f065c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x133f06a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x133f06ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x133f07310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x133f07780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x133f07e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x133f08990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x133f09140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x133f09950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x133f0a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x133f0a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x133f0aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x133f0b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x133f0bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x133f0c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x133f0cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x133f0d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x133f0d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x133f0e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x133f0e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x133f0e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x133f0ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x133f0ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x133f0f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x133f0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x133f0fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x133f10180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x133f10440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x133f108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x133f10d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x133f11190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x133f11600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x133f11a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x133f11ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x133f12350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x133f127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x133f12c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x133f130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x133f13510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x133f13980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x133f13df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x133f14260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x133f146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x133f14b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x133f14fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x133f15420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x133f15890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x133f15d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x133f16170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x133f165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x133f16b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x133f17050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x133f174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x133f17930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x133f17da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x133f18210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x133f18680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x133f18af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x133f18f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x133f193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x133f19840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x133f19cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x133f1a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x133f1a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x133f1aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x133f1ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x133f1b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x133f1b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x133f1bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x133f1c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x133f1c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x133f1c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x133f1cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x133f1d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x133f1d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x133f1dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x133f1df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x133f1e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x133f1e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x133f1ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x133f1f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x133f1f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x133f1f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x133f1fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x133f202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x133f20730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x133f20ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x133f21010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x133f21480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x133f218f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x133f21d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x133f221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x133f22640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x133f22ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x133f22f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x133f23390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x133f23800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x133f23c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x133f240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x133f24550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x133f249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x133f24e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x133f252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x133f25710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x133f25b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x133f25ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x133f26460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x133f268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x133f26d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x133f271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x133f27620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x133f27a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x133f27f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x133f28370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x133f287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x133f28c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x133f290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x133f29530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x133f299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x133f29e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x133f2a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x133f2a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x133f2ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x133f2afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x133f2b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x133f2b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x133f2bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x133f2c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x133f2c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x133f2ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x133f2cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x133f2d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x133f2d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x133f2dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x133f2e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x133f2e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x133f2e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x133f2edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x133f2f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x133f2f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x133f2fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x133f2ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x133f30420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x133f30890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x133f30d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x133f31170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x133f315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x133f31a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x133f31ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x133f32330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x133f327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x133f32c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x133f33080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x133f334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x133f33960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x133f33dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x133f34240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x133f346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x133f34b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x133f34f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x133f35bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x133f35e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x133f36140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x133f365b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x133f36a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x133f36e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x133f37300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x133f37770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x133f37be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x133f38050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x133f384c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x133f38930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x133f38da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x133f39210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x133f39680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x133f39af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x133f39f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x133f3a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x133f3a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x133f3acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x133f3b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x133f3b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x133f3ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x133f3be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x133f3c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x133f3c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x133f3cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x133f3d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x133f3d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x133f3d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x133f3dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x133f3e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x133f3e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x133f3ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x133f3ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x133f3f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x133f3f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x133f3fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x133f40290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x133f40700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x133f40b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x133f40fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x133f41500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x133f41a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x133f42580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x133f42840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x133f42e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x133f433c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x133f43980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x133f43f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x133f44500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x133f44ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x133f45080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x133f45640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x133f45c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x133f461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x133f46780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x133f46d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x133f47300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x133f478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x133f47e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x133f48440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x133f48a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x133f48fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x133f49580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x133f49b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x133f4a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x133f4a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x133f4ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x133f4b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x133f4b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x133f4bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x133f4c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x133f4c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x133f4cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x133f4d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x133f4da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x133f4e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x133f4e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x133f4ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x133f4f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x133f4f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x133f4fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x133f502c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x133f50880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x133f50e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x133f51400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x133f519c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x133f51f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x133f52540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x133f52b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x133f530c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x133f53680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x133f53c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x133f54200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x133f547c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x133f54d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x133f55340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x133f55900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x133f55ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x133f56480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x133f56a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x133f56f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x133f57440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x133f57940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x133f57e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x133f58340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x133f58840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x133f58d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x133f59240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x133f59740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x133f59c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x133f5a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x133f5a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x133f5ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x133f5b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x133f5b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x133f5bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x133f5c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x133f5cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x133f5d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x133f5d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x133f5df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x133f5e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x133f5e830 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x133f5b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x133f4c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x133f4b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x133f48140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x133f45900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x133f55040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x133f52800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x133f50580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x133f4e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x133f46480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x133f43c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x133f48cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x133f49e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x133f4f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x133f4c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x133f53f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x133f47b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x133f51100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x133f4a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x133f4cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x133f475c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x133f55600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x133f447c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x133f430c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x133f45340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x133f55bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x133f4af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x133f53380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x133f49280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x133f4bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x133f4fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x133f47000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x133f4ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x133f516c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x133f45ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x133f544c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x133f51c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x133f4d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x133f56740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x133f44d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x133f56180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x133f44200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x133f54a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x133f4e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x133f50b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x133f53940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x133f52240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x133f4a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x133f41cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x133f04680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x133f5da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x133f0b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x133f5ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x133f5f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x133f5f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x133f5f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x133f5fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x133f5fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x133f5ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x133f60250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x133f60510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x133f607d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x133f60a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x133f60d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x133f61010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x133f612d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x133f61590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x133f61850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x133f61b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x133f61dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x133f62090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x133f62350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x133f62610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x133f628d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x133f62b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x133f62e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x133f63110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x133f633d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x133f63690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x133f63950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x133f63c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x133f63ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x133f64190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x133f64450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x133f64710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x133f649d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x133f64c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x133f64f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x133f65210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x133f654d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x133f65790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x133f65a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x133f65d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x133f65fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x133f66290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x133f66550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x133f66810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x133f66ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x133f66d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x133f67050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x133f67310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x133f675d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x133f67890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x133f67b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x133f67e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x133f680d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x133f68390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x133f68650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x133f68910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x133f68bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x133f68e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x133f69150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x133f69410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x133f696d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x133f69990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x133f69c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x133f69f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x133f6a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x133f6a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x133f6a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x133f6aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x133f6acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x133f6af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x133f6b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x133f6b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x133f6b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x133f6ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x133f6bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x133f6c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x133f6c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x133f6c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x133f6c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x133f6cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x133f6cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x133f6d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x133f6d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x133f6d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x133f6d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x133f6db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x133f6de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x133f6e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x133f6e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x133f6e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x133f6e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x133f6ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x133f6eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x133f6f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x133f6f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x133f6f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x133f6f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x133f6fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x133f6ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x133f70210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x133f704d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x133f70790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x133f70a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x133f70d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x133f70fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x133f71290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x133f71550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x133f71810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x133f71ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x133f71d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x133f72050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x133f72310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x133f725d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x133f72890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x133f72b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x133f72e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x133f730d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x133f73390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x133f73650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x133f73910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x133f73bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x133f73e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x133f74150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x133f74410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x133f746d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x133f74990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x133f74c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x133f74f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x133f751d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x133f75490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x133f75750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x133f75a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x133f75cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x133f75f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x133f76250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x133f76510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x133f767d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x133f76a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x133f76d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x133f77010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x133f772d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x133f77590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x133f77850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x133f77b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x133f77dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x133f78090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x133f78350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x133f78610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x133f788d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x133f78b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x133f78e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x133f79110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x133f793d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x133f79690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x133f79950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x133f79c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x133f79ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x133f7a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x133f7a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x133f7aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x133f7ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x133f7afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x133f7b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x133f7b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x133f7b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x133f7baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x133f7bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x133f7c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x133f7ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x133f7cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x133f7d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x133f7da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x133f7dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x133f7e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x133f7ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x133f7efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x133f7f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x133f7fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x133f7ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x133f80500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x133f80a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x133f80fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x133f814f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x133f81a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x133f81f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x133f824e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x133f82a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x133f82f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x133f834d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x133f83a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x133f83f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x133f844c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x133f84a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x133f84f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x133f854b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x133f85a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x133f85f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x133f864a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x133f869f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x133f86f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x133f87490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x133f879e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x133f87f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x133f88480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x133f889d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x133f88f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x133f89470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x133f899c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x133f89f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x133f8a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x133f8a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x133f8af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x133f8b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x133f8b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x133f8bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x133f8bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x133f8c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x133f8c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x133f8cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x133f8cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x133f8d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x133f8d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x133f8dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x133f8e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x133f8e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x133f8e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x133f8ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x133f8f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x133f8f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x133f8fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x133f90000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x133f90cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x133f91410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x133f91b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x133f91df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x133f92260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x133f92860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x133f92e70 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.797s
user	0m0.296s
sys	0m0.308s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4479 (a4f3f5d8)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x122007640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x122007d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x122008300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1220088b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x122008e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x122009410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1220099c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x122009f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12200a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12200aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12200af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12200b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12200bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12200c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12200cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12200d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12200dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12200e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12200eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12200f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12200fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x122010190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1220108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x122011150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x122011870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x122011b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x122012140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x122012db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1220132f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1220135b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x122013a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x122013d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1220145a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x122014ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x122014da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x122015240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1220156e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x122015b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x122016020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1220164c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x122016960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x122016e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1220172a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x122017740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x122017a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x122018010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x122018620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x122018f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x122019550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x122019b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12201a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12201a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12201ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12201b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12201bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12201c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12201c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12201c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12201cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12201d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12201d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12201dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12201e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12201e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12201ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12201ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12201f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12201f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12201fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1220201f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x122020690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x122020b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x122020fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x122021520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x122021a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x122021fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x122022510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x122022a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x122022fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x122023500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x122023a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x122023fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1220244f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x122024a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x122024f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1220254e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x122025a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x122025f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1220264d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x122026a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x122026f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1220274c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x122027a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x122027f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1220284b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x122028a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x122028f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x122018c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1220293c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x122029b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12202a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12202a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12202ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12202b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12202b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12202bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12202c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12202c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12202cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12202d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12202d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12202db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12202e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12202e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12202e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12202ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12202f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12202f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12202fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1220300e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x122030580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x122030a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x122030ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x122031360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x122031800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x122031ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x122032140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1220325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x122032a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x122032f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1220333c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x122033860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x122033d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1220341a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x122034640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x122034ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x122034f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x122035420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1220358c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x122035d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x122036200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1220366a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x122036b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x122036fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x122037480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x122037920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x122037dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x122038260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x122038700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x122038ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x122039040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1220394e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x122039980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x122039e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12203a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12203a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12203ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12203b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12203b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12203b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12203be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12203c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12203c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12203cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12203d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12203d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12203da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12203dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12203e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12203e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12203ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12203f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12203f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12203faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12203ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1220403e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x122040880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x122040d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1220411c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x122041660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x122041b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x122041fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x122042440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1220428e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x122042d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x122043220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1220436c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x122043b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x122044000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1220444a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x122044940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x122044de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x122045280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1220457d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x122045d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x122046270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1220467c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x122046a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x122047090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1220476a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x122047cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1220484a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x122048940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x122048c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x122049210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x122049820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12204a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12204a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12204a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12204adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12204b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12204baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12204c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12204c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12204cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12204d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12204d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12204dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12204e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12204e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12204eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12204f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12204f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12204fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x122050000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x122050550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x122050aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x122050ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x122051540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x122051a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x122051fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x122052530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x122052a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x122052fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x122053520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x122053a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x122053fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x122054510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x122054a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x122054fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x122055500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x122055a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x122055fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1220564f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x122056a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x122056f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1220574e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x122057a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x122057f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1220584d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x122058a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x122058f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1220594c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x122059a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x122059f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12205a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12205aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12205af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12205b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12205b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12205bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12205c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12205c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12205cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12205d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12205d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12205df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12205e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12205e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12205ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12205f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12205f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12205fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12205ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x122060420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1220608c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x122060d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x122061200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1220616a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x122061b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x122061fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x122062480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1220629d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1220630f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x122063810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x122063f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x122064650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x122064910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x122065100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1220653c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1220659d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.085.700 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.704 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x120f08950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x120f08dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x120f09230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x120f0b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x120f0bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x120f0c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x120f0c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x120f0c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x120f0cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x120f0d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x120f0d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x120f0ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x120f0e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x120f0f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x120f0f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x120f0fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x120f10710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x120f10e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x120f11550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x120f11c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x120f123a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x120f12ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x120f131e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x120f13900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x120f14020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x120f142e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x120f145a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x120f14a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x120f14e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x120f152f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x120f15760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x120f15c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x120f16100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x120f163c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x120f16830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x120f16ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x120f17110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x120f17580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x120f179f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x120f17e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x120f182d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x120f18740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x120f18bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x120f19020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x120f19490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x120f19900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x120f19d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x120f1a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x120f1a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x120f1aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x120f1af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x120f1b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x120f1b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x120f1bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x120f1c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x120f1c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x120f1cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x120f1cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x120f1d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x120f1d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x120f1dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x120f1e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x120f1e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x120f1ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x120f1eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x120f1f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x120f1f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x120f1fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x120f200a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x120f20510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x120f20980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x120f20df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x120f21260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x120f216d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x120f21b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x120f21fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x120f22420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x120f22890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x120f22d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x120f23170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x120f235e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x120f23a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x120f23ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x120f24330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x120f247a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x120f24c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x120f25080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x120f254f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x120f25960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x120f25dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x120f26240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x120f266b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x120f26b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x120f26f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x120f27400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x120f27870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x120f27ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x120f28150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x120f285c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x120f28a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x120f28ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x120f29310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x120f29780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x120f29bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x120f2a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x120f2a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x120f2a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x120f2adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x120f2b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x120f2b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x120f2bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x120f2bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x120f2c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x120f2c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x120f2ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x120f2d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x120f2d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x120f2da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x120f2de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x120f2e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x120f2e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x120f2ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x120f2f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x120f2f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x120f2f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x120f2fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x120f30200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x120f30670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x120f30ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x120f30f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x120f313c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x120f31830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x120f31ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x120f32110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x120f32580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x120f329f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x120f32e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x120f332d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x120f33740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x120f33bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x120f34020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x120f34490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x120f34900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x120f34d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x120f351e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x120f35650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x120f35ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x120f35f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x120f363a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x120f36810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x120f36c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x120f370f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x120f37560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x120f379d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x120f37e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x120f382b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x120f38720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x120f38b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x120f39000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x120f39470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x120f398e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x120f39d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x120f3a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x120f3a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x120f3aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x120f3af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x120f3bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x120f3be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x120f3c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x120f3c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x120f3c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x120f3ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x120f3d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x120f3d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x120f3db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x120f3dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x120f3e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x120f3e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x120f3ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x120f3f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x120f3f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x120f3fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x120f3fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x120f40350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x120f407c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x120f40c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x120f410a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x120f41510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x120f41980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x120f41df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x120f42260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x120f426d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x120f42b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x120f42fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x120f43420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x120f43890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x120f43d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x120f44170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x120f445e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x120f44a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x120f44ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x120f45330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x120f45890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x120f45da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x120f46210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x120f46680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x120f46af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x120f46f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x120f47480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x120f47990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x120f48500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x120f487c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x120f48d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x120f49340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x120f49900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x120f49ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x120f4a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x120f4aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x120f4b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x120f4b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x120f4bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x120f4c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x120f4c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x120f4ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x120f4d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x120f4d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x120f4de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x120f4e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x120f4e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x120f4ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x120f4f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x120f4fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x120f50080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x120f50640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x120f50c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x120f511c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x120f51780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x120f51d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x120f52300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x120f528c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x120f52e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x120f53440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x120f53a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x120f53fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x120f54580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x120f54b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x120f55100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x120f556c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x120f55c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x120f56240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x120f56800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x120f56dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x120f57380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x120f57940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x120f57f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x120f584c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x120f58a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x120f59040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x120f59600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x120f59bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x120f5a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x120f5a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x120f5ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x120f5b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x120f5b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x120f5be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x120f5c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x120f5c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x120f5cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x120f5d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x120f5d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x120f5ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x120f5e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x120f5e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x120f5ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x120f5f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x120f5f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x120f5fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x120f600c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x120f605c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x120f60ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x120f60fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x120f614c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x120f61ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x120f625f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x120f62d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x120f63430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x120f636f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x120f63ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x120f641a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x120f647b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x120f61780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x120f525c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x120f51480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x120f4e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x120f4b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x120f5afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x120f58780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x120f56500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x120f54280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x120f4c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x120f49bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x120f4ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x120f4fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x120f553c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x120f52000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x120f59e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x120f4db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x120f57080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x120f50900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x120f52b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x120f4d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x120f5b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x120f4a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x120f49040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x120f4b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x120f5bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x120f50ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x120f59300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x120f4f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x120f51a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x120f55980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x120f4cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x120f55f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x120f57640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x120f4be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x120f5a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x120f57c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x120f53700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x120f5c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x120f4ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x120f5c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x120f4a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x120f5aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x120f54840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x120f56ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x120f598c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x120f581c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x120f50340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x120f47c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x120f084f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x120f639b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x120f11810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x120f64e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x120f65150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x120f65410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x120f656d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x120f65990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x120f65c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x120f65f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x120f661d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x120f66490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x120f66750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x120f66a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x120f66cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x120f66f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x120f67250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x120f67510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x120f677d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x120f67a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x120f67d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x120f68010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x120f682d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x120f68590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x120f68850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x120f68b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x120f68dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x120f69090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x120f69350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x120f69610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x120f698d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x120f69b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x120f69e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x120f6a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x120f6a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x120f6a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x120f6a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x120f6ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x120f6aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x120f6b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x120f6b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x120f6b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x120f6b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x120f6bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x120f6bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x120f6c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x120f6c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x120f6c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x120f6ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x120f6cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x120f6cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x120f6d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x120f6d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x120f6d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x120f6dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x120f6dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x120f6e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x120f6e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x120f6e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x120f6e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x120f6eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x120f6ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x120f6f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x120f6f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x120f6f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x120f6f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x120f6fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x120f6fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x120f70150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x120f70410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x120f706d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x120f70990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x120f70c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x120f70f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x120f711d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x120f71490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x120f71750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x120f71a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x120f71cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x120f71f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x120f72250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x120f72510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x120f727d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x120f72a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x120f72d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x120f73010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x120f732d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x120f73590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x120f73850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x120f73b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x120f73dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x120f74090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x120f74350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x120f74610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x120f748d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x120f74b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x120f74e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x120f75110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x120f753d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x120f75690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x120f75950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x120f75c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x120f75ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x120f76190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x120f76450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x120f76710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x120f769d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x120f76c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x120f76f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x120f77210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x120f774d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x120f77790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x120f77a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x120f77d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x120f77fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x120f78290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x120f78550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x120f78810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x120f78ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x120f78d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x120f79050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x120f79310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x120f795d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x120f79890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x120f79b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x120f79e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x120f7a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x120f7a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x120f7a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x120f7a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x120f7abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x120f7ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x120f7b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x120f7b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x120f7b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x120f7b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x120f7bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x120f7bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x120f7c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x120f7c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x120f7c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x120f7ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x120f7ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x120f7cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x120f7d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x120f7d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x120f7d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x120f7da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x120f7dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x120f7e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x120f7e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x120f7e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x120f7e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x120f7eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x120f7edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x120f7f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x120f7f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x120f7f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x114f04080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x114f044f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x114f04960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x114f054b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x114f05770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x114f05a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x114f05ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x114f06310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x114f06780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x114f06bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x114f07060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x114f074d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x114f07940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x114f07db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x114f08220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x114f08690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x114f08b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x114f08f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x114f093e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x114f09850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x114f09cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x114f0a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x114f0a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x114f0aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x114f0ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x114f0b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x114f0b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x114f0bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x114f0c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x114f0c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x114f0c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x114f0cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x114f0d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x114f0d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x114f0dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x114f0df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x114f0e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x114f0e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x114f0eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x114f0f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x114f0f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x114f0f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x114f0fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x114f102d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x114f10740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x114f10bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x114f11020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x114f11490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x114f11900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x114f11d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x114f121e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x114f12650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x114f12ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x114f12f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x114f133a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x114f13810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x114f13c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x114f140f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x114f14560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x114f149d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x114f14e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x114f152b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x114f15720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x114f15b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x114f16000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x114f16470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x114f168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x114f16d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x114f171c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x114f17630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x114f17aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x114f17f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x114f18380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x114f187f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x114f18c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x114f190d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x114f19b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x114f1a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x114f1a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x114f1b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x114f1b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x114f1b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x114f1bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x114f1c3e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.918s
user	0m0.243s
sys	0m0.135s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
