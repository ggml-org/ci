Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:312 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.691s
user	0m0.923s
sys	0m1.294s
++ nproc
+ make -j10
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  1%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target build_info
[  5%] Built target sha256
[  5%] Built target sha1
[  5%] Built target xxhash
[  6%] Linking CXX shared library ../../bin/libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 18%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 20%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Built target llama-gguf
[ 25%] Built target llama-gguf-hash
[ 26%] Linking CXX shared library ../bin/libllama.dylib
[ 26%] Built target llama
[ 26%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 27%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 28%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 29%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 32%] Linking C executable ../bin/test-c
[ 32%] Linking CXX executable ../../bin/llama-simple-chat
[ 33%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-simple
[ 33%] Built target llava
[ 33%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 36%] Linking CXX static library libllava_static.a
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 37%] Linking CXX static library libcommon.a
[ 37%] Built target llama-simple
[ 37%] Built target test-c
[ 37%] Built target llama-quantize-stats
[ 37%] Built target llama-simple-chat
[ 37%] Built target llava_static
[ 37%] Built target common
[ 37%] Built target llava_shared
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-0
[ 43%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 47%] Linking CXX executable ../bin/test-arg-parser
[ 47%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Linking CXX executable ../bin/test-log
[ 49%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 49%] Linking CXX executable ../bin/test-sampling
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Built target test-tokenizer-0
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Built target test-arg-parser
[ 49%] Built target test-llama-grammar
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 49%] Built target test-json-schema-to-grammar
[ 49%] Built target test-log
[ 49%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Built target test-sampling
[ 50%] Built target test-grammar-parser
[ 50%] Built target test-grammar-integration
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-chat-template
[ 57%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 57%] Linking CXX executable ../bin/test-model-load-cancel
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-gguf
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-backend-ops
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 59%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 60%] Linking CXX executable ../bin/test-barrier
[ 61%] Linking CXX executable ../bin/test-autorelease
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 63%] Linking CXX executable ../../bin/llama-batched-bench
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Built target test-model-load-cancel
[ 64%] Built target test-gguf
[ 64%] Built target test-backend-ops
[ 64%] Built target test-chat-template
[ 64%] Built target test-barrier
[ 64%] Built target test-quantize-fns
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Built target test-autorelease
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Built target test-quantize-perf
[ 66%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 66%] Built target llama-batched-bench
[ 67%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 67%] Built target test-rope
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 70%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-eval-callback
[ 70%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-embedding
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Linking CXX executable ../../bin/llama-gritlm
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-batched
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-gguf-split
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Built target llama-embedding
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 77%] Built target llama-gritlm
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Built target llama-infill
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Built target llama-imatrix
[ 77%] Built target llama-bench
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Built target llama-lookahead
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 78%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 78%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 78%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 79%] Built target llama-lookup
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Built target llama-lookup-create
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Built target llama-lookup-merge
[ 81%] Built target llama-lookup-stats
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 82%] Generating loading.html.hpp
[ 83%] Generating index.html.gz.hpp
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Built target llama-cli
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Built target llama-parallel
[ 85%] Built target llama-passkey
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Built target llama-perplexity
[ 87%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 87%] Built target llama-quantize
[ 88%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-save-load-state
[ 88%] Built target llama-retrieval
[ 88%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 88%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-speculative
[ 89%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 89%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-run
[ 89%] Linking CXX executable ../../bin/llama-speculative-simple
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-tokenize
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Built target llama-save-load-state
[ 92%] Built target llama-speculative
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-run
[ 93%] Built target llama-speculative-simple
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Built target llama-tts
[ 95%] Built target llama-convert-llama2c-to-ggml
[ 95%] Built target llama-tokenize
[ 95%] Built target llama-gen-docs
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Built target llama-cvector-generator
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.051s
user	0m6.230s
sys	0m9.443s

main: quantize time =  5272.22 ms
main:    total time =  5272.22 ms

main: quantize time =  2343.29 ms
main:    total time =  2343.29 ms

main: quantize time =  1975.76 ms
main:    total time =  1975.76 ms

main: quantize time =  2388.72 ms
main:    total time =  2388.72 ms

main: quantize time =  1622.64 ms
main:    total time =  1622.64 ms

main: quantize time =  4966.14 ms
main:    total time =  4966.14 ms

main: quantize time =  5656.69 ms
main:    total time =  5656.69 ms

main: quantize time =  6794.51 ms
main:    total time =  6794.51 ms

main: quantize time =  5787.73 ms
main:    total time =  5787.73 ms

main: quantize time =  4361.34 ms
main:    total time =  4361.34 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.110 I build: 4568 (a4417ddd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.274 I main: llama backend init
0.00.000.281 I main: load the model and apply lora adapter, if any
0.00.053.585 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.065.630 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.065.647 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.065.649 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.065.650 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.065.651 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.065.651 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.065.662 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.065.664 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.065.665 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.065.665 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.065.666 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.065.667 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.065.667 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.065.668 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.065.671 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.065.671 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.065.672 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.072.613 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.074.802 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.081.685 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.081.691 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.081.692 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.081.693 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.081.693 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.081.694 I llama_model_loader: - type  f32:  194 tensors
0.00.081.695 I llama_model_loader: - type  f16:   98 tensors
0.00.081.696 I print_info: file format = GGUF V3 (latest)
0.00.081.697 I print_info: file type   = all F32 (guessed)
0.00.081.699 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.090.947 I load: special tokens cache size = 25
0.00.096.792 I load: token to piece cache size = 0.2984 MB
0.00.096.796 I print_info: arch             = gptneox
0.00.096.797 I print_info: vocab_only       = 0
0.00.096.797 I print_info: n_ctx_train      = 2048
0.00.096.797 I print_info: n_embd           = 2048
0.00.096.797 I print_info: n_layer          = 24
0.00.096.802 I print_info: n_head           = 16
0.00.096.803 I print_info: n_head_kv        = 16
0.00.096.803 I print_info: n_rot            = 32
0.00.096.803 I print_info: n_swa            = 0
0.00.096.803 I print_info: n_embd_head_k    = 128
0.00.096.805 I print_info: n_embd_head_v    = 128
0.00.096.806 I print_info: n_gqa            = 1
0.00.096.807 I print_info: n_embd_k_gqa     = 2048
0.00.096.807 I print_info: n_embd_v_gqa     = 2048
0.00.096.808 I print_info: f_norm_eps       = 1.0e-05
0.00.096.808 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.096.808 I print_info: f_clamp_kqv      = 0.0e+00
0.00.096.809 I print_info: f_max_alibi_bias = 0.0e+00
0.00.096.809 I print_info: f_logit_scale    = 0.0e+00
0.00.096.809 I print_info: n_ff             = 8192
0.00.096.810 I print_info: n_expert         = 0
0.00.096.810 I print_info: n_expert_used    = 0
0.00.096.810 I print_info: causal attn      = 1
0.00.096.810 I print_info: pooling type     = 0
0.00.096.810 I print_info: rope type        = 2
0.00.096.810 I print_info: rope scaling     = linear
0.00.096.811 I print_info: freq_base_train  = 10000.0
0.00.096.811 I print_info: freq_scale_train = 1
0.00.096.811 I print_info: n_ctx_orig_yarn  = 2048
0.00.096.812 I print_info: rope_finetuned   = unknown
0.00.096.812 I print_info: ssm_d_conv       = 0
0.00.096.812 I print_info: ssm_d_inner      = 0
0.00.096.814 I print_info: ssm_d_state      = 0
0.00.096.814 I print_info: ssm_dt_rank      = 0
0.00.096.814 I print_info: ssm_dt_b_c_rms   = 0
0.00.096.814 I print_info: model type       = 1.4B
0.00.096.815 I print_info: model params     = 1.41 B
0.00.096.816 I print_info: general.name     = 1.4B
0.00.096.817 I print_info: vocab type       = BPE
0.00.096.817 I print_info: n_vocab          = 50304
0.00.096.817 I print_info: n_merges         = 50009
0.00.096.817 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.096.817 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.096.818 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.096.818 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.096.818 I print_info: LF token         = 128 'Ä'
0.00.096.818 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.096.819 I print_info: max token length = 1024
0.00.136.956 I load_tensors: offloading 24 repeating layers to GPU
0.00.136.958 I load_tensors: offloading output layer to GPU
0.00.136.958 I load_tensors: offloaded 25/25 layers to GPU
0.00.136.978 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.136.980 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.137.527 I llama_init_from_model: n_seq_max     = 1
0.00.137.528 I llama_init_from_model: n_ctx         = 2048
0.00.137.528 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.137.528 I llama_init_from_model: n_batch       = 2048
0.00.137.528 I llama_init_from_model: n_ubatch      = 512
0.00.137.529 I llama_init_from_model: flash_attn    = 0
0.00.137.529 I llama_init_from_model: freq_base     = 10000.0
0.00.137.529 I llama_init_from_model: freq_scale    = 1
0.00.137.530 I ggml_metal_init: allocating
0.00.137.547 I ggml_metal_init: found device: Apple M4
0.00.137.553 I ggml_metal_init: picking default device: Apple M4
0.00.138.140 I ggml_metal_init: using embedded metal library
0.00.186.356 I ggml_metal_init: GPU name:   Apple M4
0.00.186.361 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.186.361 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.186.361 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.186.362 I ggml_metal_init: simdgroup reduction   = true
0.00.186.362 I ggml_metal_init: simdgroup matrix mul. = true
0.00.186.362 I ggml_metal_init: has residency sets    = true
0.00.186.362 I ggml_metal_init: has bfloat            = true
0.00.186.362 I ggml_metal_init: use bfloat            = true
0.00.186.363 I ggml_metal_init: hasUnifiedMemory      = true
0.00.186.364 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.212.526 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.240.571 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.240.577 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.240.600 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.244.363 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.244.365 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.244.365 I llama_init_from_model: graph nodes  = 967
0.00.244.365 I llama_init_from_model: graph splits = 2
0.00.244.372 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.244.505 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.244.506 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.308.825 I main: llama threadpool init, n_threads = 4
0.00.308.856 I 
0.00.308.883 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.308.884 I 
0.00.309.044 I sampler seed: 1234
0.00.309.048 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.309.072 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.309.074 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.309.074 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.189.394 I llama_perf_sampler_print:    sampling time =       1.46 ms /    71 runs   (    0.02 ms per token, 48797.25 tokens per second)
0.02.189.395 I llama_perf_context_print:        load time =     254.23 ms
0.02.189.396 I llama_perf_context_print: prompt eval time =      43.77 ms /     7 tokens (    6.25 ms per token,   159.93 tokens per second)
0.02.189.397 I llama_perf_context_print:        eval time =    1833.89 ms /    63 runs   (   29.11 ms per token,    34.35 tokens per second)
0.02.189.397 I llama_perf_context_print:       total time =    1881.58 ms /    70 tokens
0.02.189.657 I ggml_metal_free: deallocating

real	0m2.568s
user	0m0.122s
sys	0m0.128s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4568 (a4417ddd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.009.870 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.027.740 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.027.746 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.748 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.748 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.749 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.749 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.749 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.750 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.751 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.751 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.751 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.752 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.752 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.753 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.754 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.754 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.755 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.510 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.608 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.588 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.590 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.590 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.591 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.591 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.591 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.036.592 I llama_model_loader: - type  f32:  194 tensors
0.00.036.592 I llama_model_loader: - type q8_0:   98 tensors
0.00.036.593 I print_info: file format = GGUF V3 (latest)
0.00.036.594 I print_info: file type   = Q8_0
0.00.036.594 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.045.558 I load: special tokens cache size = 25
0.00.052.295 I load: token to piece cache size = 0.2984 MB
0.00.052.300 I print_info: arch             = gptneox
0.00.052.300 I print_info: vocab_only       = 0
0.00.052.301 I print_info: n_ctx_train      = 2048
0.00.052.301 I print_info: n_embd           = 2048
0.00.052.301 I print_info: n_layer          = 24
0.00.052.307 I print_info: n_head           = 16
0.00.052.308 I print_info: n_head_kv        = 16
0.00.052.308 I print_info: n_rot            = 32
0.00.052.308 I print_info: n_swa            = 0
0.00.052.308 I print_info: n_embd_head_k    = 128
0.00.052.309 I print_info: n_embd_head_v    = 128
0.00.052.309 I print_info: n_gqa            = 1
0.00.052.310 I print_info: n_embd_k_gqa     = 2048
0.00.052.311 I print_info: n_embd_v_gqa     = 2048
0.00.052.311 I print_info: f_norm_eps       = 1.0e-05
0.00.052.312 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.312 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.312 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.312 I print_info: f_logit_scale    = 0.0e+00
0.00.052.313 I print_info: n_ff             = 8192
0.00.052.313 I print_info: n_expert         = 0
0.00.052.313 I print_info: n_expert_used    = 0
0.00.052.313 I print_info: causal attn      = 1
0.00.052.313 I print_info: pooling type     = 0
0.00.052.314 I print_info: rope type        = 2
0.00.052.314 I print_info: rope scaling     = linear
0.00.052.314 I print_info: freq_base_train  = 10000.0
0.00.052.314 I print_info: freq_scale_train = 1
0.00.052.315 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.315 I print_info: rope_finetuned   = unknown
0.00.052.315 I print_info: ssm_d_conv       = 0
0.00.052.315 I print_info: ssm_d_inner      = 0
0.00.052.315 I print_info: ssm_d_state      = 0
0.00.052.316 I print_info: ssm_dt_rank      = 0
0.00.052.316 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.316 I print_info: model type       = 1.4B
0.00.052.317 I print_info: model params     = 1.41 B
0.00.052.317 I print_info: general.name     = 1.4B
0.00.052.318 I print_info: vocab type       = BPE
0.00.052.318 I print_info: n_vocab          = 50304
0.00.052.318 I print_info: n_merges         = 50009
0.00.052.318 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.320 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.320 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.323 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.323 I print_info: LF token         = 128 'Ä'
0.00.052.323 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.323 I print_info: max token length = 1024
0.01.174.221 I load_tensors: offloading 24 repeating layers to GPU
0.01.174.227 I load_tensors: offloading output layer to GPU
0.01.174.228 I load_tensors: offloaded 25/25 layers to GPU
0.01.174.253 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.174.254 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.174.887 I llama_init_from_model: n_seq_max     = 1
0.01.174.889 I llama_init_from_model: n_ctx         = 2048
0.01.174.889 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.174.890 I llama_init_from_model: n_batch       = 2048
0.01.174.890 I llama_init_from_model: n_ubatch      = 512
0.01.174.890 I llama_init_from_model: flash_attn    = 0
0.01.174.891 I llama_init_from_model: freq_base     = 10000.0
0.01.174.892 I llama_init_from_model: freq_scale    = 1
0.01.174.892 I ggml_metal_init: allocating
0.01.174.902 I ggml_metal_init: found device: Apple M4
0.01.174.910 I ggml_metal_init: picking default device: Apple M4
0.01.176.077 I ggml_metal_init: using embedded metal library
0.01.181.318 I ggml_metal_init: GPU name:   Apple M4
0.01.181.321 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.181.322 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.181.323 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.181.324 I ggml_metal_init: simdgroup reduction   = true
0.01.181.324 I ggml_metal_init: simdgroup matrix mul. = true
0.01.181.324 I ggml_metal_init: has residency sets    = true
0.01.181.324 I ggml_metal_init: has bfloat            = true
0.01.181.324 I ggml_metal_init: use bfloat            = true
0.01.181.325 I ggml_metal_init: hasUnifiedMemory      = true
0.01.181.330 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.196.369 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.250.392 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.250.401 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.250.424 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.255.337 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.255.339 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.255.339 I llama_init_from_model: graph nodes  = 967
0.01.255.339 I llama_init_from_model: graph splits = 2
0.01.255.344 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.255.470 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.255.471 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.310.084 I main: llama threadpool init, n_threads = 4
0.01.310.130 I 
0.01.310.153 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.310.154 I 
0.01.310.336 I sampler seed: 1234
0.01.310.340 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.310.384 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.310.388 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.310.388 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.404.647 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54783.95 tokens per second)
0.02.404.649 I llama_perf_context_print:        load time =    1299.34 ms
0.02.404.650 I llama_perf_context_print: prompt eval time =      49.17 ms /     7 tokens (    7.02 ms per token,   142.36 tokens per second)
0.02.404.652 I llama_perf_context_print:        eval time =    1042.13 ms /    63 runs   (   16.54 ms per token,    60.45 tokens per second)
0.02.404.653 I llama_perf_context_print:       total time =    1095.44 ms /    70 tokens
0.02.404.910 I ggml_metal_free: deallocating

real	0m2.424s
user	0m0.108s
sys	0m0.270s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4568 (a4417ddd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.015.842 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.039.658 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.039.665 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.667 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.667 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.669 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.670 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.670 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.671 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.671 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.672 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.673 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.675 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.676 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.676 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.680 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.680 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.680 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.865 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.179 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.231 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.051.233 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.234 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.051.234 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.051.234 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.051.235 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.051.235 I llama_model_loader: - type  f32:  194 tensors
0.00.051.236 I llama_model_loader: - type q4_0:   97 tensors
0.00.051.236 I llama_model_loader: - type q6_K:    1 tensors
0.00.051.236 I print_info: file format = GGUF V3 (latest)
0.00.051.237 I print_info: file type   = Q4_0
0.00.051.238 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.063.472 I load: special tokens cache size = 25
0.00.074.923 I load: token to piece cache size = 0.2984 MB
0.00.074.928 I print_info: arch             = gptneox
0.00.074.929 I print_info: vocab_only       = 0
0.00.074.929 I print_info: n_ctx_train      = 2048
0.00.074.930 I print_info: n_embd           = 2048
0.00.074.930 I print_info: n_layer          = 24
0.00.074.935 I print_info: n_head           = 16
0.00.074.937 I print_info: n_head_kv        = 16
0.00.074.937 I print_info: n_rot            = 32
0.00.074.937 I print_info: n_swa            = 0
0.00.074.938 I print_info: n_embd_head_k    = 128
0.00.074.941 I print_info: n_embd_head_v    = 128
0.00.074.943 I print_info: n_gqa            = 1
0.00.074.944 I print_info: n_embd_k_gqa     = 2048
0.00.074.946 I print_info: n_embd_v_gqa     = 2048
0.00.074.947 I print_info: f_norm_eps       = 1.0e-05
0.00.074.947 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.074.948 I print_info: f_clamp_kqv      = 0.0e+00
0.00.074.950 I print_info: f_max_alibi_bias = 0.0e+00
0.00.074.952 I print_info: f_logit_scale    = 0.0e+00
0.00.074.954 I print_info: n_ff             = 8192
0.00.074.954 I print_info: n_expert         = 0
0.00.074.954 I print_info: n_expert_used    = 0
0.00.074.954 I print_info: causal attn      = 1
0.00.074.955 I print_info: pooling type     = 0
0.00.074.955 I print_info: rope type        = 2
0.00.074.955 I print_info: rope scaling     = linear
0.00.074.956 I print_info: freq_base_train  = 10000.0
0.00.074.956 I print_info: freq_scale_train = 1
0.00.074.956 I print_info: n_ctx_orig_yarn  = 2048
0.00.074.957 I print_info: rope_finetuned   = unknown
0.00.074.959 I print_info: ssm_d_conv       = 0
0.00.074.959 I print_info: ssm_d_inner      = 0
0.00.074.965 I print_info: ssm_d_state      = 0
0.00.074.966 I print_info: ssm_dt_rank      = 0
0.00.074.966 I print_info: ssm_dt_b_c_rms   = 0
0.00.074.966 I print_info: model type       = 1.4B
0.00.074.967 I print_info: model params     = 1.41 B
0.00.074.967 I print_info: general.name     = 1.4B
0.00.074.968 I print_info: vocab type       = BPE
0.00.074.969 I print_info: n_vocab          = 50304
0.00.074.969 I print_info: n_merges         = 50009
0.00.074.969 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.074.970 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.074.970 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.074.971 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.074.971 I print_info: LF token         = 128 'Ä'
0.00.074.972 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.074.972 I print_info: max token length = 1024
0.00.628.549 I load_tensors: offloading 24 repeating layers to GPU
0.00.628.564 I load_tensors: offloading output layer to GPU
0.00.628.564 I load_tensors: offloaded 25/25 layers to GPU
0.00.628.598 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.628.606 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.630.176 I llama_init_from_model: n_seq_max     = 1
0.00.630.185 I llama_init_from_model: n_ctx         = 2048
0.00.630.186 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.630.186 I llama_init_from_model: n_batch       = 2048
0.00.630.187 I llama_init_from_model: n_ubatch      = 512
0.00.630.187 I llama_init_from_model: flash_attn    = 0
0.00.630.189 I llama_init_from_model: freq_base     = 10000.0
0.00.630.190 I llama_init_from_model: freq_scale    = 1
0.00.630.192 I ggml_metal_init: allocating
0.00.630.270 I ggml_metal_init: found device: Apple M4
0.00.630.285 I ggml_metal_init: picking default device: Apple M4
0.00.632.176 I ggml_metal_init: using embedded metal library
0.00.638.316 I ggml_metal_init: GPU name:   Apple M4
0.00.638.321 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.638.322 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.638.323 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.638.324 I ggml_metal_init: simdgroup reduction   = true
0.00.638.325 I ggml_metal_init: simdgroup matrix mul. = true
0.00.638.325 I ggml_metal_init: has residency sets    = true
0.00.638.325 I ggml_metal_init: has bfloat            = true
0.00.638.325 I ggml_metal_init: use bfloat            = true
0.00.638.327 I ggml_metal_init: hasUnifiedMemory      = true
0.00.638.329 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.657.135 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.715.990 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.715.997 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.716.029 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.721.229 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.721.231 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.721.231 I llama_init_from_model: graph nodes  = 967
0.00.721.232 I llama_init_from_model: graph splits = 2
0.00.721.237 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.721.368 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.721.368 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.778.730 I main: llama threadpool init, n_threads = 4
0.00.778.772 I 
0.00.778.799 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.778.800 I 
0.00.778.951 I sampler seed: 1234
0.00.778.956 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.778.967 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.778.967 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.778.967 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.472.429 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51862.67 tokens per second)
0.01.472.430 I llama_perf_context_print:        load time =     762.00 ms
0.01.472.432 I llama_perf_context_print: prompt eval time =      49.25 ms /     7 tokens (    7.04 ms per token,   142.12 tokens per second)
0.01.472.433 I llama_perf_context_print:        eval time =     641.29 ms /    63 runs   (   10.18 ms per token,    98.24 tokens per second)
0.01.472.433 I llama_perf_context_print:       total time =     694.58 ms /    70 tokens
0.01.472.722 I ggml_metal_free: deallocating

real	0m1.506s
user	0m0.125s
sys	0m0.202s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4568 (a4417ddd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.812 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.027.818 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.027.823 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.824 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.824 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.825 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.825 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.825 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.828 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.828 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.828 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.829 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.829 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.830 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.830 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.833 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.833 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.833 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.835 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.884 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.844 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.845 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.845 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.846 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.846 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.846 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.036.847 I llama_model_loader: - type  f32:  194 tensors
0.00.036.847 I llama_model_loader: - type q4_1:   97 tensors
0.00.036.847 I llama_model_loader: - type q6_K:    1 tensors
0.00.036.848 I print_info: file format = GGUF V3 (latest)
0.00.036.848 I print_info: file type   = Q4_1
0.00.036.849 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.045.454 I load: special tokens cache size = 25
0.00.052.490 I load: token to piece cache size = 0.2984 MB
0.00.052.493 I print_info: arch             = gptneox
0.00.052.493 I print_info: vocab_only       = 0
0.00.052.494 I print_info: n_ctx_train      = 2048
0.00.052.494 I print_info: n_embd           = 2048
0.00.052.494 I print_info: n_layer          = 24
0.00.052.496 I print_info: n_head           = 16
0.00.052.497 I print_info: n_head_kv        = 16
0.00.052.498 I print_info: n_rot            = 32
0.00.052.498 I print_info: n_swa            = 0
0.00.052.498 I print_info: n_embd_head_k    = 128
0.00.052.498 I print_info: n_embd_head_v    = 128
0.00.052.499 I print_info: n_gqa            = 1
0.00.052.502 I print_info: n_embd_k_gqa     = 2048
0.00.052.502 I print_info: n_embd_v_gqa     = 2048
0.00.052.503 I print_info: f_norm_eps       = 1.0e-05
0.00.052.503 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.504 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.504 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.504 I print_info: f_logit_scale    = 0.0e+00
0.00.052.505 I print_info: n_ff             = 8192
0.00.052.505 I print_info: n_expert         = 0
0.00.052.505 I print_info: n_expert_used    = 0
0.00.052.505 I print_info: causal attn      = 1
0.00.052.505 I print_info: pooling type     = 0
0.00.052.507 I print_info: rope type        = 2
0.00.052.508 I print_info: rope scaling     = linear
0.00.052.509 I print_info: freq_base_train  = 10000.0
0.00.052.509 I print_info: freq_scale_train = 1
0.00.052.509 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.509 I print_info: rope_finetuned   = unknown
0.00.052.510 I print_info: ssm_d_conv       = 0
0.00.052.510 I print_info: ssm_d_inner      = 0
0.00.052.510 I print_info: ssm_d_state      = 0
0.00.052.510 I print_info: ssm_dt_rank      = 0
0.00.052.510 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.510 I print_info: model type       = 1.4B
0.00.052.511 I print_info: model params     = 1.41 B
0.00.052.511 I print_info: general.name     = 1.4B
0.00.052.515 I print_info: vocab type       = BPE
0.00.052.515 I print_info: n_vocab          = 50304
0.00.052.515 I print_info: n_merges         = 50009
0.00.052.516 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.516 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.516 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.516 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.517 I print_info: LF token         = 128 'Ä'
0.00.052.517 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.517 I print_info: max token length = 1024
0.00.642.450 I load_tensors: offloading 24 repeating layers to GPU
0.00.642.465 I load_tensors: offloading output layer to GPU
0.00.642.466 I load_tensors: offloaded 25/25 layers to GPU
0.00.642.500 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.642.502 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.643.864 I llama_init_from_model: n_seq_max     = 1
0.00.643.871 I llama_init_from_model: n_ctx         = 2048
0.00.643.871 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.643.872 I llama_init_from_model: n_batch       = 2048
0.00.643.872 I llama_init_from_model: n_ubatch      = 512
0.00.643.873 I llama_init_from_model: flash_attn    = 0
0.00.643.875 I llama_init_from_model: freq_base     = 10000.0
0.00.643.875 I llama_init_from_model: freq_scale    = 1
0.00.643.881 I ggml_metal_init: allocating
0.00.643.964 I ggml_metal_init: found device: Apple M4
0.00.643.979 I ggml_metal_init: picking default device: Apple M4
0.00.645.727 I ggml_metal_init: using embedded metal library
0.00.651.106 I ggml_metal_init: GPU name:   Apple M4
0.00.651.112 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.651.113 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.651.114 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.651.115 I ggml_metal_init: simdgroup reduction   = true
0.00.651.115 I ggml_metal_init: simdgroup matrix mul. = true
0.00.651.115 I ggml_metal_init: has residency sets    = true
0.00.651.116 I ggml_metal_init: has bfloat            = true
0.00.651.116 I ggml_metal_init: use bfloat            = true
0.00.651.117 I ggml_metal_init: hasUnifiedMemory      = true
0.00.651.119 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.670.450 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.724.110 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.724.118 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.724.150 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.728.510 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.728.512 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.728.512 I llama_init_from_model: graph nodes  = 967
0.00.728.513 I llama_init_from_model: graph splits = 2
0.00.728.518 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.728.642 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.728.642 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.782.231 I main: llama threadpool init, n_threads = 4
0.00.782.275 I 
0.00.782.300 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.782.300 I 
0.00.782.469 I sampler seed: 1234
0.00.782.474 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.782.493 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.782.493 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.782.494 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.523.613 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 56126.48 tokens per second)
0.01.523.614 I llama_perf_context_print:        load time =     772.56 ms
0.01.523.615 I llama_perf_context_print: prompt eval time =      49.67 ms /     7 tokens (    7.10 ms per token,   140.92 tokens per second)
0.01.523.616 I llama_perf_context_print:        eval time =     688.62 ms /    63 runs   (   10.93 ms per token,    91.49 tokens per second)
0.01.523.616 I llama_perf_context_print:       total time =     742.24 ms /    70 tokens
0.01.523.856 I ggml_metal_free: deallocating

real	0m1.541s
user	0m0.111s
sys	0m0.193s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4568 (a4417ddd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.791 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.030.116 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.030.121 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.122 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.123 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.123 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.123 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.127 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.128 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.128 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.129 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.129 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.129 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.130 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.130 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.131 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.131 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.132 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.144 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.219 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.265 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.039.267 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.267 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.268 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.268 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.268 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.039.269 I llama_model_loader: - type  f32:  194 tensors
0.00.039.269 I llama_model_loader: - type q5_0:   97 tensors
0.00.039.269 I llama_model_loader: - type q6_K:    1 tensors
0.00.039.270 I print_info: file format = GGUF V3 (latest)
0.00.039.270 I print_info: file type   = Q5_0
0.00.039.271 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.048.431 I load: special tokens cache size = 25
0.00.055.424 I load: token to piece cache size = 0.2984 MB
0.00.055.426 I print_info: arch             = gptneox
0.00.055.427 I print_info: vocab_only       = 0
0.00.055.432 I print_info: n_ctx_train      = 2048
0.00.055.433 I print_info: n_embd           = 2048
0.00.055.433 I print_info: n_layer          = 24
0.00.055.436 I print_info: n_head           = 16
0.00.055.436 I print_info: n_head_kv        = 16
0.00.055.437 I print_info: n_rot            = 32
0.00.055.437 I print_info: n_swa            = 0
0.00.055.437 I print_info: n_embd_head_k    = 128
0.00.055.437 I print_info: n_embd_head_v    = 128
0.00.055.438 I print_info: n_gqa            = 1
0.00.055.439 I print_info: n_embd_k_gqa     = 2048
0.00.055.439 I print_info: n_embd_v_gqa     = 2048
0.00.055.440 I print_info: f_norm_eps       = 1.0e-05
0.00.055.440 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.055.440 I print_info: f_clamp_kqv      = 0.0e+00
0.00.055.441 I print_info: f_max_alibi_bias = 0.0e+00
0.00.055.441 I print_info: f_logit_scale    = 0.0e+00
0.00.055.441 I print_info: n_ff             = 8192
0.00.055.442 I print_info: n_expert         = 0
0.00.055.442 I print_info: n_expert_used    = 0
0.00.055.442 I print_info: causal attn      = 1
0.00.055.442 I print_info: pooling type     = 0
0.00.055.444 I print_info: rope type        = 2
0.00.055.445 I print_info: rope scaling     = linear
0.00.055.446 I print_info: freq_base_train  = 10000.0
0.00.055.446 I print_info: freq_scale_train = 1
0.00.055.446 I print_info: n_ctx_orig_yarn  = 2048
0.00.055.446 I print_info: rope_finetuned   = unknown
0.00.055.447 I print_info: ssm_d_conv       = 0
0.00.055.448 I print_info: ssm_d_inner      = 0
0.00.055.448 I print_info: ssm_d_state      = 0
0.00.055.448 I print_info: ssm_dt_rank      = 0
0.00.055.448 I print_info: ssm_dt_b_c_rms   = 0
0.00.055.448 I print_info: model type       = 1.4B
0.00.055.449 I print_info: model params     = 1.41 B
0.00.055.449 I print_info: general.name     = 1.4B
0.00.055.450 I print_info: vocab type       = BPE
0.00.055.450 I print_info: n_vocab          = 50304
0.00.055.450 I print_info: n_merges         = 50009
0.00.055.450 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.055.450 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.055.450 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.055.451 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.055.451 I print_info: LF token         = 128 'Ä'
0.00.055.451 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.055.451 I print_info: max token length = 1024
0.00.744.252 I load_tensors: offloading 24 repeating layers to GPU
0.00.744.263 I load_tensors: offloading output layer to GPU
0.00.744.264 I load_tensors: offloaded 25/25 layers to GPU
0.00.744.298 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.744.304 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.745.747 I llama_init_from_model: n_seq_max     = 1
0.00.745.752 I llama_init_from_model: n_ctx         = 2048
0.00.745.752 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.745.753 I llama_init_from_model: n_batch       = 2048
0.00.745.753 I llama_init_from_model: n_ubatch      = 512
0.00.745.754 I llama_init_from_model: flash_attn    = 0
0.00.745.756 I llama_init_from_model: freq_base     = 10000.0
0.00.745.756 I llama_init_from_model: freq_scale    = 1
0.00.745.763 I ggml_metal_init: allocating
0.00.745.836 I ggml_metal_init: found device: Apple M4
0.00.745.850 I ggml_metal_init: picking default device: Apple M4
0.00.747.637 I ggml_metal_init: using embedded metal library
0.00.754.195 I ggml_metal_init: GPU name:   Apple M4
0.00.754.199 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.754.200 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.754.201 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.754.202 I ggml_metal_init: simdgroup reduction   = true
0.00.754.202 I ggml_metal_init: simdgroup matrix mul. = true
0.00.754.202 I ggml_metal_init: has residency sets    = true
0.00.754.202 I ggml_metal_init: has bfloat            = true
0.00.754.203 I ggml_metal_init: use bfloat            = true
0.00.754.203 I ggml_metal_init: hasUnifiedMemory      = true
0.00.754.205 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.771.922 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.825.009 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.825.017 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.825.040 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.829.186 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.829.188 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.829.189 I llama_init_from_model: graph nodes  = 967
0.00.829.189 I llama_init_from_model: graph splits = 2
0.00.829.201 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.829.323 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.829.324 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.886.789 I main: llama threadpool init, n_threads = 4
0.00.886.843 I 
0.00.886.868 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.886.869 I 
0.00.887.027 I sampler seed: 1234
0.00.887.031 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.887.079 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.887.082 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.887.082 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.691.361 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55252.92 tokens per second)
0.01.691.362 I llama_perf_context_print:        load time =     877.13 ms
0.01.691.363 I llama_perf_context_print: prompt eval time =      53.71 ms /     7 tokens (    7.67 ms per token,   130.33 tokens per second)
0.01.691.364 I llama_perf_context_print:        eval time =     747.71 ms /    63 runs   (   11.87 ms per token,    84.26 tokens per second)
0.01.691.364 I llama_perf_context_print:       total time =     805.44 ms /    70 tokens
0.01.691.601 I ggml_metal_free: deallocating

real	0m1.712s
user	0m0.113s
sys	0m0.208s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4568 (a4417ddd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.010.010 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.432 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.437 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.438 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.439 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.439 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.440 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.440 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.441 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.441 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.442 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.442 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.442 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.443 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.443 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.448 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.449 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.449 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.309 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.352 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.231 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.232 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.232 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.233 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.233 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.233 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.234 I llama_model_loader: - type  f32:  194 tensors
0.00.026.234 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.234 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.235 I print_info: file format = GGUF V3 (latest)
0.00.026.235 I print_info: file type   = Q5_1
0.00.026.241 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.433 I load: special tokens cache size = 25
0.00.040.399 I load: token to piece cache size = 0.2984 MB
0.00.040.402 I print_info: arch             = gptneox
0.00.040.402 I print_info: vocab_only       = 0
0.00.040.402 I print_info: n_ctx_train      = 2048
0.00.040.402 I print_info: n_embd           = 2048
0.00.040.402 I print_info: n_layer          = 24
0.00.040.406 I print_info: n_head           = 16
0.00.040.406 I print_info: n_head_kv        = 16
0.00.040.407 I print_info: n_rot            = 32
0.00.040.407 I print_info: n_swa            = 0
0.00.040.407 I print_info: n_embd_head_k    = 128
0.00.040.407 I print_info: n_embd_head_v    = 128
0.00.040.408 I print_info: n_gqa            = 1
0.00.040.409 I print_info: n_embd_k_gqa     = 2048
0.00.040.409 I print_info: n_embd_v_gqa     = 2048
0.00.040.410 I print_info: f_norm_eps       = 1.0e-05
0.00.040.410 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.410 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.411 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.411 I print_info: f_logit_scale    = 0.0e+00
0.00.040.411 I print_info: n_ff             = 8192
0.00.040.412 I print_info: n_expert         = 0
0.00.040.412 I print_info: n_expert_used    = 0
0.00.040.412 I print_info: causal attn      = 1
0.00.040.412 I print_info: pooling type     = 0
0.00.040.414 I print_info: rope type        = 2
0.00.040.415 I print_info: rope scaling     = linear
0.00.040.416 I print_info: freq_base_train  = 10000.0
0.00.040.416 I print_info: freq_scale_train = 1
0.00.040.416 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.416 I print_info: rope_finetuned   = unknown
0.00.040.417 I print_info: ssm_d_conv       = 0
0.00.040.417 I print_info: ssm_d_inner      = 0
0.00.040.417 I print_info: ssm_d_state      = 0
0.00.040.417 I print_info: ssm_dt_rank      = 0
0.00.040.417 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.417 I print_info: model type       = 1.4B
0.00.040.418 I print_info: model params     = 1.41 B
0.00.040.418 I print_info: general.name     = 1.4B
0.00.040.418 I print_info: vocab type       = BPE
0.00.040.419 I print_info: n_vocab          = 50304
0.00.040.419 I print_info: n_merges         = 50009
0.00.040.419 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.419 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.421 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.421 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.421 I print_info: LF token         = 128 'Ä'
0.00.040.421 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.422 I print_info: max token length = 1024
0.00.603.269 I load_tensors: offloading 24 repeating layers to GPU
0.00.603.285 I load_tensors: offloading output layer to GPU
0.00.603.286 I load_tensors: offloaded 25/25 layers to GPU
0.00.603.320 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.603.322 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.604.726 I llama_init_from_model: n_seq_max     = 1
0.00.604.731 I llama_init_from_model: n_ctx         = 2048
0.00.604.731 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.604.732 I llama_init_from_model: n_batch       = 2048
0.00.604.732 I llama_init_from_model: n_ubatch      = 512
0.00.604.732 I llama_init_from_model: flash_attn    = 0
0.00.604.734 I llama_init_from_model: freq_base     = 10000.0
0.00.604.735 I llama_init_from_model: freq_scale    = 1
0.00.604.742 I ggml_metal_init: allocating
0.00.604.818 I ggml_metal_init: found device: Apple M4
0.00.604.832 I ggml_metal_init: picking default device: Apple M4
0.00.606.406 I ggml_metal_init: using embedded metal library
0.00.612.787 I ggml_metal_init: GPU name:   Apple M4
0.00.612.791 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.612.792 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.612.793 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.612.794 I ggml_metal_init: simdgroup reduction   = true
0.00.612.794 I ggml_metal_init: simdgroup matrix mul. = true
0.00.612.794 I ggml_metal_init: has residency sets    = true
0.00.612.795 I ggml_metal_init: has bfloat            = true
0.00.612.795 I ggml_metal_init: use bfloat            = true
0.00.612.796 I ggml_metal_init: hasUnifiedMemory      = true
0.00.612.797 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.629.807 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.682.097 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.682.104 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.682.127 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.686.710 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.686.712 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.686.712 I llama_init_from_model: graph nodes  = 967
0.00.686.712 I llama_init_from_model: graph splits = 2
0.00.686.722 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.686.854 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.686.855 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.743.615 I main: llama threadpool init, n_threads = 4
0.00.743.660 I 
0.00.743.682 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.743.682 I 
0.00.743.858 I sampler seed: 1234
0.00.743.863 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.743.873 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.743.874 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.743.874 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.585.066 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52866.72 tokens per second)
0.01.585.066 I llama_perf_context_print:        load time =     732.74 ms
0.01.585.067 I llama_perf_context_print: prompt eval time =      42.24 ms /     7 tokens (    6.03 ms per token,   165.70 tokens per second)
0.01.585.068 I llama_perf_context_print:        eval time =     796.02 ms /    63 runs   (   12.64 ms per token,    79.14 tokens per second)
0.01.585.068 I llama_perf_context_print:       total time =     842.31 ms /    70 tokens
0.01.585.322 I ggml_metal_free: deallocating

real	0m1.608s
user	0m0.109s
sys	0m0.217s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4568 (a4417ddd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.009.491 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.062 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.068 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.070 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.070 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.071 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.071 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.071 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.072 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.073 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.073 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.073 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.074 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.074 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.075 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.077 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.077 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.077 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.881 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.019 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.873 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.874 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.874 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.875 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.875 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.876 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.876 I llama_model_loader: - type  f32:  194 tensors
0.00.024.877 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.877 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.877 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.878 I print_info: file format = GGUF V3 (latest)
0.00.024.878 I print_info: file type   = Q2_K - Medium
0.00.024.880 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.130 I load: special tokens cache size = 25
0.00.039.144 I load: token to piece cache size = 0.2984 MB
0.00.039.148 I print_info: arch             = gptneox
0.00.039.148 I print_info: vocab_only       = 0
0.00.039.149 I print_info: n_ctx_train      = 2048
0.00.039.149 I print_info: n_embd           = 2048
0.00.039.149 I print_info: n_layer          = 24
0.00.039.153 I print_info: n_head           = 16
0.00.039.154 I print_info: n_head_kv        = 16
0.00.039.154 I print_info: n_rot            = 32
0.00.039.154 I print_info: n_swa            = 0
0.00.039.154 I print_info: n_embd_head_k    = 128
0.00.039.154 I print_info: n_embd_head_v    = 128
0.00.039.155 I print_info: n_gqa            = 1
0.00.039.156 I print_info: n_embd_k_gqa     = 2048
0.00.039.159 I print_info: n_embd_v_gqa     = 2048
0.00.039.159 I print_info: f_norm_eps       = 1.0e-05
0.00.039.160 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.160 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.160 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.160 I print_info: f_logit_scale    = 0.0e+00
0.00.039.161 I print_info: n_ff             = 8192
0.00.039.161 I print_info: n_expert         = 0
0.00.039.161 I print_info: n_expert_used    = 0
0.00.039.161 I print_info: causal attn      = 1
0.00.039.161 I print_info: pooling type     = 0
0.00.039.162 I print_info: rope type        = 2
0.00.039.162 I print_info: rope scaling     = linear
0.00.039.163 I print_info: freq_base_train  = 10000.0
0.00.039.163 I print_info: freq_scale_train = 1
0.00.039.164 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.164 I print_info: rope_finetuned   = unknown
0.00.039.164 I print_info: ssm_d_conv       = 0
0.00.039.165 I print_info: ssm_d_inner      = 0
0.00.039.165 I print_info: ssm_d_state      = 0
0.00.039.165 I print_info: ssm_dt_rank      = 0
0.00.039.165 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.166 I print_info: model type       = 1.4B
0.00.039.166 I print_info: model params     = 1.41 B
0.00.039.166 I print_info: general.name     = 1.4B
0.00.039.167 I print_info: vocab type       = BPE
0.00.039.167 I print_info: n_vocab          = 50304
0.00.039.167 I print_info: n_merges         = 50009
0.00.039.167 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.167 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.168 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.168 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.168 I print_info: LF token         = 128 'Ä'
0.00.039.168 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.169 I print_info: max token length = 1024
0.00.337.409 I load_tensors: offloading 24 repeating layers to GPU
0.00.337.414 I load_tensors: offloading output layer to GPU
0.00.337.414 I load_tensors: offloaded 25/25 layers to GPU
0.00.337.439 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.337.441 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.338.682 I llama_init_from_model: n_seq_max     = 1
0.00.338.694 I llama_init_from_model: n_ctx         = 2048
0.00.338.694 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.338.695 I llama_init_from_model: n_batch       = 2048
0.00.338.695 I llama_init_from_model: n_ubatch      = 512
0.00.338.695 I llama_init_from_model: flash_attn    = 0
0.00.338.698 I llama_init_from_model: freq_base     = 10000.0
0.00.338.698 I llama_init_from_model: freq_scale    = 1
0.00.338.702 I ggml_metal_init: allocating
0.00.338.829 I ggml_metal_init: found device: Apple M4
0.00.338.846 I ggml_metal_init: picking default device: Apple M4
0.00.340.231 I ggml_metal_init: using embedded metal library
0.00.344.412 I ggml_metal_init: GPU name:   Apple M4
0.00.344.420 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.344.421 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.344.421 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.344.422 I ggml_metal_init: simdgroup reduction   = true
0.00.344.422 I ggml_metal_init: simdgroup matrix mul. = true
0.00.344.422 I ggml_metal_init: has residency sets    = true
0.00.344.423 I ggml_metal_init: has bfloat            = true
0.00.344.423 I ggml_metal_init: use bfloat            = true
0.00.344.424 I ggml_metal_init: hasUnifiedMemory      = true
0.00.344.426 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.360.824 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.392.637 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.392.642 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.392.666 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.396.863 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.396.865 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.396.865 I llama_init_from_model: graph nodes  = 967
0.00.396.865 I llama_init_from_model: graph splits = 2
0.00.396.872 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.396.997 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.396.998 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.453.527 I main: llama threadpool init, n_threads = 4
0.00.453.566 I 
0.00.453.589 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.453.589 I 
0.00.453.768 I sampler seed: 1234
0.00.453.773 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.453.811 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.453.812 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.453.812 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.127.072 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50354.61 tokens per second)
0.01.127.073 I llama_perf_context_print:        load time =     443.05 ms
0.01.127.073 I llama_perf_context_print: prompt eval time =      35.74 ms /     7 tokens (    5.11 ms per token,   195.86 tokens per second)
0.01.127.074 I llama_perf_context_print:        eval time =     634.65 ms /    63 runs   (   10.07 ms per token,    99.27 tokens per second)
0.01.127.074 I llama_perf_context_print:       total time =     674.53 ms /    70 tokens
0.01.127.276 I ggml_metal_free: deallocating

real	0m1.146s
user	0m0.107s
sys	0m0.123s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4568 (a4417ddd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.110 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.817 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.828 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.829 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.830 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.830 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.830 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.831 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.833 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.833 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.834 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.834 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.834 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.835 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.835 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.837 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.837 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.837 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.677 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.683 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.455 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.456 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.457 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.457 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.457 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.458 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.458 I llama_model_loader: - type  f32:  194 tensors
0.00.025.458 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.459 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.459 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.459 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.460 I print_info: file format = GGUF V3 (latest)
0.00.025.460 I print_info: file type   = Q3_K - Medium
0.00.025.461 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.290 I load: special tokens cache size = 25
0.00.039.283 I load: token to piece cache size = 0.2984 MB
0.00.039.286 I print_info: arch             = gptneox
0.00.039.286 I print_info: vocab_only       = 0
0.00.039.286 I print_info: n_ctx_train      = 2048
0.00.039.286 I print_info: n_embd           = 2048
0.00.039.287 I print_info: n_layer          = 24
0.00.039.289 I print_info: n_head           = 16
0.00.039.290 I print_info: n_head_kv        = 16
0.00.039.290 I print_info: n_rot            = 32
0.00.039.291 I print_info: n_swa            = 0
0.00.039.291 I print_info: n_embd_head_k    = 128
0.00.039.293 I print_info: n_embd_head_v    = 128
0.00.039.293 I print_info: n_gqa            = 1
0.00.039.294 I print_info: n_embd_k_gqa     = 2048
0.00.039.295 I print_info: n_embd_v_gqa     = 2048
0.00.039.295 I print_info: f_norm_eps       = 1.0e-05
0.00.039.296 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.296 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.296 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.296 I print_info: f_logit_scale    = 0.0e+00
0.00.039.297 I print_info: n_ff             = 8192
0.00.039.297 I print_info: n_expert         = 0
0.00.039.297 I print_info: n_expert_used    = 0
0.00.039.298 I print_info: causal attn      = 1
0.00.039.298 I print_info: pooling type     = 0
0.00.039.298 I print_info: rope type        = 2
0.00.039.298 I print_info: rope scaling     = linear
0.00.039.298 I print_info: freq_base_train  = 10000.0
0.00.039.299 I print_info: freq_scale_train = 1
0.00.039.299 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.299 I print_info: rope_finetuned   = unknown
0.00.039.299 I print_info: ssm_d_conv       = 0
0.00.039.299 I print_info: ssm_d_inner      = 0
0.00.039.300 I print_info: ssm_d_state      = 0
0.00.039.300 I print_info: ssm_dt_rank      = 0
0.00.039.302 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.302 I print_info: model type       = 1.4B
0.00.039.302 I print_info: model params     = 1.41 B
0.00.039.302 I print_info: general.name     = 1.4B
0.00.039.303 I print_info: vocab type       = BPE
0.00.039.303 I print_info: n_vocab          = 50304
0.00.039.303 I print_info: n_merges         = 50009
0.00.039.303 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.305 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.305 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.305 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.305 I print_info: LF token         = 128 'Ä'
0.00.039.306 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.306 I print_info: max token length = 1024
0.00.431.695 I load_tensors: offloading 24 repeating layers to GPU
0.00.431.710 I load_tensors: offloading output layer to GPU
0.00.431.710 I load_tensors: offloaded 25/25 layers to GPU
0.00.431.745 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.431.746 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.433.208 I llama_init_from_model: n_seq_max     = 1
0.00.433.213 I llama_init_from_model: n_ctx         = 2048
0.00.433.213 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.433.214 I llama_init_from_model: n_batch       = 2048
0.00.433.214 I llama_init_from_model: n_ubatch      = 512
0.00.433.214 I llama_init_from_model: flash_attn    = 0
0.00.433.216 I llama_init_from_model: freq_base     = 10000.0
0.00.433.217 I llama_init_from_model: freq_scale    = 1
0.00.433.219 I ggml_metal_init: allocating
0.00.433.293 I ggml_metal_init: found device: Apple M4
0.00.433.311 I ggml_metal_init: picking default device: Apple M4
0.00.435.117 I ggml_metal_init: using embedded metal library
0.00.440.740 I ggml_metal_init: GPU name:   Apple M4
0.00.440.745 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.440.745 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.440.746 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.440.747 I ggml_metal_init: simdgroup reduction   = true
0.00.440.747 I ggml_metal_init: simdgroup matrix mul. = true
0.00.440.748 I ggml_metal_init: has residency sets    = true
0.00.440.748 I ggml_metal_init: has bfloat            = true
0.00.440.748 I ggml_metal_init: use bfloat            = true
0.00.440.749 I ggml_metal_init: hasUnifiedMemory      = true
0.00.440.751 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.459.855 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.514.472 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.514.480 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.514.503 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.518.601 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.518.603 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.518.604 I llama_init_from_model: graph nodes  = 967
0.00.518.604 I llama_init_from_model: graph splits = 2
0.00.518.610 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.518.742 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.518.743 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.572.260 I main: llama threadpool init, n_threads = 4
0.00.572.309 I 
0.00.572.331 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.572.333 I 
0.00.572.487 I sampler seed: 1234
0.00.572.491 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.572.511 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.572.511 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.572.511 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.312.767 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53024.65 tokens per second)
0.01.312.768 I llama_perf_context_print:        load time =     562.28 ms
0.01.312.769 I llama_perf_context_print: prompt eval time =      40.12 ms /     7 tokens (    5.73 ms per token,   174.45 tokens per second)
0.01.312.769 I llama_perf_context_print:        eval time =     697.22 ms /    63 runs   (   11.07 ms per token,    90.36 tokens per second)
0.01.312.770 I llama_perf_context_print:       total time =     741.38 ms /    70 tokens
0.01.312.996 I ggml_metal_free: deallocating

real	0m1.331s
user	0m0.109s
sys	0m0.172s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4568 (a4417ddd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.013.074 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.277 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.020.282 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.288 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.288 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.289 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.289 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.289 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.290 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.291 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.291 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.291 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.292 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.292 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.292 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.294 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.294 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.294 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.035 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.027 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.725 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.726 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.726 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.727 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.727 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.727 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.028.728 I llama_model_loader: - type  f32:  194 tensors
0.00.028.728 I llama_model_loader: - type q4_K:   61 tensors
0.00.028.728 I llama_model_loader: - type q5_K:   24 tensors
0.00.028.729 I llama_model_loader: - type q6_K:   13 tensors
0.00.028.729 I print_info: file format = GGUF V3 (latest)
0.00.028.730 I print_info: file type   = Q4_K - Medium
0.00.028.734 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.036.561 I load: special tokens cache size = 25
0.00.042.303 I load: token to piece cache size = 0.2984 MB
0.00.042.305 I print_info: arch             = gptneox
0.00.042.306 I print_info: vocab_only       = 0
0.00.042.306 I print_info: n_ctx_train      = 2048
0.00.042.306 I print_info: n_embd           = 2048
0.00.042.306 I print_info: n_layer          = 24
0.00.042.309 I print_info: n_head           = 16
0.00.042.310 I print_info: n_head_kv        = 16
0.00.042.310 I print_info: n_rot            = 32
0.00.042.310 I print_info: n_swa            = 0
0.00.042.310 I print_info: n_embd_head_k    = 128
0.00.042.312 I print_info: n_embd_head_v    = 128
0.00.042.313 I print_info: n_gqa            = 1
0.00.042.314 I print_info: n_embd_k_gqa     = 2048
0.00.042.319 I print_info: n_embd_v_gqa     = 2048
0.00.042.321 I print_info: f_norm_eps       = 1.0e-05
0.00.042.322 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.322 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.322 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.322 I print_info: f_logit_scale    = 0.0e+00
0.00.042.323 I print_info: n_ff             = 8192
0.00.042.324 I print_info: n_expert         = 0
0.00.042.324 I print_info: n_expert_used    = 0
0.00.042.324 I print_info: causal attn      = 1
0.00.042.324 I print_info: pooling type     = 0
0.00.042.327 I print_info: rope type        = 2
0.00.042.328 I print_info: rope scaling     = linear
0.00.042.328 I print_info: freq_base_train  = 10000.0
0.00.042.329 I print_info: freq_scale_train = 1
0.00.042.329 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.329 I print_info: rope_finetuned   = unknown
0.00.042.329 I print_info: ssm_d_conv       = 0
0.00.042.329 I print_info: ssm_d_inner      = 0
0.00.042.329 I print_info: ssm_d_state      = 0
0.00.042.330 I print_info: ssm_dt_rank      = 0
0.00.042.330 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.330 I print_info: model type       = 1.4B
0.00.042.331 I print_info: model params     = 1.41 B
0.00.042.331 I print_info: general.name     = 1.4B
0.00.042.331 I print_info: vocab type       = BPE
0.00.042.331 I print_info: n_vocab          = 50304
0.00.042.332 I print_info: n_merges         = 50009
0.00.042.334 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.334 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.334 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.335 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.335 I print_info: LF token         = 128 'Ä'
0.00.042.335 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.335 I print_info: max token length = 1024
0.00.516.371 I load_tensors: offloading 24 repeating layers to GPU
0.00.516.388 I load_tensors: offloading output layer to GPU
0.00.516.389 I load_tensors: offloaded 25/25 layers to GPU
0.00.516.423 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.516.424 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.517.933 I llama_init_from_model: n_seq_max     = 1
0.00.517.937 I llama_init_from_model: n_ctx         = 2048
0.00.517.938 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.517.938 I llama_init_from_model: n_batch       = 2048
0.00.517.938 I llama_init_from_model: n_ubatch      = 512
0.00.517.939 I llama_init_from_model: flash_attn    = 0
0.00.517.941 I llama_init_from_model: freq_base     = 10000.0
0.00.517.941 I llama_init_from_model: freq_scale    = 1
0.00.517.943 I ggml_metal_init: allocating
0.00.518.018 I ggml_metal_init: found device: Apple M4
0.00.518.031 I ggml_metal_init: picking default device: Apple M4
0.00.519.830 I ggml_metal_init: using embedded metal library
0.00.526.401 I ggml_metal_init: GPU name:   Apple M4
0.00.526.405 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.526.406 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.526.406 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.526.407 I ggml_metal_init: simdgroup reduction   = true
0.00.526.407 I ggml_metal_init: simdgroup matrix mul. = true
0.00.526.407 I ggml_metal_init: has residency sets    = true
0.00.526.407 I ggml_metal_init: has bfloat            = true
0.00.526.408 I ggml_metal_init: use bfloat            = true
0.00.526.408 I ggml_metal_init: hasUnifiedMemory      = true
0.00.526.410 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.543.637 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.596.347 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.596.355 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.596.385 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.600.773 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.600.775 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.600.775 I llama_init_from_model: graph nodes  = 967
0.00.600.776 I llama_init_from_model: graph splits = 2
0.00.600.781 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.600.914 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.600.914 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.656.113 I main: llama threadpool init, n_threads = 4
0.00.656.157 I 
0.00.656.182 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.656.182 I 
0.00.656.354 I sampler seed: 1234
0.00.656.359 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.656.379 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.656.379 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.656.379 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.412.029 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49442.90 tokens per second)
0.01.412.030 I llama_perf_context_print:        load time =     642.16 ms
0.01.412.031 I llama_perf_context_print: prompt eval time =      46.74 ms /     7 tokens (    6.68 ms per token,   149.78 tokens per second)
0.01.412.033 I llama_perf_context_print:        eval time =     705.84 ms /    63 runs   (   11.20 ms per token,    89.26 tokens per second)
0.01.412.033 I llama_perf_context_print:       total time =     756.79 ms /    70 tokens
0.01.412.248 I ggml_metal_free: deallocating

real	0m1.429s
user	0m0.108s
sys	0m0.192s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4568 (a4417ddd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.762 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.293 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.298 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.299 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.300 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.300 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.301 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.301 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.302 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.302 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.303 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.303 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.303 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.304 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.304 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.307 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.307 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.307 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.082 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.150 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.946 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.947 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.947 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.948 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.948 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.948 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.949 I llama_model_loader: - type  f32:  194 tensors
0.00.024.949 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.949 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.950 I print_info: file format = GGUF V3 (latest)
0.00.024.951 I print_info: file type   = Q5_K - Medium
0.00.024.953 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.779 I load: special tokens cache size = 25
0.00.038.762 I load: token to piece cache size = 0.2984 MB
0.00.038.765 I print_info: arch             = gptneox
0.00.038.765 I print_info: vocab_only       = 0
0.00.038.765 I print_info: n_ctx_train      = 2048
0.00.038.765 I print_info: n_embd           = 2048
0.00.038.765 I print_info: n_layer          = 24
0.00.038.768 I print_info: n_head           = 16
0.00.038.769 I print_info: n_head_kv        = 16
0.00.038.769 I print_info: n_rot            = 32
0.00.038.769 I print_info: n_swa            = 0
0.00.038.772 I print_info: n_embd_head_k    = 128
0.00.038.772 I print_info: n_embd_head_v    = 128
0.00.038.773 I print_info: n_gqa            = 1
0.00.038.773 I print_info: n_embd_k_gqa     = 2048
0.00.038.774 I print_info: n_embd_v_gqa     = 2048
0.00.038.775 I print_info: f_norm_eps       = 1.0e-05
0.00.038.775 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.775 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.775 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.775 I print_info: f_logit_scale    = 0.0e+00
0.00.038.776 I print_info: n_ff             = 8192
0.00.038.776 I print_info: n_expert         = 0
0.00.038.776 I print_info: n_expert_used    = 0
0.00.038.777 I print_info: causal attn      = 1
0.00.038.777 I print_info: pooling type     = 0
0.00.038.778 I print_info: rope type        = 2
0.00.038.780 I print_info: rope scaling     = linear
0.00.038.780 I print_info: freq_base_train  = 10000.0
0.00.038.781 I print_info: freq_scale_train = 1
0.00.038.781 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.781 I print_info: rope_finetuned   = unknown
0.00.038.781 I print_info: ssm_d_conv       = 0
0.00.038.782 I print_info: ssm_d_inner      = 0
0.00.038.783 I print_info: ssm_d_state      = 0
0.00.038.783 I print_info: ssm_dt_rank      = 0
0.00.038.783 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.783 I print_info: model type       = 1.4B
0.00.038.783 I print_info: model params     = 1.41 B
0.00.038.784 I print_info: general.name     = 1.4B
0.00.038.784 I print_info: vocab type       = BPE
0.00.038.785 I print_info: n_vocab          = 50304
0.00.038.785 I print_info: n_merges         = 50009
0.00.038.785 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.785 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.785 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.786 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.786 I print_info: LF token         = 128 'Ä'
0.00.038.786 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.786 I print_info: max token length = 1024
0.00.601.679 I load_tensors: offloading 24 repeating layers to GPU
0.00.601.693 I load_tensors: offloading output layer to GPU
0.00.601.694 I load_tensors: offloaded 25/25 layers to GPU
0.00.601.727 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.601.729 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.603.089 I llama_init_from_model: n_seq_max     = 1
0.00.603.092 I llama_init_from_model: n_ctx         = 2048
0.00.603.093 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.603.093 I llama_init_from_model: n_batch       = 2048
0.00.603.093 I llama_init_from_model: n_ubatch      = 512
0.00.603.094 I llama_init_from_model: flash_attn    = 0
0.00.603.095 I llama_init_from_model: freq_base     = 10000.0
0.00.603.096 I llama_init_from_model: freq_scale    = 1
0.00.603.101 I ggml_metal_init: allocating
0.00.603.124 I ggml_metal_init: found device: Apple M4
0.00.603.137 I ggml_metal_init: picking default device: Apple M4
0.00.604.540 I ggml_metal_init: using embedded metal library
0.00.610.833 I ggml_metal_init: GPU name:   Apple M4
0.00.610.837 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.610.838 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.610.839 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.610.839 I ggml_metal_init: simdgroup reduction   = true
0.00.610.840 I ggml_metal_init: simdgroup matrix mul. = true
0.00.610.840 I ggml_metal_init: has residency sets    = true
0.00.610.840 I ggml_metal_init: has bfloat            = true
0.00.610.840 I ggml_metal_init: use bfloat            = true
0.00.610.841 I ggml_metal_init: hasUnifiedMemory      = true
0.00.610.851 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.628.079 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.681.686 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.681.692 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.681.722 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.686.356 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.686.357 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.686.357 I llama_init_from_model: graph nodes  = 967
0.00.686.358 I llama_init_from_model: graph splits = 2
0.00.686.364 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.686.496 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.686.497 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.747.727 I main: llama threadpool init, n_threads = 4
0.00.747.767 I 
0.00.747.789 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.747.789 I 
0.00.747.961 I sampler seed: 1234
0.00.747.966 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.747.984 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.747.984 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.747.984 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.592.976 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54699.54 tokens per second)
0.01.592.977 I llama_perf_context_print:        load time =     738.06 ms
0.01.592.978 I llama_perf_context_print: prompt eval time =      51.25 ms /     7 tokens (    7.32 ms per token,   136.58 tokens per second)
0.01.592.978 I llama_perf_context_print:        eval time =     790.84 ms /    63 runs   (   12.55 ms per token,    79.66 tokens per second)
0.01.592.978 I llama_perf_context_print:       total time =     846.15 ms /    70 tokens
0.01.593.225 I ggml_metal_free: deallocating

real	0m1.611s
user	0m0.108s
sys	0m0.211s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4568 (a4417ddd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.809 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.495 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.500 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.505 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.506 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.506 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.507 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.507 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.508 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.508 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.509 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.509 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.509 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.510 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.510 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.512 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.512 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.512 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.221 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.288 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.955 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.956 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.957 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.957 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.957 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.958 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.958 I llama_model_loader: - type  f32:  194 tensors
0.00.023.958 I llama_model_loader: - type q6_K:   98 tensors
0.00.023.959 I print_info: file format = GGUF V3 (latest)
0.00.023.959 I print_info: file type   = Q6_K
0.00.023.960 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.031.898 I load: special tokens cache size = 25
0.00.037.851 I load: token to piece cache size = 0.2984 MB
0.00.037.853 I print_info: arch             = gptneox
0.00.037.854 I print_info: vocab_only       = 0
0.00.037.854 I print_info: n_ctx_train      = 2048
0.00.037.854 I print_info: n_embd           = 2048
0.00.037.854 I print_info: n_layer          = 24
0.00.037.857 I print_info: n_head           = 16
0.00.037.858 I print_info: n_head_kv        = 16
0.00.037.858 I print_info: n_rot            = 32
0.00.037.858 I print_info: n_swa            = 0
0.00.037.858 I print_info: n_embd_head_k    = 128
0.00.037.858 I print_info: n_embd_head_v    = 128
0.00.037.861 I print_info: n_gqa            = 1
0.00.037.862 I print_info: n_embd_k_gqa     = 2048
0.00.037.862 I print_info: n_embd_v_gqa     = 2048
0.00.037.863 I print_info: f_norm_eps       = 1.0e-05
0.00.037.863 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.864 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.864 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.864 I print_info: f_logit_scale    = 0.0e+00
0.00.037.865 I print_info: n_ff             = 8192
0.00.037.865 I print_info: n_expert         = 0
0.00.037.865 I print_info: n_expert_used    = 0
0.00.037.865 I print_info: causal attn      = 1
0.00.037.865 I print_info: pooling type     = 0
0.00.037.865 I print_info: rope type        = 2
0.00.037.866 I print_info: rope scaling     = linear
0.00.037.866 I print_info: freq_base_train  = 10000.0
0.00.037.866 I print_info: freq_scale_train = 1
0.00.037.866 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.867 I print_info: rope_finetuned   = unknown
0.00.037.867 I print_info: ssm_d_conv       = 0
0.00.037.867 I print_info: ssm_d_inner      = 0
0.00.037.867 I print_info: ssm_d_state      = 0
0.00.037.867 I print_info: ssm_dt_rank      = 0
0.00.037.867 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.868 I print_info: model type       = 1.4B
0.00.037.868 I print_info: model params     = 1.41 B
0.00.037.868 I print_info: general.name     = 1.4B
0.00.037.869 I print_info: vocab type       = BPE
0.00.037.869 I print_info: n_vocab          = 50304
0.00.037.870 I print_info: n_merges         = 50009
0.00.037.870 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.871 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.871 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.871 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.871 I print_info: LF token         = 128 'Ä'
0.00.037.872 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.875 I print_info: max token length = 1024
0.00.640.283 I load_tensors: offloading 24 repeating layers to GPU
0.00.640.287 I load_tensors: offloading output layer to GPU
0.00.640.288 I load_tensors: offloaded 25/25 layers to GPU
0.00.640.311 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.640.314 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.641.643 I llama_init_from_model: n_seq_max     = 1
0.00.641.646 I llama_init_from_model: n_ctx         = 2048
0.00.641.646 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.641.647 I llama_init_from_model: n_batch       = 2048
0.00.641.647 I llama_init_from_model: n_ubatch      = 512
0.00.641.648 I llama_init_from_model: flash_attn    = 0
0.00.641.649 I llama_init_from_model: freq_base     = 10000.0
0.00.641.649 I llama_init_from_model: freq_scale    = 1
0.00.641.650 I ggml_metal_init: allocating
0.00.641.674 I ggml_metal_init: found device: Apple M4
0.00.641.686 I ggml_metal_init: picking default device: Apple M4
0.00.643.071 I ggml_metal_init: using embedded metal library
0.00.649.110 I ggml_metal_init: GPU name:   Apple M4
0.00.649.114 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.649.114 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.649.116 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.649.116 I ggml_metal_init: simdgroup reduction   = true
0.00.649.116 I ggml_metal_init: simdgroup matrix mul. = true
0.00.649.117 I ggml_metal_init: has residency sets    = true
0.00.649.117 I ggml_metal_init: has bfloat            = true
0.00.649.117 I ggml_metal_init: use bfloat            = true
0.00.649.118 I ggml_metal_init: hasUnifiedMemory      = true
0.00.649.119 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.666.005 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.716.777 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.716.788 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.716.816 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.720.995 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.720.997 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.720.997 I llama_init_from_model: graph nodes  = 967
0.00.720.997 I llama_init_from_model: graph splits = 2
0.00.721.002 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.721.135 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.721.136 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.787.305 I main: llama threadpool init, n_threads = 4
0.00.787.344 I 
0.00.787.367 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.787.369 I 
0.00.787.543 I sampler seed: 1234
0.00.787.547 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.787.595 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.787.597 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.787.598 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.668.658 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55512.12 tokens per second)
0.01.668.659 I llama_perf_context_print:        load time =     777.62 ms
0.01.668.661 I llama_perf_context_print: prompt eval time =      54.40 ms /     7 tokens (    7.77 ms per token,   128.69 tokens per second)
0.01.668.662 I llama_perf_context_print:        eval time =     823.82 ms /    63 runs   (   13.08 ms per token,    76.47 tokens per second)
0.01.668.664 I llama_perf_context_print:       total time =     882.23 ms /    70 tokens
0.01.668.955 I ggml_metal_free: deallocating

real	0m1.685s
user	0m0.107s
sys	0m0.212s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.837 I build: 4568 (a4417ddd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.519 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.040.042 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.040.049 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.050 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.051 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.057 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.058 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.058 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.060 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.060 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.061 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.061 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.062 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.062 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.063 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.066 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.067 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.068 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.048.120 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.915 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.627 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.629 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.630 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.630 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.631 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.631 I llama_model_loader: - type  f32:  194 tensors
0.00.056.632 I llama_model_loader: - type  f16:   98 tensors
0.00.056.632 I print_info: file format = GGUF V3 (latest)
0.00.056.633 I print_info: file type   = all F32 (guessed)
0.00.056.634 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.068.675 I load: special tokens cache size = 25
0.00.076.381 I load: token to piece cache size = 0.2984 MB
0.00.076.384 I print_info: arch             = gptneox
0.00.076.384 I print_info: vocab_only       = 0
0.00.076.385 I print_info: n_ctx_train      = 2048
0.00.076.385 I print_info: n_embd           = 2048
0.00.076.385 I print_info: n_layer          = 24
0.00.076.388 I print_info: n_head           = 16
0.00.076.389 I print_info: n_head_kv        = 16
0.00.076.389 I print_info: n_rot            = 32
0.00.076.390 I print_info: n_swa            = 0
0.00.076.390 I print_info: n_embd_head_k    = 128
0.00.076.390 I print_info: n_embd_head_v    = 128
0.00.076.391 I print_info: n_gqa            = 1
0.00.076.392 I print_info: n_embd_k_gqa     = 2048
0.00.076.392 I print_info: n_embd_v_gqa     = 2048
0.00.076.393 I print_info: f_norm_eps       = 1.0e-05
0.00.076.393 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.076.394 I print_info: f_clamp_kqv      = 0.0e+00
0.00.076.394 I print_info: f_max_alibi_bias = 0.0e+00
0.00.076.394 I print_info: f_logit_scale    = 0.0e+00
0.00.076.395 I print_info: n_ff             = 8192
0.00.076.395 I print_info: n_expert         = 0
0.00.076.395 I print_info: n_expert_used    = 0
0.00.076.395 I print_info: causal attn      = 1
0.00.076.395 I print_info: pooling type     = 0
0.00.076.395 I print_info: rope type        = 2
0.00.076.396 I print_info: rope scaling     = linear
0.00.076.396 I print_info: freq_base_train  = 10000.0
0.00.076.396 I print_info: freq_scale_train = 1
0.00.076.396 I print_info: n_ctx_orig_yarn  = 2048
0.00.076.397 I print_info: rope_finetuned   = unknown
0.00.076.397 I print_info: ssm_d_conv       = 0
0.00.076.397 I print_info: ssm_d_inner      = 0
0.00.076.397 I print_info: ssm_d_state      = 0
0.00.076.397 I print_info: ssm_dt_rank      = 0
0.00.076.397 I print_info: ssm_dt_b_c_rms   = 0
0.00.076.398 I print_info: model type       = 1.4B
0.00.076.401 I print_info: model params     = 1.41 B
0.00.076.401 I print_info: general.name     = 1.4B
0.00.076.401 I print_info: vocab type       = BPE
0.00.076.401 I print_info: n_vocab          = 50304
0.00.076.402 I print_info: n_merges         = 50009
0.00.076.402 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.076.402 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.076.402 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.076.402 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.076.403 I print_info: LF token         = 128 'Ä'
0.00.076.403 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.076.403 I print_info: max token length = 1024
0.01.409.359 I load_tensors: offloading 24 repeating layers to GPU
0.01.409.363 I load_tensors: offloading output layer to GPU
0.01.409.363 I load_tensors: offloaded 25/25 layers to GPU
0.01.409.388 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.409.389 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.410.295 I llama_init_from_model: n_seq_max     = 1
0.01.410.297 I llama_init_from_model: n_ctx         = 128
0.01.410.297 I llama_init_from_model: n_ctx_per_seq = 128
0.01.410.297 I llama_init_from_model: n_batch       = 128
0.01.410.297 I llama_init_from_model: n_ubatch      = 128
0.01.410.297 I llama_init_from_model: flash_attn    = 0
0.01.410.298 I llama_init_from_model: freq_base     = 10000.0
0.01.410.298 I llama_init_from_model: freq_scale    = 1
0.01.410.299 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.410.299 I ggml_metal_init: allocating
0.01.410.341 I ggml_metal_init: found device: Apple M4
0.01.410.348 I ggml_metal_init: picking default device: Apple M4
0.01.411.426 I ggml_metal_init: using embedded metal library
0.01.415.383 I ggml_metal_init: GPU name:   Apple M4
0.01.415.385 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.415.386 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.415.386 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.415.387 I ggml_metal_init: simdgroup reduction   = true
0.01.415.387 I ggml_metal_init: simdgroup matrix mul. = true
0.01.415.387 I ggml_metal_init: has residency sets    = true
0.01.415.387 I ggml_metal_init: has bfloat            = true
0.01.415.387 I ggml_metal_init: use bfloat            = true
0.01.415.388 I ggml_metal_init: hasUnifiedMemory      = true
0.01.415.389 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.426.281 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.428.007 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.428.009 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.428.039 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.429.653 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.429.654 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.429.655 I llama_init_from_model: graph nodes  = 967
0.01.429.655 I llama_init_from_model: graph splits = 2
0.01.429.656 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.429.656 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.464.013 I 
0.01.464.050 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.464.077 I perplexity: tokenizing the input ..
0.01.468.995 I perplexity: tokenization took 4.916 ms
0.01.469.016 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.587.901 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.589.521 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.589.538 I llama_perf_context_print:        load time =    1438.48 ms
0.01.589.538 I llama_perf_context_print: prompt eval time =     118.58 ms /   128 tokens (    0.93 ms per token,  1079.45 tokens per second)
0.01.589.539 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.589.539 I llama_perf_context_print:       total time =     125.53 ms /   129 tokens
0.01.589.948 I ggml_metal_free: deallocating

real	0m1.775s
user	0m0.098s
sys	0m0.247s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4568 (a4417ddd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.538 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.741 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.747 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.749 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.750 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.750 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.750 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.750 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.751 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.752 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.752 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.752 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.753 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.753 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.754 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.755 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.756 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.756 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.604 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.611 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.466 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.468 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.468 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.469 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.469 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.469 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.470 I llama_model_loader: - type  f32:  194 tensors
0.00.025.470 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.471 I print_info: file format = GGUF V3 (latest)
0.00.025.472 I print_info: file type   = Q8_0
0.00.025.473 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.717 I load: special tokens cache size = 25
0.00.039.724 I load: token to piece cache size = 0.2984 MB
0.00.039.728 I print_info: arch             = gptneox
0.00.039.728 I print_info: vocab_only       = 0
0.00.039.728 I print_info: n_ctx_train      = 2048
0.00.039.729 I print_info: n_embd           = 2048
0.00.039.729 I print_info: n_layer          = 24
0.00.039.733 I print_info: n_head           = 16
0.00.039.734 I print_info: n_head_kv        = 16
0.00.039.734 I print_info: n_rot            = 32
0.00.039.734 I print_info: n_swa            = 0
0.00.039.734 I print_info: n_embd_head_k    = 128
0.00.039.734 I print_info: n_embd_head_v    = 128
0.00.039.735 I print_info: n_gqa            = 1
0.00.039.736 I print_info: n_embd_k_gqa     = 2048
0.00.039.738 I print_info: n_embd_v_gqa     = 2048
0.00.039.739 I print_info: f_norm_eps       = 1.0e-05
0.00.039.739 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.739 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.739 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.739 I print_info: f_logit_scale    = 0.0e+00
0.00.039.740 I print_info: n_ff             = 8192
0.00.039.740 I print_info: n_expert         = 0
0.00.039.740 I print_info: n_expert_used    = 0
0.00.039.741 I print_info: causal attn      = 1
0.00.039.741 I print_info: pooling type     = 0
0.00.039.741 I print_info: rope type        = 2
0.00.039.741 I print_info: rope scaling     = linear
0.00.039.741 I print_info: freq_base_train  = 10000.0
0.00.039.742 I print_info: freq_scale_train = 1
0.00.039.742 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.742 I print_info: rope_finetuned   = unknown
0.00.039.744 I print_info: ssm_d_conv       = 0
0.00.039.744 I print_info: ssm_d_inner      = 0
0.00.039.744 I print_info: ssm_d_state      = 0
0.00.039.744 I print_info: ssm_dt_rank      = 0
0.00.039.744 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.744 I print_info: model type       = 1.4B
0.00.039.745 I print_info: model params     = 1.41 B
0.00.039.746 I print_info: general.name     = 1.4B
0.00.039.746 I print_info: vocab type       = BPE
0.00.039.747 I print_info: n_vocab          = 50304
0.00.039.747 I print_info: n_merges         = 50009
0.00.039.747 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.747 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.747 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.747 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.748 I print_info: LF token         = 128 'Ä'
0.00.039.748 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.748 I print_info: max token length = 1024
0.00.901.212 I load_tensors: offloading 24 repeating layers to GPU
0.00.901.219 I load_tensors: offloading output layer to GPU
0.00.901.220 I load_tensors: offloaded 25/25 layers to GPU
0.00.901.240 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.901.241 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.902.111 I llama_init_from_model: n_seq_max     = 1
0.00.902.116 I llama_init_from_model: n_ctx         = 128
0.00.902.116 I llama_init_from_model: n_ctx_per_seq = 128
0.00.902.117 I llama_init_from_model: n_batch       = 128
0.00.902.117 I llama_init_from_model: n_ubatch      = 128
0.00.902.117 I llama_init_from_model: flash_attn    = 0
0.00.902.118 I llama_init_from_model: freq_base     = 10000.0
0.00.902.119 I llama_init_from_model: freq_scale    = 1
0.00.902.119 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.902.121 I ggml_metal_init: allocating
0.00.902.167 I ggml_metal_init: found device: Apple M4
0.00.902.180 I ggml_metal_init: picking default device: Apple M4
0.00.903.173 I ggml_metal_init: using embedded metal library
0.00.907.272 I ggml_metal_init: GPU name:   Apple M4
0.00.907.279 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.907.280 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.907.280 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.907.281 I ggml_metal_init: simdgroup reduction   = true
0.00.907.281 I ggml_metal_init: simdgroup matrix mul. = true
0.00.907.281 I ggml_metal_init: has residency sets    = true
0.00.907.282 I ggml_metal_init: has bfloat            = true
0.00.907.282 I ggml_metal_init: use bfloat            = true
0.00.907.283 I ggml_metal_init: hasUnifiedMemory      = true
0.00.907.286 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.917.743 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.919.344 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.919.349 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.919.377 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.920.899 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.920.900 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.920.900 I llama_init_from_model: graph nodes  = 967
0.00.920.900 I llama_init_from_model: graph splits = 2
0.00.920.902 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.920.902 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.946.465 I 
0.00.946.500 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.946.510 I perplexity: tokenizing the input ..
0.00.950.335 I perplexity: tokenization took 3.824 ms
0.00.950.346 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.084.769 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.086.171 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.086.182 I llama_perf_context_print:        load time =     936.92 ms
0.01.086.183 I llama_perf_context_print: prompt eval time =     134.20 ms /   128 tokens (    1.05 ms per token,   953.83 tokens per second)
0.01.086.184 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.086.184 I llama_perf_context_print:       total time =     139.72 ms /   129 tokens
0.01.086.542 I ggml_metal_free: deallocating

real	0m1.101s
user	0m0.066s
sys	0m0.164s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4568 (a4417ddd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.612 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.816 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.821 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.824 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.825 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.825 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.826 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.826 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.827 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.827 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.828 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.828 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.829 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.829 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.829 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.831 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.832 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.832 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.623 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.683 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.459 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.460 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.460 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.461 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.461 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.462 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.462 I llama_model_loader: - type  f32:  194 tensors
0.00.026.462 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.463 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.463 I print_info: file format = GGUF V3 (latest)
0.00.026.464 I print_info: file type   = Q4_0
0.00.026.465 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.371 I load: special tokens cache size = 25
0.00.040.490 I load: token to piece cache size = 0.2984 MB
0.00.040.495 I print_info: arch             = gptneox
0.00.040.495 I print_info: vocab_only       = 0
0.00.040.495 I print_info: n_ctx_train      = 2048
0.00.040.495 I print_info: n_embd           = 2048
0.00.040.495 I print_info: n_layer          = 24
0.00.040.500 I print_info: n_head           = 16
0.00.040.500 I print_info: n_head_kv        = 16
0.00.040.501 I print_info: n_rot            = 32
0.00.040.501 I print_info: n_swa            = 0
0.00.040.501 I print_info: n_embd_head_k    = 128
0.00.040.501 I print_info: n_embd_head_v    = 128
0.00.040.502 I print_info: n_gqa            = 1
0.00.040.503 I print_info: n_embd_k_gqa     = 2048
0.00.040.503 I print_info: n_embd_v_gqa     = 2048
0.00.040.504 I print_info: f_norm_eps       = 1.0e-05
0.00.040.504 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.504 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.505 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.505 I print_info: f_logit_scale    = 0.0e+00
0.00.040.505 I print_info: n_ff             = 8192
0.00.040.506 I print_info: n_expert         = 0
0.00.040.506 I print_info: n_expert_used    = 0
0.00.040.506 I print_info: causal attn      = 1
0.00.040.506 I print_info: pooling type     = 0
0.00.040.506 I print_info: rope type        = 2
0.00.040.508 I print_info: rope scaling     = linear
0.00.040.508 I print_info: freq_base_train  = 10000.0
0.00.040.508 I print_info: freq_scale_train = 1
0.00.040.509 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.509 I print_info: rope_finetuned   = unknown
0.00.040.509 I print_info: ssm_d_conv       = 0
0.00.040.509 I print_info: ssm_d_inner      = 0
0.00.040.509 I print_info: ssm_d_state      = 0
0.00.040.509 I print_info: ssm_dt_rank      = 0
0.00.040.510 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.510 I print_info: model type       = 1.4B
0.00.040.510 I print_info: model params     = 1.41 B
0.00.040.510 I print_info: general.name     = 1.4B
0.00.040.511 I print_info: vocab type       = BPE
0.00.040.511 I print_info: n_vocab          = 50304
0.00.040.511 I print_info: n_merges         = 50009
0.00.040.512 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.512 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.512 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.512 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.513 I print_info: LF token         = 128 'Ä'
0.00.040.513 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.513 I print_info: max token length = 1024
0.00.590.601 I load_tensors: offloading 24 repeating layers to GPU
0.00.590.606 I load_tensors: offloading output layer to GPU
0.00.590.606 I load_tensors: offloaded 25/25 layers to GPU
0.00.590.624 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.590.624 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.591.567 I llama_init_from_model: n_seq_max     = 1
0.00.591.570 I llama_init_from_model: n_ctx         = 128
0.00.591.570 I llama_init_from_model: n_ctx_per_seq = 128
0.00.591.570 I llama_init_from_model: n_batch       = 128
0.00.591.570 I llama_init_from_model: n_ubatch      = 128
0.00.591.571 I llama_init_from_model: flash_attn    = 0
0.00.591.572 I llama_init_from_model: freq_base     = 10000.0
0.00.591.572 I llama_init_from_model: freq_scale    = 1
0.00.591.573 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.591.574 I ggml_metal_init: allocating
0.00.591.609 I ggml_metal_init: found device: Apple M4
0.00.591.621 I ggml_metal_init: picking default device: Apple M4
0.00.592.625 I ggml_metal_init: using embedded metal library
0.00.596.842 I ggml_metal_init: GPU name:   Apple M4
0.00.596.848 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.596.848 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.596.849 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.596.849 I ggml_metal_init: simdgroup reduction   = true
0.00.596.850 I ggml_metal_init: simdgroup matrix mul. = true
0.00.596.850 I ggml_metal_init: has residency sets    = true
0.00.596.850 I ggml_metal_init: has bfloat            = true
0.00.596.850 I ggml_metal_init: use bfloat            = true
0.00.596.851 I ggml_metal_init: hasUnifiedMemory      = true
0.00.596.853 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.613.483 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.615.132 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.615.134 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.615.151 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.616.725 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.616.726 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.616.726 I llama_init_from_model: graph nodes  = 967
0.00.616.727 I llama_init_from_model: graph splits = 2
0.00.616.728 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.616.728 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.642.393 I 
0.00.642.430 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.642.439 I perplexity: tokenizing the input ..
0.00.646.336 I perplexity: tokenization took 3.896 ms
0.00.646.347 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.776.021 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.777.530 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.777.543 I llama_perf_context_print:        load time =     631.78 ms
0.00.777.544 I llama_perf_context_print: prompt eval time =     129.44 ms /   128 tokens (    1.01 ms per token,   988.85 tokens per second)
0.00.777.545 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.777.545 I llama_perf_context_print:       total time =     135.15 ms /   129 tokens
0.00.777.926 I ggml_metal_free: deallocating

real	0m0.793s
user	0m0.070s
sys	0m0.085s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4568 (a4417ddd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.269 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.111 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.117 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.118 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.124 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.124 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.124 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.125 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.126 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.126 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.126 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.127 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.127 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.127 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.128 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.130 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.130 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.130 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.961 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.999 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.864 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.866 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.866 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.866 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.867 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.867 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.867 I llama_model_loader: - type  f32:  194 tensors
0.00.024.868 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.868 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.869 I print_info: file format = GGUF V3 (latest)
0.00.024.869 I print_info: file type   = Q4_1
0.00.024.870 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.257 I load: special tokens cache size = 25
0.00.039.190 I load: token to piece cache size = 0.2984 MB
0.00.039.194 I print_info: arch             = gptneox
0.00.039.194 I print_info: vocab_only       = 0
0.00.039.194 I print_info: n_ctx_train      = 2048
0.00.039.195 I print_info: n_embd           = 2048
0.00.039.195 I print_info: n_layer          = 24
0.00.039.200 I print_info: n_head           = 16
0.00.039.200 I print_info: n_head_kv        = 16
0.00.039.201 I print_info: n_rot            = 32
0.00.039.204 I print_info: n_swa            = 0
0.00.039.204 I print_info: n_embd_head_k    = 128
0.00.039.204 I print_info: n_embd_head_v    = 128
0.00.039.205 I print_info: n_gqa            = 1
0.00.039.205 I print_info: n_embd_k_gqa     = 2048
0.00.039.206 I print_info: n_embd_v_gqa     = 2048
0.00.039.206 I print_info: f_norm_eps       = 1.0e-05
0.00.039.207 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.207 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.207 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.207 I print_info: f_logit_scale    = 0.0e+00
0.00.039.208 I print_info: n_ff             = 8192
0.00.039.208 I print_info: n_expert         = 0
0.00.039.208 I print_info: n_expert_used    = 0
0.00.039.208 I print_info: causal attn      = 1
0.00.039.209 I print_info: pooling type     = 0
0.00.039.209 I print_info: rope type        = 2
0.00.039.209 I print_info: rope scaling     = linear
0.00.039.209 I print_info: freq_base_train  = 10000.0
0.00.039.211 I print_info: freq_scale_train = 1
0.00.039.211 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.211 I print_info: rope_finetuned   = unknown
0.00.039.211 I print_info: ssm_d_conv       = 0
0.00.039.211 I print_info: ssm_d_inner      = 0
0.00.039.211 I print_info: ssm_d_state      = 0
0.00.039.212 I print_info: ssm_dt_rank      = 0
0.00.039.212 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.212 I print_info: model type       = 1.4B
0.00.039.212 I print_info: model params     = 1.41 B
0.00.039.212 I print_info: general.name     = 1.4B
0.00.039.213 I print_info: vocab type       = BPE
0.00.039.213 I print_info: n_vocab          = 50304
0.00.039.213 I print_info: n_merges         = 50009
0.00.039.214 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.214 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.214 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.214 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.214 I print_info: LF token         = 128 'Ä'
0.00.039.219 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.219 I print_info: max token length = 1024
0.00.643.816 I load_tensors: offloading 24 repeating layers to GPU
0.00.643.829 I load_tensors: offloading output layer to GPU
0.00.643.830 I load_tensors: offloaded 25/25 layers to GPU
0.00.643.868 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.643.870 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.645.396 I llama_init_from_model: n_seq_max     = 1
0.00.645.401 I llama_init_from_model: n_ctx         = 128
0.00.645.401 I llama_init_from_model: n_ctx_per_seq = 128
0.00.645.402 I llama_init_from_model: n_batch       = 128
0.00.645.402 I llama_init_from_model: n_ubatch      = 128
0.00.645.403 I llama_init_from_model: flash_attn    = 0
0.00.645.405 I llama_init_from_model: freq_base     = 10000.0
0.00.645.405 I llama_init_from_model: freq_scale    = 1
0.00.645.406 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.645.408 I ggml_metal_init: allocating
0.00.645.518 I ggml_metal_init: found device: Apple M4
0.00.645.535 I ggml_metal_init: picking default device: Apple M4
0.00.647.261 I ggml_metal_init: using embedded metal library
0.00.653.752 I ggml_metal_init: GPU name:   Apple M4
0.00.653.761 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.653.762 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.653.763 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.653.763 I ggml_metal_init: simdgroup reduction   = true
0.00.653.764 I ggml_metal_init: simdgroup matrix mul. = true
0.00.653.764 I ggml_metal_init: has residency sets    = true
0.00.653.765 I ggml_metal_init: has bfloat            = true
0.00.653.765 I ggml_metal_init: use bfloat            = true
0.00.653.767 I ggml_metal_init: hasUnifiedMemory      = true
0.00.653.781 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.672.066 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.675.600 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.675.604 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.675.629 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.679.144 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.679.146 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.679.146 I llama_init_from_model: graph nodes  = 967
0.00.679.147 I llama_init_from_model: graph splits = 2
0.00.679.149 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.679.150 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.706.405 I 
0.00.706.485 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.706.505 I perplexity: tokenizing the input ..
0.00.713.651 I perplexity: tokenization took 7.142 ms
0.00.713.672 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.850.578 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.851.940 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.851.955 I llama_perf_context_print:        load time =     697.13 ms
0.00.851.956 I llama_perf_context_print: prompt eval time =     136.05 ms /   128 tokens (    1.06 ms per token,   940.86 tokens per second)
0.00.851.956 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.851.957 I llama_perf_context_print:       total time =     145.55 ms /   129 tokens
0.00.852.352 I ggml_metal_free: deallocating

real	0m0.866s
user	0m0.079s
sys	0m0.123s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4568 (a4417ddd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.038 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.289 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.294 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.295 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.296 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.296 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.296 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.297 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.297 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.298 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.298 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.299 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.299 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.299 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.300 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.302 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.302 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.302 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.176 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.227 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.093 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.094 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.095 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.095 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.095 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.096 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.096 I llama_model_loader: - type  f32:  194 tensors
0.00.025.097 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.097 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.098 I print_info: file format = GGUF V3 (latest)
0.00.025.098 I print_info: file type   = Q5_0
0.00.025.099 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.152 I load: special tokens cache size = 25
0.00.039.192 I load: token to piece cache size = 0.2984 MB
0.00.039.195 I print_info: arch             = gptneox
0.00.039.195 I print_info: vocab_only       = 0
0.00.039.195 I print_info: n_ctx_train      = 2048
0.00.039.196 I print_info: n_embd           = 2048
0.00.039.196 I print_info: n_layer          = 24
0.00.039.199 I print_info: n_head           = 16
0.00.039.200 I print_info: n_head_kv        = 16
0.00.039.200 I print_info: n_rot            = 32
0.00.039.200 I print_info: n_swa            = 0
0.00.039.200 I print_info: n_embd_head_k    = 128
0.00.039.201 I print_info: n_embd_head_v    = 128
0.00.039.201 I print_info: n_gqa            = 1
0.00.039.202 I print_info: n_embd_k_gqa     = 2048
0.00.039.206 I print_info: n_embd_v_gqa     = 2048
0.00.039.206 I print_info: f_norm_eps       = 1.0e-05
0.00.039.207 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.207 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.207 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.207 I print_info: f_logit_scale    = 0.0e+00
0.00.039.208 I print_info: n_ff             = 8192
0.00.039.210 I print_info: n_expert         = 0
0.00.039.210 I print_info: n_expert_used    = 0
0.00.039.210 I print_info: causal attn      = 1
0.00.039.210 I print_info: pooling type     = 0
0.00.039.210 I print_info: rope type        = 2
0.00.039.210 I print_info: rope scaling     = linear
0.00.039.211 I print_info: freq_base_train  = 10000.0
0.00.039.211 I print_info: freq_scale_train = 1
0.00.039.211 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.211 I print_info: rope_finetuned   = unknown
0.00.039.212 I print_info: ssm_d_conv       = 0
0.00.039.212 I print_info: ssm_d_inner      = 0
0.00.039.212 I print_info: ssm_d_state      = 0
0.00.039.212 I print_info: ssm_dt_rank      = 0
0.00.039.212 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.216 I print_info: model type       = 1.4B
0.00.039.217 I print_info: model params     = 1.41 B
0.00.039.217 I print_info: general.name     = 1.4B
0.00.039.217 I print_info: vocab type       = BPE
0.00.039.217 I print_info: n_vocab          = 50304
0.00.039.218 I print_info: n_merges         = 50009
0.00.039.218 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.218 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.218 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.218 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.219 I print_info: LF token         = 128 'Ä'
0.00.039.219 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.219 I print_info: max token length = 1024
0.00.680.042 I load_tensors: offloading 24 repeating layers to GPU
0.00.680.058 I load_tensors: offloading output layer to GPU
0.00.680.058 I load_tensors: offloaded 25/25 layers to GPU
0.00.680.096 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.680.097 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.681.510 I llama_init_from_model: n_seq_max     = 1
0.00.681.514 I llama_init_from_model: n_ctx         = 128
0.00.681.514 I llama_init_from_model: n_ctx_per_seq = 128
0.00.681.518 I llama_init_from_model: n_batch       = 128
0.00.681.519 I llama_init_from_model: n_ubatch      = 128
0.00.681.519 I llama_init_from_model: flash_attn    = 0
0.00.681.520 I llama_init_from_model: freq_base     = 10000.0
0.00.681.521 I llama_init_from_model: freq_scale    = 1
0.00.681.522 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.681.523 I ggml_metal_init: allocating
0.00.681.580 I ggml_metal_init: found device: Apple M4
0.00.681.591 I ggml_metal_init: picking default device: Apple M4
0.00.683.023 I ggml_metal_init: using embedded metal library
0.00.689.092 I ggml_metal_init: GPU name:   Apple M4
0.00.689.095 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.689.096 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.689.097 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.689.098 I ggml_metal_init: simdgroup reduction   = true
0.00.689.098 I ggml_metal_init: simdgroup matrix mul. = true
0.00.689.098 I ggml_metal_init: has residency sets    = true
0.00.689.098 I ggml_metal_init: has bfloat            = true
0.00.689.099 I ggml_metal_init: use bfloat            = true
0.00.689.099 I ggml_metal_init: hasUnifiedMemory      = true
0.00.689.110 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.706.050 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.709.527 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.709.534 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.709.579 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.712.722 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.712.724 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.712.724 I llama_init_from_model: graph nodes  = 967
0.00.712.725 I llama_init_from_model: graph splits = 2
0.00.712.727 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.712.727 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.741.215 I 
0.00.741.307 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.741.326 I perplexity: tokenizing the input ..
0.00.748.311 I perplexity: tokenization took 6.982 ms
0.00.748.330 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.896.362 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.897.889 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.897.902 I llama_perf_context_print:        load time =     732.16 ms
0.00.897.905 I llama_perf_context_print: prompt eval time =     147.06 ms /   128 tokens (    1.15 ms per token,   870.42 tokens per second)
0.00.897.906 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.897.906 I llama_perf_context_print:       total time =     156.70 ms /   129 tokens
0.00.898.331 I ggml_metal_free: deallocating

real	0m0.912s
user	0m0.078s
sys	0m0.133s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4568 (a4417ddd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.789 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.473 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.477 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.479 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.484 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.485 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.485 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.485 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.486 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.487 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.487 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.488 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.488 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.488 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.489 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.490 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.490 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.491 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.252 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.297 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.029 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.030 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.031 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.031 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.031 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.032 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.032 I llama_model_loader: - type  f32:  194 tensors
0.00.026.032 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.033 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.033 I print_info: file format = GGUF V3 (latest)
0.00.026.034 I print_info: file type   = Q5_1
0.00.026.034 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.931 I load: special tokens cache size = 25
0.00.039.937 I load: token to piece cache size = 0.2984 MB
0.00.039.940 I print_info: arch             = gptneox
0.00.039.940 I print_info: vocab_only       = 0
0.00.039.940 I print_info: n_ctx_train      = 2048
0.00.039.940 I print_info: n_embd           = 2048
0.00.039.940 I print_info: n_layer          = 24
0.00.039.943 I print_info: n_head           = 16
0.00.039.944 I print_info: n_head_kv        = 16
0.00.039.944 I print_info: n_rot            = 32
0.00.039.944 I print_info: n_swa            = 0
0.00.039.944 I print_info: n_embd_head_k    = 128
0.00.039.944 I print_info: n_embd_head_v    = 128
0.00.039.945 I print_info: n_gqa            = 1
0.00.039.948 I print_info: n_embd_k_gqa     = 2048
0.00.039.948 I print_info: n_embd_v_gqa     = 2048
0.00.039.949 I print_info: f_norm_eps       = 1.0e-05
0.00.039.949 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.950 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.950 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.950 I print_info: f_logit_scale    = 0.0e+00
0.00.039.951 I print_info: n_ff             = 8192
0.00.039.951 I print_info: n_expert         = 0
0.00.039.951 I print_info: n_expert_used    = 0
0.00.039.951 I print_info: causal attn      = 1
0.00.039.951 I print_info: pooling type     = 0
0.00.039.951 I print_info: rope type        = 2
0.00.039.952 I print_info: rope scaling     = linear
0.00.039.952 I print_info: freq_base_train  = 10000.0
0.00.039.952 I print_info: freq_scale_train = 1
0.00.039.952 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.953 I print_info: rope_finetuned   = unknown
0.00.039.953 I print_info: ssm_d_conv       = 0
0.00.039.953 I print_info: ssm_d_inner      = 0
0.00.039.953 I print_info: ssm_d_state      = 0
0.00.039.953 I print_info: ssm_dt_rank      = 0
0.00.039.953 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.954 I print_info: model type       = 1.4B
0.00.039.954 I print_info: model params     = 1.41 B
0.00.039.954 I print_info: general.name     = 1.4B
0.00.039.955 I print_info: vocab type       = BPE
0.00.039.955 I print_info: n_vocab          = 50304
0.00.039.955 I print_info: n_merges         = 50009
0.00.039.955 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.956 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.956 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.956 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.956 I print_info: LF token         = 128 'Ä'
0.00.039.957 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.957 I print_info: max token length = 1024
0.00.612.731 I load_tensors: offloading 24 repeating layers to GPU
0.00.612.737 I load_tensors: offloading output layer to GPU
0.00.612.738 I load_tensors: offloaded 25/25 layers to GPU
0.00.612.763 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.612.764 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.614.149 I llama_init_from_model: n_seq_max     = 1
0.00.614.152 I llama_init_from_model: n_ctx         = 128
0.00.614.152 I llama_init_from_model: n_ctx_per_seq = 128
0.00.614.152 I llama_init_from_model: n_batch       = 128
0.00.614.153 I llama_init_from_model: n_ubatch      = 128
0.00.614.153 I llama_init_from_model: flash_attn    = 0
0.00.614.154 I llama_init_from_model: freq_base     = 10000.0
0.00.614.155 I llama_init_from_model: freq_scale    = 1
0.00.614.155 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.614.157 I ggml_metal_init: allocating
0.00.614.170 I ggml_metal_init: found device: Apple M4
0.00.614.179 I ggml_metal_init: picking default device: Apple M4
0.00.615.485 I ggml_metal_init: using embedded metal library
0.00.621.759 I ggml_metal_init: GPU name:   Apple M4
0.00.621.763 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.621.764 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.621.765 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.621.766 I ggml_metal_init: simdgroup reduction   = true
0.00.621.766 I ggml_metal_init: simdgroup matrix mul. = true
0.00.621.766 I ggml_metal_init: has residency sets    = true
0.00.621.767 I ggml_metal_init: has bfloat            = true
0.00.621.767 I ggml_metal_init: use bfloat            = true
0.00.621.768 I ggml_metal_init: hasUnifiedMemory      = true
0.00.621.769 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.638.210 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.641.692 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.641.701 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.641.744 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.645.150 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.645.152 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.645.152 I llama_init_from_model: graph nodes  = 967
0.00.645.153 I llama_init_from_model: graph splits = 2
0.00.645.155 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.645.156 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.676.732 I 
0.00.676.809 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.676.828 I perplexity: tokenizing the input ..
0.00.682.670 I perplexity: tokenization took 5.84 ms
0.00.682.682 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.819.533 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.820.874 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.820.890 I llama_perf_context_print:        load time =     665.94 ms
0.00.820.891 I llama_perf_context_print: prompt eval time =     136.62 ms /   128 tokens (    1.07 ms per token,   936.93 tokens per second)
0.00.820.892 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.820.892 I llama_perf_context_print:       total time =     144.16 ms /   129 tokens
0.00.821.304 I ggml_metal_free: deallocating

real	0m0.836s
user	0m0.075s
sys	0m0.149s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4568 (a4417ddd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.039 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.917 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.922 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.928 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.928 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.929 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.929 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.929 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.930 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.931 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.931 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.931 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.932 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.932 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.932 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.934 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.935 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.935 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.795 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.855 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.661 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.663 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.663 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.663 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.663 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.664 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.664 I llama_model_loader: - type  f32:  194 tensors
0.00.024.665 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.665 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.665 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.666 I print_info: file format = GGUF V3 (latest)
0.00.024.666 I print_info: file type   = Q2_K - Medium
0.00.024.667 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.814 I load: special tokens cache size = 25
0.00.038.752 I load: token to piece cache size = 0.2984 MB
0.00.038.755 I print_info: arch             = gptneox
0.00.038.756 I print_info: vocab_only       = 0
0.00.038.756 I print_info: n_ctx_train      = 2048
0.00.038.756 I print_info: n_embd           = 2048
0.00.038.756 I print_info: n_layer          = 24
0.00.038.759 I print_info: n_head           = 16
0.00.038.760 I print_info: n_head_kv        = 16
0.00.038.760 I print_info: n_rot            = 32
0.00.038.761 I print_info: n_swa            = 0
0.00.038.761 I print_info: n_embd_head_k    = 128
0.00.038.761 I print_info: n_embd_head_v    = 128
0.00.038.762 I print_info: n_gqa            = 1
0.00.038.763 I print_info: n_embd_k_gqa     = 2048
0.00.038.763 I print_info: n_embd_v_gqa     = 2048
0.00.038.764 I print_info: f_norm_eps       = 1.0e-05
0.00.038.764 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.765 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.765 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.765 I print_info: f_logit_scale    = 0.0e+00
0.00.038.766 I print_info: n_ff             = 8192
0.00.038.766 I print_info: n_expert         = 0
0.00.038.766 I print_info: n_expert_used    = 0
0.00.038.766 I print_info: causal attn      = 1
0.00.038.766 I print_info: pooling type     = 0
0.00.038.766 I print_info: rope type        = 2
0.00.038.767 I print_info: rope scaling     = linear
0.00.038.767 I print_info: freq_base_train  = 10000.0
0.00.038.768 I print_info: freq_scale_train = 1
0.00.038.768 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.768 I print_info: rope_finetuned   = unknown
0.00.038.768 I print_info: ssm_d_conv       = 0
0.00.038.768 I print_info: ssm_d_inner      = 0
0.00.038.770 I print_info: ssm_d_state      = 0
0.00.038.770 I print_info: ssm_dt_rank      = 0
0.00.038.770 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.770 I print_info: model type       = 1.4B
0.00.038.771 I print_info: model params     = 1.41 B
0.00.038.773 I print_info: general.name     = 1.4B
0.00.038.773 I print_info: vocab type       = BPE
0.00.038.773 I print_info: n_vocab          = 50304
0.00.038.774 I print_info: n_merges         = 50009
0.00.038.774 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.774 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.774 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.774 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.775 I print_info: LF token         = 128 'Ä'
0.00.038.775 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.775 I print_info: max token length = 1024
0.00.336.122 I load_tensors: offloading 24 repeating layers to GPU
0.00.336.135 I load_tensors: offloading output layer to GPU
0.00.336.136 I load_tensors: offloaded 25/25 layers to GPU
0.00.336.167 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.336.168 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.337.729 I llama_init_from_model: n_seq_max     = 1
0.00.337.735 I llama_init_from_model: n_ctx         = 128
0.00.337.735 I llama_init_from_model: n_ctx_per_seq = 128
0.00.337.736 I llama_init_from_model: n_batch       = 128
0.00.337.736 I llama_init_from_model: n_ubatch      = 128
0.00.337.736 I llama_init_from_model: flash_attn    = 0
0.00.337.739 I llama_init_from_model: freq_base     = 10000.0
0.00.337.739 I llama_init_from_model: freq_scale    = 1
0.00.337.740 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.337.747 I ggml_metal_init: allocating
0.00.337.825 I ggml_metal_init: found device: Apple M4
0.00.337.839 I ggml_metal_init: picking default device: Apple M4
0.00.339.710 I ggml_metal_init: using embedded metal library
0.00.345.737 I ggml_metal_init: GPU name:   Apple M4
0.00.345.754 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.345.755 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.345.756 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.345.757 I ggml_metal_init: simdgroup reduction   = true
0.00.345.757 I ggml_metal_init: simdgroup matrix mul. = true
0.00.345.758 I ggml_metal_init: has residency sets    = true
0.00.345.758 I ggml_metal_init: has bfloat            = true
0.00.345.758 I ggml_metal_init: use bfloat            = true
0.00.345.763 I ggml_metal_init: hasUnifiedMemory      = true
0.00.345.767 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.367.268 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.370.966 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.370.976 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.371.007 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.374.274 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.374.276 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.374.277 I llama_init_from_model: graph nodes  = 967
0.00.374.277 I llama_init_from_model: graph splits = 2
0.00.374.280 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.374.281 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.406.188 I 
0.00.406.267 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.406.285 I perplexity: tokenizing the input ..
0.00.413.086 I perplexity: tokenization took 6.797 ms
0.00.413.104 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.557.657 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.558.995 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.559.007 I llama_perf_context_print:        load time =     397.14 ms
0.00.559.008 I llama_perf_context_print: prompt eval time =     143.85 ms /   128 tokens (    1.12 ms per token,   889.83 tokens per second)
0.00.559.009 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.559.009 I llama_perf_context_print:       total time =     152.82 ms /   129 tokens
0.00.559.390 I ggml_metal_free: deallocating

real	0m0.573s
user	0m0.080s
sys	0m0.092s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4568 (a4417ddd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.798 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.606 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.611 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.613 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.614 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.614 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.614 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.615 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.615 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.616 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.616 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.617 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.617 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.617 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.618 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.619 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.619 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.620 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.518 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.570 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.458 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.459 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.459 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.460 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.460 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.460 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.461 I llama_model_loader: - type  f32:  194 tensors
0.00.026.461 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.461 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.461 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.462 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.462 I print_info: file format = GGUF V3 (latest)
0.00.026.463 I print_info: file type   = Q3_K - Medium
0.00.026.464 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.034.625 I load: special tokens cache size = 25
0.00.040.577 I load: token to piece cache size = 0.2984 MB
0.00.040.579 I print_info: arch             = gptneox
0.00.040.579 I print_info: vocab_only       = 0
0.00.040.579 I print_info: n_ctx_train      = 2048
0.00.040.580 I print_info: n_embd           = 2048
0.00.040.580 I print_info: n_layer          = 24
0.00.040.582 I print_info: n_head           = 16
0.00.040.583 I print_info: n_head_kv        = 16
0.00.040.583 I print_info: n_rot            = 32
0.00.040.583 I print_info: n_swa            = 0
0.00.040.584 I print_info: n_embd_head_k    = 128
0.00.040.585 I print_info: n_embd_head_v    = 128
0.00.040.586 I print_info: n_gqa            = 1
0.00.040.586 I print_info: n_embd_k_gqa     = 2048
0.00.040.587 I print_info: n_embd_v_gqa     = 2048
0.00.040.588 I print_info: f_norm_eps       = 1.0e-05
0.00.040.588 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.588 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.588 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.589 I print_info: f_logit_scale    = 0.0e+00
0.00.040.589 I print_info: n_ff             = 8192
0.00.040.590 I print_info: n_expert         = 0
0.00.040.590 I print_info: n_expert_used    = 0
0.00.040.590 I print_info: causal attn      = 1
0.00.040.590 I print_info: pooling type     = 0
0.00.040.590 I print_info: rope type        = 2
0.00.040.590 I print_info: rope scaling     = linear
0.00.040.593 I print_info: freq_base_train  = 10000.0
0.00.040.593 I print_info: freq_scale_train = 1
0.00.040.593 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.593 I print_info: rope_finetuned   = unknown
0.00.040.594 I print_info: ssm_d_conv       = 0
0.00.040.594 I print_info: ssm_d_inner      = 0
0.00.040.594 I print_info: ssm_d_state      = 0
0.00.040.594 I print_info: ssm_dt_rank      = 0
0.00.040.594 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.594 I print_info: model type       = 1.4B
0.00.040.595 I print_info: model params     = 1.41 B
0.00.040.595 I print_info: general.name     = 1.4B
0.00.040.595 I print_info: vocab type       = BPE
0.00.040.596 I print_info: n_vocab          = 50304
0.00.040.596 I print_info: n_merges         = 50009
0.00.040.596 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.598 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.598 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.598 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.598 I print_info: LF token         = 128 'Ä'
0.00.040.599 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.599 I print_info: max token length = 1024
0.00.443.222 I load_tensors: offloading 24 repeating layers to GPU
0.00.443.229 I load_tensors: offloading output layer to GPU
0.00.443.230 I load_tensors: offloaded 25/25 layers to GPU
0.00.443.263 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.443.264 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.444.615 I llama_init_from_model: n_seq_max     = 1
0.00.444.620 I llama_init_from_model: n_ctx         = 128
0.00.444.620 I llama_init_from_model: n_ctx_per_seq = 128
0.00.444.621 I llama_init_from_model: n_batch       = 128
0.00.444.621 I llama_init_from_model: n_ubatch      = 128
0.00.444.622 I llama_init_from_model: flash_attn    = 0
0.00.444.622 I llama_init_from_model: freq_base     = 10000.0
0.00.444.623 I llama_init_from_model: freq_scale    = 1
0.00.444.624 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.444.627 I ggml_metal_init: allocating
0.00.444.716 I ggml_metal_init: found device: Apple M4
0.00.444.757 I ggml_metal_init: picking default device: Apple M4
0.00.446.829 I ggml_metal_init: using embedded metal library
0.00.453.002 I ggml_metal_init: GPU name:   Apple M4
0.00.453.014 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.453.015 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.453.016 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.453.016 I ggml_metal_init: simdgroup reduction   = true
0.00.453.017 I ggml_metal_init: simdgroup matrix mul. = true
0.00.453.017 I ggml_metal_init: has residency sets    = true
0.00.453.017 I ggml_metal_init: has bfloat            = true
0.00.453.017 I ggml_metal_init: use bfloat            = true
0.00.453.022 I ggml_metal_init: hasUnifiedMemory      = true
0.00.453.025 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.473.930 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.477.648 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.477.651 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.477.696 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.480.961 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.480.962 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.480.963 I llama_init_from_model: graph nodes  = 967
0.00.480.963 I llama_init_from_model: graph splits = 2
0.00.480.966 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.480.966 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.513.096 I 
0.00.513.174 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.513.195 I perplexity: tokenizing the input ..
0.00.520.118 I perplexity: tokenization took 6.922 ms
0.00.520.137 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.666.110 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.667.529 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.667.543 I llama_perf_context_print:        load time =     504.29 ms
0.00.667.544 I llama_perf_context_print: prompt eval time =     145.02 ms /   128 tokens (    1.13 ms per token,   882.66 tokens per second)
0.00.667.544 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.667.545 I llama_perf_context_print:       total time =     154.45 ms /   129 tokens
0.00.667.930 I ggml_metal_free: deallocating

real	0m0.681s
user	0m0.080s
sys	0m0.108s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4568 (a4417ddd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.947 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.946 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.951 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.953 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.953 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.954 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.954 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.954 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.955 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.956 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.956 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.956 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.957 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.957 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.958 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.960 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.961 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.961 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.758 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.818 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.550 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.551 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.551 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.552 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.552 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.552 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.553 I llama_model_loader: - type  f32:  194 tensors
0.00.025.553 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.553 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.554 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.554 I print_info: file format = GGUF V3 (latest)
0.00.025.555 I print_info: file type   = Q4_K - Medium
0.00.025.555 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.310 I load: special tokens cache size = 25
0.00.039.303 I load: token to piece cache size = 0.2984 MB
0.00.039.306 I print_info: arch             = gptneox
0.00.039.306 I print_info: vocab_only       = 0
0.00.039.306 I print_info: n_ctx_train      = 2048
0.00.039.306 I print_info: n_embd           = 2048
0.00.039.306 I print_info: n_layer          = 24
0.00.039.310 I print_info: n_head           = 16
0.00.039.311 I print_info: n_head_kv        = 16
0.00.039.311 I print_info: n_rot            = 32
0.00.039.311 I print_info: n_swa            = 0
0.00.039.311 I print_info: n_embd_head_k    = 128
0.00.039.311 I print_info: n_embd_head_v    = 128
0.00.039.315 I print_info: n_gqa            = 1
0.00.039.315 I print_info: n_embd_k_gqa     = 2048
0.00.039.316 I print_info: n_embd_v_gqa     = 2048
0.00.039.317 I print_info: f_norm_eps       = 1.0e-05
0.00.039.319 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.319 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.319 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.319 I print_info: f_logit_scale    = 0.0e+00
0.00.039.320 I print_info: n_ff             = 8192
0.00.039.320 I print_info: n_expert         = 0
0.00.039.320 I print_info: n_expert_used    = 0
0.00.039.320 I print_info: causal attn      = 1
0.00.039.320 I print_info: pooling type     = 0
0.00.039.320 I print_info: rope type        = 2
0.00.039.321 I print_info: rope scaling     = linear
0.00.039.321 I print_info: freq_base_train  = 10000.0
0.00.039.321 I print_info: freq_scale_train = 1
0.00.039.322 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.322 I print_info: rope_finetuned   = unknown
0.00.039.322 I print_info: ssm_d_conv       = 0
0.00.039.323 I print_info: ssm_d_inner      = 0
0.00.039.323 I print_info: ssm_d_state      = 0
0.00.039.323 I print_info: ssm_dt_rank      = 0
0.00.039.327 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.327 I print_info: model type       = 1.4B
0.00.039.328 I print_info: model params     = 1.41 B
0.00.039.328 I print_info: general.name     = 1.4B
0.00.039.328 I print_info: vocab type       = BPE
0.00.039.329 I print_info: n_vocab          = 50304
0.00.039.330 I print_info: n_merges         = 50009
0.00.039.330 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.330 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.330 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.330 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.331 I print_info: LF token         = 128 'Ä'
0.00.039.331 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.331 I print_info: max token length = 1024
0.00.514.227 I load_tensors: offloading 24 repeating layers to GPU
0.00.514.242 I load_tensors: offloading output layer to GPU
0.00.514.243 I load_tensors: offloaded 25/25 layers to GPU
0.00.514.280 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.514.281 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.515.725 I llama_init_from_model: n_seq_max     = 1
0.00.515.731 I llama_init_from_model: n_ctx         = 128
0.00.515.731 I llama_init_from_model: n_ctx_per_seq = 128
0.00.515.732 I llama_init_from_model: n_batch       = 128
0.00.515.732 I llama_init_from_model: n_ubatch      = 128
0.00.515.733 I llama_init_from_model: flash_attn    = 0
0.00.515.735 I llama_init_from_model: freq_base     = 10000.0
0.00.515.735 I llama_init_from_model: freq_scale    = 1
0.00.515.736 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.515.743 I ggml_metal_init: allocating
0.00.515.832 I ggml_metal_init: found device: Apple M4
0.00.515.846 I ggml_metal_init: picking default device: Apple M4
0.00.517.630 I ggml_metal_init: using embedded metal library
0.00.524.283 I ggml_metal_init: GPU name:   Apple M4
0.00.524.288 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.524.289 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.524.289 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.524.290 I ggml_metal_init: simdgroup reduction   = true
0.00.524.290 I ggml_metal_init: simdgroup matrix mul. = true
0.00.524.291 I ggml_metal_init: has residency sets    = true
0.00.524.291 I ggml_metal_init: has bfloat            = true
0.00.524.291 I ggml_metal_init: use bfloat            = true
0.00.524.292 I ggml_metal_init: hasUnifiedMemory      = true
0.00.524.294 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.542.520 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.546.032 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.546.045 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.546.073 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.549.197 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.549.199 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.549.199 I llama_init_from_model: graph nodes  = 967
0.00.549.199 I llama_init_from_model: graph splits = 2
0.00.549.202 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.549.203 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.579.660 I 
0.00.579.752 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.579.772 I perplexity: tokenizing the input ..
0.00.586.985 I perplexity: tokenization took 7.21 ms
0.00.587.007 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.734.523 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.735.934 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.735.945 I llama_perf_context_print:        load time =     569.71 ms
0.00.735.946 I llama_perf_context_print: prompt eval time =     146.97 ms /   128 tokens (    1.15 ms per token,   870.93 tokens per second)
0.00.735.947 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.735.947 I llama_perf_context_print:       total time =     156.29 ms /   129 tokens
0.00.736.339 I ggml_metal_free: deallocating

real	0m0.751s
user	0m0.078s
sys	0m0.123s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4568 (a4417ddd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.054 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.027 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.032 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.036 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.036 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.037 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.037 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.037 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.038 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.039 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.039 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.039 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.040 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.040 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.041 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.042 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.042 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.043 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.954 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.979 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.858 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.859 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.859 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.859 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.860 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.860 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.860 I llama_model_loader: - type  f32:  194 tensors
0.00.024.861 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.861 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.862 I print_info: file format = GGUF V3 (latest)
0.00.024.862 I print_info: file type   = Q5_K - Medium
0.00.024.867 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.001 I load: special tokens cache size = 25
0.00.039.035 I load: token to piece cache size = 0.2984 MB
0.00.039.038 I print_info: arch             = gptneox
0.00.039.038 I print_info: vocab_only       = 0
0.00.039.038 I print_info: n_ctx_train      = 2048
0.00.039.038 I print_info: n_embd           = 2048
0.00.039.038 I print_info: n_layer          = 24
0.00.039.041 I print_info: n_head           = 16
0.00.039.042 I print_info: n_head_kv        = 16
0.00.039.042 I print_info: n_rot            = 32
0.00.039.043 I print_info: n_swa            = 0
0.00.039.043 I print_info: n_embd_head_k    = 128
0.00.039.043 I print_info: n_embd_head_v    = 128
0.00.039.044 I print_info: n_gqa            = 1
0.00.039.045 I print_info: n_embd_k_gqa     = 2048
0.00.039.045 I print_info: n_embd_v_gqa     = 2048
0.00.039.046 I print_info: f_norm_eps       = 1.0e-05
0.00.039.046 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.047 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.047 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.047 I print_info: f_logit_scale    = 0.0e+00
0.00.039.048 I print_info: n_ff             = 8192
0.00.039.048 I print_info: n_expert         = 0
0.00.039.048 I print_info: n_expert_used    = 0
0.00.039.048 I print_info: causal attn      = 1
0.00.039.049 I print_info: pooling type     = 0
0.00.039.049 I print_info: rope type        = 2
0.00.039.049 I print_info: rope scaling     = linear
0.00.039.049 I print_info: freq_base_train  = 10000.0
0.00.039.050 I print_info: freq_scale_train = 1
0.00.039.050 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.050 I print_info: rope_finetuned   = unknown
0.00.039.050 I print_info: ssm_d_conv       = 0
0.00.039.050 I print_info: ssm_d_inner      = 0
0.00.039.050 I print_info: ssm_d_state      = 0
0.00.039.051 I print_info: ssm_dt_rank      = 0
0.00.039.051 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.051 I print_info: model type       = 1.4B
0.00.039.051 I print_info: model params     = 1.41 B
0.00.039.052 I print_info: general.name     = 1.4B
0.00.039.052 I print_info: vocab type       = BPE
0.00.039.052 I print_info: n_vocab          = 50304
0.00.039.052 I print_info: n_merges         = 50009
0.00.039.053 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.053 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.053 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.053 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.054 I print_info: LF token         = 128 'Ä'
0.00.039.054 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.056 I print_info: max token length = 1024
0.00.586.882 I load_tensors: offloading 24 repeating layers to GPU
0.00.586.898 I load_tensors: offloading output layer to GPU
0.00.586.898 I load_tensors: offloaded 25/25 layers to GPU
0.00.586.930 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.586.931 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.588.510 I llama_init_from_model: n_seq_max     = 1
0.00.588.516 I llama_init_from_model: n_ctx         = 128
0.00.588.516 I llama_init_from_model: n_ctx_per_seq = 128
0.00.588.517 I llama_init_from_model: n_batch       = 128
0.00.588.518 I llama_init_from_model: n_ubatch      = 128
0.00.588.518 I llama_init_from_model: flash_attn    = 0
0.00.588.520 I llama_init_from_model: freq_base     = 10000.0
0.00.588.520 I llama_init_from_model: freq_scale    = 1
0.00.588.521 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.588.528 I ggml_metal_init: allocating
0.00.588.605 I ggml_metal_init: found device: Apple M4
0.00.588.618 I ggml_metal_init: picking default device: Apple M4
0.00.590.148 I ggml_metal_init: using embedded metal library
0.00.596.518 I ggml_metal_init: GPU name:   Apple M4
0.00.596.522 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.596.523 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.596.524 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.596.525 I ggml_metal_init: simdgroup reduction   = true
0.00.596.525 I ggml_metal_init: simdgroup matrix mul. = true
0.00.596.525 I ggml_metal_init: has residency sets    = true
0.00.596.525 I ggml_metal_init: has bfloat            = true
0.00.596.526 I ggml_metal_init: use bfloat            = true
0.00.596.526 I ggml_metal_init: hasUnifiedMemory      = true
0.00.596.528 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.613.969 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.617.529 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.617.533 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.617.560 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.620.934 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.620.935 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.620.936 I llama_init_from_model: graph nodes  = 967
0.00.620.936 I llama_init_from_model: graph splits = 2
0.00.620.940 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.620.940 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.653.579 I 
0.00.653.661 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.653.682 I perplexity: tokenizing the input ..
0.00.660.131 I perplexity: tokenization took 6.446 ms
0.00.660.147 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.800.903 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.802.320 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.802.331 I llama_perf_context_print:        load time =     644.52 ms
0.00.802.332 I llama_perf_context_print: prompt eval time =     140.37 ms /   128 tokens (    1.10 ms per token,   911.88 tokens per second)
0.00.802.332 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.802.332 I llama_perf_context_print:       total time =     148.76 ms /   129 tokens
0.00.802.729 I ggml_metal_free: deallocating

real	0m0.817s
user	0m0.077s
sys	0m0.136s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4568 (a4417ddd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.198 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.667 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.671 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.673 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.673 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.674 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.674 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.674 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.675 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.675 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.676 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.676 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.676 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.677 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.677 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.679 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.679 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.679 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.424 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.455 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.220 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.221 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.222 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.222 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.222 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.223 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.223 I llama_model_loader: - type  f32:  194 tensors
0.00.024.223 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.224 I print_info: file format = GGUF V3 (latest)
0.00.024.225 I print_info: file type   = Q6_K
0.00.024.225 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.031.936 I load: special tokens cache size = 25
0.00.037.796 I load: token to piece cache size = 0.2984 MB
0.00.037.799 I print_info: arch             = gptneox
0.00.037.799 I print_info: vocab_only       = 0
0.00.037.800 I print_info: n_ctx_train      = 2048
0.00.037.800 I print_info: n_embd           = 2048
0.00.037.800 I print_info: n_layer          = 24
0.00.037.802 I print_info: n_head           = 16
0.00.037.803 I print_info: n_head_kv        = 16
0.00.037.803 I print_info: n_rot            = 32
0.00.037.804 I print_info: n_swa            = 0
0.00.037.804 I print_info: n_embd_head_k    = 128
0.00.037.804 I print_info: n_embd_head_v    = 128
0.00.037.805 I print_info: n_gqa            = 1
0.00.037.805 I print_info: n_embd_k_gqa     = 2048
0.00.037.806 I print_info: n_embd_v_gqa     = 2048
0.00.037.807 I print_info: f_norm_eps       = 1.0e-05
0.00.037.807 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.807 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.807 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.809 I print_info: f_logit_scale    = 0.0e+00
0.00.037.809 I print_info: n_ff             = 8192
0.00.037.810 I print_info: n_expert         = 0
0.00.037.810 I print_info: n_expert_used    = 0
0.00.037.810 I print_info: causal attn      = 1
0.00.037.810 I print_info: pooling type     = 0
0.00.037.810 I print_info: rope type        = 2
0.00.037.811 I print_info: rope scaling     = linear
0.00.037.811 I print_info: freq_base_train  = 10000.0
0.00.037.811 I print_info: freq_scale_train = 1
0.00.037.811 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.812 I print_info: rope_finetuned   = unknown
0.00.037.812 I print_info: ssm_d_conv       = 0
0.00.037.812 I print_info: ssm_d_inner      = 0
0.00.037.812 I print_info: ssm_d_state      = 0
0.00.037.812 I print_info: ssm_dt_rank      = 0
0.00.037.812 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.813 I print_info: model type       = 1.4B
0.00.037.813 I print_info: model params     = 1.41 B
0.00.037.813 I print_info: general.name     = 1.4B
0.00.037.814 I print_info: vocab type       = BPE
0.00.037.814 I print_info: n_vocab          = 50304
0.00.037.814 I print_info: n_merges         = 50009
0.00.037.814 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.815 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.815 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.815 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.815 I print_info: LF token         = 128 'Ä'
0.00.037.816 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.816 I print_info: max token length = 1024
0.00.609.407 I load_tensors: offloading 24 repeating layers to GPU
0.00.609.412 I load_tensors: offloading output layer to GPU
0.00.609.414 I load_tensors: offloaded 25/25 layers to GPU
0.00.609.438 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.609.439 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.610.834 I llama_init_from_model: n_seq_max     = 1
0.00.610.837 I llama_init_from_model: n_ctx         = 128
0.00.610.837 I llama_init_from_model: n_ctx_per_seq = 128
0.00.610.838 I llama_init_from_model: n_batch       = 128
0.00.610.838 I llama_init_from_model: n_ubatch      = 128
0.00.610.839 I llama_init_from_model: flash_attn    = 0
0.00.610.840 I llama_init_from_model: freq_base     = 10000.0
0.00.610.841 I llama_init_from_model: freq_scale    = 1
0.00.610.842 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.610.843 I ggml_metal_init: allocating
0.00.610.858 I ggml_metal_init: found device: Apple M4
0.00.610.867 I ggml_metal_init: picking default device: Apple M4
0.00.612.175 I ggml_metal_init: using embedded metal library
0.00.618.336 I ggml_metal_init: GPU name:   Apple M4
0.00.618.339 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.618.340 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.618.340 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.618.341 I ggml_metal_init: simdgroup reduction   = true
0.00.618.341 I ggml_metal_init: simdgroup matrix mul. = true
0.00.618.342 I ggml_metal_init: has residency sets    = true
0.00.618.342 I ggml_metal_init: has bfloat            = true
0.00.618.342 I ggml_metal_init: use bfloat            = true
0.00.618.343 I ggml_metal_init: hasUnifiedMemory      = true
0.00.618.344 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.634.641 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.638.044 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.638.048 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.638.073 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.641.410 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.641.412 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.641.413 I llama_init_from_model: graph nodes  = 967
0.00.641.413 I llama_init_from_model: graph splits = 2
0.00.641.415 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.641.415 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.673.567 I 
0.00.673.645 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.673.667 I perplexity: tokenizing the input ..
0.00.681.160 I perplexity: tokenization took 7.489 ms
0.00.681.181 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.822.410 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.823.738 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.823.753 I llama_perf_context_print:        load time =     664.36 ms
0.00.823.754 I llama_perf_context_print: prompt eval time =     140.33 ms /   128 tokens (    1.10 ms per token,   912.16 tokens per second)
0.00.823.755 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.823.755 I llama_perf_context_print:       total time =     150.19 ms /   129 tokens
0.00.824.143 I ggml_metal_free: deallocating

real	0m0.838s
user	0m0.077s
sys	0m0.147s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.245 I build: 4568 (a4417ddd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.711 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.282 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.290 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.293 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.294 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.294 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.295 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.296 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.297 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.298 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.299 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.299 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.300 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.300 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.302 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.304 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.305 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.306 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.732 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.752 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.501 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.504 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.504 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.505 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.505 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.506 I llama_model_loader: - type  f32:  194 tensors
0.00.055.506 I llama_model_loader: - type  f16:   98 tensors
0.00.055.507 I print_info: file format = GGUF V3 (latest)
0.00.055.508 I print_info: file type   = all F32 (guessed)
0.00.055.509 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.068.038 I load: special tokens cache size = 25
0.00.075.902 I load: token to piece cache size = 0.2984 MB
0.00.075.906 I print_info: arch             = gptneox
0.00.075.906 I print_info: vocab_only       = 0
0.00.075.906 I print_info: n_ctx_train      = 2048
0.00.075.906 I print_info: n_embd           = 2048
0.00.075.907 I print_info: n_layer          = 24
0.00.075.910 I print_info: n_head           = 16
0.00.075.911 I print_info: n_head_kv        = 16
0.00.075.911 I print_info: n_rot            = 32
0.00.075.911 I print_info: n_swa            = 0
0.00.075.911 I print_info: n_embd_head_k    = 128
0.00.075.911 I print_info: n_embd_head_v    = 128
0.00.075.912 I print_info: n_gqa            = 1
0.00.075.913 I print_info: n_embd_k_gqa     = 2048
0.00.075.915 I print_info: n_embd_v_gqa     = 2048
0.00.075.915 I print_info: f_norm_eps       = 1.0e-05
0.00.075.916 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.075.916 I print_info: f_clamp_kqv      = 0.0e+00
0.00.075.917 I print_info: f_max_alibi_bias = 0.0e+00
0.00.075.917 I print_info: f_logit_scale    = 0.0e+00
0.00.075.918 I print_info: n_ff             = 8192
0.00.075.918 I print_info: n_expert         = 0
0.00.075.918 I print_info: n_expert_used    = 0
0.00.075.919 I print_info: causal attn      = 1
0.00.075.919 I print_info: pooling type     = 0
0.00.075.919 I print_info: rope type        = 2
0.00.075.919 I print_info: rope scaling     = linear
0.00.075.922 I print_info: freq_base_train  = 10000.0
0.00.075.922 I print_info: freq_scale_train = 1
0.00.075.922 I print_info: n_ctx_orig_yarn  = 2048
0.00.075.923 I print_info: rope_finetuned   = unknown
0.00.075.923 I print_info: ssm_d_conv       = 0
0.00.075.923 I print_info: ssm_d_inner      = 0
0.00.075.923 I print_info: ssm_d_state      = 0
0.00.075.923 I print_info: ssm_dt_rank      = 0
0.00.075.923 I print_info: ssm_dt_b_c_rms   = 0
0.00.075.924 I print_info: model type       = 1.4B
0.00.075.924 I print_info: model params     = 1.41 B
0.00.075.924 I print_info: general.name     = 1.4B
0.00.075.925 I print_info: vocab type       = BPE
0.00.075.925 I print_info: n_vocab          = 50304
0.00.075.925 I print_info: n_merges         = 50009
0.00.075.925 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.075.925 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.075.926 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.075.926 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.075.926 I print_info: LF token         = 128 'Ä'
0.00.075.926 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.075.927 I print_info: max token length = 1024
0.01.325.621 I load_tensors: offloading 24 repeating layers to GPU
0.01.325.627 I load_tensors: offloading output layer to GPU
0.01.325.628 I load_tensors: offloaded 25/25 layers to GPU
0.01.325.654 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.325.656 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.326.450 I llama_init_from_model: n_seq_max     = 1
0.01.326.451 I llama_init_from_model: n_ctx         = 128
0.01.326.451 I llama_init_from_model: n_ctx_per_seq = 128
0.01.326.452 I llama_init_from_model: n_batch       = 128
0.01.326.452 I llama_init_from_model: n_ubatch      = 128
0.01.326.452 I llama_init_from_model: flash_attn    = 0
0.01.326.455 I llama_init_from_model: freq_base     = 10000.0
0.01.326.456 I llama_init_from_model: freq_scale    = 1
0.01.326.456 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.326.457 I ggml_metal_init: allocating
0.01.326.557 I ggml_metal_init: found device: Apple M4
0.01.326.564 I ggml_metal_init: picking default device: Apple M4
0.01.327.649 I ggml_metal_init: using embedded metal library
0.01.331.725 I ggml_metal_init: GPU name:   Apple M4
0.01.331.727 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.331.728 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.331.728 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.331.729 I ggml_metal_init: simdgroup reduction   = true
0.01.331.729 I ggml_metal_init: simdgroup matrix mul. = true
0.01.331.729 I ggml_metal_init: has residency sets    = true
0.01.331.729 I ggml_metal_init: has bfloat            = true
0.01.331.729 I ggml_metal_init: use bfloat            = true
0.01.331.730 I ggml_metal_init: hasUnifiedMemory      = true
0.01.331.731 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.342.787 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.344.541 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.344.543 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.344.558 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.346.157 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.346.158 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.346.158 I llama_init_from_model: graph nodes  = 967
0.01.346.159 I llama_init_from_model: graph splits = 2
0.01.346.160 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.346.160 I 
0.01.346.187 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.346.188 I compute_imatrix: tokenizing the input ..
0.01.350.452 I compute_imatrix: tokenization took 4.263 ms
0.01.350.454 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.617.812 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.620.384 I llama_perf_context_print:        load time =    1594.10 ms
0.01.620.385 I llama_perf_context_print: prompt eval time =     265.58 ms /   128 tokens (    2.07 ms per token,   481.96 tokens per second)
0.01.620.386 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.620.386 I llama_perf_context_print:       total time =    1596.66 ms /   129 tokens
0.01.620.931 I ggml_metal_free: deallocating

real	0m1.806s
user	0m0.128s
sys	0m0.242s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4568 (a4417ddd)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13a2044a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13a204b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13a205a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13a205eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13a208c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13a2090c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13a209530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13a2099a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13a209e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13a20a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13a20a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13a20ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13a20b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13a20c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13a20c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13a20cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13a20d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13a20ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13a20e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13a20eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13a20f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13a20fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13a210200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13a210aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13a2111c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13a211480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13a211740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13a211bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13a2122d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13a212740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13a212d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13a213210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13a213680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13a213940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13a213db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13a214220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13a214780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13a214c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13a215180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13a215680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13a215b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13a216080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13a216580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13a216a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13a216f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13a2173f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13a217860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13a217cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13a218460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13a2188d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13a218d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13a2191b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13a219620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13a219a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13a21a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13a21a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13a21aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13a21ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13a21b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13a21bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13a21be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13a21c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13a21c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13a21cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13a21d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13a21d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13a21d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13a21de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13a21e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13a21e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13a21ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13a21f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13a21f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13a21fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13a220030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13a220580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13a220ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13a221020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13a221570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13a221ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13a222010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13a222560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13a222ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13a223000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13a223550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13a223aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13a223ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13a224540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13a224a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13a224fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13a225530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13a225a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13a225fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13a226520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13a226a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13a226fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13a227510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13a217f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13a227980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13a228130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13a228680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13a228bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13a229120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13a229670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13a229bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13a22a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13a22a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13a22abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13a22b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13a22b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13a22bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13a22c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13a22c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13a22cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13a22cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13a22d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13a22d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13a22dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13a22e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13a22e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13a22eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13a22efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13a22f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13a22f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13a22fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13a230110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13a230610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13a230b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13a231010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13a231510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13a231a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13a231f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13a232410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13a232910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13a232e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13a233310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13a233810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13a233d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13a234210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13a234710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13a234c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13a235110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13a235610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13a235b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13a236010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13a236510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13a236a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13a236f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13a237410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13a237910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13a237e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13a238310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13a238810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13a238d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13a239210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13a239710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13a239c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13a23a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13a23a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13a23ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13a23b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13a23b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13a23ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13a23bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13a23c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13a23c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13a23ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13a23d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13a23d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13a23dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13a23e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13a23e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13a23ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13a23f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13a23f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13a23fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13a240010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13a240510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13a240a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13a240f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13a241410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13a241910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13a241e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13a242310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13a242810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13a242d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13a243210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13a243710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13a243c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13a244110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13a244610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13a244b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13a245010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13a245510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13a245ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13a246070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13a246620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13a246bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13a2471e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13a2477f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13a247e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13a2485f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13a248a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13a248d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13a249360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13a249970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13a24a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13a24a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13a24aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13a24af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13a24b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13a24bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13a24c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13a24c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13a24cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13a24d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13a24d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13a24dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13a24e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13a24e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13a24ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13a24f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13a24f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13a24fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13a250150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13a2506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13a250bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13a251140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13a251690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13a251be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13a252130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13a252680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13a252bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13a253120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13a253670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13a253bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13a254110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13a254660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13a254bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13a255100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13a255650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13a255ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13a2560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13a256640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13a256b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13a2570e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13a257630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13a257b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13a2580d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13a258620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13a258b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13a2590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13a259610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13a259b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13a25a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13a25a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13a25ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13a25b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13a25b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13a25bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13a25c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13a25c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13a25cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13a25d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13a25d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13a25db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13a25e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13a25e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13a25e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13a25ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13a25f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13a25f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13a25fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13a2600d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13a260570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13a260a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13a260eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13a261350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13a2617f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13a261c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13a262130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13a2625d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13a262b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13a263240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13a263960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13a264080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13a2647a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13a264a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13a265250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13a265510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13a265b20 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.707.920 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.707.924 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13cb04bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13cb05040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13cb054b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13cb05920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13cb05d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13cb06200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13cb06670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13cb06ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13cb06f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13cb073c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13cb07830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13cb07f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13cb08a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13cb091f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13cb09a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13cb0a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13cb0a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13cb0af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13cb0b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13cb0bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13cb0c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13cb0cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13cb0d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13cb0da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13cb0e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13cb0e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13cb0e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13cb0eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13cb0efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13cb0f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13cb0f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13cb0fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13cb10230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13cb104f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13cb10960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13cb10dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13cb11240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13cb116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13cb11b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13cb11f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13cb12400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13cb12870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13cb12ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13cb13150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13cb135c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13cb13a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13cb13ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13cb14310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13cb14780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13cb14bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13cb15060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13cb154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13cb15940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13cb15db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13cb16220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13cb16690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13cb16c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13cb17100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13cb17570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13cb179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13cb17e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13cb182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13cb18730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13cb18ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13cb19010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13cb19480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13cb198f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13cb19d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13cb1a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13cb1a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13cb1aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13cb1af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13cb1b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13cb1b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13cb1bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13cb1c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13cb1c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13cb1c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13cb1ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13cb1d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13cb1d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13cb1db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13cb1dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13cb1e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13cb1e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13cb1ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13cb1f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13cb1f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13cb1fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13cb1ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13cb20370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13cb207e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13cb20c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13cb210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13cb21530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13cb219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13cb21e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13cb22280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13cb226f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13cb22b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13cb22fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13cb23440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13cb238b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13cb23d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13cb24190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13cb24600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13cb24a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13cb24ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13cb25350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13cb257c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13cb25c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13cb260a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13cb26510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13cb26980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13cb26df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13cb27260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13cb276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13cb27b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13cb27fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13cb28420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13cb28890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13cb28d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13cb29170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13cb295e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13cb29a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13cb29ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13cb2a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13cb2a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13cb2ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13cb2b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13cb2b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13cb2b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13cb2bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13cb2c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13cb2c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13cb2cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13cb2cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13cb2d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13cb2d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13cb2dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13cb2e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13cb2e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13cb2ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13cb2eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13cb2f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13cb2f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13cb2fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13cb30060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13cb304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13cb30940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13cb30db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13cb31220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13cb31690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13cb31b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13cb31f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13cb323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13cb32850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13cb32cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13cb33130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13cb335a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13cb33a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13cb33e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13cb342f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13cb34760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13cb34bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13cb35040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13cb35c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13cb35f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13cb361f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13cb36660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13cb36ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13cb36f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13cb373b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13cb37820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13cb37c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13cb38100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13cb38570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13cb389e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13cb38e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13cb392c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13cb39730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13cb39ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13cb3a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13cb3a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13cb3a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13cb3ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13cb3b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13cb3b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13cb3bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13cb3bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13cb3c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13cb3c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13cb3cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13cb3d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13cb3d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13cb3d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13cb3de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13cb3e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13cb3e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13cb3eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13cb3eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13cb3f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13cb3f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13cb3fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13cb40340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13cb407b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13cb40c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13cb41090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13cb415b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13cb41ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13cb42630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13cb428f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13cb42eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13cb43470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13cb43a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13cb43ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13cb445b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13cb44b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13cb45130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13cb456f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13cb45cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13cb46270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13cb46830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13cb46df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13cb473b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13cb47970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13cb47f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13cb484f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13cb48ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13cb49070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13cb49630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13cb49bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13cb4a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13cb4a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13cb4ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13cb4b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13cb4b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13cb4be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13cb4c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13cb4c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13cb4cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13cb4d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13cb4db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13cb4e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13cb4e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13cb4ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13cb4f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13cb4f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13cb4fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13cb50370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13cb50930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13cb50ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13cb514b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13cb51a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13cb52030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13cb525f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13cb52bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13cb53170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13cb53730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13cb53cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13cb542b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13cb54870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13cb54e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13cb553f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13cb559b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13cb55f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13cb56530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13cb56af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13cb56ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13cb574f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13cb579f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13cb57ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13cb583f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13cb588f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13cb58df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13cb592f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13cb597f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13cb59cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13cb5a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13cb5a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13cb5abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13cb5b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13cb5b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13cb5c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13cb5c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13cb5ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13cb5d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13cb5d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13cb5e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13cb5e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13cb5e8e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13a2457d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13a2474a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13a249010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13a2657d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13a246e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13a247ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13a249620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13a210740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13a20b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13a20a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13a21b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13a219d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13a264d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13a2480c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13a265f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13a266240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13a266500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13a2667c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13a266a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13a266d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13a267000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13a2672c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13a267580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13a267840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13a267b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13a267dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13a268080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13a268340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13a268600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13a2688c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13a268b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13a268e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13a269100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13a2693c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13a269680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13a269940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13a269c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13a269ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13a26a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13a26a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13a26a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13a26a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13a26ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13a26af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13a26b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13a26b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13a26b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13a26ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13a26bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13a26bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13a26c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13a26c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13a26c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13a26cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13a26cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13a26d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13a26d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13a26d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13a26d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13a26db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13a26de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13a26e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13a26e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13a26e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13a26e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13a26ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13a26ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13a26f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13a26f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13a26f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13a26f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13a26fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13a26ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13a2701c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13a270480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13a270740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13a270a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13a270cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13a270f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13a271240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13a271500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13a2717c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13a271a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13a271d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13a272000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13a2722c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13a272580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13a272840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13a272b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13a272dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13a273080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13a273340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13a273600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13a2738c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13a273b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13a273e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13a274100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13a2743c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13a274680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13a274940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13a274c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13a274ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13a275180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13a275440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13a275700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13a2759c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13a275c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13a275f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13a276200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13a2764c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13a276780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13a276a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13a276d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13a276fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13a277280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13a277540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13a277800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13a277ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13a277d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13a278040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13a278300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13a2785c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13a278880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13a278b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13a278e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13a2790c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13a279380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13a279640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13a279900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13a279bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13a279e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13a27a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13a27a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13a27a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13a27a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13a27ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13a27af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13a27b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13a27b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13a27b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13a27ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13a27bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13a27bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13a27c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13a27c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13a27c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13a27ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13a27cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13a27d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13a27d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13a27d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13a27d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13a27db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13a27ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13a27e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13a27e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13a27e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13a27e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13a27eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13a27ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13a27f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13a27f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13a27f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13a27f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13a27fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13a27fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13a280180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13a280440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13a280700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13a2809c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13a280c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13a280f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13a281200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13a2814c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13a281780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13a281a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13a281d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13a281fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13a282280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13a282540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13a282800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13a282ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13a282d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13a283040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13a283300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13a2835c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13a283880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13a283b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13a283e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13a2840c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13a284380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13a284640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13a284900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13a284bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13a284e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13a285140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13a285400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13a2856c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13a285980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13a285c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13a285f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13a2861c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13a286480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13a286740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13a286a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13a286cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13a286f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13a287240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13a287500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13a2877c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13a287d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13a288050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13a288310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13a2885d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13a288890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13a288b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13a288e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13a2890d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13a289390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13a289650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13a289910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13a289bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13a289e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13a28a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13a28a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13a28a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13a28a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13a28ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13a28af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13a28b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13a28b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13a28b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13a28ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13a28bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13a28bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13a28c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13a28c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13a28c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13a28ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13a28cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13a28d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13a28d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13a28d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13a28d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13a28db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13a28ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13a28e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13a28e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13a28e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13a28e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13a28ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13a28f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13a28f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13a28fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13a290360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13a2908b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13a290e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13a291350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13a2918a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13a291df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13a292340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13a292890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13a292de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13a293330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13a293880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13a293dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13a294320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13a2945e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13a2948a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13a294b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13a294fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13a295440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13a2958b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13a295d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13a296190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13a296600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13a296a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13a296ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13a297350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13a2977c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13a297c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13a2980a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13a298510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13a298980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13a299670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13a299d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13a29a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13a29a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13a29abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13a29b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13a29b7f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.794s
user	0m0.282s
sys	0m0.324s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4568 (a4417ddd)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x141107780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x141107e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x141108440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1411089f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x141108fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x141109550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x141109b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14110a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14110a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14110ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14110b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14110b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14110c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14110c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14110d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14110d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14110de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14110e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14110ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14110f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14110fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1411102d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1411109f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x141111290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1411119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x141111c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x141112280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x141112ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x141113430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1411136f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x141113b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x141113e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1411146e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x141114c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x141114ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x141115380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x141115820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x141115cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x141116160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x141116600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x141116aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x141116f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1411173e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x141117880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x141117b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x141118150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x141118760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x141119080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x141119690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x141119ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14111a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14111a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14111aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14111b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14111bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14111c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14111c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14111c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14111cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14111d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14111d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14111de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14111e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14111e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14111ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14111f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14111f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14111f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14111fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x141120330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1411207d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x141120c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x141121110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x141121660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x141121bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x141122100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x141122650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x141122ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1411230f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x141123640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x141123b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1411240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x141124630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x141124b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1411250d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x141125620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x141125b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1411260c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x141126610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x141126b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1411270b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x141127600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x141127b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1411280a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1411285f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x141128b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x141129090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x141118d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x141129500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x141129cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14112a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14112a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14112aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14112b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14112b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14112bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14112c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14112c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14112cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14112d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14112d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14112dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14112e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14112e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14112eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14112efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14112f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14112f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14112fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x141130220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1411306c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x141130b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x141131000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1411314a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x141131940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x141131de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x141132280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x141132720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x141132bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x141133060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x141133500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1411339a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x141133e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1411342e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x141134780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x141134c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1411350c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x141135560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x141135a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x141135ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x141136340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1411367e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x141136c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x141137120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1411375c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x141137a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x141137f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1411383a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x141138840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x141138ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x141139180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x141139620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x141139ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x141139f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14113a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14113a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14113ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14113b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14113b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14113bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14113bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14113c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14113c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14113cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14113d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14113d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14113db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14113e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14113e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14113e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14113ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14113f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14113f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14113fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x141140080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x141140520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1411409c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x141140e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x141141300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1411417a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x141141c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1411420e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x141142580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x141142a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x141142ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x141143360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x141143800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x141143ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x141144140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1411445e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x141144a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x141144f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1411453c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x141145910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x141145e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1411463b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x141146900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x141146bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1411471d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1411477e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x141147df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1411485e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x141148a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x141148d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x141149350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x141149960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14114a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14114a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14114aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14114af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14114b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14114bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14114c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14114c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14114cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14114d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14114d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14114dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14114e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14114e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14114ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14114f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14114f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14114fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x141150140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x141150690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x141150be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x141151130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x141151680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x141151bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x141152120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x141152670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x141152bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x141153110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x141153660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x141153bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x141154100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x141154650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x141154ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1411550f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x141155640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x141155b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1411560e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x141156630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x141156b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1411570d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x141157620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x141157b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1411580c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x141158610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x141158b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1411590b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x141159600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x141159b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14115a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14115a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14115ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14115b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14115b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14115bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14115c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14115c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14115cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14115d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14115d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14115db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14115e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14115e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14115e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14115ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14115f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14115f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14115fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1411600c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x141160560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x141160a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x141160ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x141161340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1411617e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x141161c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x141162120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1411625c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x141162b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x141163230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x141163950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x141164070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x141164790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x141164a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x141165240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x141165500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x141165b10 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.101.283 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.101.287 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13fe07b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13fe08190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13fe08600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13fe0ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13fe0b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13fe0b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13fe0b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13fe0be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13fe0c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13fe0c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13fe0cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13fe0d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13fe0dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13fe0e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13fe0eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13fe0f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13fe0fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13fe10300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13fe10a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13fe111f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13fe11910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13fe12030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13fe12750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13fe12e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13fe13590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13fe13850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13fe13b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13fe13f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13fe143f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13fe14860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13fe14cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13fe15200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13fe15670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13fe15930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13fe15da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13fe16210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13fe16680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13fe16af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13fe16f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13fe173d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13fe17840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13fe17cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13fe18120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13fe18590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13fe18a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13fe18e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13fe192e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13fe19750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13fe19bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13fe1a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13fe1a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13fe1a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13fe1ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13fe1b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13fe1b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13fe1bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13fe1c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13fe1c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13fe1c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13fe1ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13fe1d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13fe1d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13fe1db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13fe1dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13fe1e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13fe1e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13fe1ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13fe1f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13fe1f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13fe1fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13fe1fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13fe20360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13fe207d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13fe20c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13fe210b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13fe21520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13fe21990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13fe21e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13fe22270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13fe226e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13fe22b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13fe22fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13fe23430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13fe238a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13fe23d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13fe24180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13fe245f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13fe24a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13fe24ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13fe25340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13fe257b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13fe25c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13fe26090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13fe26500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13fe26970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13fe26de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13fe27250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13fe276c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13fe27b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13fe27fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13fe28410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13fe28880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13fe28cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13fe29160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13fe295d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13fe29a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13fe29eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13fe2a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13fe2a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13fe2ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13fe2b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13fe2b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13fe2b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13fe2bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13fe2c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13fe2c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13fe2cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13fe2cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13fe2d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13fe2d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13fe2dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13fe2e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13fe2e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13fe2ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13fe2ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13fe2f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13fe2f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13fe2fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13fe30050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13fe304c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13fe30930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13fe30da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13fe31210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13fe31680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13fe31af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13fe31f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13fe323d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13fe32840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13fe32cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13fe33120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13fe33590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13fe33a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13fe33e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13fe342e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13fe34750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13fe34bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13fe35030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13fe354a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13fe35910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13fe35d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13fe361f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13fe36660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13fe36ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13fe36f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13fe373b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13fe37820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13fe37c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13fe38100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13fe38570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13fe389e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13fe38e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13fe392c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13fe39730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13fe39ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13fe3a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13fe3a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13fe3b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13fe3b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13fe3b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13fe3baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13fe3bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13fe3c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13fe3c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13fe3cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13fe3d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13fe3d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13fe3d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13fe3de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13fe3e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13fe3e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13fe3eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13fe3efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13fe3f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13fe3f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13fe3fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13fe401a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13fe40610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13fe40a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13fe40ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13fe41360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13fe417d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13fe41c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13fe420b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13fe42520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13fe42990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13fe42e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13fe43270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13fe436e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13fe43b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13fe43fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13fe44430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13fe448a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13fe44e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13fe45310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13fe45780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13fe45bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13fe46060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13fe464d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13fe469f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13fe46f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13fe47a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13fe47d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13fe482f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13fe488b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13fe48e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13fe49430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13fe499f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13fe49fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13fe4a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13fe4ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13fe4b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13fe4b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13fe4bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13fe4c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13fe4c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13fe4cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13fe4d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13fe4d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13fe4def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13fe4e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13fe4ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13fe4f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13fe4f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13fe4fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13fe50170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13fe50730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13fe50cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13fe512b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13fe51870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13fe51e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13fe523f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13fe529b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13fe52f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13fe53530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13fe53af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13fe540b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13fe54670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13fe54c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13fe551f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13fe557b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13fe55d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13fe56330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13fe568f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13fe56eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13fe57470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13fe57a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13fe57ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13fe585b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13fe58b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13fe59130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13fe596f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13fe59cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13fe5a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13fe5a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13fe5adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13fe5b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13fe5b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13fe5bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13fe5c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13fe5c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13fe5ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13fe5d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13fe5d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13fe5dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13fe5e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13fe5e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13fe5ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13fe5f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13fe5f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13fe5fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13fe60030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13fe60530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13fe60a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13fe61440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13fe61b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13fe62280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13fe629a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13fe62c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13fe63450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13fe63710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13fe63d20 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13ff07850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13ff07cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13ff08130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13ff085a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13ff08a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13ff08e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13ff092f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13ff09760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13ff09bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13ff0a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13ff0a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13ff0ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13ff0b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13ff0be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13ff0c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13ff0cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13ff0d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13ff0db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13ff0e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13ff0ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13ff0f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13ff0f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13ff0ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13ff10700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13ff10e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13ff110e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13ff113a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13ff11810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13ff11c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13ff120f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13ff125f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13ff12b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13ff12f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13ff13230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13ff136a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13ff13b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13ff14070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13ff14570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13ff14a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13ff14f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13ff15470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13ff15970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13ff15e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13ff16370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13ff16870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13ff16ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13ff17150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13ff175c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13ff17a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13ff17ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13ff18310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13ff18780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13ff18bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13ff19060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13ff194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13ff19ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13ff1a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13ff1a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13ff1aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13ff1b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13ff1b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13ff1bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13ff1bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13ff1c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13ff1c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13ff1cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13ff1d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13ff1d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13ff1dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13ff1e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13ff1e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13ff1e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13ff1ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13ff1f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13ff1f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13ff1fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13ff20360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13ff208b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13ff20e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13ff21350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13ff218a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13ff21df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13ff22340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13ff22890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13ff22de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13ff23330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13ff23880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13ff23dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13ff24320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13ff24870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13ff24dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13ff25310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13ff25860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13ff25db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13ff26300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13ff26850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13ff26da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13ff272f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13ff27840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13ff27d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13ff282e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13ff28830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13ff28d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13ff292d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13ff29820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13ff29d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13ff2a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13ff2a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13ff2ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13ff2b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13ff2b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13ff2bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13ff2c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13ff2c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13ff2cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13ff2d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13ff2d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13ff2d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13ff2de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13ff2e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13ff2e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13ff2ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13ff2f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13ff2f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13ff2fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13ff2fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13ff30360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13ff30800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13ff30ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13ff31140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13ff315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13ff31a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13ff31f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13ff323c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13ff32860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13ff32d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13ff331a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13ff33640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13ff33ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13ff33f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13ff34420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13ff348c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13ff34d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13ff35200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13ff356a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13ff35b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13ff35fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13ff36480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13ff36920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13ff36dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13ff37260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13ff37700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13ff37ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13ff38040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13ff384e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13ff38980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13ff38e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13ff392c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13ff39760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13ff39c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13ff3a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13ff3a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13ff3a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13ff3ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13ff3b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13ff3b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13ff3bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13ff3c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13ff3c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13ff3ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13ff3cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13ff3d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13ff3d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13ff3dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13ff3e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13ff3e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13ff3eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13ff3ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13ff3f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13ff3f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13ff3fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13ff401c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13ff40660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13ff40b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13ff40fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13ff41440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13ff418e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13ff41d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13ff42220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13ff426c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13ff42b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13ff43000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13ff434a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13ff439f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13ff43f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13ff44490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13ff449e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13ff44ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13ff452b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13ff458c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13ff45ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13ff466c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13ff46b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13ff46e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13ff47430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13ff47a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13ff48230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13ff486d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13ff48b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13ff49010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13ff497c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13ff49d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13ff4a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13ff4a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13ff4ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13ff4b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13ff4b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13ff4bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13ff4c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13ff4c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13ff4cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13ff4d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13ff4d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13ff4dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13ff4e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13ff4e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13ff4ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13ff4f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13ff4f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13ff4fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13ff50200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13ff50750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13ff50ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13ff511f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13ff51740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13ff51c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13ff521e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13ff52730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13ff52c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13ff531d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13ff53720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13ff53c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13ff541c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13ff54710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13ff54c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13ff551b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13ff55700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13ff55c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13ff561a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13ff566f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13ff56c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13ff57190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13ff576e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13ff57c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13ff58180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13ff586d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13ff58c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13ff59170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13ff596c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13ff59c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13ff5a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13ff5a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13ff5ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13ff5b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13ff5b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13ff5bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13ff5c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13ff5c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13ff5ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13ff5cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13ff5d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13ff5d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13ff5dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13ff5e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13ff5e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13ff5eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13ff5ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13ff5f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13ff5f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13ff5fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13ff60200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13ff606a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13ff60bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13ff61310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13ff61a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13ff62150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13ff62870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13ff62b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13ff63320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13ff635e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13ff63bf0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.975s
user	0m0.236s
sys	0m0.190s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
