### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.37 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    1.76 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.67 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.42 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.98 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.32 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.32 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    1.06 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.21 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.27 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    2.17 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.18 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.18 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.23 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.20 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed  180.93 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.94 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   25.83 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.35 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.20 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.23 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    = 223.28 sec*proc (27 tests)

Total Test time (real) = 223.29 sec

real	3m43.321s
user	7m40.717s
sys	0m6.150s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.22 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.18 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.20 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    0.90 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.17 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.17 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.17 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.25 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed   31.14 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.37 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   14.15 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.21 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.19 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.17 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    =  52.98 sec*proc (27 tests)

Total Test time (real) =  52.99 sec

real	0m52.988s
user	1m11.204s
sys	0m5.368s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.069 I build: 4253 (a4a350c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.535 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.249 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.019.256 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.258 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.019.259 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.260 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.019.260 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.019.261 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.019.276 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.019.278 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.019.279 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.019.279 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.019.280 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.019.283 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.019.284 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.019.284 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.019.285 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.019.286 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.019.286 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.019.294 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.024.124 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.025.300 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.302 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.025.303 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.025.303 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.025.304 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.025.304 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.025.305 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.025.305 I llama_model_loader: - type  f32:  124 tensors
0.00.025.306 I llama_model_loader: - type  f16:   73 tensors
0.00.029.458 I llm_load_vocab: special tokens cache size = 5
0.00.031.647 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.031.651 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.031.652 I llm_load_print_meta: arch             = bert
0.00.031.652 I llm_load_print_meta: vocab type       = WPM
0.00.031.652 I llm_load_print_meta: n_vocab          = 30522
0.00.031.653 I llm_load_print_meta: n_merges         = 0
0.00.031.653 I llm_load_print_meta: vocab_only       = 0
0.00.031.653 I llm_load_print_meta: n_ctx_train      = 512
0.00.031.653 I llm_load_print_meta: n_embd           = 384
0.00.031.654 I llm_load_print_meta: n_layer          = 12
0.00.031.657 I llm_load_print_meta: n_head           = 12
0.00.031.658 I llm_load_print_meta: n_head_kv        = 12
0.00.031.658 I llm_load_print_meta: n_rot            = 32
0.00.031.658 I llm_load_print_meta: n_swa            = 0
0.00.031.659 I llm_load_print_meta: n_embd_head_k    = 32
0.00.031.659 I llm_load_print_meta: n_embd_head_v    = 32
0.00.031.660 I llm_load_print_meta: n_gqa            = 1
0.00.031.661 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.031.667 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.031.667 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.031.668 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.031.668 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.031.668 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.031.669 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.031.670 I llm_load_print_meta: n_ff             = 1536
0.00.031.670 I llm_load_print_meta: n_expert         = 0
0.00.031.670 I llm_load_print_meta: n_expert_used    = 0
0.00.031.670 I llm_load_print_meta: causal attn      = 0
0.00.031.671 I llm_load_print_meta: pooling type     = 2
0.00.031.671 I llm_load_print_meta: rope type        = 2
0.00.031.671 I llm_load_print_meta: rope scaling     = linear
0.00.031.672 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.031.672 I llm_load_print_meta: freq_scale_train = 1
0.00.031.673 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.031.673 I llm_load_print_meta: rope_finetuned   = unknown
0.00.031.673 I llm_load_print_meta: ssm_d_conv       = 0
0.00.031.673 I llm_load_print_meta: ssm_d_inner      = 0
0.00.031.674 I llm_load_print_meta: ssm_d_state      = 0
0.00.031.674 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.031.674 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.031.687 I llm_load_print_meta: model type       = 33M
0.00.031.687 I llm_load_print_meta: model ftype      = F16
0.00.031.688 I llm_load_print_meta: model params     = 33.21 M
0.00.031.688 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.031.689 I llm_load_print_meta: general.name     = Bge Small
0.00.031.689 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.031.690 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.031.695 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.031.696 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.031.696 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.031.696 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.031.697 I llm_load_print_meta: max token length = 21
0.00.033.509 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.033.510 I llm_load_tensors: offloading output layer to GPU
0.00.033.516 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.033.542 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.033.544 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.034.094 I llama_new_context_with_model: n_seq_max     = 1
0.00.034.095 I llama_new_context_with_model: n_ctx         = 512
0.00.034.095 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.034.096 I llama_new_context_with_model: n_batch       = 2048
0.00.034.096 I llama_new_context_with_model: n_ubatch      = 2048
0.00.034.096 I llama_new_context_with_model: flash_attn    = 0
0.00.034.097 I llama_new_context_with_model: freq_base     = 10000.0
0.00.034.097 I llama_new_context_with_model: freq_scale    = 1
0.00.034.098 I ggml_metal_init: allocating
0.00.034.102 I ggml_metal_init: found device: Apple M4
0.00.034.105 I ggml_metal_init: picking default device: Apple M4
0.00.034.912 I ggml_metal_init: using embedded metal library
0.00.039.031 I ggml_metal_init: GPU name:   Apple M4
0.00.039.034 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.039.034 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.039.035 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.039.035 I ggml_metal_init: simdgroup reduction   = true
0.00.039.035 I ggml_metal_init: simdgroup matrix mul. = true
0.00.039.035 I ggml_metal_init: has bfloat            = true
0.00.039.036 I ggml_metal_init: use bfloat            = true
0.00.039.036 I ggml_metal_init: hasUnifiedMemory      = true
0.00.039.037 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.051.556 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.051.559 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.051.560 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.052.297 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.052.298 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.052.299 I llama_new_context_with_model: graph nodes  = 429
0.00.052.299 I llama_new_context_with_model: graph splits = 2
0.00.052.320 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.059.160 I 
0.00.059.187 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.059.879 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.064.614 I llama_perf_context_print:        load time =      43.62 ms
0.00.064.615 I llama_perf_context_print: prompt eval time =       4.58 ms /     9 tokens (    0.51 ms per token,  1965.92 tokens per second)
0.00.064.616 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.064.616 I llama_perf_context_print:       total time =       5.45 ms /    10 tokens
0.00.064.743 I ggml_metal_free: deallocating

real	0m0.241s
user	0m0.048s
sys	0m0.028s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.038 I build: 4253 (a4a350c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.567 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.602 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.605 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.606 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.607 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.608 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.609 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.609 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.616 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.616 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.617 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.617 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.617 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.619 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.622 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.622 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.625 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.626 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.626 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.626 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.005 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.657 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.658 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.659 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.659 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.659 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.659 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.660 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.014.660 I llama_model_loader: - type  f32:  124 tensors
0.00.014.660 I llama_model_loader: - type q8_0:   73 tensors
0.00.017.094 I llm_load_vocab: special tokens cache size = 5
0.00.018.368 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.018.371 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.018.371 I llm_load_print_meta: arch             = bert
0.00.018.372 I llm_load_print_meta: vocab type       = WPM
0.00.018.372 I llm_load_print_meta: n_vocab          = 30522
0.00.018.372 I llm_load_print_meta: n_merges         = 0
0.00.018.372 I llm_load_print_meta: vocab_only       = 0
0.00.018.373 I llm_load_print_meta: n_ctx_train      = 512
0.00.018.373 I llm_load_print_meta: n_embd           = 384
0.00.018.373 I llm_load_print_meta: n_layer          = 12
0.00.018.375 I llm_load_print_meta: n_head           = 12
0.00.018.376 I llm_load_print_meta: n_head_kv        = 12
0.00.018.376 I llm_load_print_meta: n_rot            = 32
0.00.018.377 I llm_load_print_meta: n_swa            = 0
0.00.018.377 I llm_load_print_meta: n_embd_head_k    = 32
0.00.018.377 I llm_load_print_meta: n_embd_head_v    = 32
0.00.018.377 I llm_load_print_meta: n_gqa            = 1
0.00.018.378 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.018.379 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.018.379 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.018.380 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.018.380 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.018.380 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.018.380 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.018.381 I llm_load_print_meta: n_ff             = 1536
0.00.018.381 I llm_load_print_meta: n_expert         = 0
0.00.018.382 I llm_load_print_meta: n_expert_used    = 0
0.00.018.382 I llm_load_print_meta: causal attn      = 0
0.00.018.382 I llm_load_print_meta: pooling type     = 2
0.00.018.382 I llm_load_print_meta: rope type        = 2
0.00.018.382 I llm_load_print_meta: rope scaling     = linear
0.00.018.383 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.018.383 I llm_load_print_meta: freq_scale_train = 1
0.00.018.383 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.018.383 I llm_load_print_meta: rope_finetuned   = unknown
0.00.018.383 I llm_load_print_meta: ssm_d_conv       = 0
0.00.018.384 I llm_load_print_meta: ssm_d_inner      = 0
0.00.018.384 I llm_load_print_meta: ssm_d_state      = 0
0.00.018.384 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.018.384 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.018.390 I llm_load_print_meta: model type       = 33M
0.00.018.391 I llm_load_print_meta: model ftype      = Q8_0
0.00.018.391 I llm_load_print_meta: model params     = 33.21 M
0.00.018.393 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.018.393 I llm_load_print_meta: general.name     = Bge Small
0.00.018.393 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.018.394 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.018.394 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.018.394 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.018.394 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.018.394 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.018.394 I llm_load_print_meta: max token length = 21
0.00.019.621 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.019.622 I llm_load_tensors: offloading output layer to GPU
0.00.019.622 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.019.629 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.631 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.019.972 I llama_new_context_with_model: n_seq_max     = 1
0.00.019.973 I llama_new_context_with_model: n_ctx         = 512
0.00.019.973 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.019.974 I llama_new_context_with_model: n_batch       = 2048
0.00.019.974 I llama_new_context_with_model: n_ubatch      = 2048
0.00.019.974 I llama_new_context_with_model: flash_attn    = 0
0.00.019.974 I llama_new_context_with_model: freq_base     = 10000.0
0.00.019.975 I llama_new_context_with_model: freq_scale    = 1
0.00.019.975 I ggml_metal_init: allocating
0.00.019.978 I ggml_metal_init: found device: Apple M4
0.00.019.980 I ggml_metal_init: picking default device: Apple M4
0.00.020.525 I ggml_metal_init: using embedded metal library
0.00.023.018 I ggml_metal_init: GPU name:   Apple M4
0.00.023.020 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.020 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.021 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.021 I ggml_metal_init: simdgroup reduction   = true
0.00.023.021 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.021 I ggml_metal_init: has bfloat            = true
0.00.023.021 I ggml_metal_init: use bfloat            = true
0.00.023.022 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.023 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.570 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.033.572 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.033.573 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.034.166 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.034.167 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.034.167 I llama_new_context_with_model: graph nodes  = 429
0.00.034.167 I llama_new_context_with_model: graph splits = 2
0.00.034.180 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.038.752 I 
0.00.038.777 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.039.300 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.043.745 I llama_perf_context_print:        load time =      29.18 ms
0.00.043.747 I llama_perf_context_print: prompt eval time =       4.30 ms /     9 tokens (    0.48 ms per token,  2091.56 tokens per second)
0.00.043.748 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.043.749 I llama_perf_context_print:       total time =       4.99 ms /    10 tokens
0.00.043.946 I ggml_metal_free: deallocating

real	0m0.055s
user	0m0.030s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.122 I build: 4253 (a4a350c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.398 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.288 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.033.293 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.295 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.033.296 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.297 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.033.306 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.033.307 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.033.332 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.033.335 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.033.335 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.033.336 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.033.336 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.033.340 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.033.340 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.033.341 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.033.341 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.342 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.041.118 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.043.234 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.047.847 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.047.849 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.047.849 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.047.850 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.047.850 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.047.851 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.047.851 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.047.851 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.047.852 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.047.852 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.047.852 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.047.853 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.047.857 I llama_model_loader: - type  f32:   41 tensors
0.00.047.858 I llama_model_loader: - type  f16:   29 tensors
0.00.065.704 W llm_load_vocab: empty token at index 5
0.00.070.167 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.071.416 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.071.460 I llm_load_vocab: special tokens cache size = 5
0.00.337.736 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.337.745 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.337.747 I llm_load_print_meta: arch             = jina-bert-v2
0.00.337.752 I llm_load_print_meta: vocab type       = BPE
0.00.337.752 I llm_load_print_meta: n_vocab          = 61056
0.00.337.752 I llm_load_print_meta: n_merges         = 39382
0.00.337.753 I llm_load_print_meta: vocab_only       = 0
0.00.337.753 I llm_load_print_meta: n_ctx_train      = 8192
0.00.337.753 I llm_load_print_meta: n_embd           = 384
0.00.337.753 I llm_load_print_meta: n_layer          = 4
0.00.337.757 I llm_load_print_meta: n_head           = 12
0.00.337.758 I llm_load_print_meta: n_head_kv        = 12
0.00.337.758 I llm_load_print_meta: n_rot            = 32
0.00.337.758 I llm_load_print_meta: n_swa            = 0
0.00.337.758 I llm_load_print_meta: n_embd_head_k    = 32
0.00.337.758 I llm_load_print_meta: n_embd_head_v    = 32
0.00.337.759 I llm_load_print_meta: n_gqa            = 1
0.00.337.759 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.337.759 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.337.760 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.337.760 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.337.761 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.337.761 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.337.763 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.337.763 I llm_load_print_meta: n_ff             = 1536
0.00.337.763 I llm_load_print_meta: n_expert         = 0
0.00.337.763 I llm_load_print_meta: n_expert_used    = 0
0.00.337.765 I llm_load_print_meta: causal attn      = 0
0.00.337.765 I llm_load_print_meta: pooling type     = -1
0.00.337.765 I llm_load_print_meta: rope type        = -1
0.00.337.765 I llm_load_print_meta: rope scaling     = linear
0.00.337.765 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.337.767 I llm_load_print_meta: freq_scale_train = 1
0.00.337.767 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.337.767 I llm_load_print_meta: rope_finetuned   = unknown
0.00.337.767 I llm_load_print_meta: ssm_d_conv       = 0
0.00.337.767 I llm_load_print_meta: ssm_d_inner      = 0
0.00.337.768 I llm_load_print_meta: ssm_d_state      = 0
0.00.337.768 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.337.768 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.337.785 I llm_load_print_meta: model type       = 33M
0.00.337.786 I llm_load_print_meta: model ftype      = F16
0.00.337.786 I llm_load_print_meta: model params     = 32.90 M
0.00.337.787 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.337.787 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.337.787 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.337.788 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.337.788 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.337.790 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.337.790 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.337.790 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.337.790 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.337.790 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.337.791 I llm_load_print_meta: max token length = 45
0.00.338.531 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.338.531 I llm_load_tensors: offloading output layer to GPU
0.00.338.532 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.338.546 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.338.547 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.339.215 I llama_new_context_with_model: n_seq_max     = 1
0.00.339.216 I llama_new_context_with_model: n_ctx         = 8192
0.00.339.216 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.339.217 I llama_new_context_with_model: n_batch       = 2048
0.00.339.217 I llama_new_context_with_model: n_ubatch      = 2048
0.00.339.217 I llama_new_context_with_model: flash_attn    = 0
0.00.339.217 I llama_new_context_with_model: freq_base     = 10000.0
0.00.339.218 I llama_new_context_with_model: freq_scale    = 1
0.00.339.218 I ggml_metal_init: allocating
0.00.339.221 I ggml_metal_init: found device: Apple M4
0.00.339.224 I ggml_metal_init: picking default device: Apple M4
0.00.339.812 I ggml_metal_init: using embedded metal library
0.00.342.281 I ggml_metal_init: GPU name:   Apple M4
0.00.342.283 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.342.284 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.342.284 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.342.285 I ggml_metal_init: simdgroup reduction   = true
0.00.342.285 I ggml_metal_init: simdgroup matrix mul. = true
0.00.342.285 I ggml_metal_init: has bfloat            = true
0.00.342.285 I ggml_metal_init: use bfloat            = true
0.00.342.286 I ggml_metal_init: hasUnifiedMemory      = true
0.00.342.287 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.354.669 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.354.671 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.354.673 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.355.143 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.355.144 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.355.144 I llama_new_context_with_model: graph nodes  = 154
0.00.355.145 I llama_new_context_with_model: graph splits = 2
0.00.355.158 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.366.073 I 
0.00.366.111 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.366.253 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.366.254 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.366.257 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.366.257 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.366.262 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.366.262 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.366.785 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.370.294 I llama_perf_context_print:        load time =     343.67 ms
0.00.370.295 I llama_perf_context_print: prompt eval time =       3.50 ms /    62 tokens (    0.06 ms per token, 17709.23 tokens per second)
0.00.370.296 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.370.296 I llama_perf_context_print:       total time =       4.22 ms /    63 tokens
0.00.370.507 I ggml_metal_free: deallocating

real	0m1.060s
user	0m0.345s
sys	0m0.042s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.103 I build: 4253 (a4a350c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.233 I main: llama backend init
0.00.000.239 I main: load the model and apply lora adapter, if any
0.00.094.025 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.104.992 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.105.005 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.105.008 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.105.009 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.105.009 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.105.010 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.105.011 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.105.030 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.105.030 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.105.031 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.105.031 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.105.032 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.105.033 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.105.033 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.105.039 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.105.039 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.105.039 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.111.917 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.114.136 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.121.075 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.121.080 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.121.081 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.121.082 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.121.082 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.121.084 I llama_model_loader: - type  f32:  194 tensors
0.00.121.085 I llama_model_loader: - type  f16:   98 tensors
0.00.160.233 I llm_load_vocab: special tokens cache size = 25
0.00.167.679 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.167.683 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.167.683 I llm_load_print_meta: arch             = gptneox
0.00.167.684 I llm_load_print_meta: vocab type       = BPE
0.00.167.684 I llm_load_print_meta: n_vocab          = 50304
0.00.167.684 I llm_load_print_meta: n_merges         = 50009
0.00.167.684 I llm_load_print_meta: vocab_only       = 0
0.00.167.684 I llm_load_print_meta: n_ctx_train      = 2048
0.00.167.685 I llm_load_print_meta: n_embd           = 2048
0.00.167.685 I llm_load_print_meta: n_layer          = 24
0.00.167.688 I llm_load_print_meta: n_head           = 16
0.00.167.689 I llm_load_print_meta: n_head_kv        = 16
0.00.167.690 I llm_load_print_meta: n_rot            = 32
0.00.167.690 I llm_load_print_meta: n_swa            = 0
0.00.167.690 I llm_load_print_meta: n_embd_head_k    = 128
0.00.167.690 I llm_load_print_meta: n_embd_head_v    = 128
0.00.167.693 I llm_load_print_meta: n_gqa            = 1
0.00.167.694 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.167.695 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.167.695 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.167.696 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.167.696 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.167.696 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.167.696 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.167.697 I llm_load_print_meta: n_ff             = 8192
0.00.167.697 I llm_load_print_meta: n_expert         = 0
0.00.167.697 I llm_load_print_meta: n_expert_used    = 0
0.00.167.697 I llm_load_print_meta: causal attn      = 1
0.00.167.697 I llm_load_print_meta: pooling type     = 0
0.00.167.698 I llm_load_print_meta: rope type        = 2
0.00.167.698 I llm_load_print_meta: rope scaling     = linear
0.00.167.698 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.167.698 I llm_load_print_meta: freq_scale_train = 1
0.00.167.699 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.167.699 I llm_load_print_meta: rope_finetuned   = unknown
0.00.167.699 I llm_load_print_meta: ssm_d_conv       = 0
0.00.167.699 I llm_load_print_meta: ssm_d_inner      = 0
0.00.167.699 I llm_load_print_meta: ssm_d_state      = 0
0.00.167.699 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.167.699 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.167.711 I llm_load_print_meta: model type       = 1.4B
0.00.167.712 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.167.712 I llm_load_print_meta: model params     = 1.41 B
0.00.167.713 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.167.713 I llm_load_print_meta: general.name     = 1.4B
0.00.167.713 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.167.713 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.167.714 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.167.714 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.167.714 I llm_load_print_meta: LF token         = 128 ''
0.00.167.714 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.167.715 I llm_load_print_meta: max token length = 1024
0.00.170.415 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.170.415 I llm_load_tensors: offloading output layer to GPU
0.00.170.415 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.170.434 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.170.435 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.171.442 I llama_new_context_with_model: n_seq_max     = 1
0.00.171.443 I llama_new_context_with_model: n_ctx         = 2048
0.00.171.443 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.171.444 I llama_new_context_with_model: n_batch       = 2048
0.00.171.444 I llama_new_context_with_model: n_ubatch      = 512
0.00.171.444 I llama_new_context_with_model: flash_attn    = 0
0.00.171.445 I llama_new_context_with_model: freq_base     = 10000.0
0.00.171.445 I llama_new_context_with_model: freq_scale    = 1
0.00.171.445 I ggml_metal_init: allocating
0.00.171.449 I ggml_metal_init: found device: Apple M4
0.00.171.451 I ggml_metal_init: picking default device: Apple M4
0.00.172.114 I ggml_metal_init: using embedded metal library
0.00.182.407 I ggml_metal_init: GPU name:   Apple M4
0.00.182.410 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.182.410 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.182.410 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.182.411 I ggml_metal_init: simdgroup reduction   = true
0.00.182.411 I ggml_metal_init: simdgroup matrix mul. = true
0.00.182.411 I ggml_metal_init: has bfloat            = true
0.00.182.411 I ggml_metal_init: use bfloat            = true
0.00.182.412 I ggml_metal_init: hasUnifiedMemory      = true
0.00.182.412 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.228.566 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.228.572 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.228.592 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.229.584 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.229.586 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.229.586 I llama_new_context_with_model: graph nodes  = 967
0.00.229.587 I llama_new_context_with_model: graph splits = 2
0.00.229.608 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.306.790 I main: llama threadpool init, n_threads = 4
0.00.306.825 I 
0.00.306.863 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.306.864 I 
0.00.306.948 I sampler seed: 1234
0.00.306.952 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.306.976 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.306.977 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.306.978 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.161.636 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56936.65 tokens per second)
0.02.161.637 I llama_perf_context_print:        load time =     212.75 ms
0.02.161.638 I llama_perf_context_print: prompt eval time =      43.86 ms /     7 tokens (    6.27 ms per token,   159.60 tokens per second)
0.02.161.638 I llama_perf_context_print:        eval time =    1807.89 ms /    63 runs   (   28.70 ms per token,    34.85 tokens per second)
0.02.161.639 I llama_perf_context_print:       total time =    1854.85 ms /    70 tokens
0.02.161.827 I ggml_metal_free: deallocating

real	0m2.464s
user	0m0.151s
sys	0m0.104s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.001.130 I build: 4253 (a4a350c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.034.597 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.046.727 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.046.748 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.046.751 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.046.752 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.046.752 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.046.753 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.046.754 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.046.781 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.046.781 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.046.782 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.046.782 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.046.783 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.046.783 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.046.784 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.046.787 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.046.791 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.046.791 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.054.181 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.056.471 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.063.722 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.063.731 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.063.731 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.063.732 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.063.733 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.063.734 I llama_model_loader: - type  f32:  194 tensors
0.00.063.735 I llama_model_loader: - type  f16:   98 tensors
0.00.103.330 I llm_load_vocab: special tokens cache size = 25
0.00.111.267 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.111.270 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.111.271 I llm_load_print_meta: arch             = gptneox
0.00.111.271 I llm_load_print_meta: vocab type       = BPE
0.00.111.271 I llm_load_print_meta: n_vocab          = 50304
0.00.111.271 I llm_load_print_meta: n_merges         = 50009
0.00.111.272 I llm_load_print_meta: vocab_only       = 0
0.00.111.272 I llm_load_print_meta: n_ctx_train      = 2048
0.00.111.272 I llm_load_print_meta: n_embd           = 2048
0.00.111.272 I llm_load_print_meta: n_layer          = 24
0.00.111.275 I llm_load_print_meta: n_head           = 16
0.00.111.276 I llm_load_print_meta: n_head_kv        = 16
0.00.111.276 I llm_load_print_meta: n_rot            = 32
0.00.111.277 I llm_load_print_meta: n_swa            = 0
0.00.111.277 I llm_load_print_meta: n_embd_head_k    = 128
0.00.111.277 I llm_load_print_meta: n_embd_head_v    = 128
0.00.111.278 I llm_load_print_meta: n_gqa            = 1
0.00.111.278 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.111.279 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.111.280 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.111.280 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.111.280 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.111.282 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.111.282 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.111.283 I llm_load_print_meta: n_ff             = 8192
0.00.111.283 I llm_load_print_meta: n_expert         = 0
0.00.111.283 I llm_load_print_meta: n_expert_used    = 0
0.00.111.283 I llm_load_print_meta: causal attn      = 1
0.00.111.284 I llm_load_print_meta: pooling type     = 0
0.00.111.285 I llm_load_print_meta: rope type        = 2
0.00.111.285 I llm_load_print_meta: rope scaling     = linear
0.00.111.286 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.111.286 I llm_load_print_meta: freq_scale_train = 1
0.00.111.286 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.111.288 I llm_load_print_meta: rope_finetuned   = unknown
0.00.111.288 I llm_load_print_meta: ssm_d_conv       = 0
0.00.111.288 I llm_load_print_meta: ssm_d_inner      = 0
0.00.111.288 I llm_load_print_meta: ssm_d_state      = 0
0.00.111.289 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.111.290 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.111.303 I llm_load_print_meta: model type       = 1.4B
0.00.111.303 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.111.304 I llm_load_print_meta: model params     = 1.41 B
0.00.111.304 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.111.304 I llm_load_print_meta: general.name     = 1.4B
0.00.111.305 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.111.305 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.111.305 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.111.305 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.111.306 I llm_load_print_meta: LF token         = 128 ''
0.00.111.306 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.111.306 I llm_load_print_meta: max token length = 1024
0.00.114.153 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.114.154 I llm_load_tensors: offloading output layer to GPU
0.00.114.154 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.114.165 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.114.167 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.115.311 I llama_new_context_with_model: n_seq_max     = 1
0.00.115.312 I llama_new_context_with_model: n_ctx         = 128
0.00.115.312 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.115.313 I llama_new_context_with_model: n_batch       = 128
0.00.115.313 I llama_new_context_with_model: n_ubatch      = 128
0.00.115.313 I llama_new_context_with_model: flash_attn    = 0
0.00.115.314 I llama_new_context_with_model: freq_base     = 10000.0
0.00.115.314 I llama_new_context_with_model: freq_scale    = 1
0.00.115.314 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.115.315 I ggml_metal_init: allocating
0.00.115.322 I ggml_metal_init: found device: Apple M4
0.00.115.328 I ggml_metal_init: picking default device: Apple M4
0.00.115.957 I ggml_metal_init: using embedded metal library
0.00.118.703 I ggml_metal_init: GPU name:   Apple M4
0.00.118.705 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.118.705 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.118.706 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.118.706 I ggml_metal_init: simdgroup reduction   = true
0.00.118.706 I ggml_metal_init: simdgroup matrix mul. = true
0.00.118.706 I ggml_metal_init: has bfloat            = true
0.00.118.706 I ggml_metal_init: use bfloat            = true
0.00.118.707 I ggml_metal_init: hasUnifiedMemory      = true
0.00.118.707 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.129.649 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.129.653 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.129.671 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.130.607 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.130.608 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.130.609 I llama_new_context_with_model: graph nodes  = 967
0.00.130.609 I llama_new_context_with_model: graph splits = 2
0.00.130.621 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.222.057 I 
0.01.222.100 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.01.222.149 I perplexity: tokenizing the input ..
0.01.235.979 I perplexity: tokenization took 13.825 ms
0.01.236.009 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.359.081 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.361.080 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.361.108 I llama_perf_context_print:        load time =    1187.44 ms
0.01.361.111 I llama_perf_context_print: prompt eval time =     122.12 ms /   128 tokens (    0.95 ms per token,  1048.11 tokens per second)
0.01.361.113 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.361.114 I llama_perf_context_print:       total time =     139.05 ms /   129 tokens
0.01.361.713 I ggml_metal_free: deallocating

real	0m1.561s
user	0m0.137s
sys	0m0.218s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4253 (a4a350c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.009.621 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.477 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.481 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.483 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.484 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.484 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.485 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.485 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.493 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.493 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.493 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.494 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.494 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.494 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.495 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.496 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.497 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.497 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.336 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.383 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.372 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.374 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.374 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.374 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.375 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.375 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.376 I llama_model_loader: - type  f32:  194 tensors
0.00.025.376 I llama_model_loader: - type q8_0:   98 tensors
0.00.047.533 I llm_load_vocab: special tokens cache size = 25
0.00.053.600 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.604 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.604 I llm_load_print_meta: arch             = gptneox
0.00.053.605 I llm_load_print_meta: vocab type       = BPE
0.00.053.605 I llm_load_print_meta: n_vocab          = 50304
0.00.053.605 I llm_load_print_meta: n_merges         = 50009
0.00.053.605 I llm_load_print_meta: vocab_only       = 0
0.00.053.606 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.606 I llm_load_print_meta: n_embd           = 2048
0.00.053.606 I llm_load_print_meta: n_layer          = 24
0.00.053.611 I llm_load_print_meta: n_head           = 16
0.00.053.612 I llm_load_print_meta: n_head_kv        = 16
0.00.053.616 I llm_load_print_meta: n_rot            = 32
0.00.053.616 I llm_load_print_meta: n_swa            = 0
0.00.053.616 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.616 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.617 I llm_load_print_meta: n_gqa            = 1
0.00.053.619 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.620 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.620 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.621 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.621 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.621 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.621 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.622 I llm_load_print_meta: n_ff             = 8192
0.00.053.622 I llm_load_print_meta: n_expert         = 0
0.00.053.622 I llm_load_print_meta: n_expert_used    = 0
0.00.053.622 I llm_load_print_meta: causal attn      = 1
0.00.053.622 I llm_load_print_meta: pooling type     = 0
0.00.053.623 I llm_load_print_meta: rope type        = 2
0.00.053.624 I llm_load_print_meta: rope scaling     = linear
0.00.053.625 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.625 I llm_load_print_meta: freq_scale_train = 1
0.00.053.625 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.625 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.625 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.626 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.626 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.626 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.626 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.634 I llm_load_print_meta: model type       = 1.4B
0.00.053.634 I llm_load_print_meta: model ftype      = Q8_0
0.00.053.635 I llm_load_print_meta: model params     = 1.41 B
0.00.053.636 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.053.636 I llm_load_print_meta: general.name     = 1.4B
0.00.053.636 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.636 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.637 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.637 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.637 I llm_load_print_meta: LF token         = 128 ''
0.00.053.638 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.638 I llm_load_print_meta: max token length = 1024
0.00.055.752 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.753 I llm_load_tensors: offloading output layer to GPU
0.00.055.753 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.759 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.055.760 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.056.755 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.756 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.756 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.756 I llama_new_context_with_model: n_batch       = 2048
0.00.056.757 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.757 I llama_new_context_with_model: flash_attn    = 0
0.00.056.757 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.758 I llama_new_context_with_model: freq_scale    = 1
0.00.056.758 I ggml_metal_init: allocating
0.00.056.762 I ggml_metal_init: found device: Apple M4
0.00.056.764 I ggml_metal_init: picking default device: Apple M4
0.00.057.449 I ggml_metal_init: using embedded metal library
0.00.059.982 I ggml_metal_init: GPU name:   Apple M4
0.00.059.983 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.984 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.984 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.984 I ggml_metal_init: simdgroup reduction   = true
0.00.059.985 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.985 I ggml_metal_init: has bfloat            = true
0.00.059.985 I ggml_metal_init: use bfloat            = true
0.00.059.985 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.986 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.095.765 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.095.774 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.095.797 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.096.988 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.096.990 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.096.990 I llama_new_context_with_model: graph nodes  = 967
0.00.096.990 I llama_new_context_with_model: graph splits = 2
0.00.097.005 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.963.811 I main: llama threadpool init, n_threads = 4
0.00.963.850 I 
0.00.963.873 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.963.873 I 
0.00.964.110 I sampler seed: 1234
0.00.964.114 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.964.137 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.964.138 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.964.138 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.065.279 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51938.55 tokens per second)
0.02.065.281 I llama_perf_context_print:        load time =     954.19 ms
0.02.065.281 I llama_perf_context_print: prompt eval time =      43.30 ms /     7 tokens (    6.19 ms per token,   161.67 tokens per second)
0.02.065.282 I llama_perf_context_print:        eval time =    1054.95 ms /    63 runs   (   16.75 ms per token,    59.72 tokens per second)
0.02.065.283 I llama_perf_context_print:       total time =    1101.47 ms /    70 tokens
0.02.065.494 I ggml_metal_free: deallocating

real	0m2.085s
user	0m0.113s
sys	0m0.204s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.121 I build: 4253 (a4a350c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.928 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.071 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.024.077 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.084 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.085 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.085 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.085 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.086 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.099 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.101 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.101 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.101 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.102 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.102 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.103 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.105 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.108 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.108 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.544 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.003 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.783 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.036.785 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.785 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.786 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.786 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.787 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.036.787 I llama_model_loader: - type  f32:  194 tensors
0.00.036.788 I llama_model_loader: - type q8_0:   98 tensors
0.00.066.381 I llm_load_vocab: special tokens cache size = 25
0.00.072.803 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.072.807 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.072.807 I llm_load_print_meta: arch             = gptneox
0.00.072.807 I llm_load_print_meta: vocab type       = BPE
0.00.072.808 I llm_load_print_meta: n_vocab          = 50304
0.00.072.808 I llm_load_print_meta: n_merges         = 50009
0.00.072.808 I llm_load_print_meta: vocab_only       = 0
0.00.072.808 I llm_load_print_meta: n_ctx_train      = 2048
0.00.072.808 I llm_load_print_meta: n_embd           = 2048
0.00.072.810 I llm_load_print_meta: n_layer          = 24
0.00.072.814 I llm_load_print_meta: n_head           = 16
0.00.072.814 I llm_load_print_meta: n_head_kv        = 16
0.00.072.815 I llm_load_print_meta: n_rot            = 32
0.00.072.815 I llm_load_print_meta: n_swa            = 0
0.00.072.815 I llm_load_print_meta: n_embd_head_k    = 128
0.00.072.815 I llm_load_print_meta: n_embd_head_v    = 128
0.00.072.816 I llm_load_print_meta: n_gqa            = 1
0.00.072.816 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.072.817 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.072.818 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.072.818 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.072.818 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.072.818 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.072.818 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.072.819 I llm_load_print_meta: n_ff             = 8192
0.00.072.819 I llm_load_print_meta: n_expert         = 0
0.00.072.819 I llm_load_print_meta: n_expert_used    = 0
0.00.072.820 I llm_load_print_meta: causal attn      = 1
0.00.072.820 I llm_load_print_meta: pooling type     = 0
0.00.072.820 I llm_load_print_meta: rope type        = 2
0.00.072.820 I llm_load_print_meta: rope scaling     = linear
0.00.072.820 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.072.820 I llm_load_print_meta: freq_scale_train = 1
0.00.072.821 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.072.821 I llm_load_print_meta: rope_finetuned   = unknown
0.00.072.821 I llm_load_print_meta: ssm_d_conv       = 0
0.00.072.821 I llm_load_print_meta: ssm_d_inner      = 0
0.00.072.821 I llm_load_print_meta: ssm_d_state      = 0
0.00.072.821 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.072.821 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.072.834 I llm_load_print_meta: model type       = 1.4B
0.00.072.834 I llm_load_print_meta: model ftype      = Q8_0
0.00.072.834 I llm_load_print_meta: model params     = 1.41 B
0.00.072.835 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.072.835 I llm_load_print_meta: general.name     = 1.4B
0.00.072.835 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.072.835 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.072.836 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.072.836 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.072.836 I llm_load_print_meta: LF token         = 128 ''
0.00.072.836 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.072.836 I llm_load_print_meta: max token length = 1024
0.00.075.212 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.075.212 I llm_load_tensors: offloading output layer to GPU
0.00.075.212 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.075.223 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.075.224 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.076.184 I llama_new_context_with_model: n_seq_max     = 1
0.00.076.185 I llama_new_context_with_model: n_ctx         = 128
0.00.076.185 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.076.185 I llama_new_context_with_model: n_batch       = 128
0.00.076.185 I llama_new_context_with_model: n_ubatch      = 128
0.00.076.185 I llama_new_context_with_model: flash_attn    = 0
0.00.076.186 I llama_new_context_with_model: freq_base     = 10000.0
0.00.076.186 I llama_new_context_with_model: freq_scale    = 1
0.00.076.186 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.076.186 I ggml_metal_init: allocating
0.00.076.189 I ggml_metal_init: found device: Apple M4
0.00.076.191 I ggml_metal_init: picking default device: Apple M4
0.00.076.813 I ggml_metal_init: using embedded metal library
0.00.079.386 I ggml_metal_init: GPU name:   Apple M4
0.00.079.387 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.079.388 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.079.388 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.079.388 I ggml_metal_init: simdgroup reduction   = true
0.00.079.389 I ggml_metal_init: simdgroup matrix mul. = true
0.00.079.389 I ggml_metal_init: has bfloat            = true
0.00.079.389 I ggml_metal_init: use bfloat            = true
0.00.079.389 I ggml_metal_init: hasUnifiedMemory      = true
0.00.079.390 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.089.905 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.089.910 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.089.925 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.862 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.090.863 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.090.863 I llama_new_context_with_model: graph nodes  = 967
0.00.090.863 I llama_new_context_with_model: graph splits = 2
0.00.090.876 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.039.412 I 
0.01.039.437 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.01.039.450 I perplexity: tokenizing the input ..
0.01.047.524 I perplexity: tokenization took 8.072 ms
0.01.047.534 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.172.106 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.173.484 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.173.498 I llama_perf_context_print:        load time =    1028.48 ms
0.01.173.499 I llama_perf_context_print: prompt eval time =     124.34 ms /   128 tokens (    0.97 ms per token,  1029.45 tokens per second)
0.01.173.500 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.173.500 I llama_perf_context_print:       total time =     134.09 ms /   129 tokens
0.01.173.829 I ggml_metal_free: deallocating

real	0m1.191s
user	0m0.098s
sys	0m0.163s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4253 (a4a350c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.010.820 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.407 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.411 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.413 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.413 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.414 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.414 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.414 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.424 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.425 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.425 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.425 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.426 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.426 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.426 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.428 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.428 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.429 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.166 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.232 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.142 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.144 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.144 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.144 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.145 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.145 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.146 I llama_model_loader: - type  f32:  194 tensors
0.00.026.146 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.146 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.599 I llm_load_vocab: special tokens cache size = 25
0.00.052.563 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.567 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.568 I llm_load_print_meta: arch             = gptneox
0.00.052.573 I llm_load_print_meta: vocab type       = BPE
0.00.052.573 I llm_load_print_meta: n_vocab          = 50304
0.00.052.574 I llm_load_print_meta: n_merges         = 50009
0.00.052.574 I llm_load_print_meta: vocab_only       = 0
0.00.052.574 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.576 I llm_load_print_meta: n_embd           = 2048
0.00.052.576 I llm_load_print_meta: n_layer          = 24
0.00.052.584 I llm_load_print_meta: n_head           = 16
0.00.052.585 I llm_load_print_meta: n_head_kv        = 16
0.00.052.585 I llm_load_print_meta: n_rot            = 32
0.00.052.586 I llm_load_print_meta: n_swa            = 0
0.00.052.586 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.586 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.587 I llm_load_print_meta: n_gqa            = 1
0.00.052.588 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.588 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.589 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.591 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.591 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.591 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.592 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.593 I llm_load_print_meta: n_ff             = 8192
0.00.052.593 I llm_load_print_meta: n_expert         = 0
0.00.052.593 I llm_load_print_meta: n_expert_used    = 0
0.00.052.593 I llm_load_print_meta: causal attn      = 1
0.00.052.593 I llm_load_print_meta: pooling type     = 0
0.00.052.594 I llm_load_print_meta: rope type        = 2
0.00.052.594 I llm_load_print_meta: rope scaling     = linear
0.00.052.595 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.595 I llm_load_print_meta: freq_scale_train = 1
0.00.052.595 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.595 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.596 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.596 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.596 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.596 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.596 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.605 I llm_load_print_meta: model type       = 1.4B
0.00.052.605 I llm_load_print_meta: model ftype      = Q4_0
0.00.052.605 I llm_load_print_meta: model params     = 1.41 B
0.00.052.606 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.052.606 I llm_load_print_meta: general.name     = 1.4B
0.00.052.606 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.606 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.606 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.607 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.607 I llm_load_print_meta: LF token         = 128 ''
0.00.052.607 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.607 I llm_load_print_meta: max token length = 1024
0.00.054.620 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.620 I llm_load_tensors: offloading output layer to GPU
0.00.054.620 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.627 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.054.628 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.055.683 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.684 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.684 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.684 I llama_new_context_with_model: n_batch       = 2048
0.00.055.685 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.685 I llama_new_context_with_model: flash_attn    = 0
0.00.055.685 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.686 I llama_new_context_with_model: freq_scale    = 1
0.00.055.686 I ggml_metal_init: allocating
0.00.055.693 I ggml_metal_init: found device: Apple M4
0.00.055.695 I ggml_metal_init: picking default device: Apple M4
0.00.056.365 I ggml_metal_init: using embedded metal library
0.00.058.891 I ggml_metal_init: GPU name:   Apple M4
0.00.058.892 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.893 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.893 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.893 I ggml_metal_init: simdgroup reduction   = true
0.00.058.893 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.893 I ggml_metal_init: has bfloat            = true
0.00.058.894 I ggml_metal_init: use bfloat            = true
0.00.058.894 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.895 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.094.073 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.094.080 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.094.102 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.095.179 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.095.181 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.095.181 I llama_new_context_with_model: graph nodes  = 967
0.00.095.182 I llama_new_context_with_model: graph splits = 2
0.00.095.196 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.663.747 I main: llama threadpool init, n_threads = 4
0.00.663.784 I 
0.00.663.811 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.663.811 I 
0.00.664.044 I sampler seed: 1234
0.00.664.049 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.664.093 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.664.094 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.664.094 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.348.248 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60580.20 tokens per second)
0.01.348.249 I llama_perf_context_print:        load time =     652.92 ms
0.01.348.249 I llama_perf_context_print: prompt eval time =      46.07 ms /     7 tokens (    6.58 ms per token,   151.93 tokens per second)
0.01.348.250 I llama_perf_context_print:        eval time =     635.14 ms /    63 runs   (   10.08 ms per token,    99.19 tokens per second)
0.01.348.250 I llama_perf_context_print:       total time =     684.50 ms /    70 tokens
0.01.348.427 I ggml_metal_free: deallocating

real	0m1.366s
user	0m0.109s
sys	0m0.152s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4253 (a4a350c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.653 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.915 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.919 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.921 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.921 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.921 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.922 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.922 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.928 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.929 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.931 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.931 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.932 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.932 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.932 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.934 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.934 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.934 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.672 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.683 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.441 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.442 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.442 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.443 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.443 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.443 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.444 I llama_model_loader: - type  f32:  194 tensors
0.00.027.444 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.444 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.442 I llm_load_vocab: special tokens cache size = 25
0.00.053.386 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.389 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.389 I llm_load_print_meta: arch             = gptneox
0.00.053.389 I llm_load_print_meta: vocab type       = BPE
0.00.053.390 I llm_load_print_meta: n_vocab          = 50304
0.00.053.390 I llm_load_print_meta: n_merges         = 50009
0.00.053.390 I llm_load_print_meta: vocab_only       = 0
0.00.053.390 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.390 I llm_load_print_meta: n_embd           = 2048
0.00.053.390 I llm_load_print_meta: n_layer          = 24
0.00.053.393 I llm_load_print_meta: n_head           = 16
0.00.053.396 I llm_load_print_meta: n_head_kv        = 16
0.00.053.396 I llm_load_print_meta: n_rot            = 32
0.00.053.396 I llm_load_print_meta: n_swa            = 0
0.00.053.396 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.397 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.397 I llm_load_print_meta: n_gqa            = 1
0.00.053.398 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.399 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.399 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.400 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.400 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.400 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.400 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.401 I llm_load_print_meta: n_ff             = 8192
0.00.053.401 I llm_load_print_meta: n_expert         = 0
0.00.053.401 I llm_load_print_meta: n_expert_used    = 0
0.00.053.401 I llm_load_print_meta: causal attn      = 1
0.00.053.401 I llm_load_print_meta: pooling type     = 0
0.00.053.401 I llm_load_print_meta: rope type        = 2
0.00.053.402 I llm_load_print_meta: rope scaling     = linear
0.00.053.402 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.402 I llm_load_print_meta: freq_scale_train = 1
0.00.053.403 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.403 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.403 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.403 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.404 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.404 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.405 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.417 I llm_load_print_meta: model type       = 1.4B
0.00.053.417 I llm_load_print_meta: model ftype      = Q4_0
0.00.053.417 I llm_load_print_meta: model params     = 1.41 B
0.00.053.418 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.053.418 I llm_load_print_meta: general.name     = 1.4B
0.00.053.418 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.418 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.418 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.418 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.419 I llm_load_print_meta: LF token         = 128 ''
0.00.053.419 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.419 I llm_load_print_meta: max token length = 1024
0.00.055.335 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.335 I llm_load_tensors: offloading output layer to GPU
0.00.055.335 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.346 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.055.347 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.056.254 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.254 I llama_new_context_with_model: n_ctx         = 128
0.00.056.255 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.056.255 I llama_new_context_with_model: n_batch       = 128
0.00.056.255 I llama_new_context_with_model: n_ubatch      = 128
0.00.056.255 I llama_new_context_with_model: flash_attn    = 0
0.00.056.256 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.256 I llama_new_context_with_model: freq_scale    = 1
0.00.056.256 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.056.257 I ggml_metal_init: allocating
0.00.056.263 I ggml_metal_init: found device: Apple M4
0.00.056.265 I ggml_metal_init: picking default device: Apple M4
0.00.056.811 I ggml_metal_init: using embedded metal library
0.00.059.163 I ggml_metal_init: GPU name:   Apple M4
0.00.059.165 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.165 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.166 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.166 I ggml_metal_init: simdgroup reduction   = true
0.00.059.166 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.166 I ggml_metal_init: has bfloat            = true
0.00.059.167 I ggml_metal_init: use bfloat            = true
0.00.059.167 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.167 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.052 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.070.054 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.070.068 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.966 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.967 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.967 I llama_new_context_with_model: graph nodes  = 967
0.00.070.968 I llama_new_context_with_model: graph splits = 2
0.00.070.980 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.612.887 I 
0.00.612.956 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.612.994 I perplexity: tokenizing the input ..
0.00.621.101 I perplexity: tokenization took 8.105 ms
0.00.621.111 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.743.683 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.744.993 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.745.014 I llama_perf_context_print:        load time =     601.22 ms
0.00.745.014 I llama_perf_context_print: prompt eval time =     122.34 ms /   128 tokens (    0.96 ms per token,  1046.26 tokens per second)
0.00.745.015 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.745.016 I llama_perf_context_print:       total time =     132.13 ms /   129 tokens
0.00.745.550 I ggml_metal_free: deallocating

real	0m0.761s
user	0m0.078s
sys	0m0.118s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.031 I build: 4253 (a4a350c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.059 I main: llama backend init
0.00.000.061 I main: load the model and apply lora adapter, if any
0.00.008.441 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.362 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.366 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.368 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.369 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.369 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.369 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.370 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.376 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.376 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.377 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.377 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.377 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.378 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.378 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.380 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.381 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.381 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.212 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.308 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.103 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.104 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.105 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.105 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.105 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.105 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.106 I llama_model_loader: - type  f32:  194 tensors
0.00.024.106 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.106 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.365 I llm_load_vocab: special tokens cache size = 25
0.00.050.258 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.260 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.261 I llm_load_print_meta: arch             = gptneox
0.00.050.261 I llm_load_print_meta: vocab type       = BPE
0.00.050.261 I llm_load_print_meta: n_vocab          = 50304
0.00.050.261 I llm_load_print_meta: n_merges         = 50009
0.00.050.262 I llm_load_print_meta: vocab_only       = 0
0.00.050.262 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.262 I llm_load_print_meta: n_embd           = 2048
0.00.050.262 I llm_load_print_meta: n_layer          = 24
0.00.050.264 I llm_load_print_meta: n_head           = 16
0.00.050.265 I llm_load_print_meta: n_head_kv        = 16
0.00.050.265 I llm_load_print_meta: n_rot            = 32
0.00.050.266 I llm_load_print_meta: n_swa            = 0
0.00.050.266 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.266 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.267 I llm_load_print_meta: n_gqa            = 1
0.00.050.267 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.268 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.268 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.269 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.271 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.271 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.272 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.272 I llm_load_print_meta: n_ff             = 8192
0.00.050.272 I llm_load_print_meta: n_expert         = 0
0.00.050.272 I llm_load_print_meta: n_expert_used    = 0
0.00.050.274 I llm_load_print_meta: causal attn      = 1
0.00.050.276 I llm_load_print_meta: pooling type     = 0
0.00.050.276 I llm_load_print_meta: rope type        = 2
0.00.050.276 I llm_load_print_meta: rope scaling     = linear
0.00.050.276 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.277 I llm_load_print_meta: freq_scale_train = 1
0.00.050.277 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.277 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.277 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.277 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.277 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.277 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.278 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.290 I llm_load_print_meta: model type       = 1.4B
0.00.050.290 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.290 I llm_load_print_meta: model params     = 1.41 B
0.00.050.291 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.291 I llm_load_print_meta: general.name     = 1.4B
0.00.050.291 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.291 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.293 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.293 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.293 I llm_load_print_meta: LF token         = 128 ''
0.00.050.294 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.294 I llm_load_print_meta: max token length = 1024
0.00.052.268 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.268 I llm_load_tensors: offloading output layer to GPU
0.00.052.268 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.278 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.279 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.195 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.195 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.196 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.196 I llama_new_context_with_model: n_batch       = 2048
0.00.053.196 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.196 I llama_new_context_with_model: flash_attn    = 0
0.00.053.197 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.197 I llama_new_context_with_model: freq_scale    = 1
0.00.053.197 I ggml_metal_init: allocating
0.00.053.200 I ggml_metal_init: found device: Apple M4
0.00.053.202 I ggml_metal_init: picking default device: Apple M4
0.00.053.754 I ggml_metal_init: using embedded metal library
0.00.056.033 I ggml_metal_init: GPU name:   Apple M4
0.00.056.035 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.035 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.036 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.036 I ggml_metal_init: simdgroup reduction   = true
0.00.056.036 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.036 I ggml_metal_init: has bfloat            = true
0.00.056.037 I ggml_metal_init: use bfloat            = true
0.00.056.038 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.042 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.844 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.850 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.869 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.821 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.822 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.823 I llama_new_context_with_model: graph nodes  = 967
0.00.085.823 I llama_new_context_with_model: graph splits = 2
0.00.085.831 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.653.034 I main: llama threadpool init, n_threads = 4
0.00.653.076 I 
0.00.653.113 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.653.113 I 
0.00.653.362 I sampler seed: 1234
0.00.653.366 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.653.405 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.653.410 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.653.410 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.382.691 I llama_perf_sampler_print:    sampling time =       1.13 ms /    71 runs   (    0.02 ms per token, 62999.11 tokens per second)
0.01.382.692 I llama_perf_context_print:        load time =     644.59 ms
0.01.382.693 I llama_perf_context_print: prompt eval time =      43.52 ms /     7 tokens (    6.22 ms per token,   160.85 tokens per second)
0.01.382.693 I llama_perf_context_print:        eval time =     682.83 ms /    63 runs   (   10.84 ms per token,    92.26 tokens per second)
0.01.382.694 I llama_perf_context_print:       total time =     729.66 ms /    70 tokens
0.01.382.877 I ggml_metal_free: deallocating

real	0m1.398s
user	0m0.109s
sys	0m0.148s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.080 I build: 4253 (a4a350c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.453 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.252 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.256 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.257 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.260 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.260 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.260 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.261 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.268 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.268 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.270 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.270 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.270 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.271 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.271 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.272 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.272 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.273 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.995 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.055 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.819 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.820 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.821 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.821 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.821 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.822 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.022.822 I llama_model_loader: - type  f32:  194 tensors
0.00.022.823 I llama_model_loader: - type q4_1:   97 tensors
0.00.022.823 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.777 I llm_load_vocab: special tokens cache size = 25
0.00.048.683 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.686 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.686 I llm_load_print_meta: arch             = gptneox
0.00.048.687 I llm_load_print_meta: vocab type       = BPE
0.00.048.687 I llm_load_print_meta: n_vocab          = 50304
0.00.048.687 I llm_load_print_meta: n_merges         = 50009
0.00.048.687 I llm_load_print_meta: vocab_only       = 0
0.00.048.688 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.688 I llm_load_print_meta: n_embd           = 2048
0.00.048.688 I llm_load_print_meta: n_layer          = 24
0.00.048.691 I llm_load_print_meta: n_head           = 16
0.00.048.691 I llm_load_print_meta: n_head_kv        = 16
0.00.048.692 I llm_load_print_meta: n_rot            = 32
0.00.048.692 I llm_load_print_meta: n_swa            = 0
0.00.048.692 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.693 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.694 I llm_load_print_meta: n_gqa            = 1
0.00.048.695 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.696 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.696 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.697 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.697 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.697 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.698 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.698 I llm_load_print_meta: n_ff             = 8192
0.00.048.699 I llm_load_print_meta: n_expert         = 0
0.00.048.699 I llm_load_print_meta: n_expert_used    = 0
0.00.048.699 I llm_load_print_meta: causal attn      = 1
0.00.048.699 I llm_load_print_meta: pooling type     = 0
0.00.048.699 I llm_load_print_meta: rope type        = 2
0.00.048.699 I llm_load_print_meta: rope scaling     = linear
0.00.048.700 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.700 I llm_load_print_meta: freq_scale_train = 1
0.00.048.700 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.700 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.700 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.701 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.701 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.702 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.703 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.713 I llm_load_print_meta: model type       = 1.4B
0.00.048.714 I llm_load_print_meta: model ftype      = Q4_1
0.00.048.714 I llm_load_print_meta: model params     = 1.41 B
0.00.048.715 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.048.715 I llm_load_print_meta: general.name     = 1.4B
0.00.048.715 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.716 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.716 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.716 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.716 I llm_load_print_meta: LF token         = 128 ''
0.00.048.717 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.717 I llm_load_print_meta: max token length = 1024
0.00.050.259 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.259 I llm_load_tensors: offloading output layer to GPU
0.00.050.259 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.269 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.050.270 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.051.081 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.082 I llama_new_context_with_model: n_ctx         = 128
0.00.051.082 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.082 I llama_new_context_with_model: n_batch       = 128
0.00.051.082 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.083 I llama_new_context_with_model: flash_attn    = 0
0.00.051.083 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.083 I llama_new_context_with_model: freq_scale    = 1
0.00.051.084 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.084 I ggml_metal_init: allocating
0.00.051.090 I ggml_metal_init: found device: Apple M4
0.00.051.093 I ggml_metal_init: picking default device: Apple M4
0.00.051.632 I ggml_metal_init: using embedded metal library
0.00.053.924 I ggml_metal_init: GPU name:   Apple M4
0.00.053.926 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.926 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.927 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.927 I ggml_metal_init: simdgroup reduction   = true
0.00.053.927 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.927 I ggml_metal_init: has bfloat            = true
0.00.053.927 I ggml_metal_init: use bfloat            = true
0.00.053.928 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.928 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.604 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.606 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.619 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.517 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.519 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.519 I llama_new_context_with_model: graph nodes  = 967
0.00.065.519 I llama_new_context_with_model: graph splits = 2
0.00.065.531 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.606.803 I 
0.00.606.848 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.606.865 I perplexity: tokenizing the input ..
0.00.614.958 I perplexity: tokenization took 8.09 ms
0.00.614.969 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.738.214 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.739.604 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.739.620 I llama_perf_context_print:        load time =     598.34 ms
0.00.739.621 I llama_perf_context_print: prompt eval time =     123.02 ms /   128 tokens (    0.96 ms per token,  1040.49 tokens per second)
0.00.739.622 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.739.623 I llama_perf_context_print:       total time =     132.82 ms /   129 tokens
0.00.740.085 I ggml_metal_free: deallocating

real	0m0.754s
user	0m0.078s
sys	0m0.116s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4253 (a4a350c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.010.451 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.630 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.635 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.641 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.642 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.642 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.644 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.644 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.652 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.653 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.653 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.654 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.654 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.654 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.655 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.656 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.656 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.657 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.533 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.575 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.417 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.418 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.418 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.418 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.419 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.419 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.420 I llama_model_loader: - type  f32:  194 tensors
0.00.025.420 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.420 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.516 I llm_load_vocab: special tokens cache size = 25
0.00.051.266 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.268 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.269 I llm_load_print_meta: arch             = gptneox
0.00.051.269 I llm_load_print_meta: vocab type       = BPE
0.00.051.269 I llm_load_print_meta: n_vocab          = 50304
0.00.051.269 I llm_load_print_meta: n_merges         = 50009
0.00.051.270 I llm_load_print_meta: vocab_only       = 0
0.00.051.270 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.270 I llm_load_print_meta: n_embd           = 2048
0.00.051.270 I llm_load_print_meta: n_layer          = 24
0.00.051.273 I llm_load_print_meta: n_head           = 16
0.00.051.274 I llm_load_print_meta: n_head_kv        = 16
0.00.051.274 I llm_load_print_meta: n_rot            = 32
0.00.051.274 I llm_load_print_meta: n_swa            = 0
0.00.051.274 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.277 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.277 I llm_load_print_meta: n_gqa            = 1
0.00.051.278 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.279 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.279 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.280 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.280 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.280 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.281 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.281 I llm_load_print_meta: n_ff             = 8192
0.00.051.281 I llm_load_print_meta: n_expert         = 0
0.00.051.281 I llm_load_print_meta: n_expert_used    = 0
0.00.051.282 I llm_load_print_meta: causal attn      = 1
0.00.051.282 I llm_load_print_meta: pooling type     = 0
0.00.051.282 I llm_load_print_meta: rope type        = 2
0.00.051.282 I llm_load_print_meta: rope scaling     = linear
0.00.051.283 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.283 I llm_load_print_meta: freq_scale_train = 1
0.00.051.283 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.283 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.283 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.284 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.284 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.284 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.284 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.290 I llm_load_print_meta: model type       = 1.4B
0.00.051.291 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.291 I llm_load_print_meta: model params     = 1.41 B
0.00.051.292 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.292 I llm_load_print_meta: general.name     = 1.4B
0.00.051.292 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.292 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.292 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.293 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.293 I llm_load_print_meta: LF token         = 128 ''
0.00.051.293 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.294 I llm_load_print_meta: max token length = 1024
0.00.053.061 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.061 I llm_load_tensors: offloading output layer to GPU
0.00.053.062 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.067 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.068 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.014 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.015 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.015 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.015 I llama_new_context_with_model: n_batch       = 2048
0.00.054.015 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.015 I llama_new_context_with_model: flash_attn    = 0
0.00.054.016 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.016 I llama_new_context_with_model: freq_scale    = 1
0.00.054.017 I ggml_metal_init: allocating
0.00.054.022 I ggml_metal_init: found device: Apple M4
0.00.054.026 I ggml_metal_init: picking default device: Apple M4
0.00.054.559 I ggml_metal_init: using embedded metal library
0.00.056.892 I ggml_metal_init: GPU name:   Apple M4
0.00.056.893 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.894 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.894 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.894 I ggml_metal_init: simdgroup reduction   = true
0.00.056.894 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.894 I ggml_metal_init: has bfloat            = true
0.00.056.895 I ggml_metal_init: use bfloat            = true
0.00.056.895 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.896 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.425 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.430 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.449 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.492 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.493 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.493 I llama_new_context_with_model: graph nodes  = 967
0.00.087.494 I llama_new_context_with_model: graph splits = 2
0.00.087.508 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.713.988 I main: llama threadpool init, n_threads = 4
0.00.714.027 I 
0.00.714.056 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.714.058 I 
0.00.714.276 I sampler seed: 1234
0.00.714.280 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.714.291 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.714.292 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.714.292 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.512.615 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61312.61 tokens per second)
0.01.512.616 I llama_perf_context_print:        load time =     703.53 ms
0.01.512.616 I llama_perf_context_print: prompt eval time =      47.08 ms /     7 tokens (    6.73 ms per token,   148.70 tokens per second)
0.01.512.620 I llama_perf_context_print:        eval time =     748.34 ms /    63 runs   (   11.88 ms per token,    84.19 tokens per second)
0.01.512.620 I llama_perf_context_print:       total time =     798.63 ms /    70 tokens
0.01.512.817 I ggml_metal_free: deallocating

real	0m1.531s
user	0m0.108s
sys	0m0.154s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4253 (a4a350c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.577 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.262 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.266 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.268 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.269 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.269 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.270 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.270 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.277 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.277 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.278 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.278 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.278 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.281 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.282 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.283 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.283 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.284 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.097 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.144 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.975 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.976 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.976 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.976 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.977 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.977 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.023.977 I llama_model_loader: - type  f32:  194 tensors
0.00.023.978 I llama_model_loader: - type q5_0:   97 tensors
0.00.023.978 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.984 I llm_load_vocab: special tokens cache size = 25
0.00.050.924 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.927 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.927 I llm_load_print_meta: arch             = gptneox
0.00.050.928 I llm_load_print_meta: vocab type       = BPE
0.00.050.928 I llm_load_print_meta: n_vocab          = 50304
0.00.050.928 I llm_load_print_meta: n_merges         = 50009
0.00.050.928 I llm_load_print_meta: vocab_only       = 0
0.00.050.928 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.929 I llm_load_print_meta: n_embd           = 2048
0.00.050.929 I llm_load_print_meta: n_layer          = 24
0.00.050.932 I llm_load_print_meta: n_head           = 16
0.00.050.932 I llm_load_print_meta: n_head_kv        = 16
0.00.050.933 I llm_load_print_meta: n_rot            = 32
0.00.050.933 I llm_load_print_meta: n_swa            = 0
0.00.050.933 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.933 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.936 I llm_load_print_meta: n_gqa            = 1
0.00.050.937 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.938 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.939 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.940 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.940 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.940 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.940 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.941 I llm_load_print_meta: n_ff             = 8192
0.00.050.941 I llm_load_print_meta: n_expert         = 0
0.00.050.941 I llm_load_print_meta: n_expert_used    = 0
0.00.050.941 I llm_load_print_meta: causal attn      = 1
0.00.050.941 I llm_load_print_meta: pooling type     = 0
0.00.050.942 I llm_load_print_meta: rope type        = 2
0.00.050.942 I llm_load_print_meta: rope scaling     = linear
0.00.050.942 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.942 I llm_load_print_meta: freq_scale_train = 1
0.00.050.943 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.943 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.943 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.943 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.943 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.944 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.944 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.956 I llm_load_print_meta: model type       = 1.4B
0.00.050.956 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.956 I llm_load_print_meta: model params     = 1.41 B
0.00.050.957 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.957 I llm_load_print_meta: general.name     = 1.4B
0.00.050.957 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.958 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.958 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.958 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.959 I llm_load_print_meta: LF token         = 128 ''
0.00.050.959 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.959 I llm_load_print_meta: max token length = 1024
0.00.052.913 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.914 I llm_load_tensors: offloading output layer to GPU
0.00.052.914 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.925 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.926 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.795 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.796 I llama_new_context_with_model: n_ctx         = 128
0.00.053.796 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.796 I llama_new_context_with_model: n_batch       = 128
0.00.053.797 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.797 I llama_new_context_with_model: flash_attn    = 0
0.00.053.797 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.797 I llama_new_context_with_model: freq_scale    = 1
0.00.053.798 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.798 I ggml_metal_init: allocating
0.00.053.803 I ggml_metal_init: found device: Apple M4
0.00.053.805 I ggml_metal_init: picking default device: Apple M4
0.00.054.330 I ggml_metal_init: using embedded metal library
0.00.056.651 I ggml_metal_init: GPU name:   Apple M4
0.00.056.652 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.653 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.653 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.653 I ggml_metal_init: simdgroup reduction   = true
0.00.056.653 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.653 I ggml_metal_init: has bfloat            = true
0.00.056.654 I ggml_metal_init: use bfloat            = true
0.00.056.654 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.655 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.135 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.138 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.151 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.006 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.007 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.008 I llama_new_context_with_model: graph nodes  = 967
0.00.068.008 I llama_new_context_with_model: graph splits = 2
0.00.068.020 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.673.867 I 
0.00.673.904 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.673.926 I perplexity: tokenizing the input ..
0.00.681.632 I perplexity: tokenization took 7.705 ms
0.00.681.643 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.816.748 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.818.130 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.818.145 I llama_perf_context_print:        load time =     664.28 ms
0.00.818.146 I llama_perf_context_print: prompt eval time =     134.88 ms /   128 tokens (    1.05 ms per token,   949.01 tokens per second)
0.00.818.147 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.818.148 I llama_perf_context_print:       total time =     144.28 ms /   129 tokens
0.00.818.708 I ggml_metal_free: deallocating

real	0m0.833s
user	0m0.080s
sys	0m0.126s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4253 (a4a350c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.008.678 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.210 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.214 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.216 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.216 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.217 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.222 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.223 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.230 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.233 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.233 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.233 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.234 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.234 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.234 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.236 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.237 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.237 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.080 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.143 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.977 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.978 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.978 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.979 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.979 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.979 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.980 I llama_model_loader: - type  f32:  194 tensors
0.00.024.980 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.980 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.446 I llm_load_vocab: special tokens cache size = 25
0.00.051.164 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.167 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.168 I llm_load_print_meta: arch             = gptneox
0.00.051.168 I llm_load_print_meta: vocab type       = BPE
0.00.051.168 I llm_load_print_meta: n_vocab          = 50304
0.00.051.168 I llm_load_print_meta: n_merges         = 50009
0.00.051.169 I llm_load_print_meta: vocab_only       = 0
0.00.051.169 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.169 I llm_load_print_meta: n_embd           = 2048
0.00.051.169 I llm_load_print_meta: n_layer          = 24
0.00.051.172 I llm_load_print_meta: n_head           = 16
0.00.051.173 I llm_load_print_meta: n_head_kv        = 16
0.00.051.173 I llm_load_print_meta: n_rot            = 32
0.00.051.173 I llm_load_print_meta: n_swa            = 0
0.00.051.175 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.175 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.176 I llm_load_print_meta: n_gqa            = 1
0.00.051.177 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.177 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.178 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.178 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.178 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.179 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.179 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.179 I llm_load_print_meta: n_ff             = 8192
0.00.051.179 I llm_load_print_meta: n_expert         = 0
0.00.051.180 I llm_load_print_meta: n_expert_used    = 0
0.00.051.180 I llm_load_print_meta: causal attn      = 1
0.00.051.180 I llm_load_print_meta: pooling type     = 0
0.00.051.180 I llm_load_print_meta: rope type        = 2
0.00.051.180 I llm_load_print_meta: rope scaling     = linear
0.00.051.181 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.181 I llm_load_print_meta: freq_scale_train = 1
0.00.051.181 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.182 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.182 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.182 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.182 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.182 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.182 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.194 I llm_load_print_meta: model type       = 1.4B
0.00.051.194 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.195 I llm_load_print_meta: model params     = 1.41 B
0.00.051.197 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.197 I llm_load_print_meta: general.name     = 1.4B
0.00.051.197 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.197 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.197 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.197 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.198 I llm_load_print_meta: LF token         = 128 ''
0.00.051.198 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.199 I llm_load_print_meta: max token length = 1024
0.00.053.167 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.167 I llm_load_tensors: offloading output layer to GPU
0.00.053.167 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.178 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.179 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.090 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.091 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.091 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.091 I llama_new_context_with_model: n_batch       = 2048
0.00.054.091 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.091 I llama_new_context_with_model: flash_attn    = 0
0.00.054.092 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.092 I llama_new_context_with_model: freq_scale    = 1
0.00.054.092 I ggml_metal_init: allocating
0.00.054.095 I ggml_metal_init: found device: Apple M4
0.00.054.097 I ggml_metal_init: picking default device: Apple M4
0.00.054.660 I ggml_metal_init: using embedded metal library
0.00.056.952 I ggml_metal_init: GPU name:   Apple M4
0.00.056.953 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.954 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.954 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.954 I ggml_metal_init: simdgroup reduction   = true
0.00.056.954 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.954 I ggml_metal_init: has bfloat            = true
0.00.056.955 I ggml_metal_init: use bfloat            = true
0.00.056.955 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.957 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.325 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.334 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.357 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.360 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.361 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.362 I llama_new_context_with_model: graph nodes  = 967
0.00.087.362 I llama_new_context_with_model: graph splits = 2
0.00.087.375 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.753.129 I main: llama threadpool init, n_threads = 4
0.00.753.163 I 
0.00.753.190 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.753.190 I 
0.00.753.406 I sampler seed: 1234
0.00.753.410 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.753.447 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.753.448 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.753.448 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.595.803 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61418.69 tokens per second)
0.01.595.804 I llama_perf_context_print:        load time =     744.45 ms
0.01.595.805 I llama_perf_context_print: prompt eval time =      42.24 ms /     7 tokens (    6.03 ms per token,   165.73 tokens per second)
0.01.595.805 I llama_perf_context_print:        eval time =     797.25 ms /    63 runs   (   12.65 ms per token,    79.02 tokens per second)
0.01.595.806 I llama_perf_context_print:       total time =     842.68 ms /    70 tokens
0.01.595.997 I ggml_metal_free: deallocating

real	0m1.612s
user	0m0.108s
sys	0m0.162s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4253 (a4a350c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.758 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.669 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.674 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.675 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.680 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.680 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.681 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.681 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.688 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.689 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.689 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.689 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.690 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.690 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.690 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.692 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.692 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.693 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.618 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.653 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.590 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.591 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.592 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.592 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.592 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.593 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.593 I llama_model_loader: - type  f32:  194 tensors
0.00.023.594 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.594 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.445 I llm_load_vocab: special tokens cache size = 25
0.00.050.429 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.431 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.432 I llm_load_print_meta: arch             = gptneox
0.00.050.432 I llm_load_print_meta: vocab type       = BPE
0.00.050.432 I llm_load_print_meta: n_vocab          = 50304
0.00.050.432 I llm_load_print_meta: n_merges         = 50009
0.00.050.433 I llm_load_print_meta: vocab_only       = 0
0.00.050.433 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.433 I llm_load_print_meta: n_embd           = 2048
0.00.050.433 I llm_load_print_meta: n_layer          = 24
0.00.050.436 I llm_load_print_meta: n_head           = 16
0.00.050.437 I llm_load_print_meta: n_head_kv        = 16
0.00.050.437 I llm_load_print_meta: n_rot            = 32
0.00.050.437 I llm_load_print_meta: n_swa            = 0
0.00.050.437 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.437 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.438 I llm_load_print_meta: n_gqa            = 1
0.00.050.439 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.440 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.440 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.441 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.441 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.441 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.441 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.443 I llm_load_print_meta: n_ff             = 8192
0.00.050.443 I llm_load_print_meta: n_expert         = 0
0.00.050.443 I llm_load_print_meta: n_expert_used    = 0
0.00.050.444 I llm_load_print_meta: causal attn      = 1
0.00.050.444 I llm_load_print_meta: pooling type     = 0
0.00.050.444 I llm_load_print_meta: rope type        = 2
0.00.050.444 I llm_load_print_meta: rope scaling     = linear
0.00.050.446 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.446 I llm_load_print_meta: freq_scale_train = 1
0.00.050.447 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.447 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.447 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.447 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.447 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.447 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.447 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.459 I llm_load_print_meta: model type       = 1.4B
0.00.050.459 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.459 I llm_load_print_meta: model params     = 1.41 B
0.00.050.460 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.460 I llm_load_print_meta: general.name     = 1.4B
0.00.050.461 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.461 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.461 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.461 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.462 I llm_load_print_meta: LF token         = 128 ''
0.00.050.462 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.462 I llm_load_print_meta: max token length = 1024
0.00.052.025 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.025 I llm_load_tensors: offloading output layer to GPU
0.00.052.025 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.035 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.036 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.849 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.850 I llama_new_context_with_model: n_ctx         = 128
0.00.052.851 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.851 I llama_new_context_with_model: n_batch       = 128
0.00.052.851 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.851 I llama_new_context_with_model: flash_attn    = 0
0.00.052.852 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.852 I llama_new_context_with_model: freq_scale    = 1
0.00.052.852 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.852 I ggml_metal_init: allocating
0.00.052.862 I ggml_metal_init: found device: Apple M4
0.00.052.864 I ggml_metal_init: picking default device: Apple M4
0.00.053.381 I ggml_metal_init: using embedded metal library
0.00.055.714 I ggml_metal_init: GPU name:   Apple M4
0.00.055.716 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.716 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.717 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.717 I ggml_metal_init: simdgroup reduction   = true
0.00.055.717 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.717 I ggml_metal_init: has bfloat            = true
0.00.055.717 I ggml_metal_init: use bfloat            = true
0.00.055.718 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.718 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.250 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.255 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.268 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.114 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.116 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.116 I llama_new_context_with_model: graph nodes  = 967
0.00.067.116 I llama_new_context_with_model: graph splits = 2
0.00.067.128 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.708.253 I 
0.00.708.283 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.708.296 I perplexity: tokenizing the input ..
0.00.715.957 I perplexity: tokenization took 7.658 ms
0.00.715.970 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.850.882 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.852.237 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.852.268 I llama_perf_context_print:        load time =     699.49 ms
0.00.852.269 I llama_perf_context_print: prompt eval time =     134.69 ms /   128 tokens (    1.05 ms per token,   950.36 tokens per second)
0.00.852.270 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.852.270 I llama_perf_context_print:       total time =     144.02 ms /   129 tokens
0.00.852.726 I ggml_metal_free: deallocating

real	0m0.867s
user	0m0.079s
sys	0m0.129s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4253 (a4a350c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.009.783 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.352 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.357 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.358 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.359 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.359 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.359 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.360 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.367 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.367 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.367 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.368 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.368 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.369 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.369 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.370 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.371 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.371 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.224 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.286 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.191 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.192 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.193 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.193 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.193 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.194 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.194 I llama_model_loader: - type  f32:  194 tensors
0.00.024.195 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.195 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.195 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.243 I llm_load_vocab: special tokens cache size = 25
0.00.051.253 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.255 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.256 I llm_load_print_meta: arch             = gptneox
0.00.051.256 I llm_load_print_meta: vocab type       = BPE
0.00.051.256 I llm_load_print_meta: n_vocab          = 50304
0.00.051.256 I llm_load_print_meta: n_merges         = 50009
0.00.051.257 I llm_load_print_meta: vocab_only       = 0
0.00.051.257 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.257 I llm_load_print_meta: n_embd           = 2048
0.00.051.257 I llm_load_print_meta: n_layer          = 24
0.00.051.260 I llm_load_print_meta: n_head           = 16
0.00.051.261 I llm_load_print_meta: n_head_kv        = 16
0.00.051.261 I llm_load_print_meta: n_rot            = 32
0.00.051.261 I llm_load_print_meta: n_swa            = 0
0.00.051.261 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.261 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.265 I llm_load_print_meta: n_gqa            = 1
0.00.051.265 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.266 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.267 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.267 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.268 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.268 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.269 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.270 I llm_load_print_meta: n_ff             = 8192
0.00.051.270 I llm_load_print_meta: n_expert         = 0
0.00.051.270 I llm_load_print_meta: n_expert_used    = 0
0.00.051.276 I llm_load_print_meta: causal attn      = 1
0.00.051.278 I llm_load_print_meta: pooling type     = 0
0.00.051.278 I llm_load_print_meta: rope type        = 2
0.00.051.278 I llm_load_print_meta: rope scaling     = linear
0.00.051.279 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.279 I llm_load_print_meta: freq_scale_train = 1
0.00.051.279 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.279 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.279 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.280 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.280 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.280 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.281 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.293 I llm_load_print_meta: model type       = 1.4B
0.00.051.293 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.294 I llm_load_print_meta: model params     = 1.41 B
0.00.051.294 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.294 I llm_load_print_meta: general.name     = 1.4B
0.00.051.295 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.295 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.296 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.296 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.296 I llm_load_print_meta: LF token         = 128 ''
0.00.051.298 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.298 I llm_load_print_meta: max token length = 1024
0.00.053.090 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.090 I llm_load_tensors: offloading output layer to GPU
0.00.053.090 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.100 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.101 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.974 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.975 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.975 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.975 I llama_new_context_with_model: n_batch       = 2048
0.00.053.975 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.976 I llama_new_context_with_model: flash_attn    = 0
0.00.053.976 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.976 I llama_new_context_with_model: freq_scale    = 1
0.00.053.977 I ggml_metal_init: allocating
0.00.053.983 I ggml_metal_init: found device: Apple M4
0.00.053.985 I ggml_metal_init: picking default device: Apple M4
0.00.054.526 I ggml_metal_init: using embedded metal library
0.00.056.835 I ggml_metal_init: GPU name:   Apple M4
0.00.056.837 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.837 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.837 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.838 I ggml_metal_init: simdgroup reduction   = true
0.00.056.838 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.838 I ggml_metal_init: has bfloat            = true
0.00.056.838 I ggml_metal_init: use bfloat            = true
0.00.056.839 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.839 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.483 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.488 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.504 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.468 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.469 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.470 I llama_new_context_with_model: graph nodes  = 967
0.00.086.470 I llama_new_context_with_model: graph splits = 2
0.00.086.483 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.501.248 I main: llama threadpool init, n_threads = 4
0.00.501.285 I 
0.00.501.320 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.501.322 I 
0.00.501.546 I sampler seed: 1234
0.00.501.551 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.501.573 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.501.573 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.501.573 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.184.105 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60220.53 tokens per second)
0.01.184.106 I llama_perf_context_print:        load time =     491.46 ms
0.01.184.107 I llama_perf_context_print: prompt eval time =      39.72 ms /     7 tokens (    5.67 ms per token,   176.25 tokens per second)
0.01.184.108 I llama_perf_context_print:        eval time =     639.88 ms /    63 runs   (   10.16 ms per token,    98.46 tokens per second)
0.01.184.108 I llama_perf_context_print:       total time =     682.86 ms /    70 tokens
0.01.184.324 I ggml_metal_free: deallocating

real	0m1.204s
user	0m0.109s
sys	0m0.124s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4253 (a4a350c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.584 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.895 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.018.899 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.900 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.900 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.901 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.901 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.901 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.908 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.908 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.908 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.909 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.909 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.909 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.910 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.911 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.912 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.913 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.625 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.652 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.387 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.389 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.389 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.389 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.390 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.390 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.027.391 I llama_model_loader: - type  f32:  194 tensors
0.00.027.391 I llama_model_loader: - type q2_K:   49 tensors
0.00.027.391 I llama_model_loader: - type q3_K:   48 tensors
0.00.027.391 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.353 I llm_load_vocab: special tokens cache size = 25
0.00.053.261 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.264 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.264 I llm_load_print_meta: arch             = gptneox
0.00.053.265 I llm_load_print_meta: vocab type       = BPE
0.00.053.265 I llm_load_print_meta: n_vocab          = 50304
0.00.053.265 I llm_load_print_meta: n_merges         = 50009
0.00.053.265 I llm_load_print_meta: vocab_only       = 0
0.00.053.265 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.265 I llm_load_print_meta: n_embd           = 2048
0.00.053.266 I llm_load_print_meta: n_layer          = 24
0.00.053.269 I llm_load_print_meta: n_head           = 16
0.00.053.270 I llm_load_print_meta: n_head_kv        = 16
0.00.053.270 I llm_load_print_meta: n_rot            = 32
0.00.053.270 I llm_load_print_meta: n_swa            = 0
0.00.053.270 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.273 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.273 I llm_load_print_meta: n_gqa            = 1
0.00.053.274 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.276 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.277 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.277 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.277 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.278 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.278 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.278 I llm_load_print_meta: n_ff             = 8192
0.00.053.279 I llm_load_print_meta: n_expert         = 0
0.00.053.279 I llm_load_print_meta: n_expert_used    = 0
0.00.053.279 I llm_load_print_meta: causal attn      = 1
0.00.053.279 I llm_load_print_meta: pooling type     = 0
0.00.053.279 I llm_load_print_meta: rope type        = 2
0.00.053.279 I llm_load_print_meta: rope scaling     = linear
0.00.053.280 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.280 I llm_load_print_meta: freq_scale_train = 1
0.00.053.280 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.281 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.281 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.281 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.281 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.282 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.282 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.294 I llm_load_print_meta: model type       = 1.4B
0.00.053.294 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.053.294 I llm_load_print_meta: model params     = 1.41 B
0.00.053.295 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.053.295 I llm_load_print_meta: general.name     = 1.4B
0.00.053.295 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.295 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.295 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.296 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.296 I llm_load_print_meta: LF token         = 128 ''
0.00.053.296 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.296 I llm_load_print_meta: max token length = 1024
0.00.055.323 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.323 I llm_load_tensors: offloading output layer to GPU
0.00.055.323 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.334 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.055.335 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.056.231 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.232 I llama_new_context_with_model: n_ctx         = 128
0.00.056.232 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.056.233 I llama_new_context_with_model: n_batch       = 128
0.00.056.233 I llama_new_context_with_model: n_ubatch      = 128
0.00.056.233 I llama_new_context_with_model: flash_attn    = 0
0.00.056.233 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.233 I llama_new_context_with_model: freq_scale    = 1
0.00.056.234 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.056.234 I ggml_metal_init: allocating
0.00.056.237 I ggml_metal_init: found device: Apple M4
0.00.056.239 I ggml_metal_init: picking default device: Apple M4
0.00.056.917 I ggml_metal_init: using embedded metal library
0.00.059.167 I ggml_metal_init: GPU name:   Apple M4
0.00.059.169 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.169 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.169 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.170 I ggml_metal_init: simdgroup reduction   = true
0.00.059.170 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.170 I ggml_metal_init: has bfloat            = true
0.00.059.170 I ggml_metal_init: use bfloat            = true
0.00.059.170 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.171 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.791 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.793 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.808 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.746 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.747 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.747 I llama_new_context_with_model: graph nodes  = 967
0.00.070.747 I llama_new_context_with_model: graph splits = 2
0.00.070.760 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.450.673 I 
0.00.450.703 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.450.718 I perplexity: tokenizing the input ..
0.00.458.354 I perplexity: tokenization took 7.634 ms
0.00.458.368 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.590.193 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.591.540 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.591.555 I llama_perf_context_print:        load time =     439.08 ms
0.00.591.556 I llama_perf_context_print: prompt eval time =     131.60 ms /   128 tokens (    1.03 ms per token,   972.66 tokens per second)
0.00.591.557 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.591.558 I llama_perf_context_print:       total time =     140.88 ms /   129 tokens
0.00.592.013 I ggml_metal_free: deallocating

real	0m0.608s
user	0m0.079s
sys	0m0.082s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4253 (a4a350c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.008.568 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.820 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.825 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.830 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.831 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.831 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.833 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.833 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.841 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.841 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.842 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.842 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.842 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.843 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.843 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.845 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.845 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.846 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.832 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.904 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.755 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.756 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.756 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.756 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.760 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.760 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.761 I llama_model_loader: - type  f32:  194 tensors
0.00.023.761 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.762 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.762 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.762 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.995 I llm_load_vocab: special tokens cache size = 25
0.00.051.124 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.129 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.130 I llm_load_print_meta: arch             = gptneox
0.00.051.130 I llm_load_print_meta: vocab type       = BPE
0.00.051.130 I llm_load_print_meta: n_vocab          = 50304
0.00.051.130 I llm_load_print_meta: n_merges         = 50009
0.00.051.131 I llm_load_print_meta: vocab_only       = 0
0.00.051.133 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.133 I llm_load_print_meta: n_embd           = 2048
0.00.051.134 I llm_load_print_meta: n_layer          = 24
0.00.051.137 I llm_load_print_meta: n_head           = 16
0.00.051.138 I llm_load_print_meta: n_head_kv        = 16
0.00.051.138 I llm_load_print_meta: n_rot            = 32
0.00.051.139 I llm_load_print_meta: n_swa            = 0
0.00.051.139 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.139 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.139 I llm_load_print_meta: n_gqa            = 1
0.00.051.140 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.140 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.141 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.141 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.142 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.142 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.142 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.142 I llm_load_print_meta: n_ff             = 8192
0.00.051.143 I llm_load_print_meta: n_expert         = 0
0.00.051.143 I llm_load_print_meta: n_expert_used    = 0
0.00.051.143 I llm_load_print_meta: causal attn      = 1
0.00.051.143 I llm_load_print_meta: pooling type     = 0
0.00.051.143 I llm_load_print_meta: rope type        = 2
0.00.051.143 I llm_load_print_meta: rope scaling     = linear
0.00.051.144 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.145 I llm_load_print_meta: freq_scale_train = 1
0.00.051.145 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.145 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.145 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.145 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.145 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.146 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.146 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.160 I llm_load_print_meta: model type       = 1.4B
0.00.051.160 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.160 I llm_load_print_meta: model params     = 1.41 B
0.00.051.161 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.161 I llm_load_print_meta: general.name     = 1.4B
0.00.051.161 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.161 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.162 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.163 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.163 I llm_load_print_meta: LF token         = 128 ''
0.00.051.163 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.164 I llm_load_print_meta: max token length = 1024
0.00.053.104 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.105 I llm_load_tensors: offloading output layer to GPU
0.00.053.105 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.116 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.117 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.149 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.150 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.150 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.150 I llama_new_context_with_model: n_batch       = 2048
0.00.054.151 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.151 I llama_new_context_with_model: flash_attn    = 0
0.00.054.151 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.152 I llama_new_context_with_model: freq_scale    = 1
0.00.054.152 I ggml_metal_init: allocating
0.00.054.158 I ggml_metal_init: found device: Apple M4
0.00.054.160 I ggml_metal_init: picking default device: Apple M4
0.00.054.777 I ggml_metal_init: using embedded metal library
0.00.057.405 I ggml_metal_init: GPU name:   Apple M4
0.00.057.407 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.408 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.408 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.408 I ggml_metal_init: simdgroup reduction   = true
0.00.057.409 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.409 I ggml_metal_init: has bfloat            = true
0.00.057.409 I ggml_metal_init: use bfloat            = true
0.00.057.409 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.410 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.034 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.040 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.060 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.988 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.989 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.990 I llama_new_context_with_model: graph nodes  = 967
0.00.086.990 I llama_new_context_with_model: graph splits = 2
0.00.087.002 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.526.176 I main: llama threadpool init, n_threads = 4
0.00.526.217 I 
0.00.526.246 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.526.247 I 
0.00.526.471 I sampler seed: 1234
0.00.526.477 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.526.505 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.526.507 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.526.509 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.268.461 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50641.94 tokens per second)
0.01.268.461 I llama_perf_context_print:        load time =     517.60 ms
0.01.268.463 I llama_perf_context_print: prompt eval time =      40.30 ms /     7 tokens (    5.76 ms per token,   173.68 tokens per second)
0.01.268.465 I llama_perf_context_print:        eval time =     698.88 ms /    63 runs   (   11.09 ms per token,    90.14 tokens per second)
0.01.268.465 I llama_perf_context_print:       total time =     742.29 ms /    70 tokens
0.01.268.673 I ggml_metal_free: deallocating

real	0m1.285s
user	0m0.110s
sys	0m0.121s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4253 (a4a350c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.678 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.464 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.469 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.471 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.471 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.472 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.472 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.472 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.480 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.480 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.481 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.481 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.481 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.482 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.482 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.484 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.484 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.485 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.228 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.276 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.119 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.120 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.120 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.121 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.121 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.121 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.122 I llama_model_loader: - type  f32:  194 tensors
0.00.023.122 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.122 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.122 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.122 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.995 I llm_load_vocab: special tokens cache size = 25
0.00.049.915 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.918 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.919 I llm_load_print_meta: arch             = gptneox
0.00.049.919 I llm_load_print_meta: vocab type       = BPE
0.00.049.919 I llm_load_print_meta: n_vocab          = 50304
0.00.049.919 I llm_load_print_meta: n_merges         = 50009
0.00.049.920 I llm_load_print_meta: vocab_only       = 0
0.00.049.920 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.920 I llm_load_print_meta: n_embd           = 2048
0.00.049.920 I llm_load_print_meta: n_layer          = 24
0.00.049.923 I llm_load_print_meta: n_head           = 16
0.00.049.926 I llm_load_print_meta: n_head_kv        = 16
0.00.049.926 I llm_load_print_meta: n_rot            = 32
0.00.049.926 I llm_load_print_meta: n_swa            = 0
0.00.049.927 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.927 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.928 I llm_load_print_meta: n_gqa            = 1
0.00.049.928 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.929 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.929 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.930 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.930 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.930 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.930 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.931 I llm_load_print_meta: n_ff             = 8192
0.00.049.931 I llm_load_print_meta: n_expert         = 0
0.00.049.931 I llm_load_print_meta: n_expert_used    = 0
0.00.049.932 I llm_load_print_meta: causal attn      = 1
0.00.049.932 I llm_load_print_meta: pooling type     = 0
0.00.049.936 I llm_load_print_meta: rope type        = 2
0.00.049.937 I llm_load_print_meta: rope scaling     = linear
0.00.049.937 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.937 I llm_load_print_meta: freq_scale_train = 1
0.00.049.937 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.938 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.938 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.938 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.938 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.938 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.938 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.950 I llm_load_print_meta: model type       = 1.4B
0.00.049.951 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.951 I llm_load_print_meta: model params     = 1.41 B
0.00.049.952 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.952 I llm_load_print_meta: general.name     = 1.4B
0.00.049.952 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.952 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.953 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.953 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.953 I llm_load_print_meta: LF token         = 128 ''
0.00.049.953 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.953 I llm_load_print_meta: max token length = 1024
0.00.051.821 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.821 I llm_load_tensors: offloading output layer to GPU
0.00.051.822 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.832 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.833 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.792 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.793 I llama_new_context_with_model: n_ctx         = 128
0.00.052.793 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.793 I llama_new_context_with_model: n_batch       = 128
0.00.052.794 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.794 I llama_new_context_with_model: flash_attn    = 0
0.00.052.794 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.794 I llama_new_context_with_model: freq_scale    = 1
0.00.052.795 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.795 I ggml_metal_init: allocating
0.00.052.800 I ggml_metal_init: found device: Apple M4
0.00.052.803 I ggml_metal_init: picking default device: Apple M4
0.00.053.340 I ggml_metal_init: using embedded metal library
0.00.055.632 I ggml_metal_init: GPU name:   Apple M4
0.00.055.634 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.634 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.635 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.635 I ggml_metal_init: simdgroup reduction   = true
0.00.055.635 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.635 I ggml_metal_init: has bfloat            = true
0.00.055.635 I ggml_metal_init: use bfloat            = true
0.00.055.636 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.636 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.392 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.395 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.408 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.349 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.350 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.351 I llama_new_context_with_model: graph nodes  = 967
0.00.067.351 I llama_new_context_with_model: graph splits = 2
0.00.067.364 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.494.014 I 
0.00.494.060 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.494.075 I perplexity: tokenizing the input ..
0.00.502.054 I perplexity: tokenization took 7.976 ms
0.00.502.064 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.633.415 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.634.745 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.634.763 I llama_perf_context_print:        load time =     485.33 ms
0.00.634.766 I llama_perf_context_print: prompt eval time =     131.12 ms /   128 tokens (    1.02 ms per token,   976.18 tokens per second)
0.00.634.766 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.634.767 I llama_perf_context_print:       total time =     140.75 ms /   129 tokens
0.00.635.290 I ggml_metal_free: deallocating

real	0m0.650s
user	0m0.080s
sys	0m0.090s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4253 (a4a350c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.008.672 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.141 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.146 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.148 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.153 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.153 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.154 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.154 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.161 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.162 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.162 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.162 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.163 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.163 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.163 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.166 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.166 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.167 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.062 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.135 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.965 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.967 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.967 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.968 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.968 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.968 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.969 I llama_model_loader: - type  f32:  194 tensors
0.00.023.969 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.970 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.970 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.837 I llm_load_vocab: special tokens cache size = 25
0.00.050.748 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.751 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.751 I llm_load_print_meta: arch             = gptneox
0.00.050.751 I llm_load_print_meta: vocab type       = BPE
0.00.050.752 I llm_load_print_meta: n_vocab          = 50304
0.00.050.752 I llm_load_print_meta: n_merges         = 50009
0.00.050.752 I llm_load_print_meta: vocab_only       = 0
0.00.050.752 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.752 I llm_load_print_meta: n_embd           = 2048
0.00.050.753 I llm_load_print_meta: n_layer          = 24
0.00.050.755 I llm_load_print_meta: n_head           = 16
0.00.050.756 I llm_load_print_meta: n_head_kv        = 16
0.00.050.756 I llm_load_print_meta: n_rot            = 32
0.00.050.756 I llm_load_print_meta: n_swa            = 0
0.00.050.756 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.759 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.760 I llm_load_print_meta: n_gqa            = 1
0.00.050.760 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.761 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.762 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.762 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.762 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.763 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.763 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.764 I llm_load_print_meta: n_ff             = 8192
0.00.050.764 I llm_load_print_meta: n_expert         = 0
0.00.050.765 I llm_load_print_meta: n_expert_used    = 0
0.00.050.766 I llm_load_print_meta: causal attn      = 1
0.00.050.766 I llm_load_print_meta: pooling type     = 0
0.00.050.766 I llm_load_print_meta: rope type        = 2
0.00.050.766 I llm_load_print_meta: rope scaling     = linear
0.00.050.767 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.767 I llm_load_print_meta: freq_scale_train = 1
0.00.050.767 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.767 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.768 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.768 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.768 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.768 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.768 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.780 I llm_load_print_meta: model type       = 1.4B
0.00.050.780 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.780 I llm_load_print_meta: model params     = 1.41 B
0.00.050.781 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.781 I llm_load_print_meta: general.name     = 1.4B
0.00.050.781 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.781 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.782 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.782 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.782 I llm_load_print_meta: LF token         = 128 ''
0.00.050.782 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.782 I llm_load_print_meta: max token length = 1024
0.00.052.302 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.302 I llm_load_tensors: offloading output layer to GPU
0.00.052.303 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.312 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.313 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.110 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.111 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.111 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.111 I llama_new_context_with_model: n_batch       = 2048
0.00.053.112 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.112 I llama_new_context_with_model: flash_attn    = 0
0.00.053.112 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.113 I llama_new_context_with_model: freq_scale    = 1
0.00.053.113 I ggml_metal_init: allocating
0.00.053.116 I ggml_metal_init: found device: Apple M4
0.00.053.118 I ggml_metal_init: picking default device: Apple M4
0.00.053.655 I ggml_metal_init: using embedded metal library
0.00.056.011 I ggml_metal_init: GPU name:   Apple M4
0.00.056.013 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.013 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.013 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.014 I ggml_metal_init: simdgroup reduction   = true
0.00.056.015 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.015 I ggml_metal_init: has bfloat            = true
0.00.056.015 I ggml_metal_init: use bfloat            = true
0.00.056.015 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.016 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.884 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.891 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.913 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.877 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.878 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.879 I llama_new_context_with_model: graph nodes  = 967
0.00.085.879 I llama_new_context_with_model: graph splits = 2
0.00.085.893 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.628.101 I main: llama threadpool init, n_threads = 4
0.00.628.174 I 
0.00.628.202 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.628.202 I 
0.00.628.436 I sampler seed: 1234
0.00.628.440 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.628.481 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.628.483 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.628.483 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.392.046 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58244.46 tokens per second)
0.01.392.046 I llama_perf_context_print:        load time =     619.42 ms
0.01.392.047 I llama_perf_context_print: prompt eval time =      51.12 ms /     7 tokens (    7.30 ms per token,   136.92 tokens per second)
0.01.392.048 I llama_perf_context_print:        eval time =     709.47 ms /    63 runs   (   11.26 ms per token,    88.80 tokens per second)
0.01.392.048 I llama_perf_context_print:       total time =     763.95 ms /    70 tokens
0.01.392.235 I ggml_metal_free: deallocating

real	0m1.408s
user	0m0.109s
sys	0m0.149s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4253 (a4a350c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.987 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.782 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.787 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.789 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.789 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.790 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.790 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.790 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.798 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.798 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.798 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.799 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.799 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.799 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.800 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.801 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.802 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.802 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.615 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.635 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.394 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.395 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.395 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.396 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.396 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.396 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.397 I llama_model_loader: - type  f32:  194 tensors
0.00.023.397 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.397 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.397 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.251 I llm_load_vocab: special tokens cache size = 25
0.00.050.378 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.381 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.381 I llm_load_print_meta: arch             = gptneox
0.00.050.382 I llm_load_print_meta: vocab type       = BPE
0.00.050.382 I llm_load_print_meta: n_vocab          = 50304
0.00.050.382 I llm_load_print_meta: n_merges         = 50009
0.00.050.382 I llm_load_print_meta: vocab_only       = 0
0.00.050.383 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.383 I llm_load_print_meta: n_embd           = 2048
0.00.050.383 I llm_load_print_meta: n_layer          = 24
0.00.050.386 I llm_load_print_meta: n_head           = 16
0.00.050.387 I llm_load_print_meta: n_head_kv        = 16
0.00.050.387 I llm_load_print_meta: n_rot            = 32
0.00.050.387 I llm_load_print_meta: n_swa            = 0
0.00.050.387 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.387 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.388 I llm_load_print_meta: n_gqa            = 1
0.00.050.389 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.389 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.390 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.392 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.392 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.392 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.393 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.395 I llm_load_print_meta: n_ff             = 8192
0.00.050.396 I llm_load_print_meta: n_expert         = 0
0.00.050.396 I llm_load_print_meta: n_expert_used    = 0
0.00.050.396 I llm_load_print_meta: causal attn      = 1
0.00.050.396 I llm_load_print_meta: pooling type     = 0
0.00.050.396 I llm_load_print_meta: rope type        = 2
0.00.050.396 I llm_load_print_meta: rope scaling     = linear
0.00.050.397 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.397 I llm_load_print_meta: freq_scale_train = 1
0.00.050.397 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.398 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.398 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.398 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.398 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.398 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.398 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.410 I llm_load_print_meta: model type       = 1.4B
0.00.050.410 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.411 I llm_load_print_meta: model params     = 1.41 B
0.00.050.411 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.411 I llm_load_print_meta: general.name     = 1.4B
0.00.050.412 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.412 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.412 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.412 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.413 I llm_load_print_meta: LF token         = 128 ''
0.00.050.413 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.413 I llm_load_print_meta: max token length = 1024
0.00.052.371 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.371 I llm_load_tensors: offloading output layer to GPU
0.00.052.372 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.382 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.383 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.256 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.257 I llama_new_context_with_model: n_ctx         = 128
0.00.053.257 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.257 I llama_new_context_with_model: n_batch       = 128
0.00.053.257 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.257 I llama_new_context_with_model: flash_attn    = 0
0.00.053.258 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.258 I llama_new_context_with_model: freq_scale    = 1
0.00.053.258 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.259 I ggml_metal_init: allocating
0.00.053.264 I ggml_metal_init: found device: Apple M4
0.00.053.267 I ggml_metal_init: picking default device: Apple M4
0.00.053.785 I ggml_metal_init: using embedded metal library
0.00.056.076 I ggml_metal_init: GPU name:   Apple M4
0.00.056.078 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.078 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.078 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.079 I ggml_metal_init: simdgroup reduction   = true
0.00.056.079 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.079 I ggml_metal_init: has bfloat            = true
0.00.056.079 I ggml_metal_init: use bfloat            = true
0.00.056.079 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.081 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.959 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.961 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.974 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.870 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.871 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.871 I llama_new_context_with_model: graph nodes  = 967
0.00.068.872 I llama_new_context_with_model: graph splits = 2
0.00.068.884 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.617.443 I 
0.00.617.478 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.617.493 I perplexity: tokenizing the input ..
0.00.625.561 I perplexity: tokenization took 8.066 ms
0.00.625.571 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.759.936 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.761.279 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.761.292 I llama_perf_context_print:        load time =     608.45 ms
0.00.761.293 I llama_perf_context_print: prompt eval time =     134.14 ms /   128 tokens (    1.05 ms per token,   954.24 tokens per second)
0.00.761.294 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.761.294 I llama_perf_context_print:       total time =     143.85 ms /   129 tokens
0.00.761.809 I ggml_metal_free: deallocating

real	0m0.778s
user	0m0.079s
sys	0m0.123s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4253 (a4a350c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.011.299 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.834 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.838 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.843 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.843 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.844 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.845 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.845 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.852 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.853 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.853 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.853 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.854 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.854 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.854 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.856 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.856 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.857 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.810 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.894 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.786 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.787 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.787 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.787 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.788 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.788 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.789 I llama_model_loader: - type  f32:  194 tensors
0.00.026.789 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.789 I llama_model_loader: - type q6_K:   37 tensors
0.00.047.780 I llm_load_vocab: special tokens cache size = 25
0.00.053.786 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.788 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.789 I llm_load_print_meta: arch             = gptneox
0.00.053.789 I llm_load_print_meta: vocab type       = BPE
0.00.053.789 I llm_load_print_meta: n_vocab          = 50304
0.00.053.790 I llm_load_print_meta: n_merges         = 50009
0.00.053.790 I llm_load_print_meta: vocab_only       = 0
0.00.053.790 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.790 I llm_load_print_meta: n_embd           = 2048
0.00.053.790 I llm_load_print_meta: n_layer          = 24
0.00.053.793 I llm_load_print_meta: n_head           = 16
0.00.053.796 I llm_load_print_meta: n_head_kv        = 16
0.00.053.796 I llm_load_print_meta: n_rot            = 32
0.00.053.796 I llm_load_print_meta: n_swa            = 0
0.00.053.797 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.797 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.797 I llm_load_print_meta: n_gqa            = 1
0.00.053.798 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.799 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.799 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.800 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.800 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.800 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.800 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.806 I llm_load_print_meta: n_ff             = 8192
0.00.053.808 I llm_load_print_meta: n_expert         = 0
0.00.053.808 I llm_load_print_meta: n_expert_used    = 0
0.00.053.808 I llm_load_print_meta: causal attn      = 1
0.00.053.808 I llm_load_print_meta: pooling type     = 0
0.00.053.808 I llm_load_print_meta: rope type        = 2
0.00.053.809 I llm_load_print_meta: rope scaling     = linear
0.00.053.809 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.810 I llm_load_print_meta: freq_scale_train = 1
0.00.053.810 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.810 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.810 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.810 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.811 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.811 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.811 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.819 I llm_load_print_meta: model type       = 1.4B
0.00.053.819 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.053.820 I llm_load_print_meta: model params     = 1.41 B
0.00.053.820 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.053.820 I llm_load_print_meta: general.name     = 1.4B
0.00.053.821 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.821 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.821 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.821 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.821 I llm_load_print_meta: LF token         = 128 ''
0.00.053.823 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.823 I llm_load_print_meta: max token length = 1024
0.00.055.658 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.658 I llm_load_tensors: offloading output layer to GPU
0.00.055.659 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.664 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.055.664 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.056.655 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.656 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.656 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.656 I llama_new_context_with_model: n_batch       = 2048
0.00.056.657 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.657 I llama_new_context_with_model: flash_attn    = 0
0.00.056.657 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.658 I llama_new_context_with_model: freq_scale    = 1
0.00.056.658 I ggml_metal_init: allocating
0.00.056.662 I ggml_metal_init: found device: Apple M4
0.00.056.664 I ggml_metal_init: picking default device: Apple M4
0.00.057.228 I ggml_metal_init: using embedded metal library
0.00.059.551 I ggml_metal_init: GPU name:   Apple M4
0.00.059.552 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.552 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.553 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.553 I ggml_metal_init: simdgroup reduction   = true
0.00.059.553 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.553 I ggml_metal_init: has bfloat            = true
0.00.059.553 I ggml_metal_init: use bfloat            = true
0.00.059.554 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.556 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.089.085 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.092 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.108 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.160 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.161 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.162 I llama_new_context_with_model: graph nodes  = 967
0.00.090.162 I llama_new_context_with_model: graph splits = 2
0.00.090.175 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.710.732 I main: llama threadpool init, n_threads = 4
0.00.710.765 I 
0.00.710.793 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.710.793 I 
0.00.711.013 I sampler seed: 1234
0.00.711.018 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.711.067 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.711.069 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.711.069 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.564.408 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61846.69 tokens per second)
0.01.564.408 I llama_perf_context_print:        load time =     699.43 ms
0.01.564.409 I llama_perf_context_print: prompt eval time =      51.49 ms /     7 tokens (    7.36 ms per token,   135.95 tokens per second)
0.01.564.410 I llama_perf_context_print:        eval time =     798.96 ms /    63 runs   (   12.68 ms per token,    78.85 tokens per second)
0.01.564.410 I llama_perf_context_print:       total time =     853.68 ms /    70 tokens
0.01.564.599 I ggml_metal_free: deallocating

real	0m1.584s
user	0m0.111s
sys	0m0.156s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4253 (a4a350c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.004 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.653 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.657 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.663 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.664 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.664 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.664 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.665 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.673 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.675 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.675 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.676 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.676 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.676 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.677 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.678 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.678 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.679 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.444 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.496 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.306 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.307 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.307 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.308 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.308 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.308 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.309 I llama_model_loader: - type  f32:  194 tensors
0.00.024.309 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.309 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.123 I llm_load_vocab: special tokens cache size = 25
0.00.051.004 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.007 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.008 I llm_load_print_meta: arch             = gptneox
0.00.051.008 I llm_load_print_meta: vocab type       = BPE
0.00.051.008 I llm_load_print_meta: n_vocab          = 50304
0.00.051.008 I llm_load_print_meta: n_merges         = 50009
0.00.051.009 I llm_load_print_meta: vocab_only       = 0
0.00.051.009 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.009 I llm_load_print_meta: n_embd           = 2048
0.00.051.009 I llm_load_print_meta: n_layer          = 24
0.00.051.014 I llm_load_print_meta: n_head           = 16
0.00.051.015 I llm_load_print_meta: n_head_kv        = 16
0.00.051.015 I llm_load_print_meta: n_rot            = 32
0.00.051.015 I llm_load_print_meta: n_swa            = 0
0.00.051.015 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.015 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.016 I llm_load_print_meta: n_gqa            = 1
0.00.051.018 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.019 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.020 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.020 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.020 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.020 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.021 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.021 I llm_load_print_meta: n_ff             = 8192
0.00.051.022 I llm_load_print_meta: n_expert         = 0
0.00.051.022 I llm_load_print_meta: n_expert_used    = 0
0.00.051.022 I llm_load_print_meta: causal attn      = 1
0.00.051.022 I llm_load_print_meta: pooling type     = 0
0.00.051.022 I llm_load_print_meta: rope type        = 2
0.00.051.023 I llm_load_print_meta: rope scaling     = linear
0.00.051.024 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.024 I llm_load_print_meta: freq_scale_train = 1
0.00.051.025 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.025 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.025 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.025 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.025 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.025 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.026 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.037 I llm_load_print_meta: model type       = 1.4B
0.00.051.038 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.039 I llm_load_print_meta: model params     = 1.41 B
0.00.051.039 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.039 I llm_load_print_meta: general.name     = 1.4B
0.00.051.039 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.040 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.041 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.041 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.041 I llm_load_print_meta: LF token         = 128 ''
0.00.051.041 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.041 I llm_load_print_meta: max token length = 1024
0.00.053.006 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.006 I llm_load_tensors: offloading output layer to GPU
0.00.053.007 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.017 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.019 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.975 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.976 I llama_new_context_with_model: n_ctx         = 128
0.00.053.976 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.976 I llama_new_context_with_model: n_batch       = 128
0.00.053.976 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.977 I llama_new_context_with_model: flash_attn    = 0
0.00.053.977 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.977 I llama_new_context_with_model: freq_scale    = 1
0.00.053.978 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.978 I ggml_metal_init: allocating
0.00.053.981 I ggml_metal_init: found device: Apple M4
0.00.053.984 I ggml_metal_init: picking default device: Apple M4
0.00.054.520 I ggml_metal_init: using embedded metal library
0.00.056.818 I ggml_metal_init: GPU name:   Apple M4
0.00.056.819 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.820 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.820 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.820 I ggml_metal_init: simdgroup reduction   = true
0.00.056.821 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.821 I ggml_metal_init: has bfloat            = true
0.00.056.821 I ggml_metal_init: use bfloat            = true
0.00.056.821 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.822 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.754 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.756 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.769 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.687 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.688 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.689 I llama_new_context_with_model: graph nodes  = 967
0.00.068.689 I llama_new_context_with_model: graph splits = 2
0.00.068.701 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.664.324 I 
0.00.664.357 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.664.368 I perplexity: tokenizing the input ..
0.00.672.696 I perplexity: tokenization took 8.326 ms
0.00.672.709 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.812.804 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.814.164 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.814.177 I llama_perf_context_print:        load time =     654.32 ms
0.00.814.178 I llama_perf_context_print: prompt eval time =     139.87 ms /   128 tokens (    1.09 ms per token,   915.14 tokens per second)
0.00.814.179 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.814.179 I llama_perf_context_print:       total time =     149.85 ms /   129 tokens
0.00.814.619 I ggml_metal_free: deallocating

real	0m0.830s
user	0m0.080s
sys	0m0.124s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4253 (a4a350c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.008.625 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.007 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.011 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.013 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.013 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.014 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.014 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.014 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.021 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.021 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.022 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.022 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.024 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.026 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.026 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.027 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.028 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.028 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.884 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.964 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.767 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.768 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.769 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.769 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.769 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.770 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.770 I llama_model_loader: - type  f32:  194 tensors
0.00.023.771 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.897 I llm_load_vocab: special tokens cache size = 25
0.00.049.811 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.814 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.814 I llm_load_print_meta: arch             = gptneox
0.00.049.814 I llm_load_print_meta: vocab type       = BPE
0.00.049.815 I llm_load_print_meta: n_vocab          = 50304
0.00.049.815 I llm_load_print_meta: n_merges         = 50009
0.00.049.815 I llm_load_print_meta: vocab_only       = 0
0.00.049.815 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.815 I llm_load_print_meta: n_embd           = 2048
0.00.049.816 I llm_load_print_meta: n_layer          = 24
0.00.049.818 I llm_load_print_meta: n_head           = 16
0.00.049.819 I llm_load_print_meta: n_head_kv        = 16
0.00.049.819 I llm_load_print_meta: n_rot            = 32
0.00.049.819 I llm_load_print_meta: n_swa            = 0
0.00.049.819 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.820 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.820 I llm_load_print_meta: n_gqa            = 1
0.00.049.821 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.822 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.822 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.824 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.824 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.824 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.824 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.825 I llm_load_print_meta: n_ff             = 8192
0.00.049.825 I llm_load_print_meta: n_expert         = 0
0.00.049.825 I llm_load_print_meta: n_expert_used    = 0
0.00.049.825 I llm_load_print_meta: causal attn      = 1
0.00.049.827 I llm_load_print_meta: pooling type     = 0
0.00.049.829 I llm_load_print_meta: rope type        = 2
0.00.049.829 I llm_load_print_meta: rope scaling     = linear
0.00.049.830 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.830 I llm_load_print_meta: freq_scale_train = 1
0.00.049.830 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.830 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.830 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.831 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.831 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.831 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.831 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.841 I llm_load_print_meta: model type       = 1.4B
0.00.049.842 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.843 I llm_load_print_meta: model params     = 1.41 B
0.00.049.843 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.843 I llm_load_print_meta: general.name     = 1.4B
0.00.049.843 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.843 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.844 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.844 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.844 I llm_load_print_meta: LF token         = 128 ''
0.00.049.844 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.844 I llm_load_print_meta: max token length = 1024
0.00.051.852 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.852 I llm_load_tensors: offloading output layer to GPU
0.00.051.853 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.863 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.864 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.753 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.754 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.754 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.754 I llama_new_context_with_model: n_batch       = 2048
0.00.052.754 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.755 I llama_new_context_with_model: flash_attn    = 0
0.00.052.755 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.755 I llama_new_context_with_model: freq_scale    = 1
0.00.052.756 I ggml_metal_init: allocating
0.00.052.763 I ggml_metal_init: found device: Apple M4
0.00.052.765 I ggml_metal_init: picking default device: Apple M4
0.00.053.316 I ggml_metal_init: using embedded metal library
0.00.055.641 I ggml_metal_init: GPU name:   Apple M4
0.00.055.643 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.643 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.643 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.644 I ggml_metal_init: simdgroup reduction   = true
0.00.055.645 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.645 I ggml_metal_init: has bfloat            = true
0.00.055.645 I ggml_metal_init: use bfloat            = true
0.00.055.646 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.646 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.797 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.803 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.820 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.739 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.741 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.741 I llama_new_context_with_model: graph nodes  = 967
0.00.085.741 I llama_new_context_with_model: graph splits = 2
0.00.085.750 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.767.952 I main: llama threadpool init, n_threads = 4
0.00.767.993 I 
0.00.768.024 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.768.025 I 
0.00.768.362 I sampler seed: 1234
0.00.768.367 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.768.395 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.768.398 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.768.398 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.656.696 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49271.34 tokens per second)
0.01.656.697 I llama_perf_context_print:        load time =     759.32 ms
0.01.656.697 I llama_perf_context_print: prompt eval time =      54.46 ms /     7 tokens (    7.78 ms per token,   128.53 tokens per second)
0.01.656.698 I llama_perf_context_print:        eval time =     830.86 ms /    63 runs   (   13.19 ms per token,    75.83 tokens per second)
0.01.656.698 I llama_perf_context_print:       total time =     888.75 ms /    70 tokens
0.01.656.906 I ggml_metal_free: deallocating

real	0m1.673s
user	0m0.110s
sys	0m0.175s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4253 (a4a350c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.629 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.297 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.301 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.303 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.303 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.303 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.304 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.304 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.311 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.311 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.312 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.312 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.312 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.313 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.313 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.315 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.315 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.315 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.082 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.146 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.942 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.943 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.944 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.944 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.945 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.945 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.022.946 I llama_model_loader: - type  f32:  194 tensors
0.00.022.946 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.906 I llm_load_vocab: special tokens cache size = 25
0.00.049.895 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.898 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.898 I llm_load_print_meta: arch             = gptneox
0.00.049.898 I llm_load_print_meta: vocab type       = BPE
0.00.049.899 I llm_load_print_meta: n_vocab          = 50304
0.00.049.899 I llm_load_print_meta: n_merges         = 50009
0.00.049.899 I llm_load_print_meta: vocab_only       = 0
0.00.049.899 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.899 I llm_load_print_meta: n_embd           = 2048
0.00.049.900 I llm_load_print_meta: n_layer          = 24
0.00.049.902 I llm_load_print_meta: n_head           = 16
0.00.049.903 I llm_load_print_meta: n_head_kv        = 16
0.00.049.903 I llm_load_print_meta: n_rot            = 32
0.00.049.903 I llm_load_print_meta: n_swa            = 0
0.00.049.904 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.904 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.904 I llm_load_print_meta: n_gqa            = 1
0.00.049.905 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.906 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.906 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.907 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.907 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.907 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.907 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.908 I llm_load_print_meta: n_ff             = 8192
0.00.049.908 I llm_load_print_meta: n_expert         = 0
0.00.049.908 I llm_load_print_meta: n_expert_used    = 0
0.00.049.908 I llm_load_print_meta: causal attn      = 1
0.00.049.909 I llm_load_print_meta: pooling type     = 0
0.00.049.909 I llm_load_print_meta: rope type        = 2
0.00.049.909 I llm_load_print_meta: rope scaling     = linear
0.00.049.909 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.910 I llm_load_print_meta: freq_scale_train = 1
0.00.049.910 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.910 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.910 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.911 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.911 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.911 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.911 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.923 I llm_load_print_meta: model type       = 1.4B
0.00.049.923 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.923 I llm_load_print_meta: model params     = 1.41 B
0.00.049.924 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.924 I llm_load_print_meta: general.name     = 1.4B
0.00.049.924 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.924 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.924 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.925 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.925 I llm_load_print_meta: LF token         = 128 ''
0.00.049.925 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.925 I llm_load_print_meta: max token length = 1024
0.00.051.937 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.937 I llm_load_tensors: offloading output layer to GPU
0.00.051.937 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.948 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.949 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.885 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.886 I llama_new_context_with_model: n_ctx         = 128
0.00.052.886 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.886 I llama_new_context_with_model: n_batch       = 128
0.00.052.886 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.886 I llama_new_context_with_model: flash_attn    = 0
0.00.052.887 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.887 I llama_new_context_with_model: freq_scale    = 1
0.00.052.887 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.888 I ggml_metal_init: allocating
0.00.052.891 I ggml_metal_init: found device: Apple M4
0.00.052.893 I ggml_metal_init: picking default device: Apple M4
0.00.053.437 I ggml_metal_init: using embedded metal library
0.00.055.759 I ggml_metal_init: GPU name:   Apple M4
0.00.055.761 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.761 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.762 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.762 I ggml_metal_init: simdgroup reduction   = true
0.00.055.762 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.762 I ggml_metal_init: has bfloat            = true
0.00.055.762 I ggml_metal_init: use bfloat            = true
0.00.055.763 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.763 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.627 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.630 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.644 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.548 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.549 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.550 I llama_new_context_with_model: graph nodes  = 967
0.00.067.550 I llama_new_context_with_model: graph splits = 2
0.00.067.562 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.453.129 I 
0.00.453.160 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.453.172 I perplexity: tokenizing the input ..
0.00.461.256 I perplexity: tokenization took 8.082 ms
0.00.461.272 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.601.139 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.602.497 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.602.511 I llama_perf_context_print:        load time =     444.50 ms
0.00.602.512 I llama_perf_context_print: prompt eval time =     139.64 ms /   128 tokens (    1.09 ms per token,   916.65 tokens per second)
0.00.602.513 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.602.513 I llama_perf_context_print:       total time =     149.38 ms /   129 tokens
0.00.602.912 I ggml_metal_free: deallocating

real	0m0.616s
user	0m0.079s
sys	0m0.097s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4253 (a4a350c6)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10ce0a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10ce0ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10ce0b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10ce0b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10ce0bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10ce0c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10ce0c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10ce0cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10ce0d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10ce0d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10ce0dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10ce0e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10ce0ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10ce0f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10ce0fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10ce10420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10ce10b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10ce11260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10ce11980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10ce12150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10ce12870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10ce12f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10ce136b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10ce13f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10ce14670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10ce14930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10ce14f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10ce15bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10ce160f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10ce163b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10ce16850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10ce16b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10ce173a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10ce178e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10ce17ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10ce18040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10ce184e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10ce18980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10ce18e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10ce192c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10ce19760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10ce19c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10ce1a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10ce1a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10ce1a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10ce1ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10ce1b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10ce1bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10ce1c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10ce1c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10ce1cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10ce1d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10ce1db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10ce1e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10ce1e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10ce1ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10ce1f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10ce1f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10ce1fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10ce20390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10ce20650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10ce20af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10ce20f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10ce21430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10ce218d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10ce21d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10ce22210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10ce226b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10ce22b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10ce22ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10ce23490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10ce23930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10ce23dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10ce24320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10ce24870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10ce24dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10ce25310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10ce25860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10ce25db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10ce26300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10ce26850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10ce26da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10ce272f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10ce27840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10ce27d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10ce282e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10ce28830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10ce28d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10ce292d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10ce29820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10ce29d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10ce2a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10ce2a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10ce2ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10ce2b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10ce2b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10ce2bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10ce1ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10ce2c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10ce2c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10ce2cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10ce2d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10ce2d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10ce2deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10ce2e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10ce2e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10ce2eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10ce2f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10ce2f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10ce2fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10ce303e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10ce30930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10ce30e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10ce31320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10ce317c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10ce31c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10ce32100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10ce325a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10ce32a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10ce32ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10ce33380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10ce33820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10ce33cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10ce34160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10ce34600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10ce34aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10ce34f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10ce353e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10ce35880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10ce35d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10ce361c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10ce36660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10ce36b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10ce36fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10ce37440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10ce378e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10ce37d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10ce38220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10ce386c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10ce38b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10ce39000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10ce394a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10ce39940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10ce39de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10ce3a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10ce3a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10ce3abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10ce3b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10ce3b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10ce3b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10ce3be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10ce3c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10ce3c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10ce3cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10ce3d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10ce3d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10ce3da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10ce3dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10ce3e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10ce3e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10ce3ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10ce3f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10ce3f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10ce3fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10ce3ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10ce403a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10ce40840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10ce40ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10ce41180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10ce41620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10ce41ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10ce41f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10ce42400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10ce428a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10ce42d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10ce431e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10ce43680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10ce43b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10ce43fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10ce44460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10ce44900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10ce44da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10ce45240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10ce456e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10ce45b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10ce46020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10ce464c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10ce46960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10ce46e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10ce472a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10ce47740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10ce47be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10ce48080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10ce485d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10ce48b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10ce49070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10ce495c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10ce49880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10ce49e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10ce4a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10ce4aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10ce4b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10ce4b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10ce4ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10ce4c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10ce4c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10ce4cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10ce4d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10ce4d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10ce4dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10ce4e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10ce4e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10ce4ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10ce4f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10ce4f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10ce4fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10ce502c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10ce50810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10ce50d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10ce512b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10ce51800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10ce51d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10ce522a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10ce527f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10ce52d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10ce53290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10ce537e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10ce53d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10ce54280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10ce547d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10ce54d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10ce55270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10ce557c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10ce55d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10ce56260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10ce567b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10ce56d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10ce57250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10ce577a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10ce57cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10ce58240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10ce58790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10ce58ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10ce59230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10ce59780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10ce59cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10ce5a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10ce5a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10ce5acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10ce5b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10ce5b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10ce5bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10ce5c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10ce5c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10ce5cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10ce5d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10ce5d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10ce5dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10ce5e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10ce5e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10ce5ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10ce5f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10ce5f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10ce5fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10ce601c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10ce60710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10ce60bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10ce61050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10ce614f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10ce61990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10ce61e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10ce622d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10ce62770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10ce62c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10ce630b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10ce63550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10ce639f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10ce63e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10ce64330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10ce64880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10ce64fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10ce656c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10ce65de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10ce66500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10ce667c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10ce66fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10ce67270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10ce67880 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.142.812 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10ce0c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10ce0c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10ce0cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10ce0d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10ce0bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10ce0b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10ce0a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10ce24d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10ce24ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10ce25460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10ce258d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10ce25d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10ce26630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10ce26db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10ce27590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10ce27c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10ce28370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10ce28a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10ce29150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10ce29ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10ce2a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10ce2a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10ce2afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10ce2b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10ce2bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10ce2c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10ce2c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10ce2cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10ce2cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10ce2d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10ce2d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10ce2dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10ce2e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10ce2e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10ce2e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10ce2eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10ce2f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10ce2f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10ce2f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10ce2fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10ce302d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10ce30740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10ce30bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10ce31020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10ce31490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10ce31900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10ce31d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10ce321e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10ce32650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10ce32ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10ce32f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10ce333a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10ce33810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10ce33c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10ce340f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10ce34560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10ce349d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10ce34e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10ce352b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10ce35720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10ce35b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10ce36000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10ce36470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10ce368e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10ce36d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10ce371c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10ce37630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10ce37aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10ce37f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10ce38380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10ce387f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10ce38c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10ce390d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10ce39540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10ce399b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10ce39e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10ce3a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10ce3a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10ce3ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10ce3afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10ce3b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10ce3b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10ce3bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10ce3c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10ce3c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10ce3ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10ce3cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10ce3d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10ce3d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10ce3dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10ce3e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10ce3e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10ce3e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10ce3ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10ce3f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10ce3f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10ce3fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10ce3ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10ce40430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10ce408a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10ce40d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10ce41180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10ce415f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10ce41a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10ce41ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10ce42340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10ce427b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10ce42c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10ce43090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10ce43500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10ce43970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10ce43de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10ce44250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10ce446c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10ce44b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10ce44fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10ce45410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10ce45880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10ce45cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10ce46160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10ce465d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10ce46a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10ce46eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10ce47320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10ce47790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10ce47c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10ce48070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10ce484e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10ce48950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10ce48dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10ce49230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10ce496a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10ce49b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10ce49f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10ce4a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10ce4a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10ce4acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10ce4b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10ce4b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10ce4ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10ce4be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10ce4c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10ce4c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10ce4cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10ce4d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10ce4d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10ce4d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10ce4dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10ce4e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10ce4e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10ce4eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10ce4ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10ce4f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10ce4f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10ce4fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10ce50120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10ce50590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10ce50a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10ce50e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10ce512e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10ce51750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10ce51bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10ce52030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10ce524a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10ce52910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10ce52d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10ce531f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10ce53660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10ce53ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10ce53f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10ce543b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10ce54820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10ce54c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10ce55100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10ce55570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10ce559e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10ce55e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10ce562c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10ce56730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10ce56ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10ce57010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10ce57480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10ce578f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10ce57d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10ce581d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10ce58640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10ce58ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10ce58f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10ce59390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10ce59800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10ce59c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10ce5a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10ce5a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10ce5a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10ce5ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10ce5b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10ce5b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10ce5bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10ce5bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10ce5c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10ce5c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10ce5cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10ce5d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10ce5d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10ce5da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10ce5df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10ce5e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10ce5e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10ce5ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10ce5f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10ce5f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10ce5fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10ce60120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10ce60590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10ce60a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10ce60e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10ce612e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10ce61750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10ce61bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10ce62030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10ce624a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10ce62910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10ce62d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10ce631f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10ce63660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10ce63ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10ce63f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10ce643b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10ce64820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10ce64c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10ce65100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10ce65570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10ce659e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10ce65e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10ce662c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10ce66730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10ce66ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10ce67010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10ce67480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10ce678f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10ce17a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10ce17ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10ce18360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10ce187d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10ce18c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10ce190b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10ce19520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10ce19990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10ce19e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10ce1a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10ce1a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10ce1ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10ce1afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10ce1b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10ce1b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10ce1bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10ce1c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10ce1c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10ce1ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10ce1ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10ce1d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10ce1d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10ce1dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10ce1e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10ce1e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10ce1e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10ce1ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10ce1f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10ce1f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10ce1fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10ce1ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10ce20410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10ce20880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10ce20cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10ce21160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10ce215d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10ce21a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10ce21eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10ce22320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10ce22790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10ce22e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10ce23570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10ce23c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10ce24350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10ce247c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10ce16290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10ce16700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10ce16b70 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10ce178d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10ce17d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10ce181b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10ce18620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10ce18a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10ce18f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10ce19370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10ce197e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10ce19c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10ce1a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10ce1a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10ce1a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10ce1b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10ce1ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10ce1c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10ce1c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10ce1cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10ce1d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10ce1ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10ce1e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10ce1ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10ce1f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10ce1fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10ce202f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10ce209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10ce20e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10ce212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10ce21730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10ce21ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10ce22010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10ce22480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10ce228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10ce22d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10ce23020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10ce23490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10ce23900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10ce23d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10ce241e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10ce24650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10ce0a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10ce160e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10ce16550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10ce169c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10ce16e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10ce172a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10ce24ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10ce25350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10ce257c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10ce25c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10ce260a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10ce26510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10ce26980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10ce26df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10ce27260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10ce276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10ce27b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10ce27fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10ce28420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10ce28890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10ce28d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10ce29170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10ce295e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10ce29a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10ce29ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10ce2a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10ce2a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10ce2ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10ce2b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10ce2b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10ce2b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10ce2bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10ce2c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10ce2c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10ce2cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10ce2cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10ce2d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10ce2d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10ce2dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10ce2e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10ce2e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10ce2ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10ce2eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10ce2f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10ce2f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10ce2fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10ce30060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10ce304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10ce30940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10ce30db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10ce31220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10ce31690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10ce31b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10ce31f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10ce323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10ce32850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10ce32cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10ce33130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10ce335a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10ce33a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10ce33e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10ce342f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10ce34760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10ce34bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10ce35040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10ce354b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10ce35920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10ce35d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10ce36200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10ce36670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10ce36ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10ce36f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10ce373c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10ce37830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10ce37ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10ce38110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10ce38580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10ce389f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10ce38e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10ce392d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10ce39740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10ce39bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10ce3a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10ce3a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10ce3a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10ce3ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10ce3b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10ce3b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10ce3bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10ce3bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10ce3c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10ce3c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10ce3cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10ce3d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10ce3d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10ce3d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10ce3de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10ce3e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10ce3e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10ce3eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10ce3f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10ce3f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10ce3f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10ce3fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10ce401c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10ce40630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10ce40aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10ce40f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10ce41380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10ce417f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10ce41c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10ce420d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10ce42540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10ce429b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10ce42e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10ce43290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10ce43700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10ce43b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10ce43fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10ce44450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10ce448c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10ce44d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10ce451a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10ce45610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10ce45a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10ce45ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10ce46360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10ce467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10ce46c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10ce470b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10ce47520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10ce47990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10ce47e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10ce48270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10ce486e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10ce48b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10ce48fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10ce49430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10ce498a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10ce49d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10ce4a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10ce4a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10ce4aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10ce4aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10ce4b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10ce4b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10ce4bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10ce4c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10ce4c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10ce4c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10ce4cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10ce4d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10ce4d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10ce4db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10ce4dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10ce4e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10ce4e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10ce4ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10ce4f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10ce4f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10ce4fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10ce4feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10ce50320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10ce50790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10ce50c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10ce51070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10ce514e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10ce51950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10ce51dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10ce52230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10ce529b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10ce52e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10ce53290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10ce53700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10ce53b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10ce53fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10ce54450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10ce548c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10ce54d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10ce551a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10ce55610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10ce55a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10ce55ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10ce56360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10ce567d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10ce56c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10ce570b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10ce57520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10ce57990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10ce57e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10ce58270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10ce586e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10ce58b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10ce58fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10ce59430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10ce598a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10ce59d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10ce5a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10ce5a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10ce5aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10ce5aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10ce5b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10ce5b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10ce5bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10ce5c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10ce5c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10ce5c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10ce5cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10ce5d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10ce5d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10ce5db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10ce5dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10ce5e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10ce5e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10ce5ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10ce5f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10ce5f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10ce5fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10ce5feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10ce60320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10ce60790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10ce60c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10ce61070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10ce614e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10ce61950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10ce61dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10ce62230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10ce626a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10ce62b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10ce62f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10ce633f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10ce63860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10ce63cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10ce64140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10ce645b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10ce64a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10ce64e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10ce65300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10ce65770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10ce65be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10ce66050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10ce66740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10ce66e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10ce67520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10ce0baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10ce0c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10ce0c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10ce0c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10ce0cdb0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.831s
user	0m0.294s
sys	0m0.323s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4253 (a4a350c6)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15c807130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15c807850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15c807e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15c8083b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15c808960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15c808f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15c8094c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15c809a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15c80a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15c80a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15c80aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15c80af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15c80ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15c80c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15c80ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15c80d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15c80d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15c80df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15c80e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15c80ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15c80f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15c80fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15c8103b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15c810c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15c811370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15c811630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15c811c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15c8128b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15c812df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15c8130b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15c813550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15c813810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15c8140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15c8145e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15c8148a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15c814d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15c8151e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15c815680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15c815b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15c815fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15c816460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15c816900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15c816da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15c817240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15c817500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15c817b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15c818120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15c818a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15c819050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15c819660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15c819c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15c81a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15c81a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15c81aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15c81b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15c81bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15c81bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15c81c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15c81c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15c81d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15c81d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15c81d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15c81dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15c81e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15c81e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15c81ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15c81ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15c81f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15c81f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15c81fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15c820190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15c820630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15c820ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15c821020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15c821570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15c821ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15c822010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15c822560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15c822ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15c823000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15c823550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15c823aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15c823ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15c824540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15c824a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15c824fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15c825530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15c825a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15c825fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15c826520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15c826a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15c826fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15c827510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15c827a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15c827fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15c828500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15c828a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15c818730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15c828ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15c829670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15c829bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15c82a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15c82a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15c82abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15c82b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15c82b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15c82bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15c82c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15c82c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15c82cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15c82d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15c82d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15c82db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15c82e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15c82e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15c82e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15c82ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15c82f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15c82f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15c82fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15c830080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15c830520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15c8309c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15c830e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15c831300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15c8317a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15c831c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15c8320e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15c832580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15c832a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15c832ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15c833360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15c833800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15c833ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15c834140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15c8345e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15c834a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15c834f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15c8353c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15c835860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15c835d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15c8361a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15c836640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15c836ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15c836f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15c837420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15c8378c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15c837d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15c838200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15c8386a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15c838b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15c838fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15c839480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15c839920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15c839dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15c83a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15c83a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15c83aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15c83b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15c83b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15c83b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15c83be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15c83c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15c83c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15c83cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15c83d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15c83d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15c83d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15c83de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15c83e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15c83e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15c83ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15c83f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15c83f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15c83fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15c83fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15c840380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15c840820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15c840cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15c841160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15c841600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15c841aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15c841f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15c8423e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15c842880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15c842d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15c8431c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15c843660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15c843b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15c843fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15c844440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15c8448e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15c844d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15c8452d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15c845820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15c845d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15c8462c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15c846580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15c846b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15c8471a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15c8477b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15c847fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15c848440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15c848700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15c848d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15c849500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15c8499a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15c849e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15c84a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15c84aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15c84afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15c84b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15c84ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15c84bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15c84c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15c84ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15c84cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15c84d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15c84da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15c84dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15c84e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15c84ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15c84efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15c84f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15c84fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15c84ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15c8504e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15c850a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15c850f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15c8514d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15c851a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15c851f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15c8524c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15c852a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15c852f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15c8534b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15c853a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15c853f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15c8544a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15c8549f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15c854f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15c855490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15c8559e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15c855f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15c856480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15c8569d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15c856f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15c857470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15c8579c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15c857f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15c858460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15c8589b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15c858f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15c859450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15c8599a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15c859ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15c85a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15c85a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15c85aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15c85b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15c85b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15c85bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15c85c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15c85c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15c85cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15c85d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15c85d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15c85dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15c85e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15c85e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15c85eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15c85efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15c85f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15c85f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15c85fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15c860250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15c8606f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15c860b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15c861030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15c861580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15c861ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15c8623c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15c862ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15c863200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15c8634c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15c863cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15c863f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15c864580 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.095.163 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15b605910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15b605d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15b6061f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15b609a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15b609d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15b60a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15b60a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15b60aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15b60aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15b60b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15b60b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15b60bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15b60ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15b60d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15b60da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15b60e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15b60e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15b60ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15b60f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15b60fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15b610590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15b610cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15b6113d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15b611af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15b612210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15b6124d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15b612790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15b612c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15b613070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15b6134e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15b613950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15b613e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15b6142f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15b6145b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15b614a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15b614e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15b615300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15b615770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15b615be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15b616050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15b6164c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15b616930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15b616da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15b617210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15b617680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15b617af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15b617f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15b6183d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15b618840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15b618cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15b619120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15b619590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15b619a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15b619e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15b61a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15b61a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15b61acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15b61b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15b61b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15b61baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15b61bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15b61c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15b61c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15b61cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15b61d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15b61d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15b61d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15b61de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15b61e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15b61e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15b61eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15b61efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15b61f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15b61f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15b61fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15b6201a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15b620610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15b620a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15b620ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15b621360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15b6217d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15b621c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15b6220b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15b622520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15b622990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15b622e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15b623270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15b6236e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15b623b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15b623fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15b624430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15b6248a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15b624d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15b625180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15b6255f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15b625a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15b625ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15b626340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15b6267b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15b626c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15b627090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15b627500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15b627970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15b627de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15b628250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15b6286c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15b628b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15b628fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15b629410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15b629880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15b629cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15b62a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15b62a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15b62aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15b62aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15b62b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15b62b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15b62bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15b62c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15b62c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15b62c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15b62cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15b62d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15b62d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15b62db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15b62df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15b62e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15b62e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15b62ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15b62f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15b62f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15b62fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15b62fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15b630300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15b630770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15b630be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15b631050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15b6314c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15b631930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15b631da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15b632210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15b632680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15b632af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15b632f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15b6333d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15b633840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15b633cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15b634120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15b634590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15b634a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15b634e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15b6352e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15b635750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15b635bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15b636030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15b6364a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15b636910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15b636d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15b6371f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15b637660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15b637ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15b637f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15b6383b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15b638820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15b638c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15b639100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15b639570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15b6399e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15b639e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15b63a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15b63a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15b63aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15b63b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15b63b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15b63b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15b63bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15b63c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15b63c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15b63cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15b63cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15b63d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15b63d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15b63dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15b63e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15b63e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15b63e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15b63ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15b63f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15b63f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15b63fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15b63fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15b640460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15b6408d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15b640d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15b6411b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15b641620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15b641a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15b641f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15b642370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15b6427e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15b642c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15b6430c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15b643530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15b6439a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15b643e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15b644280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15b6446f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15b644b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15b644fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15b645b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15b645e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15b6460c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15b646530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15b6469a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15b646e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15b647280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15b6476f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15b647b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15b647fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15b648440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15b6488b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15b648d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15b649190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15b649600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15b649a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15b649ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15b64a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15b64a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15b64ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15b64b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15b64b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15b64b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15b64bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15b64c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15b64c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15b64cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15b64cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15b64d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15b64d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15b64dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15b64e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15b64e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15b64ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15b64eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15b64f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15b64f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15b64fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15b650080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15b6504f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15b650960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15b650dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15b651240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15b6516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15b651b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15b651f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15b652400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15b652870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15b652ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15b653150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15b6535c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15b653a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15b653ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15b654310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15b654780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15b654bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15b655060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15b6554d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15b655940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15b655db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15b656220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15b656690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15b656b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15b656f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15b6573e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15b657850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15b657cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15b658130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15b6585a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15b658a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15b658e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15b6599c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15b65a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15b65a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15b65af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15b65b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15b65b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15b65b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15b65bd80 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15c829030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15c8294a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15c829910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15c829d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15c82a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15c82a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15c82aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15c82af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15c82b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15c82b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15c82bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15c82c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15c82cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15c82d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15c82dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15c82e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15c82e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15c82ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15c82f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15c830000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15c8306f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15c830de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15c8314d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15c831bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15c8322b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15c832720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15c832b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15c833000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15c833470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15c8338e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15c833d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15c8341c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15c834630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15c8348f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15c834d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15c8351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15c835640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15c835ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15c835f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15c836390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15c836800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15c836c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15c8370e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15c837550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15c8379c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15c837e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15c8382a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15c838710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15c838b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15c838ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15c839460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15c8398d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15c839d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15c83a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15c83a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15c83aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15c83af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15c83b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15c83b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15c83bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15c83c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15c83c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15c83c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15c83ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15c83d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15c83d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15c83db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15c83dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15c83e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15c83e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15c83ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15c83f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15c83f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15c83fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15c83fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15c840350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15c8407c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15c840c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15c8410a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15c841510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15c841980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15c841df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15c842260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15c8426d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15c842b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15c842fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15c843420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15c843890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15c843d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15c844170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15c8445e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15c844a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15c844ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15c845330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15c8457a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15c845c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15c846080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15c8464f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15c846960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15c846dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15c847240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15c8476b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15c847b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15c847f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15c848400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15c848870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15c848ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15c849150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15c8495c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15c849a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15c849ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15c84a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15c84a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15c84abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15c84b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15c84b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15c84b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15c84bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15c84c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15c84c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15c84cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15c84cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15c84d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15c84d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15c84dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15c84e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15c84e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15c84ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15c84ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15c84f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15c84f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15c84fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15c850040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15c8504b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15c850920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15c850d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15c851200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15c851670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15c851ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15c851f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15c8523c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15c852830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15c852ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15c853110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15c853580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15c8539f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15c853e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15c8542d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15c854740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15c854bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15c855020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15c855490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15c855900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15c855d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15c8561e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15c856650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15c856ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15c856f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15c8573a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15c857810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15c857c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15c8580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15c858560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15c8589d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15c858e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15c8592b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15c859720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15c859b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15c85a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15c85a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15c85a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15c85ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15c85b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15c85b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15c85baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15c85bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15c85c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15c85c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15c85cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15c85d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15c85d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15c85d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15c85de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15c85e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15c85e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15c85eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15c85efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15c85f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15c85f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15c85fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15c8601a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15c860610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15c860a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15c860ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15c861360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15c8617d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15c861c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15c8620b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15c862520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15c862990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15c862e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15c863270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15c8636e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15c863b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15c863fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15c864430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15c8081f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15c807c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15c8068d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15c821a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15c821ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15c822310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15c822780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15c822bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15c823060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15c8234d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15c823940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15c823db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15c824220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15c824690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15c824b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15c824f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15c8253e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15c825850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15c825cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15c826130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15c8265a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15c826a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15c826e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15c8272f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15c827760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15c827bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15c828040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15c8284b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15c828920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15c8145d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15c814a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15c814eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15c815320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15c815790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15c815c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15c816070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15c8164e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15c816950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15c816dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15c817230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15c8176a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15c817b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15c817f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15c8183f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15c818860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15c818cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15c819140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15c8195b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15c819a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15c819e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15c81a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15c81a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15c81abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15c81b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15c81b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15c81b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15c81bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15c81c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15c81c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15c81caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15c81cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15c81d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15c81d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15c81dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15c81e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15c81e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15c81ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15c81ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15c81f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15c81f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15c81fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15c820030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15c8204a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15c820910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15c821000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15c812de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15c8134d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15c813bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15c814030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15c80a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15c80ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15c80af70 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.948s
user	0m0.244s
sys	0m0.145s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.54 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.58 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.11 sec*proc (2 tests)

Total Test time (real) =   1.12 sec
        1.15 real         0.72 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.25 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.51 sec*proc (2 tests)

Total Test time (real) =   0.52 sec
        0.52 real         0.15 user         0.04 sys
```
