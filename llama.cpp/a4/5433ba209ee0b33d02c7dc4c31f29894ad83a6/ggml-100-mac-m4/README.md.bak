### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.28 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.77 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.68 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.43 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.97 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.32 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.32 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.14 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.19 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.21 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.03 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.22 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.27 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.18 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.51 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  177.00 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.91 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   26.03 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.33 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 219.86 sec*proc (28 tests)

Total Test time (real) = 219.87 sec

real	3m39.902s
user	7m32.508s
sys	0m6.037s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.29 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.34 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.05 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.24 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.90 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.21 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.13 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.32 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.24 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.32 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.13 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.27 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.13 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.21 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  51.32 sec*proc (28 tests)

Total Test time (real) =  51.34 sec

real	0m51.347s
user	1m11.181s
sys	0m5.342s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.072 I build: 4405 (a45433ba) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.973 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.699 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.014.702 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.704 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.014.704 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.705 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.014.705 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.014.706 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.014.706 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.014.707 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.014.707 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.014.708 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.014.708 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.014.710 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.014.711 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.014.711 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.014.711 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.014.711 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.014.712 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.014.714 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.016.851 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.017.408 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.017.409 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.017.409 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.017.410 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.017.410 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.017.410 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.017.410 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.017.411 I llama_model_loader: - type  f32:  124 tensors
0.00.017.411 I llama_model_loader: - type  f16:   73 tensors
0.00.019.685 I llm_load_vocab: special tokens cache size = 5
0.00.020.878 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.020.894 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.020.895 I llm_load_print_meta: arch             = bert
0.00.020.896 I llm_load_print_meta: vocab type       = WPM
0.00.020.896 I llm_load_print_meta: n_vocab          = 30522
0.00.020.896 I llm_load_print_meta: n_merges         = 0
0.00.020.896 I llm_load_print_meta: vocab_only       = 0
0.00.020.897 I llm_load_print_meta: n_ctx_train      = 512
0.00.020.897 I llm_load_print_meta: n_embd           = 384
0.00.020.897 I llm_load_print_meta: n_layer          = 12
0.00.020.899 I llm_load_print_meta: n_head           = 12
0.00.020.900 I llm_load_print_meta: n_head_kv        = 12
0.00.020.900 I llm_load_print_meta: n_rot            = 32
0.00.020.900 I llm_load_print_meta: n_swa            = 0
0.00.020.900 I llm_load_print_meta: n_embd_head_k    = 32
0.00.020.901 I llm_load_print_meta: n_embd_head_v    = 32
0.00.020.901 I llm_load_print_meta: n_gqa            = 1
0.00.020.902 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.020.902 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.020.903 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.020.904 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.020.904 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.020.904 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.020.906 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.020.906 I llm_load_print_meta: n_ff             = 1536
0.00.020.907 I llm_load_print_meta: n_expert         = 0
0.00.020.907 I llm_load_print_meta: n_expert_used    = 0
0.00.020.907 I llm_load_print_meta: causal attn      = 0
0.00.020.907 I llm_load_print_meta: pooling type     = 2
0.00.020.907 I llm_load_print_meta: rope type        = 2
0.00.020.907 I llm_load_print_meta: rope scaling     = linear
0.00.020.908 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.020.908 I llm_load_print_meta: freq_scale_train = 1
0.00.020.908 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.020.908 I llm_load_print_meta: rope_finetuned   = unknown
0.00.020.908 I llm_load_print_meta: ssm_d_conv       = 0
0.00.020.909 I llm_load_print_meta: ssm_d_inner      = 0
0.00.020.909 I llm_load_print_meta: ssm_d_state      = 0
0.00.020.909 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.020.911 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.020.911 I llm_load_print_meta: model type       = 33M
0.00.020.912 I llm_load_print_meta: model ftype      = F16
0.00.020.912 I llm_load_print_meta: model params     = 33.21 M
0.00.020.912 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.020.913 I llm_load_print_meta: general.name     = Bge Small
0.00.020.913 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.020.913 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.020.913 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.020.913 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.020.914 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.020.914 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.020.914 I llm_load_print_meta: max token length = 21
0.00.022.132 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.022.133 I llm_load_tensors: offloading output layer to GPU
0.00.022.133 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.022.150 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.022.151 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.022.512 I llama_new_context_with_model: n_seq_max     = 1
0.00.022.513 I llama_new_context_with_model: n_ctx         = 512
0.00.022.513 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.022.513 I llama_new_context_with_model: n_batch       = 2048
0.00.022.513 I llama_new_context_with_model: n_ubatch      = 2048
0.00.022.513 I llama_new_context_with_model: flash_attn    = 0
0.00.022.514 I llama_new_context_with_model: freq_base     = 10000.0
0.00.022.514 I llama_new_context_with_model: freq_scale    = 1
0.00.022.530 I ggml_metal_init: allocating
0.00.022.578 I ggml_metal_init: found device: Apple M4
0.00.022.589 I ggml_metal_init: picking default device: Apple M4
0.00.023.219 I ggml_metal_init: using embedded metal library
0.00.025.818 I ggml_metal_init: GPU name:   Apple M4
0.00.025.821 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.025.821 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.025.821 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.025.821 I ggml_metal_init: simdgroup reduction   = true
0.00.025.823 I ggml_metal_init: simdgroup matrix mul. = true
0.00.025.823 I ggml_metal_init: has bfloat            = true
0.00.025.823 I ggml_metal_init: use bfloat            = true
0.00.025.823 I ggml_metal_init: hasUnifiedMemory      = true
0.00.025.824 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.882 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12
0.00.035.369 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.035.371 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.035.373 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.035.932 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.035.934 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.035.934 I llama_new_context_with_model: graph nodes  = 429
0.00.035.934 I llama_new_context_with_model: graph splits = 2
0.00.035.950 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.035.951 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.041.051 I 
0.00.041.070 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.041.663 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.045.868 I llama_perf_context_print:        load time =      28.07 ms
0.00.045.869 I llama_perf_context_print: prompt eval time =       4.10 ms /     9 tokens (    0.46 ms per token,  2196.73 tokens per second)
0.00.045.869 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.045.870 I llama_perf_context_print:       total time =       4.82 ms /    10 tokens
0.00.046.062 I ggml_metal_free: deallocating

real	0m0.219s
user	0m0.032s
sys	0m0.023s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.035 I build: 4405 (a45433ba) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.670 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.010.632 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.010.636 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.010.637 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.010.638 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.010.638 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.010.638 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.010.639 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.010.640 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.010.640 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.010.641 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.010.641 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.010.641 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.010.643 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.010.644 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.010.644 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.010.645 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.010.645 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.010.645 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.010.646 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.012.900 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.013.481 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.013.482 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.013.482 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.013.483 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.013.483 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.013.483 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.013.484 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.013.484 I llama_model_loader: - type  f32:  124 tensors
0.00.013.484 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.799 I llm_load_vocab: special tokens cache size = 5
0.00.017.043 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.017.053 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.017.054 I llm_load_print_meta: arch             = bert
0.00.017.054 I llm_load_print_meta: vocab type       = WPM
0.00.017.054 I llm_load_print_meta: n_vocab          = 30522
0.00.017.055 I llm_load_print_meta: n_merges         = 0
0.00.017.055 I llm_load_print_meta: vocab_only       = 0
0.00.017.055 I llm_load_print_meta: n_ctx_train      = 512
0.00.017.055 I llm_load_print_meta: n_embd           = 384
0.00.017.055 I llm_load_print_meta: n_layer          = 12
0.00.017.058 I llm_load_print_meta: n_head           = 12
0.00.017.059 I llm_load_print_meta: n_head_kv        = 12
0.00.017.059 I llm_load_print_meta: n_rot            = 32
0.00.017.060 I llm_load_print_meta: n_swa            = 0
0.00.017.061 I llm_load_print_meta: n_embd_head_k    = 32
0.00.017.061 I llm_load_print_meta: n_embd_head_v    = 32
0.00.017.062 I llm_load_print_meta: n_gqa            = 1
0.00.017.062 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.017.063 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.017.064 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.017.065 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.017.065 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.017.065 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.017.065 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.017.066 I llm_load_print_meta: n_ff             = 1536
0.00.017.066 I llm_load_print_meta: n_expert         = 0
0.00.017.066 I llm_load_print_meta: n_expert_used    = 0
0.00.017.067 I llm_load_print_meta: causal attn      = 0
0.00.017.067 I llm_load_print_meta: pooling type     = 2
0.00.017.067 I llm_load_print_meta: rope type        = 2
0.00.017.067 I llm_load_print_meta: rope scaling     = linear
0.00.017.068 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.017.068 I llm_load_print_meta: freq_scale_train = 1
0.00.017.068 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.017.069 I llm_load_print_meta: rope_finetuned   = unknown
0.00.017.069 I llm_load_print_meta: ssm_d_conv       = 0
0.00.017.069 I llm_load_print_meta: ssm_d_inner      = 0
0.00.017.069 I llm_load_print_meta: ssm_d_state      = 0
0.00.017.069 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.017.070 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.017.070 I llm_load_print_meta: model type       = 33M
0.00.017.070 I llm_load_print_meta: model ftype      = Q8_0
0.00.017.070 I llm_load_print_meta: model params     = 33.21 M
0.00.017.071 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.017.071 I llm_load_print_meta: general.name     = Bge Small
0.00.017.072 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.017.072 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.017.072 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.017.072 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.017.072 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.017.073 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.017.073 I llm_load_print_meta: max token length = 21
0.00.018.225 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.018.225 I llm_load_tensors: offloading output layer to GPU
0.00.018.226 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.018.234 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.018.235 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.018.591 I llama_new_context_with_model: n_seq_max     = 1
0.00.018.592 I llama_new_context_with_model: n_ctx         = 512
0.00.018.592 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.018.593 I llama_new_context_with_model: n_batch       = 2048
0.00.018.593 I llama_new_context_with_model: n_ubatch      = 2048
0.00.018.593 I llama_new_context_with_model: flash_attn    = 0
0.00.018.593 I llama_new_context_with_model: freq_base     = 10000.0
0.00.018.594 I llama_new_context_with_model: freq_scale    = 1
0.00.018.594 I ggml_metal_init: allocating
0.00.018.597 I ggml_metal_init: found device: Apple M4
0.00.018.599 I ggml_metal_init: picking default device: Apple M4
0.00.019.215 I ggml_metal_init: using embedded metal library
0.00.021.566 I ggml_metal_init: GPU name:   Apple M4
0.00.021.567 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.021.568 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.021.568 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.021.569 I ggml_metal_init: simdgroup reduction   = true
0.00.021.569 I ggml_metal_init: simdgroup matrix mul. = true
0.00.021.569 I ggml_metal_init: has bfloat            = true
0.00.021.569 I ggml_metal_init: use bfloat            = true
0.00.021.570 I ggml_metal_init: hasUnifiedMemory      = true
0.00.021.571 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.031.768 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12
0.00.032.263 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.032.265 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.032.267 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.032.879 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.032.880 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.032.880 I llama_new_context_with_model: graph nodes  = 429
0.00.032.881 I llama_new_context_with_model: graph splits = 2
0.00.032.893 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.032.894 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.037.442 I 
0.00.037.461 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.038.001 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.042.431 I llama_perf_context_print:        load time =      28.77 ms
0.00.042.432 I llama_perf_context_print: prompt eval time =       4.29 ms /     9 tokens (    0.48 ms per token,  2095.95 tokens per second)
0.00.042.433 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.042.435 I llama_perf_context_print:       total time =       4.99 ms /    10 tokens
0.00.042.629 I ggml_metal_free: deallocating

real	0m0.054s
user	0m0.028s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.203 I build: 4405 (a45433ba) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.815 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.166 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.171 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.174 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.034.175 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.178 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.034.179 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.034.179 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.034.180 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.034.182 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.034.182 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.034.183 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.034.184 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.034.187 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.034.191 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.034.192 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.034.193 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.193 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.041.873 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.044.247 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.000 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.049.002 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.002 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.049.003 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.049.003 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.049.003 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.049.004 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.049.004 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.049.004 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.049.005 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.049.005 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.049.005 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.049.006 I llama_model_loader: - type  f32:   40 tensors
0.00.049.012 I llama_model_loader: - type  f16:   30 tensors
0.00.067.884 W llm_load_vocab: empty token at index 5
0.00.072.680 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.074.056 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.074.087 I llm_load_vocab: special tokens cache size = 5
0.00.329.554 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.329.559 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.329.560 I llm_load_print_meta: arch             = jina-bert-v2
0.00.329.560 I llm_load_print_meta: vocab type       = BPE
0.00.329.564 I llm_load_print_meta: n_vocab          = 61056
0.00.329.564 I llm_load_print_meta: n_merges         = 39382
0.00.329.564 I llm_load_print_meta: vocab_only       = 0
0.00.329.564 I llm_load_print_meta: n_ctx_train      = 8192
0.00.329.564 I llm_load_print_meta: n_embd           = 384
0.00.329.565 I llm_load_print_meta: n_layer          = 4
0.00.329.572 I llm_load_print_meta: n_head           = 12
0.00.329.572 I llm_load_print_meta: n_head_kv        = 12
0.00.329.572 I llm_load_print_meta: n_rot            = 32
0.00.329.572 I llm_load_print_meta: n_swa            = 0
0.00.329.573 I llm_load_print_meta: n_embd_head_k    = 32
0.00.329.573 I llm_load_print_meta: n_embd_head_v    = 32
0.00.329.573 I llm_load_print_meta: n_gqa            = 1
0.00.329.574 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.329.575 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.329.576 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.329.576 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.329.576 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.329.577 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.329.577 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.329.578 I llm_load_print_meta: n_ff             = 1536
0.00.329.578 I llm_load_print_meta: n_expert         = 0
0.00.329.578 I llm_load_print_meta: n_expert_used    = 0
0.00.329.578 I llm_load_print_meta: causal attn      = 0
0.00.329.578 I llm_load_print_meta: pooling type     = -1
0.00.329.578 I llm_load_print_meta: rope type        = -1
0.00.329.579 I llm_load_print_meta: rope scaling     = linear
0.00.329.579 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.329.579 I llm_load_print_meta: freq_scale_train = 1
0.00.329.579 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.329.580 I llm_load_print_meta: rope_finetuned   = unknown
0.00.329.580 I llm_load_print_meta: ssm_d_conv       = 0
0.00.329.580 I llm_load_print_meta: ssm_d_inner      = 0
0.00.329.580 I llm_load_print_meta: ssm_d_state      = 0
0.00.329.580 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.329.581 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.329.582 I llm_load_print_meta: model type       = 33M
0.00.329.583 I llm_load_print_meta: model ftype      = F16
0.00.329.583 I llm_load_print_meta: model params     = 32.90 M
0.00.329.584 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.329.584 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.329.584 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.329.584 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.329.585 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.329.586 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.329.586 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.329.586 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.329.587 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.329.587 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.329.587 I llm_load_print_meta: max token length = 45
0.00.330.833 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.330.833 I llm_load_tensors: offloading output layer to GPU
0.00.330.834 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.330.857 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.330.858 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.331.761 I llama_new_context_with_model: n_seq_max     = 1
0.00.331.762 I llama_new_context_with_model: n_ctx         = 8192
0.00.331.762 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.331.763 I llama_new_context_with_model: n_batch       = 2048
0.00.331.763 I llama_new_context_with_model: n_ubatch      = 2048
0.00.331.763 I llama_new_context_with_model: flash_attn    = 0
0.00.331.763 I llama_new_context_with_model: freq_base     = 10000.0
0.00.331.764 I llama_new_context_with_model: freq_scale    = 1
0.00.331.764 I ggml_metal_init: allocating
0.00.331.767 I ggml_metal_init: found device: Apple M4
0.00.331.769 I ggml_metal_init: picking default device: Apple M4
0.00.332.783 I ggml_metal_init: using embedded metal library
0.00.335.678 I ggml_metal_init: GPU name:   Apple M4
0.00.335.679 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.335.680 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.335.680 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.335.680 I ggml_metal_init: simdgroup reduction   = true
0.00.335.681 I ggml_metal_init: simdgroup matrix mul. = true
0.00.335.681 I ggml_metal_init: has bfloat            = true
0.00.335.681 I ggml_metal_init: use bfloat            = true
0.00.335.681 I ggml_metal_init: hasUnifiedMemory      = true
0.00.335.682 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.345.096 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4
0.00.347.466 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.347.468 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.347.469 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.348.073 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.348.074 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.348.074 I llama_new_context_with_model: graph nodes  = 154
0.00.348.075 I llama_new_context_with_model: graph splits = 2
0.00.348.093 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.348.094 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.357.922 I 
0.00.357.943 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.358.092 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.358.093 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.358.096 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.358.096 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.358.098 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.358.098 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.358.609 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.362.325 I llama_perf_context_print:        load time =     335.10 ms
0.00.362.327 I llama_perf_context_print: prompt eval time =       3.71 ms /    62 tokens (    0.06 ms per token, 16734.14 tokens per second)
0.00.362.328 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.362.328 I llama_perf_context_print:       total time =       4.40 ms /    63 tokens
0.00.362.561 I ggml_metal_free: deallocating

real	0m1.084s
user	0m0.336s
sys	0m0.043s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.134 I build: 4405 (a45433ba) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.253 I main: llama backend init
0.00.000.260 I main: load the model and apply lora adapter, if any
0.00.067.969 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.079.188 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.079.204 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.079.213 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.079.214 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.079.214 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.079.215 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.079.216 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.079.218 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.079.219 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.079.220 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.079.220 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.079.221 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.079.222 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.079.222 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.079.228 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.079.228 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.079.233 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.086.307 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.088.581 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.095.633 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.095.639 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.095.640 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.095.641 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.095.641 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.095.643 I llama_model_loader: - type  f32:  194 tensors
0.00.095.644 I llama_model_loader: - type  f16:   98 tensors
0.00.134.587 I llm_load_vocab: special tokens cache size = 25
0.00.142.103 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.142.106 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.142.107 I llm_load_print_meta: arch             = gptneox
0.00.142.107 I llm_load_print_meta: vocab type       = BPE
0.00.142.107 I llm_load_print_meta: n_vocab          = 50304
0.00.142.107 I llm_load_print_meta: n_merges         = 50009
0.00.142.108 I llm_load_print_meta: vocab_only       = 0
0.00.142.108 I llm_load_print_meta: n_ctx_train      = 2048
0.00.142.108 I llm_load_print_meta: n_embd           = 2048
0.00.142.108 I llm_load_print_meta: n_layer          = 24
0.00.142.112 I llm_load_print_meta: n_head           = 16
0.00.142.116 I llm_load_print_meta: n_head_kv        = 16
0.00.142.116 I llm_load_print_meta: n_rot            = 32
0.00.142.116 I llm_load_print_meta: n_swa            = 0
0.00.142.116 I llm_load_print_meta: n_embd_head_k    = 128
0.00.142.116 I llm_load_print_meta: n_embd_head_v    = 128
0.00.142.117 I llm_load_print_meta: n_gqa            = 1
0.00.142.118 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.142.119 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.142.120 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.142.120 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.142.120 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.142.120 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.142.120 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.142.121 I llm_load_print_meta: n_ff             = 8192
0.00.142.121 I llm_load_print_meta: n_expert         = 0
0.00.142.122 I llm_load_print_meta: n_expert_used    = 0
0.00.142.122 I llm_load_print_meta: causal attn      = 1
0.00.142.122 I llm_load_print_meta: pooling type     = 0
0.00.142.122 I llm_load_print_meta: rope type        = 2
0.00.142.122 I llm_load_print_meta: rope scaling     = linear
0.00.142.123 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.142.123 I llm_load_print_meta: freq_scale_train = 1
0.00.142.123 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.142.123 I llm_load_print_meta: rope_finetuned   = unknown
0.00.142.123 I llm_load_print_meta: ssm_d_conv       = 0
0.00.142.123 I llm_load_print_meta: ssm_d_inner      = 0
0.00.142.124 I llm_load_print_meta: ssm_d_state      = 0
0.00.142.124 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.142.124 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.142.124 I llm_load_print_meta: model type       = 1.4B
0.00.142.125 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.142.125 I llm_load_print_meta: model params     = 1.41 B
0.00.142.126 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.142.126 I llm_load_print_meta: general.name     = 1.4B
0.00.142.126 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.142.126 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.142.126 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.142.127 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.142.127 I llm_load_print_meta: LF token         = 128 ''
0.00.142.127 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.142.127 I llm_load_print_meta: max token length = 1024
0.00.144.888 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.144.888 I llm_load_tensors: offloading output layer to GPU
0.00.144.888 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.144.907 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.144.909 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.145.914 I llama_new_context_with_model: n_seq_max     = 1
0.00.145.915 I llama_new_context_with_model: n_ctx         = 2048
0.00.145.915 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.145.915 I llama_new_context_with_model: n_batch       = 2048
0.00.145.915 I llama_new_context_with_model: n_ubatch      = 512
0.00.145.915 I llama_new_context_with_model: flash_attn    = 0
0.00.145.916 I llama_new_context_with_model: freq_base     = 10000.0
0.00.145.916 I llama_new_context_with_model: freq_scale    = 1
0.00.145.917 I ggml_metal_init: allocating
0.00.145.920 I ggml_metal_init: found device: Apple M4
0.00.145.924 I ggml_metal_init: picking default device: Apple M4
0.00.146.632 I ggml_metal_init: using embedded metal library
0.00.156.608 I ggml_metal_init: GPU name:   Apple M4
0.00.156.610 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.156.611 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.156.611 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.156.611 I ggml_metal_init: simdgroup reduction   = true
0.00.156.612 I ggml_metal_init: simdgroup matrix mul. = true
0.00.156.612 I ggml_metal_init: has bfloat            = true
0.00.156.612 I ggml_metal_init: use bfloat            = true
0.00.156.612 I ggml_metal_init: hasUnifiedMemory      = true
0.00.156.613 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.181.354 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.203.411 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.203.420 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.203.442 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.204.524 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.204.527 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.204.528 I llama_new_context_with_model: graph nodes  = 967
0.00.204.528 I llama_new_context_with_model: graph splits = 2
0.00.204.560 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.204.704 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.204.705 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.286.179 I main: llama threadpool init, n_threads = 4
0.00.286.229 I 
0.00.286.251 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.286.251 I 
0.00.286.328 I sampler seed: 1234
0.00.286.332 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.286.356 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.286.358 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.286.358 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.125.085 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57676.69 tokens per second)
0.02.125.085 I llama_perf_context_print:        load time =     218.20 ms
0.02.125.086 I llama_perf_context_print: prompt eval time =      43.65 ms /     7 tokens (    6.24 ms per token,   160.38 tokens per second)
0.02.125.088 I llama_perf_context_print:        eval time =    1792.13 ms /    63 runs   (   28.45 ms per token,    35.15 tokens per second)
0.02.125.088 I llama_perf_context_print:       total time =    1838.91 ms /    70 tokens
0.02.125.282 I ggml_metal_free: deallocating

real	0m2.470s
user	0m0.151s
sys	0m0.106s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.856 I build: 4405 (a45433ba) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.257 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.444 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.450 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.452 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.455 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.455 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.456 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.460 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.461 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.462 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.462 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.463 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.463 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.464 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.464 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.466 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.466 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.467 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.098 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.164 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.209 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.055.211 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.212 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.212 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.213 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.214 I llama_model_loader: - type  f32:  194 tensors
0.00.055.214 I llama_model_loader: - type  f16:   98 tensors
0.00.084.827 I llm_load_vocab: special tokens cache size = 25
0.00.091.476 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.091.479 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.091.479 I llm_load_print_meta: arch             = gptneox
0.00.091.479 I llm_load_print_meta: vocab type       = BPE
0.00.091.479 I llm_load_print_meta: n_vocab          = 50304
0.00.091.480 I llm_load_print_meta: n_merges         = 50009
0.00.091.480 I llm_load_print_meta: vocab_only       = 0
0.00.091.480 I llm_load_print_meta: n_ctx_train      = 2048
0.00.091.480 I llm_load_print_meta: n_embd           = 2048
0.00.091.480 I llm_load_print_meta: n_layer          = 24
0.00.091.483 I llm_load_print_meta: n_head           = 16
0.00.091.484 I llm_load_print_meta: n_head_kv        = 16
0.00.091.484 I llm_load_print_meta: n_rot            = 32
0.00.091.484 I llm_load_print_meta: n_swa            = 0
0.00.091.486 I llm_load_print_meta: n_embd_head_k    = 128
0.00.091.486 I llm_load_print_meta: n_embd_head_v    = 128
0.00.091.487 I llm_load_print_meta: n_gqa            = 1
0.00.091.488 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.091.488 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.091.489 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.091.489 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.091.489 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.091.489 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.091.489 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.091.490 I llm_load_print_meta: n_ff             = 8192
0.00.091.490 I llm_load_print_meta: n_expert         = 0
0.00.091.490 I llm_load_print_meta: n_expert_used    = 0
0.00.091.490 I llm_load_print_meta: causal attn      = 1
0.00.091.491 I llm_load_print_meta: pooling type     = 0
0.00.091.491 I llm_load_print_meta: rope type        = 2
0.00.091.491 I llm_load_print_meta: rope scaling     = linear
0.00.091.492 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.091.493 I llm_load_print_meta: freq_scale_train = 1
0.00.091.493 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.091.493 I llm_load_print_meta: rope_finetuned   = unknown
0.00.091.493 I llm_load_print_meta: ssm_d_conv       = 0
0.00.091.493 I llm_load_print_meta: ssm_d_inner      = 0
0.00.091.494 I llm_load_print_meta: ssm_d_state      = 0
0.00.091.494 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.091.494 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.091.494 I llm_load_print_meta: model type       = 1.4B
0.00.091.495 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.091.495 I llm_load_print_meta: model params     = 1.41 B
0.00.091.496 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.091.496 I llm_load_print_meta: general.name     = 1.4B
0.00.091.496 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.091.496 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.091.497 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.091.497 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.091.497 I llm_load_print_meta: LF token         = 128 ''
0.00.091.497 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.091.497 I llm_load_print_meta: max token length = 1024
0.00.094.120 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.094.120 I llm_load_tensors: offloading output layer to GPU
0.00.094.121 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.094.131 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.094.133 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.095.089 I llama_new_context_with_model: n_seq_max     = 1
0.00.095.090 I llama_new_context_with_model: n_ctx         = 128
0.00.095.090 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.095.090 I llama_new_context_with_model: n_batch       = 128
0.00.095.090 I llama_new_context_with_model: n_ubatch      = 128
0.00.095.090 I llama_new_context_with_model: flash_attn    = 0
0.00.095.091 I llama_new_context_with_model: freq_base     = 10000.0
0.00.095.091 I llama_new_context_with_model: freq_scale    = 1
0.00.095.092 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.095.092 I ggml_metal_init: allocating
0.00.095.100 I ggml_metal_init: found device: Apple M4
0.00.095.102 I ggml_metal_init: picking default device: Apple M4
0.00.095.724 I ggml_metal_init: using embedded metal library
0.00.098.302 I ggml_metal_init: GPU name:   Apple M4
0.00.098.304 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.098.304 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.098.304 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.098.305 I ggml_metal_init: simdgroup reduction   = true
0.00.098.305 I ggml_metal_init: simdgroup matrix mul. = true
0.00.098.305 I ggml_metal_init: has bfloat            = true
0.00.098.305 I ggml_metal_init: use bfloat            = true
0.00.098.306 I ggml_metal_init: hasUnifiedMemory      = true
0.00.098.306 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.107.376 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.108.616 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.108.624 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.108.640 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.109.514 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.109.514 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.109.515 I llama_new_context_with_model: graph nodes  = 967
0.00.109.515 I llama_new_context_with_model: graph splits = 2
0.00.109.527 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.109.528 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.114.182 I 
0.01.114.220 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.114.260 I perplexity: tokenizing the input ..
0.01.127.730 I perplexity: tokenization took 13.467 ms
0.01.127.736 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.250.322 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.252.057 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.252.085 I llama_perf_context_print:        load time =    1088.91 ms
0.01.252.087 I llama_perf_context_print: prompt eval time =     121.66 ms /   128 tokens (    0.95 ms per token,  1052.09 tokens per second)
0.01.252.088 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.252.092 I llama_perf_context_print:       total time =     137.91 ms /   129 tokens
0.01.252.910 I ggml_metal_free: deallocating

real	0m1.451s
user	0m0.125s
sys	0m0.213s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4405 (a45433ba) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.009.785 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.245 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.250 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.253 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.253 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.253 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.254 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.254 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.255 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.255 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.256 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.257 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.258 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.258 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.258 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.260 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.261 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.261 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.191 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.277 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.230 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.232 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.232 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.232 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.233 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.233 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.234 I llama_model_loader: - type  f32:  194 tensors
0.00.034.234 I llama_model_loader: - type q8_0:   98 tensors
0.00.056.980 I llm_load_vocab: special tokens cache size = 25
0.00.062.888 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.062.892 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.062.892 I llm_load_print_meta: arch             = gptneox
0.00.062.893 I llm_load_print_meta: vocab type       = BPE
0.00.062.893 I llm_load_print_meta: n_vocab          = 50304
0.00.062.893 I llm_load_print_meta: n_merges         = 50009
0.00.062.893 I llm_load_print_meta: vocab_only       = 0
0.00.062.893 I llm_load_print_meta: n_ctx_train      = 2048
0.00.062.895 I llm_load_print_meta: n_embd           = 2048
0.00.062.895 I llm_load_print_meta: n_layer          = 24
0.00.062.901 I llm_load_print_meta: n_head           = 16
0.00.062.901 I llm_load_print_meta: n_head_kv        = 16
0.00.062.902 I llm_load_print_meta: n_rot            = 32
0.00.062.904 I llm_load_print_meta: n_swa            = 0
0.00.062.904 I llm_load_print_meta: n_embd_head_k    = 128
0.00.062.904 I llm_load_print_meta: n_embd_head_v    = 128
0.00.062.905 I llm_load_print_meta: n_gqa            = 1
0.00.062.906 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.062.907 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.062.907 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.062.908 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.062.908 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.062.908 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.062.909 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.062.909 I llm_load_print_meta: n_ff             = 8192
0.00.062.910 I llm_load_print_meta: n_expert         = 0
0.00.062.910 I llm_load_print_meta: n_expert_used    = 0
0.00.062.910 I llm_load_print_meta: causal attn      = 1
0.00.062.910 I llm_load_print_meta: pooling type     = 0
0.00.062.910 I llm_load_print_meta: rope type        = 2
0.00.062.911 I llm_load_print_meta: rope scaling     = linear
0.00.062.911 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.062.911 I llm_load_print_meta: freq_scale_train = 1
0.00.062.912 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.062.912 I llm_load_print_meta: rope_finetuned   = unknown
0.00.062.912 I llm_load_print_meta: ssm_d_conv       = 0
0.00.062.912 I llm_load_print_meta: ssm_d_inner      = 0
0.00.062.912 I llm_load_print_meta: ssm_d_state      = 0
0.00.062.914 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.062.914 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.062.914 I llm_load_print_meta: model type       = 1.4B
0.00.062.915 I llm_load_print_meta: model ftype      = Q8_0
0.00.062.915 I llm_load_print_meta: model params     = 1.41 B
0.00.062.916 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.062.916 I llm_load_print_meta: general.name     = 1.4B
0.00.062.916 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.062.916 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.062.917 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.062.920 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.062.921 I llm_load_print_meta: LF token         = 128 ''
0.00.062.921 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.062.921 I llm_load_print_meta: max token length = 1024
0.00.065.517 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.065.517 I llm_load_tensors: offloading output layer to GPU
0.00.065.517 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.065.529 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.065.530 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.066.534 I llama_new_context_with_model: n_seq_max     = 1
0.00.066.535 I llama_new_context_with_model: n_ctx         = 2048
0.00.066.535 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.066.535 I llama_new_context_with_model: n_batch       = 2048
0.00.066.535 I llama_new_context_with_model: n_ubatch      = 512
0.00.066.536 I llama_new_context_with_model: flash_attn    = 0
0.00.066.536 I llama_new_context_with_model: freq_base     = 10000.0
0.00.066.536 I llama_new_context_with_model: freq_scale    = 1
0.00.066.537 I ggml_metal_init: allocating
0.00.066.543 I ggml_metal_init: found device: Apple M4
0.00.066.545 I ggml_metal_init: picking default device: Apple M4
0.00.067.291 I ggml_metal_init: using embedded metal library
0.00.069.895 I ggml_metal_init: GPU name:   Apple M4
0.00.069.896 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.069.897 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.069.897 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.069.898 I ggml_metal_init: simdgroup reduction   = true
0.00.069.898 I ggml_metal_init: simdgroup matrix mul. = true
0.00.069.898 I ggml_metal_init: has bfloat            = true
0.00.069.898 I ggml_metal_init: use bfloat            = true
0.00.069.899 I ggml_metal_init: hasUnifiedMemory      = true
0.00.069.899 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.080.443 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.104.864 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.104.879 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.104.904 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.106.224 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.106.227 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.106.227 I llama_new_context_with_model: graph nodes  = 967
0.00.106.227 I llama_new_context_with_model: graph splits = 2
0.00.106.246 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.106.398 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.106.399 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.336.874 I main: llama threadpool init, n_threads = 4
0.01.336.913 I 
0.01.336.937 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.336.937 I 
0.01.337.177 I sampler seed: 1234
0.01.337.181 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.337.192 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.337.193 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.337.193 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.426.168 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56215.36 tokens per second)
0.02.426.168 I llama_perf_context_print:        load time =    1327.09 ms
0.02.426.169 I llama_perf_context_print: prompt eval time =      46.07 ms /     7 tokens (    6.58 ms per token,   151.95 tokens per second)
0.02.426.171 I llama_perf_context_print:        eval time =    1039.81 ms /    63 runs   (   16.50 ms per token,    60.59 tokens per second)
0.02.426.174 I llama_perf_context_print:       total time =    1089.29 ms /    70 tokens
0.02.426.362 I ggml_metal_free: deallocating

real	0m2.445s
user	0m0.114s
sys	0m0.208s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.124 I build: 4405 (a45433ba) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.696 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.897 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.902 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.910 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.911 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.911 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.912 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.912 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.913 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.913 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.914 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.914 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.915 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.915 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.917 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.919 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.920 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.920 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.353 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.820 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.147 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.032.149 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.149 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.150 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.150 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.150 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.032.151 I llama_model_loader: - type  f32:  194 tensors
0.00.032.151 I llama_model_loader: - type q8_0:   98 tensors
0.00.057.364 I llm_load_vocab: special tokens cache size = 25
0.00.064.027 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.064.030 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.064.030 I llm_load_print_meta: arch             = gptneox
0.00.064.030 I llm_load_print_meta: vocab type       = BPE
0.00.064.031 I llm_load_print_meta: n_vocab          = 50304
0.00.064.031 I llm_load_print_meta: n_merges         = 50009
0.00.064.031 I llm_load_print_meta: vocab_only       = 0
0.00.064.031 I llm_load_print_meta: n_ctx_train      = 2048
0.00.064.031 I llm_load_print_meta: n_embd           = 2048
0.00.064.032 I llm_load_print_meta: n_layer          = 24
0.00.064.035 I llm_load_print_meta: n_head           = 16
0.00.064.036 I llm_load_print_meta: n_head_kv        = 16
0.00.064.036 I llm_load_print_meta: n_rot            = 32
0.00.064.036 I llm_load_print_meta: n_swa            = 0
0.00.064.037 I llm_load_print_meta: n_embd_head_k    = 128
0.00.064.037 I llm_load_print_meta: n_embd_head_v    = 128
0.00.064.037 I llm_load_print_meta: n_gqa            = 1
0.00.064.038 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.064.040 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.064.043 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.064.043 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.064.044 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.064.044 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.064.044 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.064.044 I llm_load_print_meta: n_ff             = 8192
0.00.064.045 I llm_load_print_meta: n_expert         = 0
0.00.064.045 I llm_load_print_meta: n_expert_used    = 0
0.00.064.045 I llm_load_print_meta: causal attn      = 1
0.00.064.045 I llm_load_print_meta: pooling type     = 0
0.00.064.045 I llm_load_print_meta: rope type        = 2
0.00.064.045 I llm_load_print_meta: rope scaling     = linear
0.00.064.046 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.064.046 I llm_load_print_meta: freq_scale_train = 1
0.00.064.046 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.064.047 I llm_load_print_meta: rope_finetuned   = unknown
0.00.064.047 I llm_load_print_meta: ssm_d_conv       = 0
0.00.064.047 I llm_load_print_meta: ssm_d_inner      = 0
0.00.064.047 I llm_load_print_meta: ssm_d_state      = 0
0.00.064.047 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.064.051 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.064.051 I llm_load_print_meta: model type       = 1.4B
0.00.064.051 I llm_load_print_meta: model ftype      = Q8_0
0.00.064.052 I llm_load_print_meta: model params     = 1.41 B
0.00.064.052 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.064.052 I llm_load_print_meta: general.name     = 1.4B
0.00.064.053 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.064.053 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.064.053 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.064.053 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.064.053 I llm_load_print_meta: LF token         = 128 ''
0.00.064.054 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.064.054 I llm_load_print_meta: max token length = 1024
0.00.066.425 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.066.425 I llm_load_tensors: offloading output layer to GPU
0.00.066.425 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.066.437 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.066.438 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.067.408 I llama_new_context_with_model: n_seq_max     = 1
0.00.067.409 I llama_new_context_with_model: n_ctx         = 128
0.00.067.410 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.067.410 I llama_new_context_with_model: n_batch       = 128
0.00.067.410 I llama_new_context_with_model: n_ubatch      = 128
0.00.067.410 I llama_new_context_with_model: flash_attn    = 0
0.00.067.411 I llama_new_context_with_model: freq_base     = 10000.0
0.00.067.411 I llama_new_context_with_model: freq_scale    = 1
0.00.067.411 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.067.412 I ggml_metal_init: allocating
0.00.067.418 I ggml_metal_init: found device: Apple M4
0.00.067.421 I ggml_metal_init: picking default device: Apple M4
0.00.068.044 I ggml_metal_init: using embedded metal library
0.00.070.642 I ggml_metal_init: GPU name:   Apple M4
0.00.070.643 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.644 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.644 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.644 I ggml_metal_init: simdgroup reduction   = true
0.00.070.644 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.645 I ggml_metal_init: has bfloat            = true
0.00.070.645 I ggml_metal_init: use bfloat            = true
0.00.070.645 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.646 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.080.058 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.081.575 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.081.579 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.081.598 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.082.541 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.082.542 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.082.543 I llama_new_context_with_model: graph nodes  = 967
0.00.082.543 I llama_new_context_with_model: graph splits = 2
0.00.082.556 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.082.557 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.922.036 I 
0.00.922.066 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.922.102 I perplexity: tokenizing the input ..
0.00.930.367 I perplexity: tokenization took 8.263 ms
0.00.930.371 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.054.765 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.055.928 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.055.941 I llama_perf_context_print:        load time =     910.33 ms
0.01.055.942 I llama_perf_context_print: prompt eval time =     124.17 ms /   128 tokens (    0.97 ms per token,  1030.86 tokens per second)
0.01.055.943 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.055.943 I llama_perf_context_print:       total time =     133.91 ms /   129 tokens
0.01.056.395 I ggml_metal_free: deallocating

real	0m1.076s
user	0m0.093s
sys	0m0.151s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4405 (a45433ba) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.011.055 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.705 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.710 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.713 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.713 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.714 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.714 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.714 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.715 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.716 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.716 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.716 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.718 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.718 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.720 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.721 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.722 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.722 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.641 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.771 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.713 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.714 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.714 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.715 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.715 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.715 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.716 I llama_model_loader: - type  f32:  194 tensors
0.00.026.716 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.717 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.177 I llm_load_vocab: special tokens cache size = 25
0.00.054.162 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.165 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.165 I llm_load_print_meta: arch             = gptneox
0.00.054.166 I llm_load_print_meta: vocab type       = BPE
0.00.054.166 I llm_load_print_meta: n_vocab          = 50304
0.00.054.166 I llm_load_print_meta: n_merges         = 50009
0.00.054.166 I llm_load_print_meta: vocab_only       = 0
0.00.054.167 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.167 I llm_load_print_meta: n_embd           = 2048
0.00.054.167 I llm_load_print_meta: n_layer          = 24
0.00.054.172 I llm_load_print_meta: n_head           = 16
0.00.054.174 I llm_load_print_meta: n_head_kv        = 16
0.00.054.174 I llm_load_print_meta: n_rot            = 32
0.00.054.174 I llm_load_print_meta: n_swa            = 0
0.00.054.175 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.175 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.176 I llm_load_print_meta: n_gqa            = 1
0.00.054.176 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.177 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.178 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.178 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.178 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.178 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.178 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.179 I llm_load_print_meta: n_ff             = 8192
0.00.054.179 I llm_load_print_meta: n_expert         = 0
0.00.054.179 I llm_load_print_meta: n_expert_used    = 0
0.00.054.180 I llm_load_print_meta: causal attn      = 1
0.00.054.180 I llm_load_print_meta: pooling type     = 0
0.00.054.180 I llm_load_print_meta: rope type        = 2
0.00.054.180 I llm_load_print_meta: rope scaling     = linear
0.00.054.182 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.182 I llm_load_print_meta: freq_scale_train = 1
0.00.054.182 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.183 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.183 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.183 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.183 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.183 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.183 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.183 I llm_load_print_meta: model type       = 1.4B
0.00.054.184 I llm_load_print_meta: model ftype      = Q4_0
0.00.054.184 I llm_load_print_meta: model params     = 1.41 B
0.00.054.187 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.054.187 I llm_load_print_meta: general.name     = 1.4B
0.00.054.187 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.187 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.187 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.188 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.188 I llm_load_print_meta: LF token         = 128 ''
0.00.054.188 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.188 I llm_load_print_meta: max token length = 1024
0.00.056.515 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.515 I llm_load_tensors: offloading output layer to GPU
0.00.056.516 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.528 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.056.529 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.057.524 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.525 I llama_new_context_with_model: n_ctx         = 2048
0.00.057.525 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.057.525 I llama_new_context_with_model: n_batch       = 2048
0.00.057.525 I llama_new_context_with_model: n_ubatch      = 512
0.00.057.525 I llama_new_context_with_model: flash_attn    = 0
0.00.057.526 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.526 I llama_new_context_with_model: freq_scale    = 1
0.00.057.527 I ggml_metal_init: allocating
0.00.057.532 I ggml_metal_init: found device: Apple M4
0.00.057.536 I ggml_metal_init: picking default device: Apple M4
0.00.058.252 I ggml_metal_init: using embedded metal library
0.00.060.903 I ggml_metal_init: GPU name:   Apple M4
0.00.060.905 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.905 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.905 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.906 I ggml_metal_init: simdgroup reduction   = true
0.00.060.906 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.906 I ggml_metal_init: has bfloat            = true
0.00.060.906 I ggml_metal_init: use bfloat            = true
0.00.060.906 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.907 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.071.881 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.095.644 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.095.654 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.095.680 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.096.864 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.096.865 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.096.865 I llama_new_context_with_model: graph nodes  = 967
0.00.096.866 I llama_new_context_with_model: graph splits = 2
0.00.096.884 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.097.026 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.097.027 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.699.668 I main: llama threadpool init, n_threads = 4
0.00.699.707 I 
0.00.699.730 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.699.730 I 
0.00.699.962 I sampler seed: 1234
0.00.699.967 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.700.004 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.700.016 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.700.016 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.383.742 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56573.71 tokens per second)
0.01.383.742 I llama_perf_context_print:        load time =     688.61 ms
0.01.383.743 I llama_perf_context_print: prompt eval time =      43.69 ms /     7 tokens (    6.24 ms per token,   160.21 tokens per second)
0.01.383.744 I llama_perf_context_print:        eval time =     636.98 ms /    63 runs   (   10.11 ms per token,    98.90 tokens per second)
0.01.383.744 I llama_perf_context_print:       total time =     684.08 ms /    70 tokens
0.01.383.948 I ggml_metal_free: deallocating

real	0m1.401s
user	0m0.111s
sys	0m0.147s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4405 (a45433ba) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.389 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.441 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.445 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.446 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.447 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.447 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.447 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.448 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.449 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.449 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.449 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.450 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.450 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.450 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.451 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.452 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.452 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.453 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.251 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.276 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.095 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.096 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.096 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.096 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.097 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.097 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.098 I llama_model_loader: - type  f32:  194 tensors
0.00.024.098 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.098 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.313 I llm_load_vocab: special tokens cache size = 25
0.00.051.258 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.261 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.262 I llm_load_print_meta: arch             = gptneox
0.00.051.262 I llm_load_print_meta: vocab type       = BPE
0.00.051.262 I llm_load_print_meta: n_vocab          = 50304
0.00.051.262 I llm_load_print_meta: n_merges         = 50009
0.00.051.262 I llm_load_print_meta: vocab_only       = 0
0.00.051.263 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.263 I llm_load_print_meta: n_embd           = 2048
0.00.051.263 I llm_load_print_meta: n_layer          = 24
0.00.051.266 I llm_load_print_meta: n_head           = 16
0.00.051.268 I llm_load_print_meta: n_head_kv        = 16
0.00.051.268 I llm_load_print_meta: n_rot            = 32
0.00.051.268 I llm_load_print_meta: n_swa            = 0
0.00.051.269 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.269 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.270 I llm_load_print_meta: n_gqa            = 1
0.00.051.270 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.271 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.272 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.272 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.272 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.272 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.272 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.273 I llm_load_print_meta: n_ff             = 8192
0.00.051.273 I llm_load_print_meta: n_expert         = 0
0.00.051.273 I llm_load_print_meta: n_expert_used    = 0
0.00.051.274 I llm_load_print_meta: causal attn      = 1
0.00.051.274 I llm_load_print_meta: pooling type     = 0
0.00.051.274 I llm_load_print_meta: rope type        = 2
0.00.051.274 I llm_load_print_meta: rope scaling     = linear
0.00.051.275 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.275 I llm_load_print_meta: freq_scale_train = 1
0.00.051.275 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.276 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.276 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.276 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.276 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.276 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.276 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.277 I llm_load_print_meta: model type       = 1.4B
0.00.051.277 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.279 I llm_load_print_meta: model params     = 1.41 B
0.00.051.280 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.280 I llm_load_print_meta: general.name     = 1.4B
0.00.051.280 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.281 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.281 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.281 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.281 I llm_load_print_meta: LF token         = 128 ''
0.00.051.282 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.283 I llm_load_print_meta: max token length = 1024
0.00.053.275 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.276 I llm_load_tensors: offloading output layer to GPU
0.00.053.276 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.287 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.288 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.054.181 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.182 I llama_new_context_with_model: n_ctx         = 128
0.00.054.183 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.183 I llama_new_context_with_model: n_batch       = 128
0.00.054.183 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.183 I llama_new_context_with_model: flash_attn    = 0
0.00.054.184 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.184 I llama_new_context_with_model: freq_scale    = 1
0.00.054.184 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.185 I ggml_metal_init: allocating
0.00.054.191 I ggml_metal_init: found device: Apple M4
0.00.054.194 I ggml_metal_init: picking default device: Apple M4
0.00.054.770 I ggml_metal_init: using embedded metal library
0.00.057.088 I ggml_metal_init: GPU name:   Apple M4
0.00.057.089 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.090 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.090 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.090 I ggml_metal_init: simdgroup reduction   = true
0.00.057.091 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.091 I ggml_metal_init: has bfloat            = true
0.00.057.091 I ggml_metal_init: use bfloat            = true
0.00.057.091 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.092 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.507 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.745 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.747 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.761 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.663 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.664 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.665 I llama_new_context_with_model: graph nodes  = 967
0.00.068.665 I llama_new_context_with_model: graph splits = 2
0.00.068.677 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.678 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.596.023 I 
0.00.596.063 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.596.077 I perplexity: tokenizing the input ..
0.00.603.784 I perplexity: tokenization took 7.706 ms
0.00.603.788 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.726.659 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.727.885 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.727.901 I llama_perf_context_print:        load time =     586.63 ms
0.00.727.902 I llama_perf_context_print: prompt eval time =     122.65 ms /   128 tokens (    0.96 ms per token,  1043.62 tokens per second)
0.00.727.903 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.727.903 I llama_perf_context_print:       total time =     131.88 ms /   129 tokens
0.00.728.408 I ggml_metal_free: deallocating

real	0m0.743s
user	0m0.079s
sys	0m0.089s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4405 (a45433ba) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.009.054 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.316 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.320 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.322 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.322 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.329 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.329 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.330 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.330 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.331 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.331 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.332 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.332 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.332 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.333 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.334 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.335 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.335 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.169 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.200 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.994 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.995 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.995 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.996 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.996 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.996 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.997 I llama_model_loader: - type  f32:  194 tensors
0.00.024.997 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.998 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.379 I llm_load_vocab: special tokens cache size = 25
0.00.051.246 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.249 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.249 I llm_load_print_meta: arch             = gptneox
0.00.051.250 I llm_load_print_meta: vocab type       = BPE
0.00.051.250 I llm_load_print_meta: n_vocab          = 50304
0.00.051.250 I llm_load_print_meta: n_merges         = 50009
0.00.051.250 I llm_load_print_meta: vocab_only       = 0
0.00.051.250 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.250 I llm_load_print_meta: n_embd           = 2048
0.00.051.251 I llm_load_print_meta: n_layer          = 24
0.00.051.253 I llm_load_print_meta: n_head           = 16
0.00.051.254 I llm_load_print_meta: n_head_kv        = 16
0.00.051.254 I llm_load_print_meta: n_rot            = 32
0.00.051.254 I llm_load_print_meta: n_swa            = 0
0.00.051.254 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.255 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.255 I llm_load_print_meta: n_gqa            = 1
0.00.051.256 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.257 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.258 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.258 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.258 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.258 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.259 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.259 I llm_load_print_meta: n_ff             = 8192
0.00.051.259 I llm_load_print_meta: n_expert         = 0
0.00.051.260 I llm_load_print_meta: n_expert_used    = 0
0.00.051.260 I llm_load_print_meta: causal attn      = 1
0.00.051.260 I llm_load_print_meta: pooling type     = 0
0.00.051.260 I llm_load_print_meta: rope type        = 2
0.00.051.260 I llm_load_print_meta: rope scaling     = linear
0.00.051.261 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.261 I llm_load_print_meta: freq_scale_train = 1
0.00.051.261 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.262 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.262 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.262 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.262 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.262 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.262 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.263 I llm_load_print_meta: model type       = 1.4B
0.00.051.263 I llm_load_print_meta: model ftype      = Q4_1
0.00.051.263 I llm_load_print_meta: model params     = 1.41 B
0.00.051.264 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.051.264 I llm_load_print_meta: general.name     = 1.4B
0.00.051.264 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.265 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.265 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.265 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.267 I llm_load_print_meta: LF token         = 128 ''
0.00.051.268 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.268 I llm_load_print_meta: max token length = 1024
0.00.052.994 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.994 I llm_load_tensors: offloading output layer to GPU
0.00.052.995 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.000 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.053.001 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.993 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.994 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.994 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.995 I llama_new_context_with_model: n_batch       = 2048
0.00.053.995 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.995 I llama_new_context_with_model: flash_attn    = 0
0.00.053.995 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.996 I llama_new_context_with_model: freq_scale    = 1
0.00.053.996 I ggml_metal_init: allocating
0.00.054.002 I ggml_metal_init: found device: Apple M4
0.00.054.004 I ggml_metal_init: picking default device: Apple M4
0.00.054.584 I ggml_metal_init: using embedded metal library
0.00.056.943 I ggml_metal_init: GPU name:   Apple M4
0.00.056.945 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.945 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.946 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.946 I ggml_metal_init: simdgroup reduction   = true
0.00.056.946 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.946 I ggml_metal_init: has bfloat            = true
0.00.056.946 I ggml_metal_init: use bfloat            = true
0.00.056.947 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.947 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.448 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.086.509 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.518 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.546 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.583 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.584 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.585 I llama_new_context_with_model: graph nodes  = 967
0.00.087.585 I llama_new_context_with_model: graph splits = 2
0.00.087.601 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.742 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.743 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.750.711 I main: llama threadpool init, n_threads = 4
0.00.750.764 I 
0.00.750.807 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.750.807 I 
0.00.751.036 I sampler seed: 1234
0.00.751.040 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.751.051 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.751.052 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.751.052 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.484.809 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61312.61 tokens per second)
0.01.484.810 I llama_perf_context_print:        load time =     741.65 ms
0.01.484.811 I llama_perf_context_print: prompt eval time =      43.46 ms /     7 tokens (    6.21 ms per token,   161.08 tokens per second)
0.01.484.811 I llama_perf_context_print:        eval time =     687.34 ms /    63 runs   (   10.91 ms per token,    91.66 tokens per second)
0.01.484.812 I llama_perf_context_print:       total time =     734.10 ms /    70 tokens
0.01.485.024 I ggml_metal_free: deallocating

real	0m1.503s
user	0m0.110s
sys	0m0.147s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4405 (a45433ba) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.881 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.812 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.816 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.820 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.820 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.820 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.821 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.821 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.822 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.822 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.823 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.823 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.823 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.824 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.824 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.827 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.827 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.828 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.689 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.754 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.675 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.676 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.677 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.677 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.677 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.678 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.678 I llama_model_loader: - type  f32:  194 tensors
0.00.023.679 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.679 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.815 I llm_load_vocab: special tokens cache size = 25
0.00.050.545 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.548 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.548 I llm_load_print_meta: arch             = gptneox
0.00.050.548 I llm_load_print_meta: vocab type       = BPE
0.00.050.549 I llm_load_print_meta: n_vocab          = 50304
0.00.050.549 I llm_load_print_meta: n_merges         = 50009
0.00.050.549 I llm_load_print_meta: vocab_only       = 0
0.00.050.549 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.549 I llm_load_print_meta: n_embd           = 2048
0.00.050.550 I llm_load_print_meta: n_layer          = 24
0.00.050.552 I llm_load_print_meta: n_head           = 16
0.00.050.553 I llm_load_print_meta: n_head_kv        = 16
0.00.050.553 I llm_load_print_meta: n_rot            = 32
0.00.050.553 I llm_load_print_meta: n_swa            = 0
0.00.050.554 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.554 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.554 I llm_load_print_meta: n_gqa            = 1
0.00.050.556 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.556 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.557 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.557 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.557 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.557 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.558 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.560 I llm_load_print_meta: n_ff             = 8192
0.00.050.560 I llm_load_print_meta: n_expert         = 0
0.00.050.560 I llm_load_print_meta: n_expert_used    = 0
0.00.050.560 I llm_load_print_meta: causal attn      = 1
0.00.050.562 I llm_load_print_meta: pooling type     = 0
0.00.050.562 I llm_load_print_meta: rope type        = 2
0.00.050.562 I llm_load_print_meta: rope scaling     = linear
0.00.050.563 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.563 I llm_load_print_meta: freq_scale_train = 1
0.00.050.563 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.564 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.564 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.564 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.564 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.564 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.564 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.564 I llm_load_print_meta: model type       = 1.4B
0.00.050.565 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.565 I llm_load_print_meta: model params     = 1.41 B
0.00.050.566 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.566 I llm_load_print_meta: general.name     = 1.4B
0.00.050.566 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.566 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.567 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.567 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.567 I llm_load_print_meta: LF token         = 128 ''
0.00.050.567 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.568 I llm_load_print_meta: max token length = 1024
0.00.052.631 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.632 I llm_load_tensors: offloading output layer to GPU
0.00.052.632 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.643 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.644 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.591 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.592 I llama_new_context_with_model: n_ctx         = 128
0.00.053.592 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.593 I llama_new_context_with_model: n_batch       = 128
0.00.053.593 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.593 I llama_new_context_with_model: flash_attn    = 0
0.00.053.593 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.594 I llama_new_context_with_model: freq_scale    = 1
0.00.053.594 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.594 I ggml_metal_init: allocating
0.00.053.600 I ggml_metal_init: found device: Apple M4
0.00.053.602 I ggml_metal_init: picking default device: Apple M4
0.00.054.173 I ggml_metal_init: using embedded metal library
0.00.056.475 I ggml_metal_init: GPU name:   Apple M4
0.00.056.477 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.477 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.478 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.478 I ggml_metal_init: simdgroup reduction   = true
0.00.056.478 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.478 I ggml_metal_init: has bfloat            = true
0.00.056.478 I ggml_metal_init: use bfloat            = true
0.00.056.479 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.479 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.895 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.184 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.186 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.199 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.049 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.050 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.051 I llama_new_context_with_model: graph nodes  = 967
0.00.068.051 I llama_new_context_with_model: graph splits = 2
0.00.068.063 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.064 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.675.956 I 
0.00.676.009 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.676.033 I perplexity: tokenizing the input ..
0.00.683.581 I perplexity: tokenization took 7.544 ms
0.00.683.584 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.806.039 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.807.192 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.807.210 I llama_perf_context_print:        load time =     667.07 ms
0.00.807.211 I llama_perf_context_print: prompt eval time =     122.23 ms /   128 tokens (    0.95 ms per token,  1047.23 tokens per second)
0.00.807.212 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.807.213 I llama_perf_context_print:       total time =     131.26 ms /   129 tokens
0.00.807.633 I ggml_metal_free: deallocating

real	0m0.822s
user	0m0.078s
sys	0m0.102s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4405 (a45433ba) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.010.473 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.643 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.647 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.649 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.650 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.650 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.652 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.652 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.653 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.654 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.654 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.655 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.655 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.656 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.656 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.661 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.661 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.661 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.550 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.549 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.386 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.388 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.388 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.388 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.389 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.389 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.389 I llama_model_loader: - type  f32:  194 tensors
0.00.025.390 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.390 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.724 I llm_load_vocab: special tokens cache size = 25
0.00.051.742 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.745 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.746 I llm_load_print_meta: arch             = gptneox
0.00.051.746 I llm_load_print_meta: vocab type       = BPE
0.00.051.746 I llm_load_print_meta: n_vocab          = 50304
0.00.051.746 I llm_load_print_meta: n_merges         = 50009
0.00.051.747 I llm_load_print_meta: vocab_only       = 0
0.00.051.747 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.747 I llm_load_print_meta: n_embd           = 2048
0.00.051.747 I llm_load_print_meta: n_layer          = 24
0.00.051.750 I llm_load_print_meta: n_head           = 16
0.00.051.750 I llm_load_print_meta: n_head_kv        = 16
0.00.051.753 I llm_load_print_meta: n_rot            = 32
0.00.051.753 I llm_load_print_meta: n_swa            = 0
0.00.051.753 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.753 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.754 I llm_load_print_meta: n_gqa            = 1
0.00.051.755 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.756 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.756 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.756 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.757 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.757 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.757 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.758 I llm_load_print_meta: n_ff             = 8192
0.00.051.758 I llm_load_print_meta: n_expert         = 0
0.00.051.758 I llm_load_print_meta: n_expert_used    = 0
0.00.051.759 I llm_load_print_meta: causal attn      = 1
0.00.051.760 I llm_load_print_meta: pooling type     = 0
0.00.051.760 I llm_load_print_meta: rope type        = 2
0.00.051.760 I llm_load_print_meta: rope scaling     = linear
0.00.051.761 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.761 I llm_load_print_meta: freq_scale_train = 1
0.00.051.761 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.761 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.761 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.762 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.762 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.762 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.762 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.762 I llm_load_print_meta: model type       = 1.4B
0.00.051.762 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.763 I llm_load_print_meta: model params     = 1.41 B
0.00.051.764 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.764 I llm_load_print_meta: general.name     = 1.4B
0.00.051.765 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.765 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.765 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.765 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.765 I llm_load_print_meta: LF token         = 128 ''
0.00.051.766 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.766 I llm_load_print_meta: max token length = 1024
0.00.053.775 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.775 I llm_load_tensors: offloading output layer to GPU
0.00.053.775 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.785 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.787 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.693 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.694 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.694 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.694 I llama_new_context_with_model: n_batch       = 2048
0.00.054.694 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.694 I llama_new_context_with_model: flash_attn    = 0
0.00.054.695 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.695 I llama_new_context_with_model: freq_scale    = 1
0.00.054.696 I ggml_metal_init: allocating
0.00.054.701 I ggml_metal_init: found device: Apple M4
0.00.054.706 I ggml_metal_init: picking default device: Apple M4
0.00.055.312 I ggml_metal_init: using embedded metal library
0.00.057.650 I ggml_metal_init: GPU name:   Apple M4
0.00.057.651 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.652 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.652 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.652 I ggml_metal_init: simdgroup reduction   = true
0.00.057.652 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.653 I ggml_metal_init: has bfloat            = true
0.00.057.653 I ggml_metal_init: use bfloat            = true
0.00.057.653 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.655 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.214 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.086.953 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.958 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.976 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.099 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.101 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.101 I llama_new_context_with_model: graph nodes  = 967
0.00.088.101 I llama_new_context_with_model: graph splits = 2
0.00.088.117 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.265 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.265 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.797.319 I main: llama threadpool init, n_threads = 4
0.00.797.356 I 
0.00.797.381 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.797.383 I 
0.00.797.624 I sampler seed: 1234
0.00.797.631 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.797.665 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.797.676 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.797.676 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.584.497 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56891.03 tokens per second)
0.01.584.497 I llama_perf_context_print:        load time =     786.84 ms
0.01.584.498 I llama_perf_context_print: prompt eval time =      50.03 ms /     7 tokens (    7.15 ms per token,   139.91 tokens per second)
0.01.584.499 I llama_perf_context_print:        eval time =     733.71 ms /    63 runs   (   11.65 ms per token,    85.86 tokens per second)
0.01.584.499 I llama_perf_context_print:       total time =     787.18 ms /    70 tokens
0.01.584.711 I ggml_metal_free: deallocating

real	0m1.604s
user	0m0.109s
sys	0m0.156s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4405 (a45433ba) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.786 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.299 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.303 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.305 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.306 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.306 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.306 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.307 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.308 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.308 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.308 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.309 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.309 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.309 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.310 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.311 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.311 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.312 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.090 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.122 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.847 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.849 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.849 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.849 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.849 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.850 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.023.850 I llama_model_loader: - type  f32:  194 tensors
0.00.023.851 I llama_model_loader: - type q5_0:   97 tensors
0.00.023.851 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.201 I llm_load_vocab: special tokens cache size = 25
0.00.050.077 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.080 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.080 I llm_load_print_meta: arch             = gptneox
0.00.050.080 I llm_load_print_meta: vocab type       = BPE
0.00.050.081 I llm_load_print_meta: n_vocab          = 50304
0.00.050.081 I llm_load_print_meta: n_merges         = 50009
0.00.050.081 I llm_load_print_meta: vocab_only       = 0
0.00.050.081 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.081 I llm_load_print_meta: n_embd           = 2048
0.00.050.081 I llm_load_print_meta: n_layer          = 24
0.00.050.084 I llm_load_print_meta: n_head           = 16
0.00.050.085 I llm_load_print_meta: n_head_kv        = 16
0.00.050.085 I llm_load_print_meta: n_rot            = 32
0.00.050.085 I llm_load_print_meta: n_swa            = 0
0.00.050.085 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.085 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.086 I llm_load_print_meta: n_gqa            = 1
0.00.050.087 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.088 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.093 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.093 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.094 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.094 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.094 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.095 I llm_load_print_meta: n_ff             = 8192
0.00.050.095 I llm_load_print_meta: n_expert         = 0
0.00.050.095 I llm_load_print_meta: n_expert_used    = 0
0.00.050.095 I llm_load_print_meta: causal attn      = 1
0.00.050.095 I llm_load_print_meta: pooling type     = 0
0.00.050.099 I llm_load_print_meta: rope type        = 2
0.00.050.099 I llm_load_print_meta: rope scaling     = linear
0.00.050.100 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.100 I llm_load_print_meta: freq_scale_train = 1
0.00.050.100 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.101 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.101 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.101 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.101 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.101 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.101 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.101 I llm_load_print_meta: model type       = 1.4B
0.00.050.102 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.102 I llm_load_print_meta: model params     = 1.41 B
0.00.050.103 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.103 I llm_load_print_meta: general.name     = 1.4B
0.00.050.104 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.104 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.104 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.105 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.105 I llm_load_print_meta: LF token         = 128 ''
0.00.050.105 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.105 I llm_load_print_meta: max token length = 1024
0.00.052.032 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.033 I llm_load_tensors: offloading output layer to GPU
0.00.052.033 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.043 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.045 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.052.936 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.937 I llama_new_context_with_model: n_ctx         = 128
0.00.052.937 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.938 I llama_new_context_with_model: n_batch       = 128
0.00.052.938 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.938 I llama_new_context_with_model: flash_attn    = 0
0.00.052.938 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.939 I llama_new_context_with_model: freq_scale    = 1
0.00.052.939 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.940 I ggml_metal_init: allocating
0.00.052.946 I ggml_metal_init: found device: Apple M4
0.00.052.949 I ggml_metal_init: picking default device: Apple M4
0.00.053.513 I ggml_metal_init: using embedded metal library
0.00.055.855 I ggml_metal_init: GPU name:   Apple M4
0.00.055.857 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.857 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.858 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.858 I ggml_metal_init: simdgroup reduction   = true
0.00.055.858 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.858 I ggml_metal_init: has bfloat            = true
0.00.055.858 I ggml_metal_init: use bfloat            = true
0.00.055.859 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.859 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.434 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.855 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.858 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.874 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.734 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.735 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.735 I llama_new_context_with_model: graph nodes  = 967
0.00.067.735 I llama_new_context_with_model: graph splits = 2
0.00.067.748 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.748 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.769.423 I 
0.00.769.457 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.769.471 I perplexity: tokenizing the input ..
0.00.777.521 I perplexity: tokenization took 8.048 ms
0.00.777.525 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.912.119 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.913.274 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.913.291 I llama_perf_context_print:        load time =     759.63 ms
0.00.913.293 I llama_perf_context_print: prompt eval time =     134.37 ms /   128 tokens (    1.05 ms per token,   952.61 tokens per second)
0.00.913.299 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.913.301 I llama_perf_context_print:       total time =     143.87 ms /   129 tokens
0.00.913.766 I ggml_metal_free: deallocating

real	0m0.929s
user	0m0.078s
sys	0m0.130s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4405 (a45433ba) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.008.638 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.689 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.693 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.699 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.699 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.700 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.700 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.700 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.701 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.701 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.702 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.702 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.703 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.703 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.703 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.705 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.705 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.705 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.646 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.695 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.498 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.500 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.500 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.500 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.500 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.501 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.501 I llama_model_loader: - type  f32:  194 tensors
0.00.023.502 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.502 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.535 I llm_load_vocab: special tokens cache size = 25
0.00.050.510 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.513 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.513 I llm_load_print_meta: arch             = gptneox
0.00.050.514 I llm_load_print_meta: vocab type       = BPE
0.00.050.514 I llm_load_print_meta: n_vocab          = 50304
0.00.050.514 I llm_load_print_meta: n_merges         = 50009
0.00.050.514 I llm_load_print_meta: vocab_only       = 0
0.00.050.514 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.515 I llm_load_print_meta: n_embd           = 2048
0.00.050.515 I llm_load_print_meta: n_layer          = 24
0.00.050.517 I llm_load_print_meta: n_head           = 16
0.00.050.518 I llm_load_print_meta: n_head_kv        = 16
0.00.050.518 I llm_load_print_meta: n_rot            = 32
0.00.050.518 I llm_load_print_meta: n_swa            = 0
0.00.050.518 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.518 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.519 I llm_load_print_meta: n_gqa            = 1
0.00.050.520 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.521 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.521 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.522 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.522 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.522 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.522 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.523 I llm_load_print_meta: n_ff             = 8192
0.00.050.523 I llm_load_print_meta: n_expert         = 0
0.00.050.523 I llm_load_print_meta: n_expert_used    = 0
0.00.050.523 I llm_load_print_meta: causal attn      = 1
0.00.050.524 I llm_load_print_meta: pooling type     = 0
0.00.050.524 I llm_load_print_meta: rope type        = 2
0.00.050.524 I llm_load_print_meta: rope scaling     = linear
0.00.050.525 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.526 I llm_load_print_meta: freq_scale_train = 1
0.00.050.526 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.526 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.526 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.526 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.526 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.528 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.529 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.529 I llm_load_print_meta: model type       = 1.4B
0.00.050.529 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.529 I llm_load_print_meta: model params     = 1.41 B
0.00.050.530 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.530 I llm_load_print_meta: general.name     = 1.4B
0.00.050.530 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.531 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.531 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.531 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.531 I llm_load_print_meta: LF token         = 128 ''
0.00.050.532 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.532 I llm_load_print_meta: max token length = 1024
0.00.052.589 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.589 I llm_load_tensors: offloading output layer to GPU
0.00.052.589 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.600 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.601 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.526 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.526 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.527 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.527 I llama_new_context_with_model: n_batch       = 2048
0.00.053.527 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.527 I llama_new_context_with_model: flash_attn    = 0
0.00.053.528 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.528 I llama_new_context_with_model: freq_scale    = 1
0.00.053.528 I ggml_metal_init: allocating
0.00.053.535 I ggml_metal_init: found device: Apple M4
0.00.053.537 I ggml_metal_init: picking default device: Apple M4
0.00.054.143 I ggml_metal_init: using embedded metal library
0.00.056.443 I ggml_metal_init: GPU name:   Apple M4
0.00.056.445 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.445 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.445 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.446 I ggml_metal_init: simdgroup reduction   = true
0.00.056.446 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.446 I ggml_metal_init: has bfloat            = true
0.00.056.446 I ggml_metal_init: use bfloat            = true
0.00.056.447 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.447 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.177 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.085.583 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.588 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.607 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.455 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.457 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.457 I llama_new_context_with_model: graph nodes  = 967
0.00.086.457 I llama_new_context_with_model: graph splits = 2
0.00.086.468 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.612 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.612 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.731.379 I main: llama threadpool init, n_threads = 4
0.00.731.415 I 
0.00.731.445 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.731.445 I 
0.00.731.669 I sampler seed: 1234
0.00.731.677 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.731.709 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.731.710 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.731.710 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.578.799 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58006.54 tokens per second)
0.01.578.800 I llama_perf_context_print:        load time =     722.74 ms
0.01.578.801 I llama_perf_context_print: prompt eval time =      46.16 ms /     7 tokens (    6.59 ms per token,   151.64 tokens per second)
0.01.578.801 I llama_perf_context_print:        eval time =     797.96 ms /    63 runs   (   12.67 ms per token,    78.95 tokens per second)
0.01.578.802 I llama_perf_context_print:       total time =     847.42 ms /    70 tokens
0.01.579.022 I ggml_metal_free: deallocating

real	0m1.596s
user	0m0.111s
sys	0m0.162s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4405 (a45433ba) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.778 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.315 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.319 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.321 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.322 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.322 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.322 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.323 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.324 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.324 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.325 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.325 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.325 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.326 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.326 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.329 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.329 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.329 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.091 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.165 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.993 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.994 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.995 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.995 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.995 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.996 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.022.996 I llama_model_loader: - type  f32:  194 tensors
0.00.022.996 I llama_model_loader: - type q5_1:   97 tensors
0.00.022.997 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.079 I llm_load_vocab: special tokens cache size = 25
0.00.048.968 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.971 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.971 I llm_load_print_meta: arch             = gptneox
0.00.048.971 I llm_load_print_meta: vocab type       = BPE
0.00.048.972 I llm_load_print_meta: n_vocab          = 50304
0.00.048.972 I llm_load_print_meta: n_merges         = 50009
0.00.048.972 I llm_load_print_meta: vocab_only       = 0
0.00.048.972 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.972 I llm_load_print_meta: n_embd           = 2048
0.00.048.973 I llm_load_print_meta: n_layer          = 24
0.00.048.976 I llm_load_print_meta: n_head           = 16
0.00.048.977 I llm_load_print_meta: n_head_kv        = 16
0.00.048.977 I llm_load_print_meta: n_rot            = 32
0.00.048.977 I llm_load_print_meta: n_swa            = 0
0.00.048.977 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.977 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.978 I llm_load_print_meta: n_gqa            = 1
0.00.048.979 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.980 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.980 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.981 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.981 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.981 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.981 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.982 I llm_load_print_meta: n_ff             = 8192
0.00.048.982 I llm_load_print_meta: n_expert         = 0
0.00.048.982 I llm_load_print_meta: n_expert_used    = 0
0.00.048.985 I llm_load_print_meta: causal attn      = 1
0.00.048.985 I llm_load_print_meta: pooling type     = 0
0.00.048.986 I llm_load_print_meta: rope type        = 2
0.00.048.986 I llm_load_print_meta: rope scaling     = linear
0.00.048.986 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.986 I llm_load_print_meta: freq_scale_train = 1
0.00.048.987 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.987 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.987 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.987 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.987 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.987 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.987 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.988 I llm_load_print_meta: model type       = 1.4B
0.00.048.988 I llm_load_print_meta: model ftype      = Q5_1
0.00.048.988 I llm_load_print_meta: model params     = 1.41 B
0.00.048.989 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.048.989 I llm_load_print_meta: general.name     = 1.4B
0.00.048.991 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.992 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.992 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.992 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.992 I llm_load_print_meta: LF token         = 128 ''
0.00.048.993 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.993 I llm_load_print_meta: max token length = 1024
0.00.050.950 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.950 I llm_load_tensors: offloading output layer to GPU
0.00.050.950 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.960 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.050.961 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.051.894 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.895 I llama_new_context_with_model: n_ctx         = 128
0.00.051.895 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.895 I llama_new_context_with_model: n_batch       = 128
0.00.051.895 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.895 I llama_new_context_with_model: flash_attn    = 0
0.00.051.896 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.896 I llama_new_context_with_model: freq_scale    = 1
0.00.051.896 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.897 I ggml_metal_init: allocating
0.00.051.900 I ggml_metal_init: found device: Apple M4
0.00.051.902 I ggml_metal_init: picking default device: Apple M4
0.00.052.474 I ggml_metal_init: using embedded metal library
0.00.054.822 I ggml_metal_init: GPU name:   Apple M4
0.00.054.823 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.823 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.824 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.824 I ggml_metal_init: simdgroup reduction   = true
0.00.054.824 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.824 I ggml_metal_init: has bfloat            = true
0.00.054.824 I ggml_metal_init: use bfloat            = true
0.00.054.825 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.825 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.367 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.065.642 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.644 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.658 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.530 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.531 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.531 I llama_new_context_with_model: graph nodes  = 967
0.00.066.531 I llama_new_context_with_model: graph splits = 2
0.00.066.544 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.545 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.647.121 I 
0.00.647.165 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.647.180 I perplexity: tokenizing the input ..
0.00.655.256 I perplexity: tokenization took 8.074 ms
0.00.655.259 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.790.413 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.791.671 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.791.690 I llama_perf_context_print:        load time =     638.34 ms
0.00.791.691 I llama_perf_context_print: prompt eval time =     134.93 ms /   128 tokens (    1.05 ms per token,   948.66 tokens per second)
0.00.791.694 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.791.694 I llama_perf_context_print:       total time =     144.57 ms /   129 tokens
0.00.792.135 I ggml_metal_free: deallocating

real	0m0.806s
user	0m0.077s
sys	0m0.116s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4405 (a45433ba) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.010.353 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.799 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.804 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.806 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.806 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.806 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.807 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.807 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.808 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.808 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.808 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.809 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.809 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.810 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.810 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.811 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.812 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.812 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.559 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.598 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.365 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.367 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.367 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.367 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.368 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.368 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.368 I llama_model_loader: - type  f32:  194 tensors
0.00.024.369 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.369 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.369 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.743 I llm_load_vocab: special tokens cache size = 25
0.00.050.782 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.784 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.784 I llm_load_print_meta: arch             = gptneox
0.00.050.785 I llm_load_print_meta: vocab type       = BPE
0.00.050.785 I llm_load_print_meta: n_vocab          = 50304
0.00.050.785 I llm_load_print_meta: n_merges         = 50009
0.00.050.785 I llm_load_print_meta: vocab_only       = 0
0.00.050.786 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.786 I llm_load_print_meta: n_embd           = 2048
0.00.050.786 I llm_load_print_meta: n_layer          = 24
0.00.050.789 I llm_load_print_meta: n_head           = 16
0.00.050.790 I llm_load_print_meta: n_head_kv        = 16
0.00.050.790 I llm_load_print_meta: n_rot            = 32
0.00.050.790 I llm_load_print_meta: n_swa            = 0
0.00.050.793 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.793 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.794 I llm_load_print_meta: n_gqa            = 1
0.00.050.795 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.795 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.796 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.796 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.796 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.797 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.797 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.797 I llm_load_print_meta: n_ff             = 8192
0.00.050.798 I llm_load_print_meta: n_expert         = 0
0.00.050.798 I llm_load_print_meta: n_expert_used    = 0
0.00.050.798 I llm_load_print_meta: causal attn      = 1
0.00.050.798 I llm_load_print_meta: pooling type     = 0
0.00.050.798 I llm_load_print_meta: rope type        = 2
0.00.050.798 I llm_load_print_meta: rope scaling     = linear
0.00.050.800 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.800 I llm_load_print_meta: freq_scale_train = 1
0.00.050.801 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.801 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.801 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.801 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.801 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.801 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.802 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.802 I llm_load_print_meta: model type       = 1.4B
0.00.050.802 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.803 I llm_load_print_meta: model params     = 1.41 B
0.00.050.803 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.803 I llm_load_print_meta: general.name     = 1.4B
0.00.050.804 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.804 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.804 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.804 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.804 I llm_load_print_meta: LF token         = 128 ''
0.00.050.805 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.805 I llm_load_print_meta: max token length = 1024
0.00.052.678 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.678 I llm_load_tensors: offloading output layer to GPU
0.00.052.678 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.689 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.690 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.587 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.587 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.588 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.588 I llama_new_context_with_model: n_batch       = 2048
0.00.053.588 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.588 I llama_new_context_with_model: flash_attn    = 0
0.00.053.589 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.589 I llama_new_context_with_model: freq_scale    = 1
0.00.053.589 I ggml_metal_init: allocating
0.00.053.593 I ggml_metal_init: found device: Apple M4
0.00.053.595 I ggml_metal_init: picking default device: Apple M4
0.00.054.176 I ggml_metal_init: using embedded metal library
0.00.056.508 I ggml_metal_init: GPU name:   Apple M4
0.00.056.509 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.510 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.510 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.510 I ggml_metal_init: simdgroup reduction   = true
0.00.056.511 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.511 I ggml_metal_init: has bfloat            = true
0.00.056.511 I ggml_metal_init: use bfloat            = true
0.00.056.511 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.512 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.193 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.085.641 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.648 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.670 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.752 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.753 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.753 I llama_new_context_with_model: graph nodes  = 967
0.00.086.753 I llama_new_context_with_model: graph splits = 2
0.00.086.768 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.926 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.927 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.477.669 I main: llama threadpool init, n_threads = 4
0.00.477.717 I 
0.00.477.741 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.477.741 I 
0.00.477.979 I sampler seed: 1234
0.00.477.985 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.478.020 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.478.024 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.478.025 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.159.280 I llama_perf_sampler_print:    sampling time =       1.11 ms /    71 runs   (    0.02 ms per token, 64079.42 tokens per second)
0.01.159.281 I llama_perf_context_print:        load time =     467.31 ms
0.01.159.281 I llama_perf_context_print: prompt eval time =      35.74 ms /     7 tokens (    5.11 ms per token,   195.84 tokens per second)
0.01.159.282 I llama_perf_context_print:        eval time =     642.66 ms /    63 runs   (   10.20 ms per token,    98.03 tokens per second)
0.01.159.282 I llama_perf_context_print:       total time =     681.62 ms /    70 tokens
0.01.159.461 I ggml_metal_free: deallocating

real	0m1.178s
user	0m0.111s
sys	0m0.110s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4405 (a45433ba) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.123 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.757 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.761 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.762 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.763 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.763 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.768 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.768 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.769 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.769 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.769 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.770 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.770 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.770 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.771 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.775 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.775 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.775 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.541 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.575 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.434 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.435 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.435 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.436 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.436 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.436 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.437 I llama_model_loader: - type  f32:  194 tensors
0.00.024.437 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.437 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.438 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.564 I llm_load_vocab: special tokens cache size = 25
0.00.050.414 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.416 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.417 I llm_load_print_meta: arch             = gptneox
0.00.050.417 I llm_load_print_meta: vocab type       = BPE
0.00.050.417 I llm_load_print_meta: n_vocab          = 50304
0.00.050.417 I llm_load_print_meta: n_merges         = 50009
0.00.050.418 I llm_load_print_meta: vocab_only       = 0
0.00.050.418 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.418 I llm_load_print_meta: n_embd           = 2048
0.00.050.418 I llm_load_print_meta: n_layer          = 24
0.00.050.421 I llm_load_print_meta: n_head           = 16
0.00.050.422 I llm_load_print_meta: n_head_kv        = 16
0.00.050.422 I llm_load_print_meta: n_rot            = 32
0.00.050.422 I llm_load_print_meta: n_swa            = 0
0.00.050.423 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.423 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.424 I llm_load_print_meta: n_gqa            = 1
0.00.050.425 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.426 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.426 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.427 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.427 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.427 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.428 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.428 I llm_load_print_meta: n_ff             = 8192
0.00.050.429 I llm_load_print_meta: n_expert         = 0
0.00.050.429 I llm_load_print_meta: n_expert_used    = 0
0.00.050.429 I llm_load_print_meta: causal attn      = 1
0.00.050.429 I llm_load_print_meta: pooling type     = 0
0.00.050.429 I llm_load_print_meta: rope type        = 2
0.00.050.431 I llm_load_print_meta: rope scaling     = linear
0.00.050.433 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.433 I llm_load_print_meta: freq_scale_train = 1
0.00.050.434 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.434 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.434 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.438 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.438 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.438 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.439 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.439 I llm_load_print_meta: model type       = 1.4B
0.00.050.439 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.440 I llm_load_print_meta: model params     = 1.41 B
0.00.050.440 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.440 I llm_load_print_meta: general.name     = 1.4B
0.00.050.441 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.441 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.441 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.441 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.441 I llm_load_print_meta: LF token         = 128 ''
0.00.050.441 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.442 I llm_load_print_meta: max token length = 1024
0.00.052.216 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.216 I llm_load_tensors: offloading output layer to GPU
0.00.052.216 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.222 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.222 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.274 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.275 I llama_new_context_with_model: n_ctx         = 128
0.00.053.275 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.276 I llama_new_context_with_model: n_batch       = 128
0.00.053.276 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.276 I llama_new_context_with_model: flash_attn    = 0
0.00.053.276 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.277 I llama_new_context_with_model: freq_scale    = 1
0.00.053.277 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.277 I ggml_metal_init: allocating
0.00.053.283 I ggml_metal_init: found device: Apple M4
0.00.053.285 I ggml_metal_init: picking default device: Apple M4
0.00.053.860 I ggml_metal_init: using embedded metal library
0.00.056.215 I ggml_metal_init: GPU name:   Apple M4
0.00.056.217 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.217 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.217 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.218 I ggml_metal_init: simdgroup reduction   = true
0.00.056.218 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.218 I ggml_metal_init: has bfloat            = true
0.00.056.218 I ggml_metal_init: use bfloat            = true
0.00.056.218 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.220 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.659 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.036 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.039 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.057 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.960 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.961 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.961 I llama_new_context_with_model: graph nodes  = 967
0.00.067.961 I llama_new_context_with_model: graph splits = 2
0.00.067.973 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.974 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.470.276 I 
0.00.470.310 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.470.324 I perplexity: tokenizing the input ..
0.00.478.067 I perplexity: tokenization took 7.741 ms
0.00.478.070 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.610.635 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.611.853 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.611.874 I llama_perf_context_print:        load time =     460.15 ms
0.00.611.875 I llama_perf_context_print: prompt eval time =     132.34 ms /   128 tokens (    1.03 ms per token,   967.21 tokens per second)
0.00.611.876 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.611.877 I llama_perf_context_print:       total time =     141.60 ms /   129 tokens
0.00.612.353 I ggml_metal_free: deallocating

real	0m0.627s
user	0m0.078s
sys	0m0.082s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4405 (a45433ba) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.008.769 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.171 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.176 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.178 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.179 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.179 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.179 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.180 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.181 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.181 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.181 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.182 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.184 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.185 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.185 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.188 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.188 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.188 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.128 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.244 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.110 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.112 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.112 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.113 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.113 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.113 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.114 I llama_model_loader: - type  f32:  194 tensors
0.00.024.114 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.114 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.115 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.115 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.274 I llm_load_vocab: special tokens cache size = 25
0.00.051.154 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.157 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.157 I llm_load_print_meta: arch             = gptneox
0.00.051.158 I llm_load_print_meta: vocab type       = BPE
0.00.051.158 I llm_load_print_meta: n_vocab          = 50304
0.00.051.158 I llm_load_print_meta: n_merges         = 50009
0.00.051.158 I llm_load_print_meta: vocab_only       = 0
0.00.051.158 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.159 I llm_load_print_meta: n_embd           = 2048
0.00.051.159 I llm_load_print_meta: n_layer          = 24
0.00.051.162 I llm_load_print_meta: n_head           = 16
0.00.051.163 I llm_load_print_meta: n_head_kv        = 16
0.00.051.163 I llm_load_print_meta: n_rot            = 32
0.00.051.163 I llm_load_print_meta: n_swa            = 0
0.00.051.166 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.166 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.166 I llm_load_print_meta: n_gqa            = 1
0.00.051.167 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.168 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.169 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.169 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.169 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.169 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.170 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.170 I llm_load_print_meta: n_ff             = 8192
0.00.051.172 I llm_load_print_meta: n_expert         = 0
0.00.051.173 I llm_load_print_meta: n_expert_used    = 0
0.00.051.173 I llm_load_print_meta: causal attn      = 1
0.00.051.173 I llm_load_print_meta: pooling type     = 0
0.00.051.173 I llm_load_print_meta: rope type        = 2
0.00.051.173 I llm_load_print_meta: rope scaling     = linear
0.00.051.174 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.174 I llm_load_print_meta: freq_scale_train = 1
0.00.051.174 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.179 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.179 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.179 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.179 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.179 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.180 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.180 I llm_load_print_meta: model type       = 1.4B
0.00.051.180 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.180 I llm_load_print_meta: model params     = 1.41 B
0.00.051.181 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.181 I llm_load_print_meta: general.name     = 1.4B
0.00.051.181 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.181 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.182 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.182 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.182 I llm_load_print_meta: LF token         = 128 ''
0.00.051.182 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.182 I llm_load_print_meta: max token length = 1024
0.00.053.166 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.166 I llm_load_tensors: offloading output layer to GPU
0.00.053.166 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.177 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.178 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.077 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.078 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.078 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.078 I llama_new_context_with_model: n_batch       = 2048
0.00.054.078 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.078 I llama_new_context_with_model: flash_attn    = 0
0.00.054.079 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.079 I llama_new_context_with_model: freq_scale    = 1
0.00.054.080 I ggml_metal_init: allocating
0.00.054.083 I ggml_metal_init: found device: Apple M4
0.00.054.085 I ggml_metal_init: picking default device: Apple M4
0.00.054.690 I ggml_metal_init: using embedded metal library
0.00.057.041 I ggml_metal_init: GPU name:   Apple M4
0.00.057.044 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.044 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.045 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.045 I ggml_metal_init: simdgroup reduction   = true
0.00.057.045 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.045 I ggml_metal_init: has bfloat            = true
0.00.057.045 I ggml_metal_init: use bfloat            = true
0.00.057.046 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.046 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.946 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.086.857 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.867 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.897 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.994 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.996 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.996 I llama_new_context_with_model: graph nodes  = 967
0.00.087.996 I llama_new_context_with_model: graph splits = 2
0.00.088.011 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.151 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.152 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.537.649 I main: llama threadpool init, n_threads = 4
0.00.537.688 I 
0.00.537.709 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.537.709 I 
0.00.537.946 I sampler seed: 1234
0.00.537.950 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.538.001 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.538.003 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.538.003 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.286.483 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52321.30 tokens per second)
0.01.286.483 I llama_perf_context_print:        load time =     528.88 ms
0.01.286.485 I llama_perf_context_print: prompt eval time =      44.55 ms /     7 tokens (    6.36 ms per token,   157.14 tokens per second)
0.01.286.486 I llama_perf_context_print:        eval time =     701.37 ms /    63 runs   (   11.13 ms per token,    89.82 tokens per second)
0.01.286.486 I llama_perf_context_print:       total time =     748.84 ms /    70 tokens
0.01.286.666 I ggml_metal_free: deallocating

real	0m1.303s
user	0m0.110s
sys	0m0.124s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4405 (a45433ba) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.722 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.436 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.441 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.443 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.443 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.444 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.444 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.444 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.445 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.445 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.448 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.448 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.448 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.449 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.449 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.452 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.453 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.453 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.241 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.252 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.936 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.937 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.937 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.937 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.938 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.938 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.022.938 I llama_model_loader: - type  f32:  194 tensors
0.00.022.939 I llama_model_loader: - type q3_K:   25 tensors
0.00.022.939 I llama_model_loader: - type q4_K:   71 tensors
0.00.022.939 I llama_model_loader: - type q5_K:    1 tensors
0.00.022.940 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.078 I llm_load_vocab: special tokens cache size = 25
0.00.048.983 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.985 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.986 I llm_load_print_meta: arch             = gptneox
0.00.048.986 I llm_load_print_meta: vocab type       = BPE
0.00.048.986 I llm_load_print_meta: n_vocab          = 50304
0.00.048.987 I llm_load_print_meta: n_merges         = 50009
0.00.048.987 I llm_load_print_meta: vocab_only       = 0
0.00.048.987 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.987 I llm_load_print_meta: n_embd           = 2048
0.00.048.987 I llm_load_print_meta: n_layer          = 24
0.00.048.990 I llm_load_print_meta: n_head           = 16
0.00.048.991 I llm_load_print_meta: n_head_kv        = 16
0.00.048.991 I llm_load_print_meta: n_rot            = 32
0.00.048.991 I llm_load_print_meta: n_swa            = 0
0.00.048.991 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.991 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.994 I llm_load_print_meta: n_gqa            = 1
0.00.048.995 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.996 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.997 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.998 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.998 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.998 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.999 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.999 I llm_load_print_meta: n_ff             = 8192
0.00.049.001 I llm_load_print_meta: n_expert         = 0
0.00.049.001 I llm_load_print_meta: n_expert_used    = 0
0.00.049.001 I llm_load_print_meta: causal attn      = 1
0.00.049.001 I llm_load_print_meta: pooling type     = 0
0.00.049.001 I llm_load_print_meta: rope type        = 2
0.00.049.001 I llm_load_print_meta: rope scaling     = linear
0.00.049.002 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.002 I llm_load_print_meta: freq_scale_train = 1
0.00.049.002 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.002 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.002 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.003 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.003 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.003 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.003 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.007 I llm_load_print_meta: model type       = 1.4B
0.00.049.007 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.008 I llm_load_print_meta: model params     = 1.41 B
0.00.049.009 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.009 I llm_load_print_meta: general.name     = 1.4B
0.00.049.009 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.009 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.009 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.010 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.010 I llm_load_print_meta: LF token         = 128 ''
0.00.049.010 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.010 I llm_load_print_meta: max token length = 1024
0.00.050.887 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.887 I llm_load_tensors: offloading output layer to GPU
0.00.050.887 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.898 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.050.899 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.051.819 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.819 I llama_new_context_with_model: n_ctx         = 128
0.00.051.820 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.820 I llama_new_context_with_model: n_batch       = 128
0.00.051.820 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.820 I llama_new_context_with_model: flash_attn    = 0
0.00.051.821 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.821 I llama_new_context_with_model: freq_scale    = 1
0.00.051.821 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.822 I ggml_metal_init: allocating
0.00.051.825 I ggml_metal_init: found device: Apple M4
0.00.051.827 I ggml_metal_init: picking default device: Apple M4
0.00.052.390 I ggml_metal_init: using embedded metal library
0.00.054.694 I ggml_metal_init: GPU name:   Apple M4
0.00.054.695 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.696 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.696 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.696 I ggml_metal_init: simdgroup reduction   = true
0.00.054.696 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.697 I ggml_metal_init: has bfloat            = true
0.00.054.697 I ggml_metal_init: use bfloat            = true
0.00.054.697 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.698 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.231 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.065.477 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.482 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.498 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.405 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.406 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.406 I llama_new_context_with_model: graph nodes  = 967
0.00.066.406 I llama_new_context_with_model: graph splits = 2
0.00.066.418 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.419 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.477.260 I 
0.00.477.292 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.477.305 I perplexity: tokenizing the input ..
0.00.485.570 I perplexity: tokenization took 8.263 ms
0.00.485.573 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.617.986 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.619.161 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.619.182 I llama_perf_context_print:        load time =     468.53 ms
0.00.619.185 I llama_perf_context_print: prompt eval time =     132.18 ms /   128 tokens (    1.03 ms per token,   968.37 tokens per second)
0.00.619.186 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.619.186 I llama_perf_context_print:       total time =     141.92 ms /   129 tokens
0.00.619.677 I ggml_metal_free: deallocating

real	0m0.633s
user	0m0.078s
sys	0m0.086s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4405 (a45433ba) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.008.945 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.587 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.592 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.594 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.595 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.595 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.595 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.596 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.597 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.597 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.598 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.598 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.598 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.599 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.599 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.601 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.601 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.601 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.300 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.398 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.202 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.204 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.204 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.205 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.205 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.205 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.206 I llama_model_loader: - type  f32:  194 tensors
0.00.023.206 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.207 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.207 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.198 I llm_load_vocab: special tokens cache size = 25
0.00.050.188 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.192 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.192 I llm_load_print_meta: arch             = gptneox
0.00.050.193 I llm_load_print_meta: vocab type       = BPE
0.00.050.193 I llm_load_print_meta: n_vocab          = 50304
0.00.050.193 I llm_load_print_meta: n_merges         = 50009
0.00.050.193 I llm_load_print_meta: vocab_only       = 0
0.00.050.193 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.194 I llm_load_print_meta: n_embd           = 2048
0.00.050.194 I llm_load_print_meta: n_layer          = 24
0.00.050.198 I llm_load_print_meta: n_head           = 16
0.00.050.199 I llm_load_print_meta: n_head_kv        = 16
0.00.050.199 I llm_load_print_meta: n_rot            = 32
0.00.050.199 I llm_load_print_meta: n_swa            = 0
0.00.050.199 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.199 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.200 I llm_load_print_meta: n_gqa            = 1
0.00.050.201 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.202 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.202 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.203 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.203 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.203 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.203 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.204 I llm_load_print_meta: n_ff             = 8192
0.00.050.204 I llm_load_print_meta: n_expert         = 0
0.00.050.204 I llm_load_print_meta: n_expert_used    = 0
0.00.050.204 I llm_load_print_meta: causal attn      = 1
0.00.050.206 I llm_load_print_meta: pooling type     = 0
0.00.050.206 I llm_load_print_meta: rope type        = 2
0.00.050.206 I llm_load_print_meta: rope scaling     = linear
0.00.050.208 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.209 I llm_load_print_meta: freq_scale_train = 1
0.00.050.209 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.209 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.210 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.211 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.211 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.211 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.211 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.212 I llm_load_print_meta: model type       = 1.4B
0.00.050.212 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.212 I llm_load_print_meta: model params     = 1.41 B
0.00.050.213 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.213 I llm_load_print_meta: general.name     = 1.4B
0.00.050.213 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.213 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.213 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.213 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.214 I llm_load_print_meta: LF token         = 128 ''
0.00.050.214 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.214 I llm_load_print_meta: max token length = 1024
0.00.052.175 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.175 I llm_load_tensors: offloading output layer to GPU
0.00.052.175 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.186 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.187 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.095 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.095 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.096 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.096 I llama_new_context_with_model: n_batch       = 2048
0.00.053.096 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.096 I llama_new_context_with_model: flash_attn    = 0
0.00.053.097 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.097 I llama_new_context_with_model: freq_scale    = 1
0.00.053.098 I ggml_metal_init: allocating
0.00.053.102 I ggml_metal_init: found device: Apple M4
0.00.053.104 I ggml_metal_init: picking default device: Apple M4
0.00.053.793 I ggml_metal_init: using embedded metal library
0.00.056.253 I ggml_metal_init: GPU name:   Apple M4
0.00.056.255 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.255 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.256 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.256 I ggml_metal_init: simdgroup reduction   = true
0.00.056.256 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.256 I ggml_metal_init: has bfloat            = true
0.00.056.256 I ggml_metal_init: use bfloat            = true
0.00.056.257 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.258 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.424 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.085.563 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.571 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.595 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.556 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.558 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.558 I llama_new_context_with_model: graph nodes  = 967
0.00.086.558 I llama_new_context_with_model: graph splits = 2
0.00.086.574 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.730 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.730 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.616.205 I main: llama threadpool init, n_threads = 4
0.00.616.252 I 
0.00.616.299 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.616.300 I 
0.00.616.525 I sampler seed: 1234
0.00.616.530 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.616.568 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.616.569 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.616.569 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.372.334 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58388.16 tokens per second)
0.01.372.334 I llama_perf_context_print:        load time =     607.25 ms
0.01.372.335 I llama_perf_context_print: prompt eval time =      51.03 ms /     7 tokens (    7.29 ms per token,   137.16 tokens per second)
0.01.372.336 I llama_perf_context_print:        eval time =     701.77 ms /    63 runs   (   11.14 ms per token,    89.77 tokens per second)
0.01.372.336 I llama_perf_context_print:       total time =     756.13 ms /    70 tokens
0.01.372.542 I ggml_metal_free: deallocating

real	0m1.391s
user	0m0.111s
sys	0m0.139s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4405 (a45433ba) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.667 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.262 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.267 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.269 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.269 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.270 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.270 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.270 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.271 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.271 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.272 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.274 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.275 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.275 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.275 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.277 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.277 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.277 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.138 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.230 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.073 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.074 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.074 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.075 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.075 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.075 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.076 I llama_model_loader: - type  f32:  194 tensors
0.00.024.076 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.076 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.076 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.165 I llm_load_vocab: special tokens cache size = 25
0.00.051.005 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.008 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.009 I llm_load_print_meta: arch             = gptneox
0.00.051.009 I llm_load_print_meta: vocab type       = BPE
0.00.051.009 I llm_load_print_meta: n_vocab          = 50304
0.00.051.009 I llm_load_print_meta: n_merges         = 50009
0.00.051.010 I llm_load_print_meta: vocab_only       = 0
0.00.051.010 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.010 I llm_load_print_meta: n_embd           = 2048
0.00.051.010 I llm_load_print_meta: n_layer          = 24
0.00.051.013 I llm_load_print_meta: n_head           = 16
0.00.051.014 I llm_load_print_meta: n_head_kv        = 16
0.00.051.016 I llm_load_print_meta: n_rot            = 32
0.00.051.016 I llm_load_print_meta: n_swa            = 0
0.00.051.016 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.016 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.017 I llm_load_print_meta: n_gqa            = 1
0.00.051.018 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.019 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.019 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.019 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.020 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.020 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.020 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.021 I llm_load_print_meta: n_ff             = 8192
0.00.051.021 I llm_load_print_meta: n_expert         = 0
0.00.051.021 I llm_load_print_meta: n_expert_used    = 0
0.00.051.021 I llm_load_print_meta: causal attn      = 1
0.00.051.026 I llm_load_print_meta: pooling type     = 0
0.00.051.028 I llm_load_print_meta: rope type        = 2
0.00.051.028 I llm_load_print_meta: rope scaling     = linear
0.00.051.029 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.029 I llm_load_print_meta: freq_scale_train = 1
0.00.051.029 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.030 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.030 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.030 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.030 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.030 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.031 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.031 I llm_load_print_meta: model type       = 1.4B
0.00.051.031 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.032 I llm_load_print_meta: model params     = 1.41 B
0.00.051.032 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.032 I llm_load_print_meta: general.name     = 1.4B
0.00.051.033 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.033 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.033 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.033 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.033 I llm_load_print_meta: LF token         = 128 ''
0.00.051.034 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.034 I llm_load_print_meta: max token length = 1024
0.00.053.025 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.025 I llm_load_tensors: offloading output layer to GPU
0.00.053.025 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.036 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.037 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.014 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.015 I llama_new_context_with_model: n_ctx         = 128
0.00.054.015 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.016 I llama_new_context_with_model: n_batch       = 128
0.00.054.016 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.016 I llama_new_context_with_model: flash_attn    = 0
0.00.054.016 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.017 I llama_new_context_with_model: freq_scale    = 1
0.00.054.017 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.017 I ggml_metal_init: allocating
0.00.054.020 I ggml_metal_init: found device: Apple M4
0.00.054.022 I ggml_metal_init: picking default device: Apple M4
0.00.054.589 I ggml_metal_init: using embedded metal library
0.00.056.938 I ggml_metal_init: GPU name:   Apple M4
0.00.056.939 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.939 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.940 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.940 I ggml_metal_init: simdgroup reduction   = true
0.00.056.940 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.940 I ggml_metal_init: has bfloat            = true
0.00.056.940 I ggml_metal_init: use bfloat            = true
0.00.056.941 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.941 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.581 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.875 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.878 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.893 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.856 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.857 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.857 I llama_new_context_with_model: graph nodes  = 967
0.00.068.857 I llama_new_context_with_model: graph splits = 2
0.00.068.870 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.870 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.560.227 I 
0.00.560.264 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.560.288 I perplexity: tokenizing the input ..
0.00.567.857 I perplexity: tokenization took 7.568 ms
0.00.567.860 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.702.018 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.703.194 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.703.221 I llama_perf_context_print:        load time =     550.55 ms
0.00.703.223 I llama_perf_context_print: prompt eval time =     133.93 ms /   128 tokens (    1.05 ms per token,   955.76 tokens per second)
0.00.703.224 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.703.224 I llama_perf_context_print:       total time =     143.00 ms /   129 tokens
0.00.703.572 I ggml_metal_free: deallocating

real	0m0.717s
user	0m0.078s
sys	0m0.090s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4405 (a45433ba) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.010.780 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.907 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.912 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.919 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.920 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.920 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.921 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.921 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.922 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.922 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.923 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.923 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.923 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.924 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.924 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.925 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.926 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.926 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.991 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.039 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.034 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.036 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.036 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.036 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.036 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.037 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.037 I llama_model_loader: - type  f32:  194 tensors
0.00.026.038 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.038 I llama_model_loader: - type q6_K:   37 tensors
0.00.047.365 I llm_load_vocab: special tokens cache size = 25
0.00.053.282 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.284 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.284 I llm_load_print_meta: arch             = gptneox
0.00.053.285 I llm_load_print_meta: vocab type       = BPE
0.00.053.285 I llm_load_print_meta: n_vocab          = 50304
0.00.053.285 I llm_load_print_meta: n_merges         = 50009
0.00.053.286 I llm_load_print_meta: vocab_only       = 0
0.00.053.286 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.286 I llm_load_print_meta: n_embd           = 2048
0.00.053.286 I llm_load_print_meta: n_layer          = 24
0.00.053.289 I llm_load_print_meta: n_head           = 16
0.00.053.290 I llm_load_print_meta: n_head_kv        = 16
0.00.053.290 I llm_load_print_meta: n_rot            = 32
0.00.053.290 I llm_load_print_meta: n_swa            = 0
0.00.053.290 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.291 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.291 I llm_load_print_meta: n_gqa            = 1
0.00.053.292 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.293 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.293 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.294 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.294 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.294 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.294 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.295 I llm_load_print_meta: n_ff             = 8192
0.00.053.295 I llm_load_print_meta: n_expert         = 0
0.00.053.295 I llm_load_print_meta: n_expert_used    = 0
0.00.053.295 I llm_load_print_meta: causal attn      = 1
0.00.053.296 I llm_load_print_meta: pooling type     = 0
0.00.053.296 I llm_load_print_meta: rope type        = 2
0.00.053.296 I llm_load_print_meta: rope scaling     = linear
0.00.053.299 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.299 I llm_load_print_meta: freq_scale_train = 1
0.00.053.299 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.300 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.300 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.300 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.300 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.300 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.300 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.300 I llm_load_print_meta: model type       = 1.4B
0.00.053.301 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.053.301 I llm_load_print_meta: model params     = 1.41 B
0.00.053.302 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.053.302 I llm_load_print_meta: general.name     = 1.4B
0.00.053.302 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.303 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.303 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.303 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.303 I llm_load_print_meta: LF token         = 128 ''
0.00.053.304 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.306 I llm_load_print_meta: max token length = 1024
0.00.055.392 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.392 I llm_load_tensors: offloading output layer to GPU
0.00.055.392 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.403 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.055.404 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.056.369 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.370 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.371 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.371 I llama_new_context_with_model: n_batch       = 2048
0.00.056.371 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.371 I llama_new_context_with_model: flash_attn    = 0
0.00.056.372 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.372 I llama_new_context_with_model: freq_scale    = 1
0.00.056.372 I ggml_metal_init: allocating
0.00.056.378 I ggml_metal_init: found device: Apple M4
0.00.056.381 I ggml_metal_init: picking default device: Apple M4
0.00.056.973 I ggml_metal_init: using embedded metal library
0.00.059.368 I ggml_metal_init: GPU name:   Apple M4
0.00.059.369 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.369 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.370 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.370 I ggml_metal_init: simdgroup reduction   = true
0.00.059.370 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.370 I ggml_metal_init: has bfloat            = true
0.00.059.370 I ggml_metal_init: use bfloat            = true
0.00.059.371 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.371 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.156 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.089.517 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.521 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.539 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.540 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.541 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.542 I llama_new_context_with_model: graph nodes  = 967
0.00.090.542 I llama_new_context_with_model: graph splits = 2
0.00.090.557 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.711 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.712 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.694.519 I main: llama threadpool init, n_threads = 4
0.00.694.560 I 
0.00.694.582 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.694.582 I 
0.00.694.807 I sampler seed: 1234
0.00.694.811 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.694.832 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.694.832 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.694.832 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.543.572 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57629.87 tokens per second)
0.01.543.572 I llama_perf_context_print:        load time =     683.73 ms
0.01.543.573 I llama_perf_context_print: prompt eval time =      51.52 ms /     7 tokens (    7.36 ms per token,   135.86 tokens per second)
0.01.543.574 I llama_perf_context_print:        eval time =     794.21 ms /    63 runs   (   12.61 ms per token,    79.32 tokens per second)
0.01.543.575 I llama_perf_context_print:       total time =     849.06 ms /    70 tokens
0.01.543.774 I ggml_metal_free: deallocating

real	0m1.561s
user	0m0.111s
sys	0m0.151s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4405 (a45433ba) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.069 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.622 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.627 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.629 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.629 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.629 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.630 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.630 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.632 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.632 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.633 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.633 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.633 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.634 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.634 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.637 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.637 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.638 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.472 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.562 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.398 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.399 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.399 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.399 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.400 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.400 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.401 I llama_model_loader: - type  f32:  194 tensors
0.00.024.401 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.401 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.414 I llm_load_vocab: special tokens cache size = 25
0.00.051.342 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.345 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.345 I llm_load_print_meta: arch             = gptneox
0.00.051.345 I llm_load_print_meta: vocab type       = BPE
0.00.051.346 I llm_load_print_meta: n_vocab          = 50304
0.00.051.346 I llm_load_print_meta: n_merges         = 50009
0.00.051.346 I llm_load_print_meta: vocab_only       = 0
0.00.051.346 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.346 I llm_load_print_meta: n_embd           = 2048
0.00.051.346 I llm_load_print_meta: n_layer          = 24
0.00.051.349 I llm_load_print_meta: n_head           = 16
0.00.051.350 I llm_load_print_meta: n_head_kv        = 16
0.00.051.350 I llm_load_print_meta: n_rot            = 32
0.00.051.350 I llm_load_print_meta: n_swa            = 0
0.00.051.351 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.351 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.352 I llm_load_print_meta: n_gqa            = 1
0.00.051.353 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.353 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.354 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.354 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.356 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.356 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.356 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.357 I llm_load_print_meta: n_ff             = 8192
0.00.051.357 I llm_load_print_meta: n_expert         = 0
0.00.051.357 I llm_load_print_meta: n_expert_used    = 0
0.00.051.357 I llm_load_print_meta: causal attn      = 1
0.00.051.357 I llm_load_print_meta: pooling type     = 0
0.00.051.357 I llm_load_print_meta: rope type        = 2
0.00.051.358 I llm_load_print_meta: rope scaling     = linear
0.00.051.359 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.359 I llm_load_print_meta: freq_scale_train = 1
0.00.051.359 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.360 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.360 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.360 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.360 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.360 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.360 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.360 I llm_load_print_meta: model type       = 1.4B
0.00.051.366 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.366 I llm_load_print_meta: model params     = 1.41 B
0.00.051.366 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.368 I llm_load_print_meta: general.name     = 1.4B
0.00.051.368 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.368 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.368 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.368 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.369 I llm_load_print_meta: LF token         = 128 ''
0.00.051.369 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.369 I llm_load_print_meta: max token length = 1024
0.00.053.378 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.379 I llm_load_tensors: offloading output layer to GPU
0.00.053.379 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.390 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.391 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.289 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.290 I llama_new_context_with_model: n_ctx         = 128
0.00.054.290 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.290 I llama_new_context_with_model: n_batch       = 128
0.00.054.291 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.291 I llama_new_context_with_model: flash_attn    = 0
0.00.054.291 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.291 I llama_new_context_with_model: freq_scale    = 1
0.00.054.292 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.292 I ggml_metal_init: allocating
0.00.054.298 I ggml_metal_init: found device: Apple M4
0.00.054.301 I ggml_metal_init: picking default device: Apple M4
0.00.054.861 I ggml_metal_init: using embedded metal library
0.00.057.187 I ggml_metal_init: GPU name:   Apple M4
0.00.057.188 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.188 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.189 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.189 I ggml_metal_init: simdgroup reduction   = true
0.00.057.189 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.189 I ggml_metal_init: has bfloat            = true
0.00.057.190 I ggml_metal_init: use bfloat            = true
0.00.057.190 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.191 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.487 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.740 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.743 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.759 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.613 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.614 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.614 I llama_new_context_with_model: graph nodes  = 967
0.00.068.615 I llama_new_context_with_model: graph splits = 2
0.00.068.627 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.628 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.641.541 I 
0.00.641.574 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.641.589 I perplexity: tokenizing the input ..
0.00.649.349 I perplexity: tokenization took 7.759 ms
0.00.649.352 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.790.326 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.791.483 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.791.504 I llama_perf_context_print:        load time =     631.46 ms
0.00.791.505 I llama_perf_context_print: prompt eval time =     140.72 ms /   128 tokens (    1.10 ms per token,   909.61 tokens per second)
0.00.791.506 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.791.506 I llama_perf_context_print:       total time =     149.97 ms /   129 tokens
0.00.791.996 I ggml_metal_free: deallocating

real	0m0.807s
user	0m0.078s
sys	0m0.116s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4405 (a45433ba) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.008.735 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.136 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.140 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.146 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.146 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.147 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.147 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.147 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.150 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.150 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.151 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.151 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.151 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.152 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.152 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.153 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.154 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.155 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.001 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.094 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.918 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.919 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.919 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.919 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.920 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.920 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.022.921 I llama_model_loader: - type  f32:  194 tensors
0.00.022.921 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.182 I llm_load_vocab: special tokens cache size = 25
0.00.049.150 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.153 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.153 I llm_load_print_meta: arch             = gptneox
0.00.049.153 I llm_load_print_meta: vocab type       = BPE
0.00.049.154 I llm_load_print_meta: n_vocab          = 50304
0.00.049.154 I llm_load_print_meta: n_merges         = 50009
0.00.049.154 I llm_load_print_meta: vocab_only       = 0
0.00.049.154 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.154 I llm_load_print_meta: n_embd           = 2048
0.00.049.154 I llm_load_print_meta: n_layer          = 24
0.00.049.157 I llm_load_print_meta: n_head           = 16
0.00.049.158 I llm_load_print_meta: n_head_kv        = 16
0.00.049.158 I llm_load_print_meta: n_rot            = 32
0.00.049.158 I llm_load_print_meta: n_swa            = 0
0.00.049.158 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.158 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.159 I llm_load_print_meta: n_gqa            = 1
0.00.049.160 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.161 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.161 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.162 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.162 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.162 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.162 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.163 I llm_load_print_meta: n_ff             = 8192
0.00.049.163 I llm_load_print_meta: n_expert         = 0
0.00.049.163 I llm_load_print_meta: n_expert_used    = 0
0.00.049.164 I llm_load_print_meta: causal attn      = 1
0.00.049.164 I llm_load_print_meta: pooling type     = 0
0.00.049.164 I llm_load_print_meta: rope type        = 2
0.00.049.164 I llm_load_print_meta: rope scaling     = linear
0.00.049.164 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.165 I llm_load_print_meta: freq_scale_train = 1
0.00.049.165 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.165 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.165 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.166 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.166 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.166 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.166 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.166 I llm_load_print_meta: model type       = 1.4B
0.00.049.167 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.167 I llm_load_print_meta: model params     = 1.41 B
0.00.049.167 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.168 I llm_load_print_meta: general.name     = 1.4B
0.00.049.168 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.168 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.168 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.168 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.171 I llm_load_print_meta: LF token         = 128 ''
0.00.049.171 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.171 I llm_load_print_meta: max token length = 1024
0.00.051.151 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.151 I llm_load_tensors: offloading output layer to GPU
0.00.051.151 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.162 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.163 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.042 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.043 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.043 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.043 I llama_new_context_with_model: n_batch       = 2048
0.00.052.043 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.044 I llama_new_context_with_model: flash_attn    = 0
0.00.052.044 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.044 I llama_new_context_with_model: freq_scale    = 1
0.00.052.045 I ggml_metal_init: allocating
0.00.052.050 I ggml_metal_init: found device: Apple M4
0.00.052.052 I ggml_metal_init: picking default device: Apple M4
0.00.052.631 I ggml_metal_init: using embedded metal library
0.00.054.992 I ggml_metal_init: GPU name:   Apple M4
0.00.054.993 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.994 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.994 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.994 I ggml_metal_init: simdgroup reduction   = true
0.00.054.994 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.994 I ggml_metal_init: has bfloat            = true
0.00.054.995 I ggml_metal_init: use bfloat            = true
0.00.054.995 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.996 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.603 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.083.763 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.772 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.796 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.923 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.924 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.924 I llama_new_context_with_model: graph nodes  = 967
0.00.084.925 I llama_new_context_with_model: graph splits = 2
0.00.084.941 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.089 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.090 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.738.477 I main: llama threadpool init, n_threads = 4
0.00.738.520 I 
0.00.738.541 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.738.541 I 
0.00.738.783 I sampler seed: 1234
0.00.738.789 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.738.827 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.738.828 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.738.828 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.619.097 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51374.82 tokens per second)
0.01.619.098 I llama_perf_context_print:        load time =     729.74 ms
0.01.619.098 I llama_perf_context_print: prompt eval time =      54.37 ms /     7 tokens (    7.77 ms per token,   128.75 tokens per second)
0.01.619.099 I llama_perf_context_print:        eval time =     823.02 ms /    63 runs   (   13.06 ms per token,    76.55 tokens per second)
0.01.619.100 I llama_perf_context_print:       total time =     880.62 ms /    70 tokens
0.01.619.332 I ggml_metal_free: deallocating

real	0m1.638s
user	0m0.110s
sys	0m0.157s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4405 (a45433ba) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.931 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.432 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.436 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.438 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.444 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.444 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.444 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.445 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.446 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.446 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.446 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.447 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.447 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.447 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.448 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.452 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.452 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.452 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.252 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.250 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.058 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.059 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.059 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.060 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.060 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.060 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.061 I llama_model_loader: - type  f32:  194 tensors
0.00.023.061 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.989 I llm_load_vocab: special tokens cache size = 25
0.00.049.993 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.996 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.996 I llm_load_print_meta: arch             = gptneox
0.00.049.996 I llm_load_print_meta: vocab type       = BPE
0.00.049.997 I llm_load_print_meta: n_vocab          = 50304
0.00.049.997 I llm_load_print_meta: n_merges         = 50009
0.00.049.997 I llm_load_print_meta: vocab_only       = 0
0.00.049.997 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.997 I llm_load_print_meta: n_embd           = 2048
0.00.049.998 I llm_load_print_meta: n_layer          = 24
0.00.050.000 I llm_load_print_meta: n_head           = 16
0.00.050.001 I llm_load_print_meta: n_head_kv        = 16
0.00.050.001 I llm_load_print_meta: n_rot            = 32
0.00.050.003 I llm_load_print_meta: n_swa            = 0
0.00.050.003 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.003 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.004 I llm_load_print_meta: n_gqa            = 1
0.00.050.005 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.005 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.006 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.006 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.006 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.006 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.007 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.007 I llm_load_print_meta: n_ff             = 8192
0.00.050.007 I llm_load_print_meta: n_expert         = 0
0.00.050.008 I llm_load_print_meta: n_expert_used    = 0
0.00.050.008 I llm_load_print_meta: causal attn      = 1
0.00.050.008 I llm_load_print_meta: pooling type     = 0
0.00.050.008 I llm_load_print_meta: rope type        = 2
0.00.050.008 I llm_load_print_meta: rope scaling     = linear
0.00.050.010 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.010 I llm_load_print_meta: freq_scale_train = 1
0.00.050.010 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.011 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.011 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.011 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.011 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.011 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.011 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.012 I llm_load_print_meta: model type       = 1.4B
0.00.050.012 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.013 I llm_load_print_meta: model params     = 1.41 B
0.00.050.013 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.013 I llm_load_print_meta: general.name     = 1.4B
0.00.050.013 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.014 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.014 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.014 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.014 I llm_load_print_meta: LF token         = 128 ''
0.00.050.014 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.015 I llm_load_print_meta: max token length = 1024
0.00.052.091 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.091 I llm_load_tensors: offloading output layer to GPU
0.00.052.091 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.102 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.103 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.056 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.057 I llama_new_context_with_model: n_ctx         = 128
0.00.053.057 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.057 I llama_new_context_with_model: n_batch       = 128
0.00.053.057 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.058 I llama_new_context_with_model: flash_attn    = 0
0.00.053.058 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.058 I llama_new_context_with_model: freq_scale    = 1
0.00.053.059 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.059 I ggml_metal_init: allocating
0.00.053.065 I ggml_metal_init: found device: Apple M4
0.00.053.067 I ggml_metal_init: picking default device: Apple M4
0.00.053.631 I ggml_metal_init: using embedded metal library
0.00.055.924 I ggml_metal_init: GPU name:   Apple M4
0.00.055.926 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.926 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.926 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.927 I ggml_metal_init: simdgroup reduction   = true
0.00.055.927 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.927 I ggml_metal_init: has bfloat            = true
0.00.055.927 I ggml_metal_init: use bfloat            = true
0.00.055.927 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.928 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.251 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.533 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.535 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.548 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.441 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.442 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.442 I llama_new_context_with_model: graph nodes  = 967
0.00.067.443 I llama_new_context_with_model: graph splits = 2
0.00.067.455 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.455 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.384.179 I 
0.00.384.217 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.384.230 I perplexity: tokenizing the input ..
0.00.391.853 I perplexity: tokenization took 7.621 ms
0.00.391.856 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.531.918 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.533.089 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.533.106 I llama_perf_context_print:        load time =     375.24 ms
0.00.533.107 I llama_perf_context_print: prompt eval time =     139.84 ms /   128 tokens (    1.09 ms per token,   915.35 tokens per second)
0.00.533.110 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.533.110 I llama_perf_context_print:       total time =     148.93 ms /   129 tokens
0.00.533.541 I ggml_metal_free: deallocating

real	0m0.547s
user	0m0.078s
sys	0m0.080s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4405 (a45433ba)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x140e0a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x140e0a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x140e0af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x140e0b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x140e0bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x140e0c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x140e0c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x140e0cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x140e0d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x140e0d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x140e0db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x140e0e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x140e0eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x140e0f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x140e0fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x140e10270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x140e10990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x140e110b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x140e117d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x140e11fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x140e126c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x140e12de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x140e13500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x140e13da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x140e144c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x140e14780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x140e14d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x140e15a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x140e15f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x140e16200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x140e166a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x140e16960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x140e171f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x140e17730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x140e179f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x140e17e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x140e18330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x140e187d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x140e18c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x140e19110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x140e195b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x140e19a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x140e19ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x140e1a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x140e1a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x140e1ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x140e1b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x140e1bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x140e1c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x140e1c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x140e1cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x140e1d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x140e1d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x140e1dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x140e1e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x140e1ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x140e1f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x140e1f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x140e1f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x140e201e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x140e204a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x140e20940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x140e20de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x140e21280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x140e21720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x140e21bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x140e22060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x140e22500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x140e229a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x140e22e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x140e232e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x140e23780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x140e23c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x140e24170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x140e246c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x140e24c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x140e25160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x140e256b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x140e25c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x140e26150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x140e266a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x140e26bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x140e27140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x140e27690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x140e27be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x140e28130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x140e28680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x140e28bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x140e29120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x140e29670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x140e29bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x140e2a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x140e2a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x140e2abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x140e2b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x140e2b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x140e2bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x140e1b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x140e2c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x140e2c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x140e2cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x140e2d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x140e2d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x140e2dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x140e2e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x140e2e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x140e2ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x140e2f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x140e2f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x140e2fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x140e30230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x140e30780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x140e30cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x140e31170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x140e31610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x140e31ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x140e31f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x140e323f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x140e32890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x140e32d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x140e331d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x140e33670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x140e33b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x140e33fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x140e34450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x140e348f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x140e34d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x140e35230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x140e356d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x140e35b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x140e36010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x140e364b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x140e36950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x140e36df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x140e37290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x140e37730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x140e37bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x140e38070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x140e38510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x140e389b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x140e38e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x140e392f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x140e39790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x140e39c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x140e3a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x140e3a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x140e3aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x140e3aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x140e3b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x140e3b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x140e3bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x140e3c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x140e3c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x140e3ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x140e3cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x140e3d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x140e3d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x140e3dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x140e3e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x140e3e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x140e3ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x140e3ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x140e3f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x140e3f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x140e3fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x140e401f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x140e40690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x140e40b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x140e40fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x140e41470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x140e41910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x140e41db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x140e42250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x140e426f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x140e42b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x140e43030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x140e434d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x140e43970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x140e43e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x140e442b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x140e44750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x140e44bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x140e45090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x140e45530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x140e459d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x140e45e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x140e46310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x140e467b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x140e46c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x140e470f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x140e47590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x140e47a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x140e47ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x140e48420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x140e48970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x140e48ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x140e49410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x140e496d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x140e49ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x140e4a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x140e4a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x140e4b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x140e4b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x140e4b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x140e4be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x140e4c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x140e4cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x140e4d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x140e4d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x140e4da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x140e4e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x140e4e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x140e4ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x140e4f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x140e4f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x140e4fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x140e501d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x140e50720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x140e50c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x140e511c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x140e51710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x140e51c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x140e521b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x140e52700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x140e52c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x140e531a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x140e536f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x140e53c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x140e54190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x140e546e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x140e54c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x140e55180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x140e556d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x140e55c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x140e56170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x140e566c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x140e56c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x140e57160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x140e576b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x140e57c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x140e58150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x140e586a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x140e58bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x140e59140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x140e59690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x140e59be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x140e5a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x140e5a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x140e5abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x140e5b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x140e5b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x140e5bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x140e5c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x140e5c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x140e5cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x140e5d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x140e5d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x140e5dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x140e5e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x140e5e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x140e5eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x140e5f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x140e5f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x140e5fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x140e600d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x140e60620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x140e60b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x140e61010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x140e614b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x140e61950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x140e61df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x140e62290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x140e62730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x140e62bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x140e63070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x140e63510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x140e639b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x140e63e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x140e642f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x140e64790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x140e64c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x140e650d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x140e65620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x140e65d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x140e66460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x140e66b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x140e672a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x140e67560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x140e67d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x140e68010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x140e68620 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.149.392 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.149.395 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x134d04b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x134d04f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x134d05400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x134d05870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x134d05ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x134d06150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x134d065c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x134d06a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x134d06ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x134d07310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x134d07780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x134d07e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x134d08990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x134d09140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x134d09950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x134d0a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x134d0a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x134d0aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x134d0b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x134d0bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x134d0c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x134d0cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x134d0d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x134d0d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x134d0e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x134d0e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x134d0e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x134d0ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x134d0ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x134d0f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x134d0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x134d0fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x134d10180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x134d10440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x134d108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x134d10d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x134d11190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x134d11600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x134d11a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x134d11ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x134d12350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x134d127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x134d12c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x134d130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x134d13510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x134d13980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x134d13df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x134d14260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x134d146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x134d14b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x134d14fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x134d15420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x134d15890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x134d15d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x134d16170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x134d165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x134d16b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x134d17050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x134d174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x134d17930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x134d17da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x134d18210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x134d18680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x134d18af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x134d18f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x134d193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x134d19840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x134d19cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x134d1a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x134d1a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x134d1aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x134d1ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x134d1b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x134d1b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x134d1bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x134d1c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x134d1c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x134d1c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x134d1cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x134d1d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x134d1d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x134d1dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x134d1df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x134d1e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x134d1e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x134d1ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x134d1f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x134d1f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x134d1f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x134d1fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x134d202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x134d20730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x134d20ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x134d21010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x134d21480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x134d218f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x134d21d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x134d221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x134d22640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x134d22ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x134d22f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x134d23390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x134d23800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x134d23c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x134d240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x134d24550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x134d249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x134d24e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x134d252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x134d25710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x134d25b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x134d25ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x134d26460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x134d268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x134d26d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x134d271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x134d27620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x134d27a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x134d27f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x134d28370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x134d287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x134d28c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x134d290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x134d29530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x134d299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x134d29e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x134d2a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x134d2a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x134d2ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x134d2afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x134d2b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x134d2b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x134d2bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x134d2c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x134d2c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x134d2ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x134d2cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x134d2d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x134d2d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x134d2dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x134d2e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x134d2e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x134d2e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x134d2edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x134d2f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x134d2f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x134d2fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x134d2ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x134d30420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x134d30890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x134d30d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x134d31170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x134d315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x134d31a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x134d31ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x134d32330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x134d327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x134d32c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x134d33080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x134d334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x134d33960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x134d33dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x134d34240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x134d346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x134d34b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x134d34f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x134d35400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x134d35870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x134d35ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x134d36150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x134d365c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x134d36a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x134d36ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x134d37310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x134d37780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x134d37bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x134d38060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x134d384d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x134d38940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x134d38db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x134d39220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x134d39690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x134d39b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x134d39f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x134d3a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x134d3a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x134d3acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x134d3b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x134d3b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x134d3ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x134d3be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x134d3c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x134d3c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x134d3cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x134d3d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x134d3d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x134d3d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x134d3dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x134d3e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x134d3e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x134d3eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x134d3ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x134d3f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x134d3f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x134d3fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x134d40110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x134d40580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x134d40b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x134d40f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x134d413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x134d41f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x134d42200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x134d424c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x134d42930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x134d42da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x134d43210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x134d43680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x134d43af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x134d43f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x134d443d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x134d44840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x134d44cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x134d45120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x134d45590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x134d45a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x134d45e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x134d462e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x134d46750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x134d46bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x134d47030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x134d474a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x134d47910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x134d47d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x134d481f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x134d48660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x134d48ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x134d48f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x134d493b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x134d49820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x134d49c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x134d4a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x134d4a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x134d4a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x134d4ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x134d4b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x134d4b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x134d4bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x134d4c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x134d4c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x134d4c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x134d4cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x134d4d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x134d4d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x134d4dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x134d4df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x134d4e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x134d4e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x134d4ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x134d4f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x134d4f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x134d4f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x134d4fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x134d502a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x134d50710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x134d50b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x134d50ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x134d51460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x134d518d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x134d51d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x134d521b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x134d52620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x134d52a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x134d52f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x134d53370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x134d537e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x134d53c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x134d540c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x134d54530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x134d549a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x134d54e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x134d55280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x134d556f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x134d55b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x134d565d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x134d56cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x134d57410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x134d57b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x134d57df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x134d58260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x134d58860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x134d58e70 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x140e24d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x140e251a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x140e25610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x140e25a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x140e25ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x140e26360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x140e267d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x140e26c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x140e270b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x140e27520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x140e27990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x140e27f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x140e28860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x140e28fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x140e297c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x140e29eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x140e2a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x140e2ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x140e2b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x140e2bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x140e2c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x140e2cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x140e2d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x140e2d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x140e2dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x140e2e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x140e2e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x140e2ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x140e2f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x140e2f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x140e2fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x140e2fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x140e30330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x140e305f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x140e30a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x140e30ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x140e31340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x140e317b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x140e31c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x140e32090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x140e32500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x140e32970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x140e32de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x140e33250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x140e336c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x140e33b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x140e33fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x140e34410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x140e34880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x140e34cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x140e35160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x140e355d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x140e35a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x140e35eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x140e36320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x140e36790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x140e36c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x140e37070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x140e374e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x140e37950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x140e37dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x140e38230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x140e386a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x140e38b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x140e38f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x140e393f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x140e39860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x140e39cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x140e3a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x140e3a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x140e3aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x140e3ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x140e3b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x140e3b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x140e3bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x140e3c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x140e3c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x140e3c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x140e3cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x140e3d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x140e3d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x140e3daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x140e3df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x140e3e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x140e3e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x140e3ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x140e3f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x140e3f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x140e3fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x140e3fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x140e402e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x140e40750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x140e40bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x140e41030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x140e414a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x140e41910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x140e41d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x140e421f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x140e42660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x140e42ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x140e42f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x140e433b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x140e43820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x140e43c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x140e44100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x140e44570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x140e449e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x140e44e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x140e452c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x140e45730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x140e45ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x140e46010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x140e46480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x140e468f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x140e46d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x140e471d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x140e47640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x140e47ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x140e47f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x140e48390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x140e48800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x140e48c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x140e490e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x140e49550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x140e499c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x140e49e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x140e4a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x140e4a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x140e4ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x140e4aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x140e4b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x140e4b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x140e4bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x140e4c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x140e4c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x140e4ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x140e4cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x140e4d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x140e4d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x140e4dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x140e4e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x140e4e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x140e4e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x140e4ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x140e4f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x140e4f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x140e4fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x140e4ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x140e50440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x140e508b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x140e50d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x140e51190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x140e51600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x140e51a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x140e51ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x140e52350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x140e527c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x140e52c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x140e530a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x140e53510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x140e53980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x140e53df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x140e54260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x140e546d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x140e54b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x140e54fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x140e55420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x140e55890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x140e55d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x140e56170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x140e565e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x140e56a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x140e56ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x140e57330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x140e577a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x140e57c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x140e58080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x140e584f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x140e58960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x140e58dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x140e59240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x140e596b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x140e59b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x140e59f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x140e5a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x140e5a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x140e5ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x140e5b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x140e5b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x140e5ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x140e5bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x140e5c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x140e5c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x140e5cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x140e5d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x140e5d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x140e5d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x140e5ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x140e5e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x140e5e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x140e5eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x140e5ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x140e5f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x140e5f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x140e5fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x140e60130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x140e605a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x140e60a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x140e60e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x140e612f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x140e61a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x140e61ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x140e62350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x140e627c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x140e62c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x140e630a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x140e63510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x140e63980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x140e63df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x140e64260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x140e646d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x140e64b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x140e64fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x140e65420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x140e65890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x140e65d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x140e66170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x140e665e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x140e66a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x140e66ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x140e67330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x140e677a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x140e67c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x140e68080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x140e684f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x140e0b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x140e0ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x140e098c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x140e0a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x140e178d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x140e17d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x140e181b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x140e18620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x140e18a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x140e18f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x140e19370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x140e197e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x140e19c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x140e1a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x140e1a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x140e1a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x140e1ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x140e1b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x140e1b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x140e1bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x140e1bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x140e1c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x140e1c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x140e1cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x140e1d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x140e1d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x140e1da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x140e1dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x140e1e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x140e1e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x140e1ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x140e1f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x140e1f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x140e1f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x140e1fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x140e20260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x140e206d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x140e20b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x140e20fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x140e21420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x140e21890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x140e21d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x140e22170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x140e225e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x140e22a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x140e22ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x140e23330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x140e237a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x140e23e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x140e24580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x140e16360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x140e16a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x140e16ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x140e0d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x140e0daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x140e0df10 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.802s
user	0m0.302s
sys	0m0.295s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4405 (a45433ba)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1527102e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1527109f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x152710fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x152711550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x152711b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1527120b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x152712660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x152712c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1527131c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1527136c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x152713bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1527140c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x152714be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x152715390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x152715ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1527162c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1527169e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x152717100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x152717820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x152717ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x152718710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x152718e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x152719550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x152719df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15271a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15271a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15271ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15271ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15271bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15271c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15271c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15271c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15271d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15271d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15271da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15271dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15271e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15271e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15271ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15271f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15271f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15271faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15271ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1527203e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1527206a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x152720cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1527212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x152721be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1527221f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x152722800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x152722e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x152723420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x152723a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x152724040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x152724830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x152724cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x152725170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x152725430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x152725a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x152726230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1527264f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x152726990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x152726e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1527272d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x152727770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x152727c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1527280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x152728550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1527289f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x152728e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x152729330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1527297d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x152729c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15272a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15272a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15272ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15272b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15272b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15272bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15272c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15272c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15272cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15272d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15272d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15272dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15272e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15272e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15272ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15272f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15272f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15272fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x152730160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1527306b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x152730c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x152731150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1527316a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x152731bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1527218d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x152732060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x152732810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x152732d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1527332b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x152733800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x152733d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1527342a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1527347f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x152734d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x152735290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1527357e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x152735d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x152736280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1527367d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x152736d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1527371c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x152737660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x152737b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x152737fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x152738440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1527388e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x152738d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x152739220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1527396c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x152739b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15273a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15273a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15273a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15273ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15273b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15273b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15273bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15273c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15273c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15273c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15273ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15273d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15273d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15273dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15273e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15273e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15273ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15273eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15273f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15273f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15273fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x152740120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1527405c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x152740a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x152740f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1527413a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x152741840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x152741ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x152742180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x152742620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x152742ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x152742f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x152743400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1527438a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x152743d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1527441e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x152744680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x152744b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x152744fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x152745460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x152745900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x152745da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x152746240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1527466e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x152746b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x152747020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1527474c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x152747960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x152747e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1527482a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x152748740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x152748be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x152749080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x152749520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1527499c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x152749e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15274a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15274a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15274ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15274b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15274b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15274ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15274bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15274c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15274c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15274cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15274d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15274d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15274da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15274df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15274e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15274e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15274ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15274f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15274f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15274fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x152750340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x152750950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x152751140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1527515e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1527518a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x152751eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1527524c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x152752cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x152753150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1527535f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x152753a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x152754240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x152754790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x152754ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x152755230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x152755780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x152755cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x152756220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x152756770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x152756cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x152757210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x152757760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x152757cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x152758200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x152758750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x152758ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1527591f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x152759740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x152759c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15275a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15275a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15275ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15275b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15275b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15275bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15275c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15275c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15275cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15275d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15275d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15275dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15275e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15275e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15275ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15275f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15275f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15275fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x152760180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1527606d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x152760c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x152761170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1527616c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x152761c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x152762160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1527626b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x152762c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x152763150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1527636a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x152763bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x152764140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x152764690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x152764be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x152765130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x152765680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x152765bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x152766120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x152766670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x152766bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x152767060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x152767500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1527679a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x152767e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1527682e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x152768780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x152768c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1527690c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x152769560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x152769a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x152769ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15276a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15276a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15276ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15276b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15276b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15276bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15276c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15276cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15276d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15276d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15276dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15276e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15276e670 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.096.378 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.096.382 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x156e04d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x156e051c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x156e05630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x156e05aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x156e05f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x156e06380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x156e067f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x156e06c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x156e070d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x156e07540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x156e079b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x156e080a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x156e08bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x156e09370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x156e09b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x156e0a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x156e0a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x156e0b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x156e0b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x156e0bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x156e0c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x156e0cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x156e0d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x156e0dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x156e0e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x156e0e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x156e0e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x156e0ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x156e0f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x156e0f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x156e0fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x156e0ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x156e103b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x156e10670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x156e10ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x156e10f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x156e113c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x156e11830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x156e11ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x156e12110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x156e12580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x156e129f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x156e12e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x156e132d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x156e13740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x156e13bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x156e14020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x156e14490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x156e14900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x156e14d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x156e151e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x156e15650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x156e15ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x156e15f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x156e163a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x156e16810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x156e16d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x156e17280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x156e176f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x156e17b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x156e17fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x156e18440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x156e188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x156e18d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x156e19190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x156e19600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x156e19a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x156e19ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x156e1a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x156e1a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x156e1ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x156e1b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x156e1b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x156e1b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x156e1bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x156e1c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x156e1c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x156e1cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x156e1cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x156e1d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x156e1d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x156e1dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x156e1e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x156e1e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x156e1ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x156e1eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x156e1f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x156e1f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x156e1fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x156e20080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x156e204f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x156e20960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x156e20dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x156e21240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x156e216b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x156e21b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x156e21f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x156e22400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x156e22870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x156e22ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x156e23150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x156e235c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x156e23a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x156e23ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x156e24310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x156e24780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x156e24bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x156e25060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x156e254d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x156e25940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x156e25db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x156e26220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x156e26690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x156e26b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x156e26f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x156e273e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x156e27850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x156e27cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x156e28130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x156e285a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x156e28a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x156e28e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x156e292f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x156e29760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x156e29bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x156e2a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x156e2a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x156e2a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x156e2ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x156e2b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x156e2b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x156e2bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x156e2bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x156e2c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x156e2c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x156e2cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x156e2d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x156e2d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x156e2d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x156e2de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x156e2e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x156e2e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x156e2ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x156e2f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x156e2f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x156e2f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x156e2fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x156e301e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x156e30650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x156e30ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x156e30f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x156e313a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x156e31810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x156e31c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x156e320f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x156e32560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x156e329d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x156e32e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x156e332b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x156e33720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x156e33b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x156e34000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x156e34470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x156e348e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x156e34d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x156e351c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x156e35630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x156e35aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x156e35f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x156e36380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x156e367f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x156e36c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x156e370d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x156e37540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x156e379b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x156e37e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x156e38290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x156e38700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x156e38b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x156e38fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x156e39450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x156e398c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x156e39d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x156e3a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x156e3a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x156e3aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x156e3aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x156e3b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x156e3b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x156e3bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x156e3c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x156e3c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x156e3c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x156e3ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x156e3d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x156e3d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x156e3db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x156e3dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x156e3e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x156e3e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x156e3ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x156e3f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x156e3f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x156e3fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x156e3fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x156e40340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x156e407b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x156e40d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x156e411b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x156e41620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x156e42170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x156e42430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x156e426f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x156e42b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x156e42fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x156e43440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x156e438b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x156e43d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x156e44190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x156e44600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x156e44a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x156e44ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x156e45350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x156e457c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x156e45c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x156e460a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x156e46510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x156e46980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x156e46df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x156e47260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x156e476d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x156e47b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x156e47fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x156e48420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x156e48890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x156e48d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x156e49170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x156e495e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x156e49a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x156e49ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x156e4a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x156e4a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x156e4ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x156e4b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x156e4b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x156e4b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x156e4bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x156e4c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x156e4c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x156e4cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x156e4cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x156e4d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x156e4d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x156e4dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x156e4e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x156e4e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x156e4ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x156e4eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x156e4f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x156e4f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x156e4fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x156e50060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x156e504d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x156e50940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x156e50db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x156e51220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x156e51690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x156e51b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x156e51f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x156e523e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x156e52850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x156e52cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x156e53130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x156e535a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x156e53a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x156e53e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x156e542f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x156e54760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x156e54bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x156e55040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x156e554b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x156e55920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x156e55d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x156e56800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x156e56f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x156e57640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x156e57d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x156e58020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x156e58490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x156e58a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x156e590a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1526053b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x152605820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x152605c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x152606100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x152606570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1526069e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x152606e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1526072c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x152607730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x152607c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1526080f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x152608770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x152609290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x152609a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15260a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15260a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15260b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15260b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15260bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15260c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15260cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15260d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15260dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15260e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15260ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15260ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15260efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15260f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15260f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15260fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x152610180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1526106b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x152610b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x152610de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x152611250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1526116c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x152611b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x152611fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x152612410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x152612880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x152612cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x152613160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1526135d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x152613a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x152613eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x152614320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x152614790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x152614c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x152615070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1526154e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x152615950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x152615dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x152616230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1526166a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x152616b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x152616f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15272abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15272b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15272b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15272b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15272bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15272c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15272c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15272cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15272cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15272d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15272d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15272dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15272e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15272e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15272e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15272ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15272f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15272f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15272fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x152730020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x152730490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x152730900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x152730d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1527311e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x152731650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x152731ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x152731f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1527323a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x152732810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x152732c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1527330f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x152733560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1527339d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x152733e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1527342b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x152734720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x152734b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x152735000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x152735470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1527358e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x152735d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1527361c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x152736630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x152736aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x152736f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x152737380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1527377f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x152737c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1527380d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x152738540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1527389b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x152738e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x152739290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x152739700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x152739b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x152739fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15273a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15273a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15273ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15273b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15273b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15273ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15273bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15273c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15273c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15273cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15273d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15273d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15273d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15273de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15273e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15273e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15273eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15273efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15273f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15273f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15273fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x152740180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1527405f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x152740a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x152740ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x152741340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1527417b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x152741c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x152742090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x152742500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x152742970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x152742de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x152743250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1527436c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x152743b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x152743fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x152744410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x152744880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x152744cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x152745160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1527455d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x152745a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x152745eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x152746320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x152746790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x152746c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x152747070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1527474e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x152747950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x152747dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x152748230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1527486a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x152748b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x152748f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1527493f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x152749860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x152749cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15274a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15274a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15274aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15274ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15274b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15274b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15274bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15274c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15274c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15274c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15274cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15274d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15274d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15274daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15274df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15274e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15274e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15274ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15274f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15274f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15274fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15274fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1527502e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x152750750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x152750bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x152751030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1527514a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x152751910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x152751d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1527521f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x152752660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x152752ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x152752f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1527533b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x152753820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x152753c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x152754100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x152754570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1527549e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x152754e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1527552c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x152755bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x152756020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x152756490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x152756900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x152756d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1527571e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x152757650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x152757ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x152757f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1527583a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x152758810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x152758c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1527590f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x152759560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1527599d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x152759e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15275a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15275a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15275ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15275b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15275b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15275b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15275bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15275c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15275c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15275caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15275cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15275d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15275d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15275dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15275e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15275e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15275e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15275ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15275f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15275f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15275fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x152760290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x152760700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x152760b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x152760fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x152761450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1527618c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x152761d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1527621a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x152762610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x152762a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x152762ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x152763360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1527637d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x152763c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1527640b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x152764520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x152764990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x152764e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x152765270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1527656e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x152765b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x152765fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x152766430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1527668a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x152766d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x152767180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1527675f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x152767a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x152767ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x152768340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1527687b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x152768c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x152769090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x152769500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x152769970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x152769de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15276a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15276abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15276b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15276b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15276be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15276c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15276c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15276cb60 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.940s
user	0m0.244s
sys	0m0.144s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.54 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.58 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.12 sec*proc (2 tests)

Total Test time (real) =   1.13 sec
        1.15 real         0.72 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.25 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.51 sec*proc (2 tests)

Total Test time (real) =   0.52 sec
        0.52 real         0.15 user         0.04 sys
```
