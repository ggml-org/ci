Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:312 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.632s
user	0m0.913s
sys	0m1.250s
++ nproc
+ make -j10
[  0%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Built target build_info
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha1
[  5%] Built target sha256
[  5%] Built target xxhash
[  6%] Linking CXX shared library ../../bin/libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 15%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 16%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 26%] Linking CXX shared library ../bin/libllama.dylib
[ 26%] Built target llama-gguf
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama
[ 26%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 27%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 31%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 31%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 31%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 32%] Linking C executable ../bin/test-c
[ 34%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-simple
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 35%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 35%] Linking CXX executable ../../bin/llama-simple-chat
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llava
[ 36%] Built target test-c
[ 36%] Built target llama-simple-chat
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Built target llama-quantize-stats
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Built target llama-simple
[ 37%] Built target common
[ 37%] Built target llava_static
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 39%] Built target llava_shared
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 44%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-0
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-llama-grammar
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 46%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 47%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 47%] Linking CXX executable ../bin/test-sampling
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 47%] Linking CXX executable ../bin/test-grammar-parser
[ 47%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-llama-grammar
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-json-schema-to-grammar
[ 48%] Built target test-sampling
[ 48%] Built target test-grammar-integration
[ 49%] Linking CXX executable ../bin/test-log
[ 49%] Linking CXX executable ../bin/test-arg-parser
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 49%] Built target test-grammar-parser
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 56%] Built target test-arg-parser
[ 57%] Linking CXX executable ../bin/test-gguf
[ 57%] Linking CXX executable ../bin/test-backend-ops
[ 57%] Linking CXX executable ../bin/test-chat-template
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Built target test-log
[ 58%] Linking CXX executable ../bin/test-model-load-cancel
[ 59%] Linking CXX executable ../bin/test-barrier
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 61%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 61%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Built target test-backend-ops
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-gguf
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-barrier
[ 63%] Linking CXX executable ../../bin/llama-batched-bench
[ 63%] Built target test-chat-template
[ 63%] Built target test-autorelease
[ 63%] Built target test-quantize-perf
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Built target test-quantize-fns
[ 65%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 67%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 68%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 69%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-batched
[ 70%] Linking CXX executable ../../bin/llama-embedding
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-eval-callback
[ 71%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Built target llama-batched-bench
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Built target test-rope
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Built target llama-embedding
[ 72%] Built target llama-batched
[ 72%] Built target llama-eval-callback
[ 72%] Built target llama-gbnf-validator
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-gritlm
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Built target llama-imatrix
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Built target llama-infill
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 79%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Built target llama-bench
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Built target llama-lookahead
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Built target llama-lookup-merge
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 81%] Built target llama-lookup-create
[ 81%] Built target llama-lookup-stats
[ 81%] Generating loading.html.hpp
[ 81%] Built target llama-parallel
[ 81%] Built target llama-passkey
[ 82%] Generating index.html.gz.hpp
[ 82%] Built target llama-cli
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Built target llama-lookup
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Built target llama-perplexity
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 87%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 88%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-save-load-state
[ 88%] Linking CXX executable ../../bin/llama-speculative
[ 89%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 89%] Built target llama-retrieval
[ 89%] Linking CXX executable ../../bin/llama-speculative-simple
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Built target llama-quantize
[ 91%] Linking CXX executable ../../bin/llama-run
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Built target llama-speculative
[ 91%] Built target llama-save-load-state
[ 91%] Built target llama-tts
[ 91%] Built target llama-tokenize
[ 91%] Built target llama-speculative-simple
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Built target llama-gen-docs
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Built target llama-run
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 97%] Linking CXX executable ../../bin/llama-vdot
[ 97%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Built target llama-cvector-generator
[ 98%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-convert-llama2c-to-ggml
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-qwen2vl-cli
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.049s
user	0m6.242s
sys	0m9.536s

main: quantize time =  5045.73 ms
main:    total time =  5045.73 ms

main: quantize time =  3087.65 ms
main:    total time =  3087.65 ms

main: quantize time =  3360.39 ms
main:    total time =  3360.39 ms

main: quantize time =  1781.08 ms
main:    total time =  1781.08 ms

main: quantize time =  2109.11 ms
main:    total time =  2109.11 ms

main: quantize time =  6623.49 ms
main:    total time =  6623.49 ms

main: quantize time =  5928.55 ms
main:    total time =  5928.55 ms

main: quantize time =  7088.10 ms
main:    total time =  7088.10 ms

main: quantize time =  5975.53 ms
main:    total time =  5975.53 ms

main: quantize time =  4547.55 ms
main:    total time =  4547.55 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.152 I build: 4581 (b636228c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.321 I main: llama backend init
0.00.000.327 I main: load the model and apply lora adapter, if any
0.00.052.379 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.067.699 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.067.722 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.067.727 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.067.728 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.067.729 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.067.729 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.067.730 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.067.734 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.067.735 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.067.735 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.067.737 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.067.737 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.067.738 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.067.739 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.067.744 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.067.745 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.067.746 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.075.345 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.077.965 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.087.015 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.087.019 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.087.019 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.087.020 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.087.020 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.087.021 I llama_model_loader: - type  f32:  194 tensors
0.00.087.021 I llama_model_loader: - type  f16:   98 tensors
0.00.087.022 I print_info: file format = GGUF V3 (latest)
0.00.087.026 I print_info: file type   = all F32 (guessed)
0.00.087.028 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.101.540 I load: special tokens cache size = 25
0.00.110.745 I load: token to piece cache size = 0.2984 MB
0.00.110.748 I print_info: arch             = gptneox
0.00.110.749 I print_info: vocab_only       = 0
0.00.110.749 I print_info: n_ctx_train      = 2048
0.00.110.749 I print_info: n_embd           = 2048
0.00.110.749 I print_info: n_layer          = 24
0.00.110.752 I print_info: n_head           = 16
0.00.110.753 I print_info: n_head_kv        = 16
0.00.110.753 I print_info: n_rot            = 32
0.00.110.754 I print_info: n_swa            = 0
0.00.110.755 I print_info: n_embd_head_k    = 128
0.00.110.755 I print_info: n_embd_head_v    = 128
0.00.110.756 I print_info: n_gqa            = 1
0.00.110.757 I print_info: n_embd_k_gqa     = 2048
0.00.110.757 I print_info: n_embd_v_gqa     = 2048
0.00.110.758 I print_info: f_norm_eps       = 1.0e-05
0.00.110.761 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.110.761 I print_info: f_clamp_kqv      = 0.0e+00
0.00.110.761 I print_info: f_max_alibi_bias = 0.0e+00
0.00.110.761 I print_info: f_logit_scale    = 0.0e+00
0.00.110.762 I print_info: n_ff             = 8192
0.00.110.762 I print_info: n_expert         = 0
0.00.110.763 I print_info: n_expert_used    = 0
0.00.110.763 I print_info: causal attn      = 1
0.00.110.763 I print_info: pooling type     = 0
0.00.110.763 I print_info: rope type        = 2
0.00.110.763 I print_info: rope scaling     = linear
0.00.110.764 I print_info: freq_base_train  = 10000.0
0.00.110.764 I print_info: freq_scale_train = 1
0.00.110.764 I print_info: n_ctx_orig_yarn  = 2048
0.00.110.765 I print_info: rope_finetuned   = unknown
0.00.110.765 I print_info: ssm_d_conv       = 0
0.00.110.765 I print_info: ssm_d_inner      = 0
0.00.110.765 I print_info: ssm_d_state      = 0
0.00.110.765 I print_info: ssm_dt_rank      = 0
0.00.110.766 I print_info: ssm_dt_b_c_rms   = 0
0.00.110.766 I print_info: model type       = 1.4B
0.00.110.767 I print_info: model params     = 1.41 B
0.00.110.767 I print_info: general.name     = 1.4B
0.00.110.768 I print_info: vocab type       = BPE
0.00.110.768 I print_info: n_vocab          = 50304
0.00.110.768 I print_info: n_merges         = 50009
0.00.110.768 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.110.769 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.110.769 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.110.769 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.110.769 I print_info: LF token         = 128 'Ä'
0.00.110.770 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.110.770 I print_info: max token length = 1024
0.00.150.511 I load_tensors: offloading 24 repeating layers to GPU
0.00.150.514 I load_tensors: offloading output layer to GPU
0.00.150.514 I load_tensors: offloaded 25/25 layers to GPU
0.00.150.537 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.150.538 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.150.839 I llama_init_from_model: n_seq_max     = 1
0.00.150.840 I llama_init_from_model: n_ctx         = 2048
0.00.150.840 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.150.840 I llama_init_from_model: n_batch       = 2048
0.00.150.840 I llama_init_from_model: n_ubatch      = 512
0.00.150.840 I llama_init_from_model: flash_attn    = 0
0.00.150.841 I llama_init_from_model: freq_base     = 10000.0
0.00.150.841 I llama_init_from_model: freq_scale    = 1
0.00.150.843 I ggml_metal_init: allocating
0.00.150.885 I ggml_metal_init: found device: Apple M4
0.00.150.891 I ggml_metal_init: picking default device: Apple M4
0.00.151.508 I ggml_metal_init: using embedded metal library
0.00.160.743 I ggml_metal_init: GPU name:   Apple M4
0.00.160.745 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.160.745 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.160.745 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.160.745 I ggml_metal_init: simdgroup reduction   = true
0.00.160.746 I ggml_metal_init: simdgroup matrix mul. = true
0.00.160.746 I ggml_metal_init: has residency sets    = true
0.00.160.746 I ggml_metal_init: has bfloat            = true
0.00.160.746 I ggml_metal_init: use bfloat            = true
0.00.160.746 I ggml_metal_init: hasUnifiedMemory      = true
0.00.160.747 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.184.102 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.214.755 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.214.763 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.214.793 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.218.512 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.218.515 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.218.515 I llama_init_from_model: graph nodes  = 967
0.00.218.515 I llama_init_from_model: graph splits = 2
0.00.218.522 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.218.655 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.218.655 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.286.055 I main: llama threadpool init, n_threads = 4
0.00.286.097 I 
0.00.286.130 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.286.132 I 
0.00.286.312 I sampler seed: 1234
0.00.286.317 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.286.341 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.286.343 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.286.343 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.116.170 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59265.44 tokens per second)
0.02.116.171 I llama_perf_context_print:        load time =     232.62 ms
0.02.116.172 I llama_perf_context_print: prompt eval time =      43.81 ms /     7 tokens (    6.26 ms per token,   159.77 tokens per second)
0.02.116.172 I llama_perf_context_print:        eval time =    1783.13 ms /    63 runs   (   28.30 ms per token,    35.33 tokens per second)
0.02.116.173 I llama_perf_context_print:       total time =    1831.15 ms /    70 tokens
0.02.116.401 I ggml_metal_free: deallocating

real	0m2.468s
user	0m0.132s
sys	0m0.139s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4581 (b636228c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.009.913 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.822 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.827 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.829 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.829 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.830 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.830 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.830 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.832 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.832 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.832 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.833 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.833 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.833 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.834 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.835 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.836 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.836 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.700 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.725 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.519 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.521 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.522 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.522 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.522 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.523 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.523 I llama_model_loader: - type  f32:  194 tensors
0.00.026.524 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.524 I print_info: file format = GGUF V3 (latest)
0.00.026.525 I print_info: file type   = Q8_0
0.00.026.526 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.034.621 I load: special tokens cache size = 25
0.00.040.659 I load: token to piece cache size = 0.2984 MB
0.00.040.664 I print_info: arch             = gptneox
0.00.040.665 I print_info: vocab_only       = 0
0.00.040.665 I print_info: n_ctx_train      = 2048
0.00.040.669 I print_info: n_embd           = 2048
0.00.040.669 I print_info: n_layer          = 24
0.00.040.676 I print_info: n_head           = 16
0.00.040.676 I print_info: n_head_kv        = 16
0.00.040.677 I print_info: n_rot            = 32
0.00.040.677 I print_info: n_swa            = 0
0.00.040.677 I print_info: n_embd_head_k    = 128
0.00.040.677 I print_info: n_embd_head_v    = 128
0.00.040.678 I print_info: n_gqa            = 1
0.00.040.678 I print_info: n_embd_k_gqa     = 2048
0.00.040.679 I print_info: n_embd_v_gqa     = 2048
0.00.040.680 I print_info: f_norm_eps       = 1.0e-05
0.00.040.680 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.680 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.680 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.680 I print_info: f_logit_scale    = 0.0e+00
0.00.040.681 I print_info: n_ff             = 8192
0.00.040.681 I print_info: n_expert         = 0
0.00.040.681 I print_info: n_expert_used    = 0
0.00.040.682 I print_info: causal attn      = 1
0.00.040.682 I print_info: pooling type     = 0
0.00.040.682 I print_info: rope type        = 2
0.00.040.683 I print_info: rope scaling     = linear
0.00.040.683 I print_info: freq_base_train  = 10000.0
0.00.040.683 I print_info: freq_scale_train = 1
0.00.040.683 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.684 I print_info: rope_finetuned   = unknown
0.00.040.684 I print_info: ssm_d_conv       = 0
0.00.040.684 I print_info: ssm_d_inner      = 0
0.00.040.684 I print_info: ssm_d_state      = 0
0.00.040.684 I print_info: ssm_dt_rank      = 0
0.00.040.684 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.685 I print_info: model type       = 1.4B
0.00.040.685 I print_info: model params     = 1.41 B
0.00.040.685 I print_info: general.name     = 1.4B
0.00.040.686 I print_info: vocab type       = BPE
0.00.040.686 I print_info: n_vocab          = 50304
0.00.040.686 I print_info: n_merges         = 50009
0.00.040.686 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.688 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.689 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.689 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.689 I print_info: LF token         = 128 'Ä'
0.00.040.689 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.689 I print_info: max token length = 1024
0.01.002.672 I load_tensors: offloading 24 repeating layers to GPU
0.01.002.676 I load_tensors: offloading output layer to GPU
0.01.002.677 I load_tensors: offloaded 25/25 layers to GPU
0.01.002.697 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.002.699 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.003.453 I llama_init_from_model: n_seq_max     = 1
0.01.003.455 I llama_init_from_model: n_ctx         = 2048
0.01.003.456 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.003.456 I llama_init_from_model: n_batch       = 2048
0.01.003.457 I llama_init_from_model: n_ubatch      = 512
0.01.003.457 I llama_init_from_model: flash_attn    = 0
0.01.003.458 I llama_init_from_model: freq_base     = 10000.0
0.01.003.458 I llama_init_from_model: freq_scale    = 1
0.01.003.459 I ggml_metal_init: allocating
0.01.003.486 I ggml_metal_init: found device: Apple M4
0.01.003.496 I ggml_metal_init: picking default device: Apple M4
0.01.004.809 I ggml_metal_init: using embedded metal library
0.01.010.386 I ggml_metal_init: GPU name:   Apple M4
0.01.010.389 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.010.389 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.010.390 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.010.390 I ggml_metal_init: simdgroup reduction   = true
0.01.010.391 I ggml_metal_init: simdgroup matrix mul. = true
0.01.010.391 I ggml_metal_init: has residency sets    = true
0.01.010.391 I ggml_metal_init: has bfloat            = true
0.01.010.391 I ggml_metal_init: use bfloat            = true
0.01.010.392 I ggml_metal_init: hasUnifiedMemory      = true
0.01.010.393 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.026.792 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.087.531 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.087.539 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.087.566 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.092.765 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.092.767 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.092.768 I llama_init_from_model: graph nodes  = 967
0.01.092.768 I llama_init_from_model: graph splits = 2
0.01.092.773 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.092.894 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.092.895 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.140.206 I main: llama threadpool init, n_threads = 4
0.01.140.243 I 
0.01.140.266 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.140.267 I 
0.01.140.399 I sampler seed: 1234
0.01.140.403 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.140.413 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.140.413 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.140.415 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.219.833 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55167.06 tokens per second)
0.02.219.835 I llama_perf_context_print:        load time =    1129.41 ms
0.02.219.836 I llama_perf_context_print: prompt eval time =      39.88 ms /     7 tokens (    5.70 ms per token,   175.55 tokens per second)
0.02.219.836 I llama_perf_context_print:        eval time =    1036.59 ms /    63 runs   (   16.45 ms per token,    60.78 tokens per second)
0.02.219.837 I llama_perf_context_print:       total time =    1080.50 ms /    70 tokens
0.02.220.106 I ggml_metal_free: deallocating

real	0m2.239s
user	0m0.108s
sys	0m0.254s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4581 (b636228c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.011.578 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.091 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.019.097 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.100 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.101 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.101 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.102 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.102 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.103 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.103 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.104 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.104 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.104 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.105 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.105 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.107 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.107 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.107 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.024 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.082 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.876 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.877 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.877 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.877 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.878 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.878 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.879 I llama_model_loader: - type  f32:  194 tensors
0.00.027.879 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.879 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.880 I print_info: file format = GGUF V3 (latest)
0.00.027.881 I print_info: file type   = Q4_0
0.00.027.881 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.035.925 I load: special tokens cache size = 25
0.00.041.819 I load: token to piece cache size = 0.2984 MB
0.00.041.823 I print_info: arch             = gptneox
0.00.041.823 I print_info: vocab_only       = 0
0.00.041.823 I print_info: n_ctx_train      = 2048
0.00.041.823 I print_info: n_embd           = 2048
0.00.041.824 I print_info: n_layer          = 24
0.00.041.829 I print_info: n_head           = 16
0.00.041.831 I print_info: n_head_kv        = 16
0.00.041.831 I print_info: n_rot            = 32
0.00.041.831 I print_info: n_swa            = 0
0.00.041.831 I print_info: n_embd_head_k    = 128
0.00.041.831 I print_info: n_embd_head_v    = 128
0.00.041.832 I print_info: n_gqa            = 1
0.00.041.833 I print_info: n_embd_k_gqa     = 2048
0.00.041.836 I print_info: n_embd_v_gqa     = 2048
0.00.041.837 I print_info: f_norm_eps       = 1.0e-05
0.00.041.837 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.837 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.837 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.837 I print_info: f_logit_scale    = 0.0e+00
0.00.041.839 I print_info: n_ff             = 8192
0.00.041.839 I print_info: n_expert         = 0
0.00.041.839 I print_info: n_expert_used    = 0
0.00.041.839 I print_info: causal attn      = 1
0.00.041.839 I print_info: pooling type     = 0
0.00.041.839 I print_info: rope type        = 2
0.00.041.840 I print_info: rope scaling     = linear
0.00.041.840 I print_info: freq_base_train  = 10000.0
0.00.041.840 I print_info: freq_scale_train = 1
0.00.041.840 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.841 I print_info: rope_finetuned   = unknown
0.00.041.841 I print_info: ssm_d_conv       = 0
0.00.041.841 I print_info: ssm_d_inner      = 0
0.00.041.846 I print_info: ssm_d_state      = 0
0.00.041.846 I print_info: ssm_dt_rank      = 0
0.00.041.846 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.846 I print_info: model type       = 1.4B
0.00.041.847 I print_info: model params     = 1.41 B
0.00.041.847 I print_info: general.name     = 1.4B
0.00.041.848 I print_info: vocab type       = BPE
0.00.041.848 I print_info: n_vocab          = 50304
0.00.041.848 I print_info: n_merges         = 50009
0.00.041.848 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.848 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.849 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.849 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.849 I print_info: LF token         = 128 'Ä'
0.00.041.849 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.849 I print_info: max token length = 1024
0.00.590.136 I load_tensors: offloading 24 repeating layers to GPU
0.00.590.147 I load_tensors: offloading output layer to GPU
0.00.590.148 I load_tensors: offloaded 25/25 layers to GPU
0.00.590.176 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.590.177 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.591.618 I llama_init_from_model: n_seq_max     = 1
0.00.591.626 I llama_init_from_model: n_ctx         = 2048
0.00.591.627 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.591.627 I llama_init_from_model: n_batch       = 2048
0.00.591.628 I llama_init_from_model: n_ubatch      = 512
0.00.591.628 I llama_init_from_model: flash_attn    = 0
0.00.591.629 I llama_init_from_model: freq_base     = 10000.0
0.00.591.629 I llama_init_from_model: freq_scale    = 1
0.00.591.632 I ggml_metal_init: allocating
0.00.591.681 I ggml_metal_init: found device: Apple M4
0.00.591.694 I ggml_metal_init: picking default device: Apple M4
0.00.593.307 I ggml_metal_init: using embedded metal library
0.00.598.839 I ggml_metal_init: GPU name:   Apple M4
0.00.598.852 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.598.853 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.598.854 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.598.855 I ggml_metal_init: simdgroup reduction   = true
0.00.598.855 I ggml_metal_init: simdgroup matrix mul. = true
0.00.598.855 I ggml_metal_init: has residency sets    = true
0.00.598.855 I ggml_metal_init: has bfloat            = true
0.00.598.856 I ggml_metal_init: use bfloat            = true
0.00.598.858 I ggml_metal_init: hasUnifiedMemory      = true
0.00.598.861 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.619.250 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.680.548 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.680.553 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.680.573 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.685.311 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.685.313 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.685.313 I llama_init_from_model: graph nodes  = 967
0.00.685.314 I llama_init_from_model: graph splits = 2
0.00.685.318 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.685.438 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.685.438 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.739.359 I main: llama threadpool init, n_threads = 4
0.00.739.402 I 
0.00.739.422 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.739.422 I 
0.00.739.577 I sampler seed: 1234
0.00.739.581 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.739.591 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.739.591 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.739.591 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.423.873 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 50105.86 tokens per second)
0.01.423.874 I llama_perf_context_print:        load time =     726.85 ms
0.01.423.875 I llama_perf_context_print: prompt eval time =      49.67 ms /     7 tokens (    7.10 ms per token,   140.93 tokens per second)
0.01.423.875 I llama_perf_context_print:        eval time =     631.71 ms /    63 runs   (   10.03 ms per token,    99.73 tokens per second)
0.01.423.876 I llama_perf_context_print:       total time =     685.45 ms /    70 tokens
0.01.424.111 I ggml_metal_free: deallocating

real	0m1.443s
user	0m0.110s
sys	0m0.213s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4581 (b636228c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.008.945 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.898 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.909 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.911 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.911 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.912 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.912 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.912 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.913 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.914 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.914 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.914 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.915 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.915 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.915 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.917 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.917 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.918 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.683 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.699 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.453 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.454 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.454 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.455 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.455 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.455 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.456 I llama_model_loader: - type  f32:  194 tensors
0.00.025.456 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.456 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.457 I print_info: file format = GGUF V3 (latest)
0.00.025.457 I print_info: file type   = Q4_1
0.00.025.458 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.572 I load: special tokens cache size = 25
0.00.039.528 I load: token to piece cache size = 0.2984 MB
0.00.039.531 I print_info: arch             = gptneox
0.00.039.531 I print_info: vocab_only       = 0
0.00.039.531 I print_info: n_ctx_train      = 2048
0.00.039.532 I print_info: n_embd           = 2048
0.00.039.532 I print_info: n_layer          = 24
0.00.039.535 I print_info: n_head           = 16
0.00.039.535 I print_info: n_head_kv        = 16
0.00.039.536 I print_info: n_rot            = 32
0.00.039.536 I print_info: n_swa            = 0
0.00.039.536 I print_info: n_embd_head_k    = 128
0.00.039.536 I print_info: n_embd_head_v    = 128
0.00.039.537 I print_info: n_gqa            = 1
0.00.039.538 I print_info: n_embd_k_gqa     = 2048
0.00.039.539 I print_info: n_embd_v_gqa     = 2048
0.00.039.539 I print_info: f_norm_eps       = 1.0e-05
0.00.039.540 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.540 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.544 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.544 I print_info: f_logit_scale    = 0.0e+00
0.00.039.545 I print_info: n_ff             = 8192
0.00.039.545 I print_info: n_expert         = 0
0.00.039.545 I print_info: n_expert_used    = 0
0.00.039.545 I print_info: causal attn      = 1
0.00.039.545 I print_info: pooling type     = 0
0.00.039.546 I print_info: rope type        = 2
0.00.039.546 I print_info: rope scaling     = linear
0.00.039.546 I print_info: freq_base_train  = 10000.0
0.00.039.546 I print_info: freq_scale_train = 1
0.00.039.547 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.547 I print_info: rope_finetuned   = unknown
0.00.039.547 I print_info: ssm_d_conv       = 0
0.00.039.547 I print_info: ssm_d_inner      = 0
0.00.039.547 I print_info: ssm_d_state      = 0
0.00.039.547 I print_info: ssm_dt_rank      = 0
0.00.039.547 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.548 I print_info: model type       = 1.4B
0.00.039.548 I print_info: model params     = 1.41 B
0.00.039.548 I print_info: general.name     = 1.4B
0.00.039.549 I print_info: vocab type       = BPE
0.00.039.549 I print_info: n_vocab          = 50304
0.00.039.549 I print_info: n_merges         = 50009
0.00.039.549 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.550 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.550 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.551 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.551 I print_info: LF token         = 128 'Ä'
0.00.039.551 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.551 I print_info: max token length = 1024
0.00.645.766 I load_tensors: offloading 24 repeating layers to GPU
0.00.645.782 I load_tensors: offloading output layer to GPU
0.00.645.783 I load_tensors: offloaded 25/25 layers to GPU
0.00.645.814 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.645.816 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.647.266 I llama_init_from_model: n_seq_max     = 1
0.00.647.271 I llama_init_from_model: n_ctx         = 2048
0.00.647.272 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.647.272 I llama_init_from_model: n_batch       = 2048
0.00.647.272 I llama_init_from_model: n_ubatch      = 512
0.00.647.273 I llama_init_from_model: flash_attn    = 0
0.00.647.275 I llama_init_from_model: freq_base     = 10000.0
0.00.647.275 I llama_init_from_model: freq_scale    = 1
0.00.647.278 I ggml_metal_init: allocating
0.00.647.352 I ggml_metal_init: found device: Apple M4
0.00.647.366 I ggml_metal_init: picking default device: Apple M4
0.00.649.223 I ggml_metal_init: using embedded metal library
0.00.655.806 I ggml_metal_init: GPU name:   Apple M4
0.00.655.810 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.655.811 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.655.812 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.655.813 I ggml_metal_init: simdgroup reduction   = true
0.00.655.813 I ggml_metal_init: simdgroup matrix mul. = true
0.00.655.814 I ggml_metal_init: has residency sets    = true
0.00.655.814 I ggml_metal_init: has bfloat            = true
0.00.655.814 I ggml_metal_init: use bfloat            = true
0.00.655.815 I ggml_metal_init: hasUnifiedMemory      = true
0.00.655.825 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.674.075 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.730.914 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.730.920 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.730.941 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.734.985 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.734.987 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.734.988 I llama_init_from_model: graph nodes  = 967
0.00.734.988 I llama_init_from_model: graph splits = 2
0.00.734.993 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.735.123 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.735.124 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.786.397 I main: llama threadpool init, n_threads = 4
0.00.786.439 I 
0.00.786.462 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.786.463 I 
0.00.786.625 I sampler seed: 1234
0.00.786.630 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.786.641 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.786.641 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.786.641 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.523.141 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55295.95 tokens per second)
0.01.523.147 I llama_perf_context_print:        load time =     776.59 ms
0.01.523.147 I llama_perf_context_print: prompt eval time =      48.71 ms /     7 tokens (    6.96 ms per token,   143.72 tokens per second)
0.01.523.148 I llama_perf_context_print:        eval time =     685.57 ms /    63 runs   (   10.88 ms per token,    91.89 tokens per second)
0.01.523.149 I llama_perf_context_print:       total time =     737.61 ms /    70 tokens
0.01.523.428 I ggml_metal_free: deallocating

real	0m1.539s
user	0m0.110s
sys	0m0.200s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4581 (b636228c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.011.499 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.158 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.020.164 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.166 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.166 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.166 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.167 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.167 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.168 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.168 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.168 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.168 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.169 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.169 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.169 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.171 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.171 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.172 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.014 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.037 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.833 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.835 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.835 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.835 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.836 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.836 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.028.837 I llama_model_loader: - type  f32:  194 tensors
0.00.028.837 I llama_model_loader: - type q5_0:   97 tensors
0.00.028.837 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.838 I print_info: file format = GGUF V3 (latest)
0.00.028.838 I print_info: file type   = Q5_0
0.00.028.839 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.037.103 I load: special tokens cache size = 25
0.00.043.248 I load: token to piece cache size = 0.2984 MB
0.00.043.252 I print_info: arch             = gptneox
0.00.043.253 I print_info: vocab_only       = 0
0.00.043.253 I print_info: n_ctx_train      = 2048
0.00.043.253 I print_info: n_embd           = 2048
0.00.043.253 I print_info: n_layer          = 24
0.00.043.258 I print_info: n_head           = 16
0.00.043.260 I print_info: n_head_kv        = 16
0.00.043.260 I print_info: n_rot            = 32
0.00.043.260 I print_info: n_swa            = 0
0.00.043.261 I print_info: n_embd_head_k    = 128
0.00.043.261 I print_info: n_embd_head_v    = 128
0.00.043.262 I print_info: n_gqa            = 1
0.00.043.262 I print_info: n_embd_k_gqa     = 2048
0.00.043.263 I print_info: n_embd_v_gqa     = 2048
0.00.043.263 I print_info: f_norm_eps       = 1.0e-05
0.00.043.264 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.043.264 I print_info: f_clamp_kqv      = 0.0e+00
0.00.043.266 I print_info: f_max_alibi_bias = 0.0e+00
0.00.043.266 I print_info: f_logit_scale    = 0.0e+00
0.00.043.267 I print_info: n_ff             = 8192
0.00.043.270 I print_info: n_expert         = 0
0.00.043.271 I print_info: n_expert_used    = 0
0.00.043.271 I print_info: causal attn      = 1
0.00.043.271 I print_info: pooling type     = 0
0.00.043.271 I print_info: rope type        = 2
0.00.043.271 I print_info: rope scaling     = linear
0.00.043.272 I print_info: freq_base_train  = 10000.0
0.00.043.272 I print_info: freq_scale_train = 1
0.00.043.272 I print_info: n_ctx_orig_yarn  = 2048
0.00.043.273 I print_info: rope_finetuned   = unknown
0.00.043.273 I print_info: ssm_d_conv       = 0
0.00.043.273 I print_info: ssm_d_inner      = 0
0.00.043.273 I print_info: ssm_d_state      = 0
0.00.043.273 I print_info: ssm_dt_rank      = 0
0.00.043.273 I print_info: ssm_dt_b_c_rms   = 0
0.00.043.274 I print_info: model type       = 1.4B
0.00.043.274 I print_info: model params     = 1.41 B
0.00.043.274 I print_info: general.name     = 1.4B
0.00.043.275 I print_info: vocab type       = BPE
0.00.043.275 I print_info: n_vocab          = 50304
0.00.043.275 I print_info: n_merges         = 50009
0.00.043.275 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.043.275 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.043.276 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.043.276 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.043.276 I print_info: LF token         = 128 'Ä'
0.00.043.276 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.043.276 I print_info: max token length = 1024
0.00.693.529 I load_tensors: offloading 24 repeating layers to GPU
0.00.693.533 I load_tensors: offloading output layer to GPU
0.00.693.534 I load_tensors: offloaded 25/25 layers to GPU
0.00.693.547 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.693.548 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.694.413 I llama_init_from_model: n_seq_max     = 1
0.00.694.420 I llama_init_from_model: n_ctx         = 2048
0.00.694.420 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.694.420 I llama_init_from_model: n_batch       = 2048
0.00.694.421 I llama_init_from_model: n_ubatch      = 512
0.00.694.421 I llama_init_from_model: flash_attn    = 0
0.00.694.422 I llama_init_from_model: freq_base     = 10000.0
0.00.694.423 I llama_init_from_model: freq_scale    = 1
0.00.694.424 I ggml_metal_init: allocating
0.00.694.454 I ggml_metal_init: found device: Apple M4
0.00.694.463 I ggml_metal_init: picking default device: Apple M4
0.00.695.467 I ggml_metal_init: using embedded metal library
0.00.699.699 I ggml_metal_init: GPU name:   Apple M4
0.00.699.706 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.699.706 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.699.707 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.699.707 I ggml_metal_init: simdgroup reduction   = true
0.00.699.707 I ggml_metal_init: simdgroup matrix mul. = true
0.00.699.708 I ggml_metal_init: has residency sets    = true
0.00.699.708 I ggml_metal_init: has bfloat            = true
0.00.699.708 I ggml_metal_init: use bfloat            = true
0.00.699.709 I ggml_metal_init: hasUnifiedMemory      = true
0.00.699.711 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.714.281 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.745.488 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.745.493 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.745.512 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.751.291 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.751.292 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.751.293 I llama_init_from_model: graph nodes  = 967
0.00.751.293 I llama_init_from_model: graph splits = 2
0.00.751.299 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.751.432 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.751.433 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.806.458 I main: llama threadpool init, n_threads = 4
0.00.806.495 I 
0.00.806.515 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.806.515 I 
0.00.806.687 I sampler seed: 1234
0.00.806.691 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.806.702 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.806.702 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.806.702 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.593.878 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53024.65 tokens per second)
0.01.593.878 I llama_perf_context_print:        load time =     794.10 ms
0.01.593.883 I llama_perf_context_print: prompt eval time =      42.71 ms /     7 tokens (    6.10 ms per token,   163.90 tokens per second)
0.01.593.885 I llama_perf_context_print:        eval time =     741.74 ms /    63 runs   (   11.77 ms per token,    84.94 tokens per second)
0.01.593.885 I llama_perf_context_print:       total time =     788.28 ms /    70 tokens
0.01.594.099 I ggml_metal_free: deallocating

real	0m1.613s
user	0m0.104s
sys	0m0.180s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4581 (b636228c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.008.733 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.027.613 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.027.617 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.618 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.619 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.619 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.619 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.619 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.620 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.620 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.621 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.621 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.621 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.621 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.622 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.625 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.626 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.627 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.230 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.234 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.168 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.169 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.169 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.170 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.170 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.170 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.036.171 I llama_model_loader: - type  f32:  194 tensors
0.00.036.171 I llama_model_loader: - type q5_1:   97 tensors
0.00.036.171 I llama_model_loader: - type q6_K:    1 tensors
0.00.036.172 I print_info: file format = GGUF V3 (latest)
0.00.036.172 I print_info: file type   = Q5_1
0.00.036.173 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.044.436 I load: special tokens cache size = 25
0.00.051.146 I load: token to piece cache size = 0.2984 MB
0.00.051.149 I print_info: arch             = gptneox
0.00.051.149 I print_info: vocab_only       = 0
0.00.051.149 I print_info: n_ctx_train      = 2048
0.00.051.150 I print_info: n_embd           = 2048
0.00.051.150 I print_info: n_layer          = 24
0.00.051.152 I print_info: n_head           = 16
0.00.051.153 I print_info: n_head_kv        = 16
0.00.051.153 I print_info: n_rot            = 32
0.00.051.154 I print_info: n_swa            = 0
0.00.051.154 I print_info: n_embd_head_k    = 128
0.00.051.156 I print_info: n_embd_head_v    = 128
0.00.051.156 I print_info: n_gqa            = 1
0.00.051.157 I print_info: n_embd_k_gqa     = 2048
0.00.051.158 I print_info: n_embd_v_gqa     = 2048
0.00.051.158 I print_info: f_norm_eps       = 1.0e-05
0.00.051.159 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.159 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.159 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.159 I print_info: f_logit_scale    = 0.0e+00
0.00.051.160 I print_info: n_ff             = 8192
0.00.051.160 I print_info: n_expert         = 0
0.00.051.160 I print_info: n_expert_used    = 0
0.00.051.160 I print_info: causal attn      = 1
0.00.051.160 I print_info: pooling type     = 0
0.00.051.162 I print_info: rope type        = 2
0.00.051.163 I print_info: rope scaling     = linear
0.00.051.163 I print_info: freq_base_train  = 10000.0
0.00.051.164 I print_info: freq_scale_train = 1
0.00.051.164 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.164 I print_info: rope_finetuned   = unknown
0.00.051.164 I print_info: ssm_d_conv       = 0
0.00.051.164 I print_info: ssm_d_inner      = 0
0.00.051.165 I print_info: ssm_d_state      = 0
0.00.051.165 I print_info: ssm_dt_rank      = 0
0.00.051.165 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.165 I print_info: model type       = 1.4B
0.00.051.165 I print_info: model params     = 1.41 B
0.00.051.165 I print_info: general.name     = 1.4B
0.00.051.166 I print_info: vocab type       = BPE
0.00.051.166 I print_info: n_vocab          = 50304
0.00.051.166 I print_info: n_merges         = 50009
0.00.051.167 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.167 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.167 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.167 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.167 I print_info: LF token         = 128 'Ä'
0.00.051.168 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.168 I print_info: max token length = 1024
0.00.772.466 I load_tensors: offloading 24 repeating layers to GPU
0.00.772.477 I load_tensors: offloading output layer to GPU
0.00.772.478 I load_tensors: offloaded 25/25 layers to GPU
0.00.772.510 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.772.511 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.773.915 I llama_init_from_model: n_seq_max     = 1
0.00.773.925 I llama_init_from_model: n_ctx         = 2048
0.00.773.926 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.773.926 I llama_init_from_model: n_batch       = 2048
0.00.773.927 I llama_init_from_model: n_ubatch      = 512
0.00.773.927 I llama_init_from_model: flash_attn    = 0
0.00.773.928 I llama_init_from_model: freq_base     = 10000.0
0.00.773.929 I llama_init_from_model: freq_scale    = 1
0.00.773.935 I ggml_metal_init: allocating
0.00.773.988 I ggml_metal_init: found device: Apple M4
0.00.774.004 I ggml_metal_init: picking default device: Apple M4
0.00.776.221 I ggml_metal_init: using embedded metal library
0.00.783.202 I ggml_metal_init: GPU name:   Apple M4
0.00.783.208 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.783.208 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.783.209 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.783.210 I ggml_metal_init: simdgroup reduction   = true
0.00.783.210 I ggml_metal_init: simdgroup matrix mul. = true
0.00.783.210 I ggml_metal_init: has residency sets    = true
0.00.783.211 I ggml_metal_init: has bfloat            = true
0.00.783.211 I ggml_metal_init: use bfloat            = true
0.00.783.212 I ggml_metal_init: hasUnifiedMemory      = true
0.00.783.213 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.800.856 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.856.631 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.856.643 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.856.673 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.861.009 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.861.010 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.861.010 I llama_init_from_model: graph nodes  = 967
0.00.861.011 I llama_init_from_model: graph splits = 2
0.00.861.019 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.861.147 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.861.148 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.922.453 I main: llama threadpool init, n_threads = 4
0.00.922.505 I 
0.00.922.532 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.922.533 I 
0.00.922.684 I sampler seed: 1234
0.00.922.689 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.922.702 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.922.702 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.922.703 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.764.610 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52437.22 tokens per second)
0.01.764.611 I llama_perf_context_print:        load time =     912.87 ms
0.01.764.611 I llama_perf_context_print: prompt eval time =      51.87 ms /     7 tokens (    7.41 ms per token,   134.95 tokens per second)
0.01.764.612 I llama_perf_context_print:        eval time =     787.10 ms /    63 runs   (   12.49 ms per token,    80.04 tokens per second)
0.01.764.613 I llama_perf_context_print:       total time =     843.01 ms /    70 tokens
0.01.764.923 I ggml_metal_free: deallocating

real	0m1.781s
user	0m0.110s
sys	0m0.213s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4581 (b636228c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.009.837 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.377 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.382 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.384 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.384 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.385 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.385 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.385 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.388 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.388 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.389 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.389 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.389 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.390 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.391 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.392 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.393 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.393 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.119 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.114 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.802 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.803 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.803 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.803 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.804 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.804 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.805 I llama_model_loader: - type  f32:  194 tensors
0.00.024.805 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.805 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.805 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.805 I print_info: file format = GGUF V3 (latest)
0.00.024.806 I print_info: file type   = Q2_K - Medium
0.00.024.810 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.567 I load: special tokens cache size = 25
0.00.038.485 I load: token to piece cache size = 0.2984 MB
0.00.038.487 I print_info: arch             = gptneox
0.00.038.487 I print_info: vocab_only       = 0
0.00.038.488 I print_info: n_ctx_train      = 2048
0.00.038.488 I print_info: n_embd           = 2048
0.00.038.488 I print_info: n_layer          = 24
0.00.038.491 I print_info: n_head           = 16
0.00.038.492 I print_info: n_head_kv        = 16
0.00.038.492 I print_info: n_rot            = 32
0.00.038.492 I print_info: n_swa            = 0
0.00.038.492 I print_info: n_embd_head_k    = 128
0.00.038.492 I print_info: n_embd_head_v    = 128
0.00.038.493 I print_info: n_gqa            = 1
0.00.038.494 I print_info: n_embd_k_gqa     = 2048
0.00.038.495 I print_info: n_embd_v_gqa     = 2048
0.00.038.495 I print_info: f_norm_eps       = 1.0e-05
0.00.038.498 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.498 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.498 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.498 I print_info: f_logit_scale    = 0.0e+00
0.00.038.499 I print_info: n_ff             = 8192
0.00.038.499 I print_info: n_expert         = 0
0.00.038.499 I print_info: n_expert_used    = 0
0.00.038.500 I print_info: causal attn      = 1
0.00.038.500 I print_info: pooling type     = 0
0.00.038.500 I print_info: rope type        = 2
0.00.038.500 I print_info: rope scaling     = linear
0.00.038.501 I print_info: freq_base_train  = 10000.0
0.00.038.501 I print_info: freq_scale_train = 1
0.00.038.501 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.501 I print_info: rope_finetuned   = unknown
0.00.038.501 I print_info: ssm_d_conv       = 0
0.00.038.501 I print_info: ssm_d_inner      = 0
0.00.038.503 I print_info: ssm_d_state      = 0
0.00.038.503 I print_info: ssm_dt_rank      = 0
0.00.038.503 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.504 I print_info: model type       = 1.4B
0.00.038.504 I print_info: model params     = 1.41 B
0.00.038.504 I print_info: general.name     = 1.4B
0.00.038.505 I print_info: vocab type       = BPE
0.00.038.505 I print_info: n_vocab          = 50304
0.00.038.505 I print_info: n_merges         = 50009
0.00.038.506 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.506 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.506 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.506 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.507 I print_info: LF token         = 128 'Ä'
0.00.038.507 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.507 I print_info: max token length = 1024
0.00.353.218 I load_tensors: offloading 24 repeating layers to GPU
0.00.353.229 I load_tensors: offloading output layer to GPU
0.00.353.230 I load_tensors: offloaded 25/25 layers to GPU
0.00.353.262 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.353.265 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.354.523 I llama_init_from_model: n_seq_max     = 1
0.00.354.527 I llama_init_from_model: n_ctx         = 2048
0.00.354.527 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.354.527 I llama_init_from_model: n_batch       = 2048
0.00.354.528 I llama_init_from_model: n_ubatch      = 512
0.00.354.528 I llama_init_from_model: flash_attn    = 0
0.00.354.530 I llama_init_from_model: freq_base     = 10000.0
0.00.354.533 I llama_init_from_model: freq_scale    = 1
0.00.354.539 I ggml_metal_init: allocating
0.00.354.602 I ggml_metal_init: found device: Apple M4
0.00.354.615 I ggml_metal_init: picking default device: Apple M4
0.00.356.155 I ggml_metal_init: using embedded metal library
0.00.361.601 I ggml_metal_init: GPU name:   Apple M4
0.00.361.615 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.361.616 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.361.617 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.361.617 I ggml_metal_init: simdgroup reduction   = true
0.00.361.618 I ggml_metal_init: simdgroup matrix mul. = true
0.00.361.618 I ggml_metal_init: has residency sets    = true
0.00.361.618 I ggml_metal_init: has bfloat            = true
0.00.361.618 I ggml_metal_init: use bfloat            = true
0.00.361.621 I ggml_metal_init: hasUnifiedMemory      = true
0.00.361.625 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.383.397 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.450.468 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.450.475 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.450.498 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.455.284 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.455.286 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.455.286 I llama_init_from_model: graph nodes  = 967
0.00.455.286 I llama_init_from_model: graph splits = 2
0.00.455.293 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.455.426 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.455.427 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.512.328 I main: llama threadpool init, n_threads = 4
0.00.512.370 I 
0.00.512.393 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.512.394 I 
0.00.512.573 I sampler seed: 1234
0.00.512.578 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.512.589 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.512.589 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.512.589 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.185.668 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54657.43 tokens per second)
0.01.185.668 I llama_perf_context_print:        load time =     501.60 ms
0.01.185.669 I llama_perf_context_print: prompt eval time =      35.88 ms /     7 tokens (    5.13 ms per token,   195.11 tokens per second)
0.01.185.670 I llama_perf_context_print:        eval time =     634.33 ms /    63 runs   (   10.07 ms per token,    99.32 tokens per second)
0.01.185.670 I llama_perf_context_print:       total time =     674.22 ms /    70 tokens
0.01.185.928 I ggml_metal_free: deallocating

real	0m1.205s
user	0m0.112s
sys	0m0.190s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4581 (b636228c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.008.695 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.366 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.371 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.373 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.374 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.375 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.375 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.375 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.376 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.377 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.377 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.377 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.378 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.378 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.379 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.381 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.382 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.382 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.135 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.181 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.930 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.931 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.931 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.932 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.932 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.932 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.933 I llama_model_loader: - type  f32:  194 tensors
0.00.024.933 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.933 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.934 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.934 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.934 I print_info: file format = GGUF V3 (latest)
0.00.024.935 I print_info: file type   = Q3_K - Medium
0.00.024.936 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.753 I load: special tokens cache size = 25
0.00.038.723 I load: token to piece cache size = 0.2984 MB
0.00.038.725 I print_info: arch             = gptneox
0.00.038.726 I print_info: vocab_only       = 0
0.00.038.726 I print_info: n_ctx_train      = 2048
0.00.038.726 I print_info: n_embd           = 2048
0.00.038.726 I print_info: n_layer          = 24
0.00.038.729 I print_info: n_head           = 16
0.00.038.730 I print_info: n_head_kv        = 16
0.00.038.730 I print_info: n_rot            = 32
0.00.038.732 I print_info: n_swa            = 0
0.00.038.733 I print_info: n_embd_head_k    = 128
0.00.038.733 I print_info: n_embd_head_v    = 128
0.00.038.734 I print_info: n_gqa            = 1
0.00.038.735 I print_info: n_embd_k_gqa     = 2048
0.00.038.735 I print_info: n_embd_v_gqa     = 2048
0.00.038.736 I print_info: f_norm_eps       = 1.0e-05
0.00.038.736 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.737 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.737 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.737 I print_info: f_logit_scale    = 0.0e+00
0.00.038.738 I print_info: n_ff             = 8192
0.00.038.738 I print_info: n_expert         = 0
0.00.038.738 I print_info: n_expert_used    = 0
0.00.038.739 I print_info: causal attn      = 1
0.00.038.740 I print_info: pooling type     = 0
0.00.038.740 I print_info: rope type        = 2
0.00.038.740 I print_info: rope scaling     = linear
0.00.038.741 I print_info: freq_base_train  = 10000.0
0.00.038.741 I print_info: freq_scale_train = 1
0.00.038.741 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.741 I print_info: rope_finetuned   = unknown
0.00.038.743 I print_info: ssm_d_conv       = 0
0.00.038.743 I print_info: ssm_d_inner      = 0
0.00.038.743 I print_info: ssm_d_state      = 0
0.00.038.743 I print_info: ssm_dt_rank      = 0
0.00.038.743 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.744 I print_info: model type       = 1.4B
0.00.038.744 I print_info: model params     = 1.41 B
0.00.038.744 I print_info: general.name     = 1.4B
0.00.038.745 I print_info: vocab type       = BPE
0.00.038.745 I print_info: n_vocab          = 50304
0.00.038.745 I print_info: n_merges         = 50009
0.00.038.745 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.745 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.747 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.747 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.747 I print_info: LF token         = 128 'Ä'
0.00.038.747 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.747 I print_info: max token length = 1024
0.00.438.277 I load_tensors: offloading 24 repeating layers to GPU
0.00.438.291 I load_tensors: offloading output layer to GPU
0.00.438.292 I load_tensors: offloaded 25/25 layers to GPU
0.00.438.323 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.438.325 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.439.875 I llama_init_from_model: n_seq_max     = 1
0.00.439.885 I llama_init_from_model: n_ctx         = 2048
0.00.439.885 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.439.886 I llama_init_from_model: n_batch       = 2048
0.00.439.886 I llama_init_from_model: n_ubatch      = 512
0.00.439.886 I llama_init_from_model: flash_attn    = 0
0.00.439.893 I llama_init_from_model: freq_base     = 10000.0
0.00.439.894 I llama_init_from_model: freq_scale    = 1
0.00.439.896 I ggml_metal_init: allocating
0.00.439.973 I ggml_metal_init: found device: Apple M4
0.00.439.988 I ggml_metal_init: picking default device: Apple M4
0.00.441.794 I ggml_metal_init: using embedded metal library
0.00.447.340 I ggml_metal_init: GPU name:   Apple M4
0.00.447.356 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.447.357 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.447.358 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.447.359 I ggml_metal_init: simdgroup reduction   = true
0.00.447.359 I ggml_metal_init: simdgroup matrix mul. = true
0.00.447.359 I ggml_metal_init: has residency sets    = true
0.00.447.360 I ggml_metal_init: has bfloat            = true
0.00.447.360 I ggml_metal_init: use bfloat            = true
0.00.447.364 I ggml_metal_init: hasUnifiedMemory      = true
0.00.447.369 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.467.285 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.522.500 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.522.507 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.522.538 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.526.961 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.526.964 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.526.964 I llama_init_from_model: graph nodes  = 967
0.00.526.964 I llama_init_from_model: graph splits = 2
0.00.526.969 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.527.105 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.527.105 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.584.220 I main: llama threadpool init, n_threads = 4
0.00.584.263 I 
0.00.584.288 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.584.288 I 
0.00.584.451 I sampler seed: 1234
0.00.584.456 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.584.467 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.584.468 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.584.469 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.334.800 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53951.37 tokens per second)
0.01.334.800 I llama_perf_context_print:        load time =     574.62 ms
0.01.334.801 I llama_perf_context_print: prompt eval time =      49.35 ms /     7 tokens (    7.05 ms per token,   141.86 tokens per second)
0.01.334.802 I llama_perf_context_print:        eval time =     698.13 ms /    63 runs   (   11.08 ms per token,    90.24 tokens per second)
0.01.334.802 I llama_perf_context_print:       total time =     751.48 ms /    70 tokens
0.01.335.035 I ggml_metal_free: deallocating

real	0m1.351s
user	0m0.109s
sys	0m0.177s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4581 (b636228c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.695 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.467 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.472 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.475 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.476 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.476 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.476 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.477 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.478 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.478 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.478 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.479 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.479 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.479 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.480 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.483 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.483 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.483 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.194 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.173 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.819 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.820 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.820 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.820 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.821 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.821 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.821 I llama_model_loader: - type  f32:  194 tensors
0.00.024.822 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.822 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.822 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.822 I print_info: file format = GGUF V3 (latest)
0.00.024.823 I print_info: file type   = Q4_K - Medium
0.00.024.823 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.574 I load: special tokens cache size = 25
0.00.038.508 I load: token to piece cache size = 0.2984 MB
0.00.038.511 I print_info: arch             = gptneox
0.00.038.511 I print_info: vocab_only       = 0
0.00.038.512 I print_info: n_ctx_train      = 2048
0.00.038.512 I print_info: n_embd           = 2048
0.00.038.512 I print_info: n_layer          = 24
0.00.038.515 I print_info: n_head           = 16
0.00.038.516 I print_info: n_head_kv        = 16
0.00.038.516 I print_info: n_rot            = 32
0.00.038.517 I print_info: n_swa            = 0
0.00.038.517 I print_info: n_embd_head_k    = 128
0.00.038.517 I print_info: n_embd_head_v    = 128
0.00.038.518 I print_info: n_gqa            = 1
0.00.038.519 I print_info: n_embd_k_gqa     = 2048
0.00.038.519 I print_info: n_embd_v_gqa     = 2048
0.00.038.520 I print_info: f_norm_eps       = 1.0e-05
0.00.038.520 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.520 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.521 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.521 I print_info: f_logit_scale    = 0.0e+00
0.00.038.523 I print_info: n_ff             = 8192
0.00.038.524 I print_info: n_expert         = 0
0.00.038.524 I print_info: n_expert_used    = 0
0.00.038.524 I print_info: causal attn      = 1
0.00.038.525 I print_info: pooling type     = 0
0.00.038.527 I print_info: rope type        = 2
0.00.038.527 I print_info: rope scaling     = linear
0.00.038.527 I print_info: freq_base_train  = 10000.0
0.00.038.527 I print_info: freq_scale_train = 1
0.00.038.528 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.528 I print_info: rope_finetuned   = unknown
0.00.038.528 I print_info: ssm_d_conv       = 0
0.00.038.528 I print_info: ssm_d_inner      = 0
0.00.038.528 I print_info: ssm_d_state      = 0
0.00.038.528 I print_info: ssm_dt_rank      = 0
0.00.038.529 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.529 I print_info: model type       = 1.4B
0.00.038.529 I print_info: model params     = 1.41 B
0.00.038.529 I print_info: general.name     = 1.4B
0.00.038.530 I print_info: vocab type       = BPE
0.00.038.530 I print_info: n_vocab          = 50304
0.00.038.530 I print_info: n_merges         = 50009
0.00.038.530 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.531 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.531 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.531 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.531 I print_info: LF token         = 128 'Ä'
0.00.038.532 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.533 I print_info: max token length = 1024
0.00.516.704 I load_tensors: offloading 24 repeating layers to GPU
0.00.516.718 I load_tensors: offloading output layer to GPU
0.00.516.719 I load_tensors: offloaded 25/25 layers to GPU
0.00.516.754 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.516.755 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.518.298 I llama_init_from_model: n_seq_max     = 1
0.00.518.304 I llama_init_from_model: n_ctx         = 2048
0.00.518.305 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.518.305 I llama_init_from_model: n_batch       = 2048
0.00.518.305 I llama_init_from_model: n_ubatch      = 512
0.00.518.306 I llama_init_from_model: flash_attn    = 0
0.00.518.308 I llama_init_from_model: freq_base     = 10000.0
0.00.518.313 I llama_init_from_model: freq_scale    = 1
0.00.518.331 I ggml_metal_init: allocating
0.00.518.410 I ggml_metal_init: found device: Apple M4
0.00.518.424 I ggml_metal_init: picking default device: Apple M4
0.00.520.265 I ggml_metal_init: using embedded metal library
0.00.526.945 I ggml_metal_init: GPU name:   Apple M4
0.00.526.950 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.526.951 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.526.952 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.526.952 I ggml_metal_init: simdgroup reduction   = true
0.00.526.952 I ggml_metal_init: simdgroup matrix mul. = true
0.00.526.953 I ggml_metal_init: has residency sets    = true
0.00.526.953 I ggml_metal_init: has bfloat            = true
0.00.526.953 I ggml_metal_init: use bfloat            = true
0.00.526.954 I ggml_metal_init: hasUnifiedMemory      = true
0.00.526.955 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.544.507 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.604.289 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.604.295 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.604.320 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.608.408 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.608.410 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.608.411 I llama_init_from_model: graph nodes  = 967
0.00.608.411 I llama_init_from_model: graph splits = 2
0.00.608.417 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.608.546 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.608.547 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.666.584 I main: llama threadpool init, n_threads = 4
0.00.666.625 I 
0.00.666.649 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.666.649 I 
0.00.666.813 I sampler seed: 1234
0.00.666.817 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.666.828 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.666.829 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.666.829 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.464.002 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49271.34 tokens per second)
0.01.464.002 I llama_perf_context_print:        load time =     657.00 ms
0.01.464.003 I llama_perf_context_print: prompt eval time =      57.57 ms /     7 tokens (    8.22 ms per token,   121.60 tokens per second)
0.01.464.004 I llama_perf_context_print:        eval time =     736.52 ms /    63 runs   (   11.69 ms per token,    85.54 tokens per second)
0.01.464.004 I llama_perf_context_print:       total time =     798.30 ms /    70 tokens
0.01.464.251 I ggml_metal_free: deallocating

real	0m1.480s
user	0m0.108s
sys	0m0.201s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4581 (b636228c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.010.189 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.687 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.691 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.693 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.697 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.698 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.698 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.698 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.699 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.699 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.700 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.700 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.700 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.701 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.703 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.705 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.705 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.706 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.415 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.429 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.045 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.046 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.046 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.047 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.047 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.047 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.048 I llama_model_loader: - type  f32:  194 tensors
0.00.026.048 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.048 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.049 I print_info: file format = GGUF V3 (latest)
0.00.026.050 I print_info: file type   = Q5_K - Medium
0.00.026.050 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.802 I load: special tokens cache size = 25
0.00.039.464 I load: token to piece cache size = 0.2984 MB
0.00.039.467 I print_info: arch             = gptneox
0.00.039.467 I print_info: vocab_only       = 0
0.00.039.467 I print_info: n_ctx_train      = 2048
0.00.039.467 I print_info: n_embd           = 2048
0.00.039.468 I print_info: n_layer          = 24
0.00.039.470 I print_info: n_head           = 16
0.00.039.471 I print_info: n_head_kv        = 16
0.00.039.471 I print_info: n_rot            = 32
0.00.039.471 I print_info: n_swa            = 0
0.00.039.472 I print_info: n_embd_head_k    = 128
0.00.039.472 I print_info: n_embd_head_v    = 128
0.00.039.472 I print_info: n_gqa            = 1
0.00.039.473 I print_info: n_embd_k_gqa     = 2048
0.00.039.476 I print_info: n_embd_v_gqa     = 2048
0.00.039.476 I print_info: f_norm_eps       = 1.0e-05
0.00.039.477 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.477 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.477 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.477 I print_info: f_logit_scale    = 0.0e+00
0.00.039.478 I print_info: n_ff             = 8192
0.00.039.478 I print_info: n_expert         = 0
0.00.039.479 I print_info: n_expert_used    = 0
0.00.039.479 I print_info: causal attn      = 1
0.00.039.479 I print_info: pooling type     = 0
0.00.039.480 I print_info: rope type        = 2
0.00.039.481 I print_info: rope scaling     = linear
0.00.039.482 I print_info: freq_base_train  = 10000.0
0.00.039.482 I print_info: freq_scale_train = 1
0.00.039.482 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.482 I print_info: rope_finetuned   = unknown
0.00.039.483 I print_info: ssm_d_conv       = 0
0.00.039.483 I print_info: ssm_d_inner      = 0
0.00.039.483 I print_info: ssm_d_state      = 0
0.00.039.483 I print_info: ssm_dt_rank      = 0
0.00.039.483 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.483 I print_info: model type       = 1.4B
0.00.039.484 I print_info: model params     = 1.41 B
0.00.039.484 I print_info: general.name     = 1.4B
0.00.039.488 I print_info: vocab type       = BPE
0.00.039.488 I print_info: n_vocab          = 50304
0.00.039.488 I print_info: n_merges         = 50009
0.00.039.489 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.489 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.489 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.489 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.489 I print_info: LF token         = 128 'Ä'
0.00.039.490 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.490 I print_info: max token length = 1024
0.00.612.163 I load_tensors: offloading 24 repeating layers to GPU
0.00.612.182 I load_tensors: offloading output layer to GPU
0.00.612.183 I load_tensors: offloaded 25/25 layers to GPU
0.00.612.217 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.612.219 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.613.452 I llama_init_from_model: n_seq_max     = 1
0.00.613.463 I llama_init_from_model: n_ctx         = 2048
0.00.613.464 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.613.464 I llama_init_from_model: n_batch       = 2048
0.00.613.465 I llama_init_from_model: n_ubatch      = 512
0.00.613.465 I llama_init_from_model: flash_attn    = 0
0.00.613.468 I llama_init_from_model: freq_base     = 10000.0
0.00.613.468 I llama_init_from_model: freq_scale    = 1
0.00.613.475 I ggml_metal_init: allocating
0.00.613.553 I ggml_metal_init: found device: Apple M4
0.00.613.568 I ggml_metal_init: picking default device: Apple M4
0.00.615.428 I ggml_metal_init: using embedded metal library
0.00.621.931 I ggml_metal_init: GPU name:   Apple M4
0.00.621.936 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.621.937 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.621.938 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.621.939 I ggml_metal_init: simdgroup reduction   = true
0.00.621.939 I ggml_metal_init: simdgroup matrix mul. = true
0.00.621.940 I ggml_metal_init: has residency sets    = true
0.00.621.940 I ggml_metal_init: has bfloat            = true
0.00.621.940 I ggml_metal_init: use bfloat            = true
0.00.621.942 I ggml_metal_init: hasUnifiedMemory      = true
0.00.621.945 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.640.167 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.698.836 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.698.845 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.698.873 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.703.578 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.703.581 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.703.581 I llama_init_from_model: graph nodes  = 967
0.00.703.581 I llama_init_from_model: graph splits = 2
0.00.703.586 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.703.714 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.703.715 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.762.195 I main: llama threadpool init, n_threads = 4
0.00.762.239 I 
0.00.762.261 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.762.262 I 
0.00.762.376 I sampler seed: 1234
0.00.762.380 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.762.390 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.762.390 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.762.392 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.653.978 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54239.88 tokens per second)
0.01.653.978 I llama_perf_context_print:        load time =     751.08 ms
0.01.653.979 I llama_perf_context_print: prompt eval time =      63.39 ms /     7 tokens (    9.06 ms per token,   110.43 tokens per second)
0.01.653.980 I llama_perf_context_print:        eval time =     825.27 ms /    63 runs   (   13.10 ms per token,    76.34 tokens per second)
0.01.653.980 I llama_perf_context_print:       total time =     892.70 ms /    70 tokens
0.01.654.210 I ggml_metal_free: deallocating

real	0m1.672s
user	0m0.111s
sys	0m0.219s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4581 (b636228c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.039 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.769 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.774 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.776 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.776 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.777 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.777 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.777 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.778 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.779 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.779 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.779 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.780 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.780 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.781 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.782 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.782 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.783 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.524 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.569 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.274 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.275 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.276 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.276 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.276 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.277 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.277 I llama_model_loader: - type  f32:  194 tensors
0.00.026.278 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.278 I print_info: file format = GGUF V3 (latest)
0.00.026.279 I print_info: file type   = Q6_K
0.00.026.279 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.034.033 I load: special tokens cache size = 25
0.00.039.782 I load: token to piece cache size = 0.2984 MB
0.00.039.785 I print_info: arch             = gptneox
0.00.039.785 I print_info: vocab_only       = 0
0.00.039.786 I print_info: n_ctx_train      = 2048
0.00.039.786 I print_info: n_embd           = 2048
0.00.039.786 I print_info: n_layer          = 24
0.00.039.789 I print_info: n_head           = 16
0.00.039.790 I print_info: n_head_kv        = 16
0.00.039.790 I print_info: n_rot            = 32
0.00.039.790 I print_info: n_swa            = 0
0.00.039.790 I print_info: n_embd_head_k    = 128
0.00.039.792 I print_info: n_embd_head_v    = 128
0.00.039.793 I print_info: n_gqa            = 1
0.00.039.794 I print_info: n_embd_k_gqa     = 2048
0.00.039.795 I print_info: n_embd_v_gqa     = 2048
0.00.039.795 I print_info: f_norm_eps       = 1.0e-05
0.00.039.796 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.796 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.796 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.796 I print_info: f_logit_scale    = 0.0e+00
0.00.039.797 I print_info: n_ff             = 8192
0.00.039.797 I print_info: n_expert         = 0
0.00.039.797 I print_info: n_expert_used    = 0
0.00.039.797 I print_info: causal attn      = 1
0.00.039.797 I print_info: pooling type     = 0
0.00.039.797 I print_info: rope type        = 2
0.00.039.798 I print_info: rope scaling     = linear
0.00.039.798 I print_info: freq_base_train  = 10000.0
0.00.039.802 I print_info: freq_scale_train = 1
0.00.039.803 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.803 I print_info: rope_finetuned   = unknown
0.00.039.803 I print_info: ssm_d_conv       = 0
0.00.039.803 I print_info: ssm_d_inner      = 0
0.00.039.803 I print_info: ssm_d_state      = 0
0.00.039.804 I print_info: ssm_dt_rank      = 0
0.00.039.804 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.804 I print_info: model type       = 1.4B
0.00.039.804 I print_info: model params     = 1.41 B
0.00.039.805 I print_info: general.name     = 1.4B
0.00.039.806 I print_info: vocab type       = BPE
0.00.039.806 I print_info: n_vocab          = 50304
0.00.039.806 I print_info: n_merges         = 50009
0.00.039.806 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.807 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.807 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.809 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.810 I print_info: LF token         = 128 'Ä'
0.00.039.810 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.810 I print_info: max token length = 1024
0.00.653.193 I load_tensors: offloading 24 repeating layers to GPU
0.00.653.212 I load_tensors: offloading output layer to GPU
0.00.653.213 I load_tensors: offloaded 25/25 layers to GPU
0.00.653.255 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.653.256 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.654.514 I llama_init_from_model: n_seq_max     = 1
0.00.654.520 I llama_init_from_model: n_ctx         = 2048
0.00.654.521 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.654.521 I llama_init_from_model: n_batch       = 2048
0.00.654.522 I llama_init_from_model: n_ubatch      = 512
0.00.654.522 I llama_init_from_model: flash_attn    = 0
0.00.654.524 I llama_init_from_model: freq_base     = 10000.0
0.00.654.524 I llama_init_from_model: freq_scale    = 1
0.00.654.527 I ggml_metal_init: allocating
0.00.654.618 I ggml_metal_init: found device: Apple M4
0.00.654.633 I ggml_metal_init: picking default device: Apple M4
0.00.656.535 I ggml_metal_init: using embedded metal library
0.00.663.122 I ggml_metal_init: GPU name:   Apple M4
0.00.663.126 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.663.127 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.663.128 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.663.129 I ggml_metal_init: simdgroup reduction   = true
0.00.663.129 I ggml_metal_init: simdgroup matrix mul. = true
0.00.663.129 I ggml_metal_init: has residency sets    = true
0.00.663.130 I ggml_metal_init: has bfloat            = true
0.00.663.130 I ggml_metal_init: use bfloat            = true
0.00.663.131 I ggml_metal_init: hasUnifiedMemory      = true
0.00.663.132 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.680.758 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.739.190 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.739.197 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.739.218 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.743.456 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.743.458 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.743.459 I llama_init_from_model: graph nodes  = 967
0.00.743.459 I llama_init_from_model: graph splits = 2
0.00.743.464 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.743.593 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.743.594 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.801.163 I main: llama threadpool init, n_threads = 4
0.00.801.210 I 
0.00.801.232 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.801.232 I 
0.00.801.364 I sampler seed: 1234
0.00.801.369 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.801.388 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.801.389 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.801.389 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.722.041 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53951.37 tokens per second)
0.01.722.041 I llama_perf_context_print:        load time =     791.22 ms
0.01.722.042 I llama_perf_context_print: prompt eval time =      54.26 ms /     7 tokens (    7.75 ms per token,   129.01 tokens per second)
0.01.722.043 I llama_perf_context_print:        eval time =     863.44 ms /    63 runs   (   13.71 ms per token,    72.96 tokens per second)
0.01.722.043 I llama_perf_context_print:       total time =     921.78 ms /    70 tokens
0.01.722.331 I ggml_metal_free: deallocating

real	0m1.738s
user	0m0.111s
sys	0m0.218s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.905 I build: 4581 (b636228c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.850 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.039.016 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.023 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.025 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.026 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.026 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.027 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.028 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.029 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.030 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.030 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.031 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.032 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.032 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.033 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.035 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.036 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.036 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.031 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.888 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.217 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.219 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.219 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.220 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.220 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.221 I llama_model_loader: - type  f32:  194 tensors
0.00.054.221 I llama_model_loader: - type  f16:   98 tensors
0.00.054.222 I print_info: file format = GGUF V3 (latest)
0.00.054.223 I print_info: file type   = all F32 (guessed)
0.00.054.224 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.065.544 I load: special tokens cache size = 25
0.00.072.838 I load: token to piece cache size = 0.2984 MB
0.00.072.841 I print_info: arch             = gptneox
0.00.072.841 I print_info: vocab_only       = 0
0.00.072.841 I print_info: n_ctx_train      = 2048
0.00.072.841 I print_info: n_embd           = 2048
0.00.072.841 I print_info: n_layer          = 24
0.00.072.845 I print_info: n_head           = 16
0.00.072.846 I print_info: n_head_kv        = 16
0.00.072.846 I print_info: n_rot            = 32
0.00.072.846 I print_info: n_swa            = 0
0.00.072.846 I print_info: n_embd_head_k    = 128
0.00.072.847 I print_info: n_embd_head_v    = 128
0.00.072.847 I print_info: n_gqa            = 1
0.00.072.848 I print_info: n_embd_k_gqa     = 2048
0.00.072.849 I print_info: n_embd_v_gqa     = 2048
0.00.072.849 I print_info: f_norm_eps       = 1.0e-05
0.00.072.850 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.072.850 I print_info: f_clamp_kqv      = 0.0e+00
0.00.072.850 I print_info: f_max_alibi_bias = 0.0e+00
0.00.072.850 I print_info: f_logit_scale    = 0.0e+00
0.00.072.851 I print_info: n_ff             = 8192
0.00.072.851 I print_info: n_expert         = 0
0.00.072.851 I print_info: n_expert_used    = 0
0.00.072.851 I print_info: causal attn      = 1
0.00.072.851 I print_info: pooling type     = 0
0.00.072.851 I print_info: rope type        = 2
0.00.072.852 I print_info: rope scaling     = linear
0.00.072.854 I print_info: freq_base_train  = 10000.0
0.00.072.855 I print_info: freq_scale_train = 1
0.00.072.855 I print_info: n_ctx_orig_yarn  = 2048
0.00.072.855 I print_info: rope_finetuned   = unknown
0.00.072.855 I print_info: ssm_d_conv       = 0
0.00.072.855 I print_info: ssm_d_inner      = 0
0.00.072.856 I print_info: ssm_d_state      = 0
0.00.072.856 I print_info: ssm_dt_rank      = 0
0.00.072.856 I print_info: ssm_dt_b_c_rms   = 0
0.00.072.856 I print_info: model type       = 1.4B
0.00.072.856 I print_info: model params     = 1.41 B
0.00.072.857 I print_info: general.name     = 1.4B
0.00.072.857 I print_info: vocab type       = BPE
0.00.072.858 I print_info: n_vocab          = 50304
0.00.072.858 I print_info: n_merges         = 50009
0.00.072.858 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.072.858 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.072.858 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.072.858 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.072.859 I print_info: LF token         = 128 'Ä'
0.00.072.859 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.072.859 I print_info: max token length = 1024
0.01.491.126 I load_tensors: offloading 24 repeating layers to GPU
0.01.491.131 I load_tensors: offloading output layer to GPU
0.01.491.132 I load_tensors: offloaded 25/25 layers to GPU
0.01.491.160 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.491.162 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.492.041 I llama_init_from_model: n_seq_max     = 1
0.01.492.042 I llama_init_from_model: n_ctx         = 128
0.01.492.042 I llama_init_from_model: n_ctx_per_seq = 128
0.01.492.043 I llama_init_from_model: n_batch       = 128
0.01.492.043 I llama_init_from_model: n_ubatch      = 128
0.01.492.043 I llama_init_from_model: flash_attn    = 0
0.01.492.044 I llama_init_from_model: freq_base     = 10000.0
0.01.492.044 I llama_init_from_model: freq_scale    = 1
0.01.492.045 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.492.047 I ggml_metal_init: allocating
0.01.492.111 I ggml_metal_init: found device: Apple M4
0.01.492.119 I ggml_metal_init: picking default device: Apple M4
0.01.493.223 I ggml_metal_init: using embedded metal library
0.01.497.336 I ggml_metal_init: GPU name:   Apple M4
0.01.497.338 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.497.339 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.497.340 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.497.340 I ggml_metal_init: simdgroup reduction   = true
0.01.497.340 I ggml_metal_init: simdgroup matrix mul. = true
0.01.497.340 I ggml_metal_init: has residency sets    = true
0.01.497.340 I ggml_metal_init: has bfloat            = true
0.01.497.341 I ggml_metal_init: use bfloat            = true
0.01.497.341 I ggml_metal_init: hasUnifiedMemory      = true
0.01.497.342 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.508.813 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.510.616 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.510.619 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.510.637 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.512.363 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.512.364 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.512.364 I llama_init_from_model: graph nodes  = 967
0.01.512.364 I llama_init_from_model: graph splits = 2
0.01.512.366 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.512.366 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.548.708 I 
0.01.548.746 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.548.767 I perplexity: tokenizing the input ..
0.01.554.061 I perplexity: tokenization took 5.293 ms
0.01.554.084 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.673.346 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.674.699 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.674.718 I llama_perf_context_print:        load time =    1525.85 ms
0.01.674.720 I llama_perf_context_print: prompt eval time =     118.91 ms /   128 tokens (    0.93 ms per token,  1076.49 tokens per second)
0.01.674.724 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.674.725 I llama_perf_context_print:       total time =     126.01 ms /   129 tokens
0.01.675.167 I ggml_metal_free: deallocating

real	0m1.887s
user	0m0.097s
sys	0m0.285s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.281 I build: 4581 (b636228c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.341 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.534 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.540 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.547 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.548 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.548 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.548 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.549 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.550 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.550 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.550 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.551 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.551 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.551 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.552 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.554 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.554 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.554 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.496 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.580 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.401 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.402 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.402 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.403 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.403 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.404 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.404 I llama_model_loader: - type  f32:  194 tensors
0.00.026.404 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.405 I print_info: file format = GGUF V3 (latest)
0.00.026.406 I print_info: file type   = Q8_0
0.00.026.407 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.034.442 I load: special tokens cache size = 25
0.00.041.294 I load: token to piece cache size = 0.2984 MB
0.00.041.298 I print_info: arch             = gptneox
0.00.041.298 I print_info: vocab_only       = 0
0.00.041.299 I print_info: n_ctx_train      = 2048
0.00.041.299 I print_info: n_embd           = 2048
0.00.041.299 I print_info: n_layer          = 24
0.00.041.304 I print_info: n_head           = 16
0.00.041.305 I print_info: n_head_kv        = 16
0.00.041.305 I print_info: n_rot            = 32
0.00.041.305 I print_info: n_swa            = 0
0.00.041.305 I print_info: n_embd_head_k    = 128
0.00.041.306 I print_info: n_embd_head_v    = 128
0.00.041.306 I print_info: n_gqa            = 1
0.00.041.307 I print_info: n_embd_k_gqa     = 2048
0.00.041.308 I print_info: n_embd_v_gqa     = 2048
0.00.041.309 I print_info: f_norm_eps       = 1.0e-05
0.00.041.309 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.309 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.309 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.310 I print_info: f_logit_scale    = 0.0e+00
0.00.041.310 I print_info: n_ff             = 8192
0.00.041.311 I print_info: n_expert         = 0
0.00.041.311 I print_info: n_expert_used    = 0
0.00.041.311 I print_info: causal attn      = 1
0.00.041.314 I print_info: pooling type     = 0
0.00.041.314 I print_info: rope type        = 2
0.00.041.314 I print_info: rope scaling     = linear
0.00.041.315 I print_info: freq_base_train  = 10000.0
0.00.041.315 I print_info: freq_scale_train = 1
0.00.041.315 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.316 I print_info: rope_finetuned   = unknown
0.00.041.316 I print_info: ssm_d_conv       = 0
0.00.041.316 I print_info: ssm_d_inner      = 0
0.00.041.316 I print_info: ssm_d_state      = 0
0.00.041.316 I print_info: ssm_dt_rank      = 0
0.00.041.316 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.316 I print_info: model type       = 1.4B
0.00.041.317 I print_info: model params     = 1.41 B
0.00.041.317 I print_info: general.name     = 1.4B
0.00.041.318 I print_info: vocab type       = BPE
0.00.041.318 I print_info: n_vocab          = 50304
0.00.041.318 I print_info: n_merges         = 50009
0.00.041.318 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.319 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.319 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.319 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.320 I print_info: LF token         = 128 'Ä'
0.00.041.320 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.320 I print_info: max token length = 1024
0.00.936.218 I load_tensors: offloading 24 repeating layers to GPU
0.00.936.221 I load_tensors: offloading output layer to GPU
0.00.936.221 I load_tensors: offloaded 25/25 layers to GPU
0.00.936.237 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.936.238 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.937.235 I llama_init_from_model: n_seq_max     = 1
0.00.937.237 I llama_init_from_model: n_ctx         = 128
0.00.937.238 I llama_init_from_model: n_ctx_per_seq = 128
0.00.937.238 I llama_init_from_model: n_batch       = 128
0.00.937.239 I llama_init_from_model: n_ubatch      = 128
0.00.937.239 I llama_init_from_model: flash_attn    = 0
0.00.937.240 I llama_init_from_model: freq_base     = 10000.0
0.00.937.240 I llama_init_from_model: freq_scale    = 1
0.00.937.241 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.937.242 I ggml_metal_init: allocating
0.00.937.279 I ggml_metal_init: found device: Apple M4
0.00.937.290 I ggml_metal_init: picking default device: Apple M4
0.00.938.429 I ggml_metal_init: using embedded metal library
0.00.943.538 I ggml_metal_init: GPU name:   Apple M4
0.00.943.541 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.943.541 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.943.542 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.943.542 I ggml_metal_init: simdgroup reduction   = true
0.00.943.543 I ggml_metal_init: simdgroup matrix mul. = true
0.00.943.543 I ggml_metal_init: has residency sets    = true
0.00.943.543 I ggml_metal_init: has bfloat            = true
0.00.943.543 I ggml_metal_init: use bfloat            = true
0.00.943.544 I ggml_metal_init: hasUnifiedMemory      = true
0.00.943.548 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.958.609 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.962.007 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.962.011 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.962.037 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.965.172 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.965.173 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.965.174 I llama_init_from_model: graph nodes  = 967
0.00.965.174 I llama_init_from_model: graph splits = 2
0.00.965.177 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.965.177 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.990.542 I 
0.00.990.619 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.990.638 I perplexity: tokenizing the input ..
0.00.998.077 I perplexity: tokenization took 7.434 ms
0.00.998.097 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.123.592 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.124.922 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.124.934 I llama_perf_context_print:        load time =     980.19 ms
0.01.124.935 I llama_perf_context_print: prompt eval time =     124.61 ms /   128 tokens (    0.97 ms per token,  1027.20 tokens per second)
0.01.124.936 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.124.936 I llama_perf_context_print:       total time =     134.40 ms /   129 tokens
0.01.125.338 I ggml_metal_free: deallocating

real	0m1.143s
user	0m0.077s
sys	0m0.185s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.273 I build: 4581 (b636228c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.019 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.350 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.357 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.359 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.359 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.360 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.360 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.360 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.361 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.362 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.362 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.363 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.363 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.363 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.364 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.366 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.366 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.367 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.092 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.112 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.936 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.937 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.938 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.938 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.938 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.939 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.939 I llama_model_loader: - type  f32:  194 tensors
0.00.025.940 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.940 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.941 I print_info: file format = GGUF V3 (latest)
0.00.025.941 I print_info: file type   = Q4_0
0.00.025.943 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.033.933 I load: special tokens cache size = 25
0.00.039.790 I load: token to piece cache size = 0.2984 MB
0.00.039.793 I print_info: arch             = gptneox
0.00.039.793 I print_info: vocab_only       = 0
0.00.039.794 I print_info: n_ctx_train      = 2048
0.00.039.794 I print_info: n_embd           = 2048
0.00.039.794 I print_info: n_layer          = 24
0.00.039.798 I print_info: n_head           = 16
0.00.039.798 I print_info: n_head_kv        = 16
0.00.039.799 I print_info: n_rot            = 32
0.00.039.799 I print_info: n_swa            = 0
0.00.039.799 I print_info: n_embd_head_k    = 128
0.00.039.799 I print_info: n_embd_head_v    = 128
0.00.039.800 I print_info: n_gqa            = 1
0.00.039.801 I print_info: n_embd_k_gqa     = 2048
0.00.039.801 I print_info: n_embd_v_gqa     = 2048
0.00.039.802 I print_info: f_norm_eps       = 1.0e-05
0.00.039.802 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.803 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.803 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.803 I print_info: f_logit_scale    = 0.0e+00
0.00.039.803 I print_info: n_ff             = 8192
0.00.039.807 I print_info: n_expert         = 0
0.00.039.807 I print_info: n_expert_used    = 0
0.00.039.807 I print_info: causal attn      = 1
0.00.039.807 I print_info: pooling type     = 0
0.00.039.807 I print_info: rope type        = 2
0.00.039.808 I print_info: rope scaling     = linear
0.00.039.808 I print_info: freq_base_train  = 10000.0
0.00.039.808 I print_info: freq_scale_train = 1
0.00.039.809 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.809 I print_info: rope_finetuned   = unknown
0.00.039.809 I print_info: ssm_d_conv       = 0
0.00.039.809 I print_info: ssm_d_inner      = 0
0.00.039.809 I print_info: ssm_d_state      = 0
0.00.039.810 I print_info: ssm_dt_rank      = 0
0.00.039.810 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.810 I print_info: model type       = 1.4B
0.00.039.810 I print_info: model params     = 1.41 B
0.00.039.810 I print_info: general.name     = 1.4B
0.00.039.811 I print_info: vocab type       = BPE
0.00.039.811 I print_info: n_vocab          = 50304
0.00.039.811 I print_info: n_merges         = 50009
0.00.039.812 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.812 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.812 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.812 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.813 I print_info: LF token         = 128 'Ä'
0.00.039.813 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.813 I print_info: max token length = 1024
0.00.577.667 I load_tensors: offloading 24 repeating layers to GPU
0.00.577.681 I load_tensors: offloading output layer to GPU
0.00.577.681 I load_tensors: offloaded 25/25 layers to GPU
0.00.577.718 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.577.719 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.578.815 I llama_init_from_model: n_seq_max     = 1
0.00.578.820 I llama_init_from_model: n_ctx         = 128
0.00.578.821 I llama_init_from_model: n_ctx_per_seq = 128
0.00.578.825 I llama_init_from_model: n_batch       = 128
0.00.578.825 I llama_init_from_model: n_ubatch      = 128
0.00.578.826 I llama_init_from_model: flash_attn    = 0
0.00.578.828 I llama_init_from_model: freq_base     = 10000.0
0.00.578.829 I llama_init_from_model: freq_scale    = 1
0.00.578.833 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.578.835 I ggml_metal_init: allocating
0.00.578.942 I ggml_metal_init: found device: Apple M4
0.00.578.957 I ggml_metal_init: picking default device: Apple M4
0.00.580.870 I ggml_metal_init: using embedded metal library
0.00.586.480 I ggml_metal_init: GPU name:   Apple M4
0.00.586.498 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.586.498 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.586.499 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.586.500 I ggml_metal_init: simdgroup reduction   = true
0.00.586.500 I ggml_metal_init: simdgroup matrix mul. = true
0.00.586.500 I ggml_metal_init: has residency sets    = true
0.00.586.501 I ggml_metal_init: has bfloat            = true
0.00.586.501 I ggml_metal_init: use bfloat            = true
0.00.586.503 I ggml_metal_init: hasUnifiedMemory      = true
0.00.586.507 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.606.476 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.609.953 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.609.960 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.609.996 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.613.438 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.613.440 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.613.441 I llama_init_from_model: graph nodes  = 967
0.00.613.441 I llama_init_from_model: graph splits = 2
0.00.613.445 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.613.448 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.640.397 I 
0.00.640.474 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.640.494 I perplexity: tokenizing the input ..
0.00.647.423 I perplexity: tokenization took 6.925 ms
0.00.647.443 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.784.228 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.785.560 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.785.575 I llama_perf_context_print:        load time =     630.37 ms
0.00.785.576 I llama_perf_context_print: prompt eval time =     135.99 ms /   128 tokens (    1.06 ms per token,   941.26 tokens per second)
0.00.785.577 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.785.577 I llama_perf_context_print:       total time =     145.18 ms /   129 tokens
0.00.785.937 I ggml_metal_free: deallocating

real	0m0.802s
user	0m0.079s
sys	0m0.125s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4581 (b636228c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.940 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.846 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.851 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.856 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.857 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.857 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.858 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.859 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.860 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.860 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.861 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.861 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.861 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.862 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.862 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.864 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.864 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.864 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.661 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.701 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.516 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.517 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.518 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.518 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.518 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.519 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.519 I llama_model_loader: - type  f32:  194 tensors
0.00.024.519 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.520 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.520 I print_info: file format = GGUF V3 (latest)
0.00.024.521 I print_info: file type   = Q4_1
0.00.024.521 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.287 I load: special tokens cache size = 25
0.00.038.179 I load: token to piece cache size = 0.2984 MB
0.00.038.182 I print_info: arch             = gptneox
0.00.038.182 I print_info: vocab_only       = 0
0.00.038.182 I print_info: n_ctx_train      = 2048
0.00.038.182 I print_info: n_embd           = 2048
0.00.038.182 I print_info: n_layer          = 24
0.00.038.185 I print_info: n_head           = 16
0.00.038.186 I print_info: n_head_kv        = 16
0.00.038.188 I print_info: n_rot            = 32
0.00.038.188 I print_info: n_swa            = 0
0.00.038.189 I print_info: n_embd_head_k    = 128
0.00.038.189 I print_info: n_embd_head_v    = 128
0.00.038.190 I print_info: n_gqa            = 1
0.00.038.197 I print_info: n_embd_k_gqa     = 2048
0.00.038.205 I print_info: n_embd_v_gqa     = 2048
0.00.038.206 I print_info: f_norm_eps       = 1.0e-05
0.00.038.206 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.206 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.206 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.206 I print_info: f_logit_scale    = 0.0e+00
0.00.038.207 I print_info: n_ff             = 8192
0.00.038.207 I print_info: n_expert         = 0
0.00.038.209 I print_info: n_expert_used    = 0
0.00.038.209 I print_info: causal attn      = 1
0.00.038.209 I print_info: pooling type     = 0
0.00.038.209 I print_info: rope type        = 2
0.00.038.209 I print_info: rope scaling     = linear
0.00.038.210 I print_info: freq_base_train  = 10000.0
0.00.038.210 I print_info: freq_scale_train = 1
0.00.038.210 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.210 I print_info: rope_finetuned   = unknown
0.00.038.210 I print_info: ssm_d_conv       = 0
0.00.038.211 I print_info: ssm_d_inner      = 0
0.00.038.212 I print_info: ssm_d_state      = 0
0.00.038.212 I print_info: ssm_dt_rank      = 0
0.00.038.212 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.212 I print_info: model type       = 1.4B
0.00.038.212 I print_info: model params     = 1.41 B
0.00.038.213 I print_info: general.name     = 1.4B
0.00.038.213 I print_info: vocab type       = BPE
0.00.038.213 I print_info: n_vocab          = 50304
0.00.038.213 I print_info: n_merges         = 50009
0.00.038.214 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.215 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.215 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.215 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.215 I print_info: LF token         = 128 'Ä'
0.00.038.216 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.216 I print_info: max token length = 1024
0.00.644.179 I load_tensors: offloading 24 repeating layers to GPU
0.00.644.195 I load_tensors: offloading output layer to GPU
0.00.644.196 I load_tensors: offloaded 25/25 layers to GPU
0.00.644.235 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.644.236 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.645.870 I llama_init_from_model: n_seq_max     = 1
0.00.645.875 I llama_init_from_model: n_ctx         = 128
0.00.645.876 I llama_init_from_model: n_ctx_per_seq = 128
0.00.645.881 I llama_init_from_model: n_batch       = 128
0.00.645.881 I llama_init_from_model: n_ubatch      = 128
0.00.645.882 I llama_init_from_model: flash_attn    = 0
0.00.645.888 I llama_init_from_model: freq_base     = 10000.0
0.00.645.889 I llama_init_from_model: freq_scale    = 1
0.00.645.890 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.645.892 I ggml_metal_init: allocating
0.00.646.016 I ggml_metal_init: found device: Apple M4
0.00.646.030 I ggml_metal_init: picking default device: Apple M4
0.00.647.872 I ggml_metal_init: using embedded metal library
0.00.654.619 I ggml_metal_init: GPU name:   Apple M4
0.00.654.623 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.654.624 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.654.625 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.654.625 I ggml_metal_init: simdgroup reduction   = true
0.00.654.625 I ggml_metal_init: simdgroup matrix mul. = true
0.00.654.626 I ggml_metal_init: has residency sets    = true
0.00.654.626 I ggml_metal_init: has bfloat            = true
0.00.654.626 I ggml_metal_init: use bfloat            = true
0.00.654.627 I ggml_metal_init: hasUnifiedMemory      = true
0.00.654.629 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.671.971 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.675.338 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.675.341 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.675.377 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.678.548 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.678.550 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.678.550 I llama_init_from_model: graph nodes  = 967
0.00.678.550 I llama_init_from_model: graph splits = 2
0.00.678.554 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.678.554 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.705.792 I 
0.00.705.872 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.705.893 I perplexity: tokenizing the input ..
0.00.713.153 I perplexity: tokenization took 7.257 ms
0.00.713.179 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.850.040 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.851.518 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.851.533 I llama_perf_context_print:        load time =     696.84 ms
0.00.851.534 I llama_perf_context_print: prompt eval time =     135.99 ms /   128 tokens (    1.06 ms per token,   941.23 tokens per second)
0.00.851.535 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.851.535 I llama_perf_context_print:       total time =     145.75 ms /   129 tokens
0.00.851.878 I ggml_metal_free: deallocating

real	0m0.866s
user	0m0.078s
sys	0m0.129s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.108 I build: 4581 (b636228c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.354 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.398 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.403 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.405 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.405 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.405 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.406 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.406 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.407 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.407 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.408 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.408 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.408 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.409 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.409 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.411 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.411 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.411 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.144 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.164 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.891 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.893 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.893 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.893 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.894 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.894 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.894 I llama_model_loader: - type  f32:  194 tensors
0.00.025.895 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.895 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.896 I print_info: file format = GGUF V3 (latest)
0.00.025.896 I print_info: file type   = Q5_0
0.00.025.897 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.083 I load: special tokens cache size = 25
0.00.039.977 I load: token to piece cache size = 0.2984 MB
0.00.039.980 I print_info: arch             = gptneox
0.00.039.980 I print_info: vocab_only       = 0
0.00.039.980 I print_info: n_ctx_train      = 2048
0.00.039.980 I print_info: n_embd           = 2048
0.00.039.980 I print_info: n_layer          = 24
0.00.039.983 I print_info: n_head           = 16
0.00.039.984 I print_info: n_head_kv        = 16
0.00.039.984 I print_info: n_rot            = 32
0.00.039.984 I print_info: n_swa            = 0
0.00.039.984 I print_info: n_embd_head_k    = 128
0.00.039.985 I print_info: n_embd_head_v    = 128
0.00.039.985 I print_info: n_gqa            = 1
0.00.039.986 I print_info: n_embd_k_gqa     = 2048
0.00.039.987 I print_info: n_embd_v_gqa     = 2048
0.00.039.987 I print_info: f_norm_eps       = 1.0e-05
0.00.039.988 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.988 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.988 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.988 I print_info: f_logit_scale    = 0.0e+00
0.00.039.989 I print_info: n_ff             = 8192
0.00.039.989 I print_info: n_expert         = 0
0.00.039.989 I print_info: n_expert_used    = 0
0.00.039.990 I print_info: causal attn      = 1
0.00.039.990 I print_info: pooling type     = 0
0.00.039.990 I print_info: rope type        = 2
0.00.039.990 I print_info: rope scaling     = linear
0.00.039.991 I print_info: freq_base_train  = 10000.0
0.00.039.991 I print_info: freq_scale_train = 1
0.00.039.991 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.991 I print_info: rope_finetuned   = unknown
0.00.039.991 I print_info: ssm_d_conv       = 0
0.00.039.992 I print_info: ssm_d_inner      = 0
0.00.039.992 I print_info: ssm_d_state      = 0
0.00.039.992 I print_info: ssm_dt_rank      = 0
0.00.039.992 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.992 I print_info: model type       = 1.4B
0.00.039.993 I print_info: model params     = 1.41 B
0.00.039.993 I print_info: general.name     = 1.4B
0.00.039.993 I print_info: vocab type       = BPE
0.00.039.994 I print_info: n_vocab          = 50304
0.00.039.994 I print_info: n_merges         = 50009
0.00.039.996 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.996 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.996 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.997 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.997 I print_info: LF token         = 128 'Ä'
0.00.039.997 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.997 I print_info: max token length = 1024
0.00.699.088 I load_tensors: offloading 24 repeating layers to GPU
0.00.699.102 I load_tensors: offloading output layer to GPU
0.00.699.103 I load_tensors: offloaded 25/25 layers to GPU
0.00.699.136 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.699.138 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.700.707 I llama_init_from_model: n_seq_max     = 1
0.00.700.711 I llama_init_from_model: n_ctx         = 128
0.00.700.712 I llama_init_from_model: n_ctx_per_seq = 128
0.00.700.713 I llama_init_from_model: n_batch       = 128
0.00.700.713 I llama_init_from_model: n_ubatch      = 128
0.00.700.713 I llama_init_from_model: flash_attn    = 0
0.00.700.716 I llama_init_from_model: freq_base     = 10000.0
0.00.700.716 I llama_init_from_model: freq_scale    = 1
0.00.700.717 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.700.719 I ggml_metal_init: allocating
0.00.700.796 I ggml_metal_init: found device: Apple M4
0.00.700.811 I ggml_metal_init: picking default device: Apple M4
0.00.702.544 I ggml_metal_init: using embedded metal library
0.00.709.331 I ggml_metal_init: GPU name:   Apple M4
0.00.709.338 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.709.339 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.709.339 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.709.340 I ggml_metal_init: simdgroup reduction   = true
0.00.709.340 I ggml_metal_init: simdgroup matrix mul. = true
0.00.709.341 I ggml_metal_init: has residency sets    = true
0.00.709.341 I ggml_metal_init: has bfloat            = true
0.00.709.341 I ggml_metal_init: use bfloat            = true
0.00.709.342 I ggml_metal_init: hasUnifiedMemory      = true
0.00.709.353 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.727.276 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.730.892 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.730.896 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.730.925 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.734.262 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.734.264 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.734.265 I llama_init_from_model: graph nodes  = 967
0.00.734.265 I llama_init_from_model: graph splits = 2
0.00.734.268 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.734.268 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.767.657 I 
0.00.767.742 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.767.765 I perplexity: tokenizing the input ..
0.00.774.617 I perplexity: tokenization took 6.849 ms
0.00.774.637 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.917.356 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.918.699 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.918.713 I llama_perf_context_print:        load time =     757.29 ms
0.00.918.714 I llama_perf_context_print: prompt eval time =     141.81 ms /   128 tokens (    1.11 ms per token,   902.63 tokens per second)
0.00.918.715 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.918.715 I llama_perf_context_print:       total time =     151.06 ms /   129 tokens
0.00.919.100 I ggml_metal_free: deallocating

real	0m0.939s
user	0m0.079s
sys	0m0.155s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4581 (b636228c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.054 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.346 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.351 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.353 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.353 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.354 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.354 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.354 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.355 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.356 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.357 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.358 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.358 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.358 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.359 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.360 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.361 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.361 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.165 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.177 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.973 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.974 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.974 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.975 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.975 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.975 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.976 I llama_model_loader: - type  f32:  194 tensors
0.00.024.976 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.976 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.977 I print_info: file format = GGUF V3 (latest)
0.00.024.977 I print_info: file type   = Q5_1
0.00.024.978 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.162 I load: special tokens cache size = 25
0.00.039.076 I load: token to piece cache size = 0.2984 MB
0.00.039.079 I print_info: arch             = gptneox
0.00.039.079 I print_info: vocab_only       = 0
0.00.039.079 I print_info: n_ctx_train      = 2048
0.00.039.079 I print_info: n_embd           = 2048
0.00.039.080 I print_info: n_layer          = 24
0.00.039.082 I print_info: n_head           = 16
0.00.039.083 I print_info: n_head_kv        = 16
0.00.039.083 I print_info: n_rot            = 32
0.00.039.083 I print_info: n_swa            = 0
0.00.039.084 I print_info: n_embd_head_k    = 128
0.00.039.084 I print_info: n_embd_head_v    = 128
0.00.039.084 I print_info: n_gqa            = 1
0.00.039.085 I print_info: n_embd_k_gqa     = 2048
0.00.039.086 I print_info: n_embd_v_gqa     = 2048
0.00.039.087 I print_info: f_norm_eps       = 1.0e-05
0.00.039.087 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.087 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.087 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.087 I print_info: f_logit_scale    = 0.0e+00
0.00.039.088 I print_info: n_ff             = 8192
0.00.039.088 I print_info: n_expert         = 0
0.00.039.088 I print_info: n_expert_used    = 0
0.00.039.088 I print_info: causal attn      = 1
0.00.039.089 I print_info: pooling type     = 0
0.00.039.089 I print_info: rope type        = 2
0.00.039.089 I print_info: rope scaling     = linear
0.00.039.090 I print_info: freq_base_train  = 10000.0
0.00.039.090 I print_info: freq_scale_train = 1
0.00.039.091 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.091 I print_info: rope_finetuned   = unknown
0.00.039.091 I print_info: ssm_d_conv       = 0
0.00.039.091 I print_info: ssm_d_inner      = 0
0.00.039.091 I print_info: ssm_d_state      = 0
0.00.039.093 I print_info: ssm_dt_rank      = 0
0.00.039.093 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.093 I print_info: model type       = 1.4B
0.00.039.094 I print_info: model params     = 1.41 B
0.00.039.094 I print_info: general.name     = 1.4B
0.00.039.094 I print_info: vocab type       = BPE
0.00.039.095 I print_info: n_vocab          = 50304
0.00.039.095 I print_info: n_merges         = 50009
0.00.039.095 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.095 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.095 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.099 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.100 I print_info: LF token         = 128 'Ä'
0.00.039.101 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.101 I print_info: max token length = 1024
0.00.639.400 I load_tensors: offloading 24 repeating layers to GPU
0.00.639.412 I load_tensors: offloading output layer to GPU
0.00.639.412 I load_tensors: offloaded 25/25 layers to GPU
0.00.639.440 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.639.441 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.640.735 I llama_init_from_model: n_seq_max     = 1
0.00.640.747 I llama_init_from_model: n_ctx         = 128
0.00.640.747 I llama_init_from_model: n_ctx_per_seq = 128
0.00.640.748 I llama_init_from_model: n_batch       = 128
0.00.640.748 I llama_init_from_model: n_ubatch      = 128
0.00.640.749 I llama_init_from_model: flash_attn    = 0
0.00.640.751 I llama_init_from_model: freq_base     = 10000.0
0.00.640.751 I llama_init_from_model: freq_scale    = 1
0.00.640.752 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.640.755 I ggml_metal_init: allocating
0.00.640.848 I ggml_metal_init: found device: Apple M4
0.00.640.861 I ggml_metal_init: picking default device: Apple M4
0.00.642.519 I ggml_metal_init: using embedded metal library
0.00.647.627 I ggml_metal_init: GPU name:   Apple M4
0.00.647.632 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.647.632 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.647.633 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.647.634 I ggml_metal_init: simdgroup reduction   = true
0.00.647.634 I ggml_metal_init: simdgroup matrix mul. = true
0.00.647.634 I ggml_metal_init: has residency sets    = true
0.00.647.634 I ggml_metal_init: has bfloat            = true
0.00.647.634 I ggml_metal_init: use bfloat            = true
0.00.647.635 I ggml_metal_init: hasUnifiedMemory      = true
0.00.647.645 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.660.182 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.662.109 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.662.112 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.662.126 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.663.934 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.663.936 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.663.936 I llama_init_from_model: graph nodes  = 967
0.00.663.936 I llama_init_from_model: graph splits = 2
0.00.663.938 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.663.938 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.689.069 I 
0.00.689.106 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.689.115 I perplexity: tokenizing the input ..
0.00.692.986 I perplexity: tokenization took 3.869 ms
0.00.692.998 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.826.959 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.828.386 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.828.400 I llama_perf_context_print:        load time =     680.01 ms
0.00.828.402 I llama_perf_context_print: prompt eval time =     133.72 ms /   128 tokens (    1.04 ms per token,   957.19 tokens per second)
0.00.828.403 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.828.403 I llama_perf_context_print:       total time =     139.33 ms /   129 tokens
0.00.828.775 I ggml_metal_free: deallocating

real	0m0.843s
user	0m0.068s
sys	0m0.121s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4581 (b636228c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.729 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.549 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.555 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.557 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.558 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.558 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.558 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.559 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.559 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.560 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.560 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.561 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.561 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.562 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.562 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.564 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.564 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.565 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.356 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.371 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.227 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.229 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.229 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.230 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.230 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.230 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.231 I llama_model_loader: - type  f32:  194 tensors
0.00.025.231 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.231 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.232 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.232 I print_info: file format = GGUF V3 (latest)
0.00.025.233 I print_info: file type   = Q2_K - Medium
0.00.025.234 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.590 I load: special tokens cache size = 25
0.00.039.562 I load: token to piece cache size = 0.2984 MB
0.00.039.568 I print_info: arch             = gptneox
0.00.039.570 I print_info: vocab_only       = 0
0.00.039.570 I print_info: n_ctx_train      = 2048
0.00.039.570 I print_info: n_embd           = 2048
0.00.039.571 I print_info: n_layer          = 24
0.00.039.575 I print_info: n_head           = 16
0.00.039.575 I print_info: n_head_kv        = 16
0.00.039.576 I print_info: n_rot            = 32
0.00.039.576 I print_info: n_swa            = 0
0.00.039.576 I print_info: n_embd_head_k    = 128
0.00.039.576 I print_info: n_embd_head_v    = 128
0.00.039.577 I print_info: n_gqa            = 1
0.00.039.577 I print_info: n_embd_k_gqa     = 2048
0.00.039.578 I print_info: n_embd_v_gqa     = 2048
0.00.039.579 I print_info: f_norm_eps       = 1.0e-05
0.00.039.579 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.579 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.579 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.581 I print_info: f_logit_scale    = 0.0e+00
0.00.039.583 I print_info: n_ff             = 8192
0.00.039.583 I print_info: n_expert         = 0
0.00.039.583 I print_info: n_expert_used    = 0
0.00.039.583 I print_info: causal attn      = 1
0.00.039.583 I print_info: pooling type     = 0
0.00.039.583 I print_info: rope type        = 2
0.00.039.584 I print_info: rope scaling     = linear
0.00.039.584 I print_info: freq_base_train  = 10000.0
0.00.039.584 I print_info: freq_scale_train = 1
0.00.039.584 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.585 I print_info: rope_finetuned   = unknown
0.00.039.585 I print_info: ssm_d_conv       = 0
0.00.039.585 I print_info: ssm_d_inner      = 0
0.00.039.585 I print_info: ssm_d_state      = 0
0.00.039.585 I print_info: ssm_dt_rank      = 0
0.00.039.585 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.585 I print_info: model type       = 1.4B
0.00.039.586 I print_info: model params     = 1.41 B
0.00.039.586 I print_info: general.name     = 1.4B
0.00.039.586 I print_info: vocab type       = BPE
0.00.039.586 I print_info: n_vocab          = 50304
0.00.039.587 I print_info: n_merges         = 50009
0.00.039.587 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.587 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.587 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.587 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.587 I print_info: LF token         = 128 'Ä'
0.00.039.588 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.588 I print_info: max token length = 1024
0.00.363.079 I load_tensors: offloading 24 repeating layers to GPU
0.00.363.090 I load_tensors: offloading output layer to GPU
0.00.363.091 I load_tensors: offloaded 25/25 layers to GPU
0.00.363.123 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.363.125 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.364.456 I llama_init_from_model: n_seq_max     = 1
0.00.364.466 I llama_init_from_model: n_ctx         = 128
0.00.364.472 I llama_init_from_model: n_ctx_per_seq = 128
0.00.364.472 I llama_init_from_model: n_batch       = 128
0.00.364.473 I llama_init_from_model: n_ubatch      = 128
0.00.364.473 I llama_init_from_model: flash_attn    = 0
0.00.364.475 I llama_init_from_model: freq_base     = 10000.0
0.00.364.475 I llama_init_from_model: freq_scale    = 1
0.00.364.476 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.364.478 I ggml_metal_init: allocating
0.00.364.537 I ggml_metal_init: found device: Apple M4
0.00.364.548 I ggml_metal_init: picking default device: Apple M4
0.00.366.888 I ggml_metal_init: using embedded metal library
0.00.373.045 I ggml_metal_init: GPU name:   Apple M4
0.00.373.057 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.373.058 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.373.059 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.373.060 I ggml_metal_init: simdgroup reduction   = true
0.00.373.060 I ggml_metal_init: simdgroup matrix mul. = true
0.00.373.060 I ggml_metal_init: has residency sets    = true
0.00.373.061 I ggml_metal_init: has bfloat            = true
0.00.373.061 I ggml_metal_init: use bfloat            = true
0.00.373.063 I ggml_metal_init: hasUnifiedMemory      = true
0.00.373.068 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.394.423 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.397.966 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.397.971 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.397.999 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.401.227 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.401.229 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.401.230 I llama_init_from_model: graph nodes  = 967
0.00.401.231 I llama_init_from_model: graph splits = 2
0.00.401.233 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.401.234 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.429.550 I 
0.00.429.634 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.429.653 I perplexity: tokenizing the input ..
0.00.436.397 I perplexity: tokenization took 6.741 ms
0.00.436.416 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.569.419 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.570.711 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.570.729 I llama_perf_context_print:        load time =     419.81 ms
0.00.570.730 I llama_perf_context_print: prompt eval time =     132.10 ms /   128 tokens (    1.03 ms per token,   968.97 tokens per second)
0.00.570.730 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.570.732 I llama_perf_context_print:       total time =     141.18 ms /   129 tokens
0.00.571.082 I ggml_metal_free: deallocating

real	0m0.587s
user	0m0.081s
sys	0m0.111s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4581 (b636228c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.809 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.827 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.834 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.836 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.836 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.836 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.837 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.837 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.838 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.838 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.839 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.839 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.840 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.840 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.840 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.842 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.842 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.843 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.608 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.602 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.405 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.407 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.407 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.408 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.408 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.408 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.409 I llama_model_loader: - type  f32:  194 tensors
0.00.024.409 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.410 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.410 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.410 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.411 I print_info: file format = GGUF V3 (latest)
0.00.024.411 I print_info: file type   = Q3_K - Medium
0.00.024.412 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.278 I load: special tokens cache size = 25
0.00.038.092 I load: token to piece cache size = 0.2984 MB
0.00.038.097 I print_info: arch             = gptneox
0.00.038.097 I print_info: vocab_only       = 0
0.00.038.097 I print_info: n_ctx_train      = 2048
0.00.038.097 I print_info: n_embd           = 2048
0.00.038.097 I print_info: n_layer          = 24
0.00.038.101 I print_info: n_head           = 16
0.00.038.102 I print_info: n_head_kv        = 16
0.00.038.102 I print_info: n_rot            = 32
0.00.038.102 I print_info: n_swa            = 0
0.00.038.103 I print_info: n_embd_head_k    = 128
0.00.038.103 I print_info: n_embd_head_v    = 128
0.00.038.103 I print_info: n_gqa            = 1
0.00.038.106 I print_info: n_embd_k_gqa     = 2048
0.00.038.107 I print_info: n_embd_v_gqa     = 2048
0.00.038.108 I print_info: f_norm_eps       = 1.0e-05
0.00.038.108 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.108 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.109 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.110 I print_info: f_logit_scale    = 0.0e+00
0.00.038.111 I print_info: n_ff             = 8192
0.00.038.111 I print_info: n_expert         = 0
0.00.038.111 I print_info: n_expert_used    = 0
0.00.038.111 I print_info: causal attn      = 1
0.00.038.112 I print_info: pooling type     = 0
0.00.038.112 I print_info: rope type        = 2
0.00.038.112 I print_info: rope scaling     = linear
0.00.038.112 I print_info: freq_base_train  = 10000.0
0.00.038.113 I print_info: freq_scale_train = 1
0.00.038.113 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.113 I print_info: rope_finetuned   = unknown
0.00.038.113 I print_info: ssm_d_conv       = 0
0.00.038.113 I print_info: ssm_d_inner      = 0
0.00.038.113 I print_info: ssm_d_state      = 0
0.00.038.114 I print_info: ssm_dt_rank      = 0
0.00.038.115 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.118 I print_info: model type       = 1.4B
0.00.038.119 I print_info: model params     = 1.41 B
0.00.038.120 I print_info: general.name     = 1.4B
0.00.038.120 I print_info: vocab type       = BPE
0.00.038.121 I print_info: n_vocab          = 50304
0.00.038.121 I print_info: n_merges         = 50009
0.00.038.121 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.121 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.121 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.121 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.121 I print_info: LF token         = 128 'Ä'
0.00.038.122 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.125 I print_info: max token length = 1024
0.00.433.386 I load_tensors: offloading 24 repeating layers to GPU
0.00.433.402 I load_tensors: offloading output layer to GPU
0.00.433.403 I load_tensors: offloaded 25/25 layers to GPU
0.00.433.433 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.433.435 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.434.897 I llama_init_from_model: n_seq_max     = 1
0.00.434.903 I llama_init_from_model: n_ctx         = 128
0.00.434.904 I llama_init_from_model: n_ctx_per_seq = 128
0.00.434.904 I llama_init_from_model: n_batch       = 128
0.00.434.905 I llama_init_from_model: n_ubatch      = 128
0.00.434.905 I llama_init_from_model: flash_attn    = 0
0.00.434.907 I llama_init_from_model: freq_base     = 10000.0
0.00.434.907 I llama_init_from_model: freq_scale    = 1
0.00.434.908 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.434.913 I ggml_metal_init: allocating
0.00.434.988 I ggml_metal_init: found device: Apple M4
0.00.435.002 I ggml_metal_init: picking default device: Apple M4
0.00.436.724 I ggml_metal_init: using embedded metal library
0.00.442.362 I ggml_metal_init: GPU name:   Apple M4
0.00.442.366 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.442.367 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.442.368 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.442.372 I ggml_metal_init: simdgroup reduction   = true
0.00.442.373 I ggml_metal_init: simdgroup matrix mul. = true
0.00.442.373 I ggml_metal_init: has residency sets    = true
0.00.442.373 I ggml_metal_init: has bfloat            = true
0.00.442.377 I ggml_metal_init: use bfloat            = true
0.00.442.379 I ggml_metal_init: hasUnifiedMemory      = true
0.00.442.384 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.461.823 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.465.349 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.465.353 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.465.379 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.468.922 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.468.923 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.468.924 I llama_init_from_model: graph nodes  = 967
0.00.468.925 I llama_init_from_model: graph splits = 2
0.00.468.928 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.468.928 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.499.899 I 
0.00.499.990 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.500.010 I perplexity: tokenizing the input ..
0.00.506.725 I perplexity: tokenization took 6.712 ms
0.00.506.743 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.648.285 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.649.691 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.649.705 I llama_perf_context_print:        load time =     491.08 ms
0.00.649.706 I llama_perf_context_print: prompt eval time =     140.99 ms /   128 tokens (    1.10 ms per token,   907.84 tokens per second)
0.00.649.706 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.649.707 I llama_perf_context_print:       total time =     149.81 ms /   129 tokens
0.00.650.054 I ggml_metal_free: deallocating

real	0m0.663s
user	0m0.078s
sys	0m0.104s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4581 (b636228c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.246 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.366 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.371 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.376 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.377 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.377 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.378 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.378 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.379 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.379 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.380 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.380 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.380 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.381 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.381 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.383 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.383 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.384 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.178 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.179 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.957 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.958 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.958 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.958 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.959 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.959 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.959 I llama_model_loader: - type  f32:  194 tensors
0.00.024.960 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.960 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.960 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.961 I print_info: file format = GGUF V3 (latest)
0.00.024.961 I print_info: file type   = Q4_K - Medium
0.00.024.963 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.078 I load: special tokens cache size = 25
0.00.038.988 I load: token to piece cache size = 0.2984 MB
0.00.038.990 I print_info: arch             = gptneox
0.00.038.991 I print_info: vocab_only       = 0
0.00.038.991 I print_info: n_ctx_train      = 2048
0.00.038.991 I print_info: n_embd           = 2048
0.00.038.991 I print_info: n_layer          = 24
0.00.038.994 I print_info: n_head           = 16
0.00.038.995 I print_info: n_head_kv        = 16
0.00.038.995 I print_info: n_rot            = 32
0.00.038.996 I print_info: n_swa            = 0
0.00.038.996 I print_info: n_embd_head_k    = 128
0.00.038.996 I print_info: n_embd_head_v    = 128
0.00.038.997 I print_info: n_gqa            = 1
0.00.038.997 I print_info: n_embd_k_gqa     = 2048
0.00.038.998 I print_info: n_embd_v_gqa     = 2048
0.00.038.999 I print_info: f_norm_eps       = 1.0e-05
0.00.038.999 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.999 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.999 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.001 I print_info: f_logit_scale    = 0.0e+00
0.00.039.002 I print_info: n_ff             = 8192
0.00.039.002 I print_info: n_expert         = 0
0.00.039.002 I print_info: n_expert_used    = 0
0.00.039.002 I print_info: causal attn      = 1
0.00.039.003 I print_info: pooling type     = 0
0.00.039.008 I print_info: rope type        = 2
0.00.039.009 I print_info: rope scaling     = linear
0.00.039.010 I print_info: freq_base_train  = 10000.0
0.00.039.010 I print_info: freq_scale_train = 1
0.00.039.010 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.011 I print_info: rope_finetuned   = unknown
0.00.039.011 I print_info: ssm_d_conv       = 0
0.00.039.011 I print_info: ssm_d_inner      = 0
0.00.039.011 I print_info: ssm_d_state      = 0
0.00.039.011 I print_info: ssm_dt_rank      = 0
0.00.039.011 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.012 I print_info: model type       = 1.4B
0.00.039.012 I print_info: model params     = 1.41 B
0.00.039.012 I print_info: general.name     = 1.4B
0.00.039.013 I print_info: vocab type       = BPE
0.00.039.013 I print_info: n_vocab          = 50304
0.00.039.013 I print_info: n_merges         = 50009
0.00.039.014 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.015 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.015 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.015 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.015 I print_info: LF token         = 128 'Ä'
0.00.039.015 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.015 I print_info: max token length = 1024
0.00.518.919 I load_tensors: offloading 24 repeating layers to GPU
0.00.518.935 I load_tensors: offloading output layer to GPU
0.00.518.936 I load_tensors: offloaded 25/25 layers to GPU
0.00.518.969 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.518.971 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.520.495 I llama_init_from_model: n_seq_max     = 1
0.00.520.499 I llama_init_from_model: n_ctx         = 128
0.00.520.500 I llama_init_from_model: n_ctx_per_seq = 128
0.00.520.500 I llama_init_from_model: n_batch       = 128
0.00.520.500 I llama_init_from_model: n_ubatch      = 128
0.00.520.501 I llama_init_from_model: flash_attn    = 0
0.00.520.503 I llama_init_from_model: freq_base     = 10000.0
0.00.520.503 I llama_init_from_model: freq_scale    = 1
0.00.520.504 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.520.506 I ggml_metal_init: allocating
0.00.520.585 I ggml_metal_init: found device: Apple M4
0.00.520.599 I ggml_metal_init: picking default device: Apple M4
0.00.522.350 I ggml_metal_init: using embedded metal library
0.00.528.927 I ggml_metal_init: GPU name:   Apple M4
0.00.528.931 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.528.932 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.528.933 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.528.934 I ggml_metal_init: simdgroup reduction   = true
0.00.528.934 I ggml_metal_init: simdgroup matrix mul. = true
0.00.528.934 I ggml_metal_init: has residency sets    = true
0.00.528.934 I ggml_metal_init: has bfloat            = true
0.00.528.935 I ggml_metal_init: use bfloat            = true
0.00.528.936 I ggml_metal_init: hasUnifiedMemory      = true
0.00.528.937 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.546.184 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.549.678 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.549.683 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.549.709 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.552.965 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.552.967 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.552.968 I llama_init_from_model: graph nodes  = 967
0.00.552.968 I llama_init_from_model: graph splits = 2
0.00.552.971 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.552.971 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.577.295 I 
0.00.577.368 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.577.388 I perplexity: tokenizing the input ..
0.00.583.397 I perplexity: tokenization took 6.007 ms
0.00.583.409 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.717.152 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.718.918 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.718.929 I llama_perf_context_print:        load time =     568.04 ms
0.00.718.930 I llama_perf_context_print: prompt eval time =     133.52 ms /   128 tokens (    1.04 ms per token,   958.67 tokens per second)
0.00.718.934 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.718.935 I llama_perf_context_print:       total time =     141.64 ms /   129 tokens
0.00.719.319 I ggml_metal_free: deallocating

real	0m0.734s
user	0m0.076s
sys	0m0.123s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4581 (b636228c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.873 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.599 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.604 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.606 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.606 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.607 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.607 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.607 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.608 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.609 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.609 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.609 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.610 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.610 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.611 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.612 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.612 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.613 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.446 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.466 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.288 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.289 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.290 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.290 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.290 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.290 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.291 I llama_model_loader: - type  f32:  194 tensors
0.00.025.291 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.291 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.292 I print_info: file format = GGUF V3 (latest)
0.00.025.292 I print_info: file type   = Q5_K - Medium
0.00.025.293 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.442 I load: special tokens cache size = 25
0.00.039.387 I load: token to piece cache size = 0.2984 MB
0.00.039.390 I print_info: arch             = gptneox
0.00.039.390 I print_info: vocab_only       = 0
0.00.039.390 I print_info: n_ctx_train      = 2048
0.00.039.390 I print_info: n_embd           = 2048
0.00.039.391 I print_info: n_layer          = 24
0.00.039.393 I print_info: n_head           = 16
0.00.039.394 I print_info: n_head_kv        = 16
0.00.039.394 I print_info: n_rot            = 32
0.00.039.396 I print_info: n_swa            = 0
0.00.039.396 I print_info: n_embd_head_k    = 128
0.00.039.397 I print_info: n_embd_head_v    = 128
0.00.039.398 I print_info: n_gqa            = 1
0.00.039.399 I print_info: n_embd_k_gqa     = 2048
0.00.039.399 I print_info: n_embd_v_gqa     = 2048
0.00.039.404 I print_info: f_norm_eps       = 1.0e-05
0.00.039.405 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.405 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.405 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.406 I print_info: f_logit_scale    = 0.0e+00
0.00.039.408 I print_info: n_ff             = 8192
0.00.039.408 I print_info: n_expert         = 0
0.00.039.408 I print_info: n_expert_used    = 0
0.00.039.408 I print_info: causal attn      = 1
0.00.039.408 I print_info: pooling type     = 0
0.00.039.409 I print_info: rope type        = 2
0.00.039.409 I print_info: rope scaling     = linear
0.00.039.410 I print_info: freq_base_train  = 10000.0
0.00.039.410 I print_info: freq_scale_train = 1
0.00.039.411 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.411 I print_info: rope_finetuned   = unknown
0.00.039.411 I print_info: ssm_d_conv       = 0
0.00.039.411 I print_info: ssm_d_inner      = 0
0.00.039.411 I print_info: ssm_d_state      = 0
0.00.039.411 I print_info: ssm_dt_rank      = 0
0.00.039.412 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.412 I print_info: model type       = 1.4B
0.00.039.412 I print_info: model params     = 1.41 B
0.00.039.412 I print_info: general.name     = 1.4B
0.00.039.413 I print_info: vocab type       = BPE
0.00.039.413 I print_info: n_vocab          = 50304
0.00.039.413 I print_info: n_merges         = 50009
0.00.039.413 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.413 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.415 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.415 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.415 I print_info: LF token         = 128 'Ä'
0.00.039.415 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.415 I print_info: max token length = 1024
0.00.606.505 I load_tensors: offloading 24 repeating layers to GPU
0.00.606.509 I load_tensors: offloading output layer to GPU
0.00.606.510 I load_tensors: offloaded 25/25 layers to GPU
0.00.606.535 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.606.536 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.607.892 I llama_init_from_model: n_seq_max     = 1
0.00.607.895 I llama_init_from_model: n_ctx         = 128
0.00.607.895 I llama_init_from_model: n_ctx_per_seq = 128
0.00.607.896 I llama_init_from_model: n_batch       = 128
0.00.607.896 I llama_init_from_model: n_ubatch      = 128
0.00.607.897 I llama_init_from_model: flash_attn    = 0
0.00.607.897 I llama_init_from_model: freq_base     = 10000.0
0.00.607.898 I llama_init_from_model: freq_scale    = 1
0.00.607.899 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.607.901 I ggml_metal_init: allocating
0.00.607.913 I ggml_metal_init: found device: Apple M4
0.00.607.921 I ggml_metal_init: picking default device: Apple M4
0.00.609.229 I ggml_metal_init: using embedded metal library
0.00.615.355 I ggml_metal_init: GPU name:   Apple M4
0.00.615.359 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.615.360 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.615.361 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.615.361 I ggml_metal_init: simdgroup reduction   = true
0.00.615.361 I ggml_metal_init: simdgroup matrix mul. = true
0.00.615.362 I ggml_metal_init: has residency sets    = true
0.00.615.362 I ggml_metal_init: has bfloat            = true
0.00.615.362 I ggml_metal_init: use bfloat            = true
0.00.615.363 I ggml_metal_init: hasUnifiedMemory      = true
0.00.615.364 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.631.769 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.635.267 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.635.278 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.635.313 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.638.756 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.638.758 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.638.759 I llama_init_from_model: graph nodes  = 967
0.00.638.759 I llama_init_from_model: graph splits = 2
0.00.638.762 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.638.762 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.674.238 I 
0.00.674.327 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.674.348 I perplexity: tokenizing the input ..
0.00.681.458 I perplexity: tokenization took 7.107 ms
0.00.681.475 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.821.328 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.822.657 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.822.669 I llama_perf_context_print:        load time =     664.35 ms
0.00.822.669 I llama_perf_context_print: prompt eval time =     139.62 ms /   128 tokens (    1.09 ms per token,   916.76 tokens per second)
0.00.822.670 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.822.671 I llama_perf_context_print:       total time =     148.44 ms /   129 tokens
0.00.823.029 I ggml_metal_free: deallocating

real	0m0.839s
user	0m0.077s
sys	0m0.149s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4581 (b636228c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.922 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.722 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.726 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.727 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.728 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.735 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.735 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.736 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.736 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.737 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.737 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.737 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.738 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.738 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.739 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.740 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.740 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.741 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.515 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.519 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.330 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.331 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.332 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.332 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.332 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.333 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.333 I llama_model_loader: - type  f32:  194 tensors
0.00.024.333 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.334 I print_info: file format = GGUF V3 (latest)
0.00.024.334 I print_info: file type   = Q6_K
0.00.024.335 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.140 I load: special tokens cache size = 25
0.00.038.045 I load: token to piece cache size = 0.2984 MB
0.00.038.047 I print_info: arch             = gptneox
0.00.038.048 I print_info: vocab_only       = 0
0.00.038.048 I print_info: n_ctx_train      = 2048
0.00.038.048 I print_info: n_embd           = 2048
0.00.038.048 I print_info: n_layer          = 24
0.00.038.051 I print_info: n_head           = 16
0.00.038.052 I print_info: n_head_kv        = 16
0.00.038.052 I print_info: n_rot            = 32
0.00.038.052 I print_info: n_swa            = 0
0.00.038.052 I print_info: n_embd_head_k    = 128
0.00.038.053 I print_info: n_embd_head_v    = 128
0.00.038.053 I print_info: n_gqa            = 1
0.00.038.054 I print_info: n_embd_k_gqa     = 2048
0.00.038.055 I print_info: n_embd_v_gqa     = 2048
0.00.038.055 I print_info: f_norm_eps       = 1.0e-05
0.00.038.056 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.056 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.056 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.056 I print_info: f_logit_scale    = 0.0e+00
0.00.038.057 I print_info: n_ff             = 8192
0.00.038.057 I print_info: n_expert         = 0
0.00.038.057 I print_info: n_expert_used    = 0
0.00.038.058 I print_info: causal attn      = 1
0.00.038.058 I print_info: pooling type     = 0
0.00.038.058 I print_info: rope type        = 2
0.00.038.058 I print_info: rope scaling     = linear
0.00.038.058 I print_info: freq_base_train  = 10000.0
0.00.038.059 I print_info: freq_scale_train = 1
0.00.038.059 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.059 I print_info: rope_finetuned   = unknown
0.00.038.059 I print_info: ssm_d_conv       = 0
0.00.038.059 I print_info: ssm_d_inner      = 0
0.00.038.060 I print_info: ssm_d_state      = 0
0.00.038.060 I print_info: ssm_dt_rank      = 0
0.00.038.060 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.060 I print_info: model type       = 1.4B
0.00.038.061 I print_info: model params     = 1.41 B
0.00.038.061 I print_info: general.name     = 1.4B
0.00.038.061 I print_info: vocab type       = BPE
0.00.038.062 I print_info: n_vocab          = 50304
0.00.038.062 I print_info: n_merges         = 50009
0.00.038.062 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.062 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.062 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.063 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.063 I print_info: LF token         = 128 'Ä'
0.00.038.063 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.063 I print_info: max token length = 1024
0.00.604.667 I load_tensors: offloading 24 repeating layers to GPU
0.00.604.672 I load_tensors: offloading output layer to GPU
0.00.604.674 I load_tensors: offloaded 25/25 layers to GPU
0.00.604.697 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.604.700 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.605.969 I llama_init_from_model: n_seq_max     = 1
0.00.605.972 I llama_init_from_model: n_ctx         = 128
0.00.605.973 I llama_init_from_model: n_ctx_per_seq = 128
0.00.605.973 I llama_init_from_model: n_batch       = 128
0.00.605.973 I llama_init_from_model: n_ubatch      = 128
0.00.605.974 I llama_init_from_model: flash_attn    = 0
0.00.605.975 I llama_init_from_model: freq_base     = 10000.0
0.00.605.975 I llama_init_from_model: freq_scale    = 1
0.00.605.976 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.605.977 I ggml_metal_init: allocating
0.00.605.998 I ggml_metal_init: found device: Apple M4
0.00.606.008 I ggml_metal_init: picking default device: Apple M4
0.00.607.371 I ggml_metal_init: using embedded metal library
0.00.613.384 I ggml_metal_init: GPU name:   Apple M4
0.00.613.387 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.613.387 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.613.388 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.613.389 I ggml_metal_init: simdgroup reduction   = true
0.00.613.389 I ggml_metal_init: simdgroup matrix mul. = true
0.00.613.390 I ggml_metal_init: has residency sets    = true
0.00.613.390 I ggml_metal_init: has bfloat            = true
0.00.613.390 I ggml_metal_init: use bfloat            = true
0.00.613.391 I ggml_metal_init: hasUnifiedMemory      = true
0.00.613.399 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.629.546 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.632.971 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.632.977 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.633.008 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.636.306 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.636.308 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.636.308 I llama_init_from_model: graph nodes  = 967
0.00.636.309 I llama_init_from_model: graph splits = 2
0.00.636.312 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.636.312 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.671.723 I 
0.00.671.806 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.671.827 I perplexity: tokenizing the input ..
0.00.678.996 I perplexity: tokenization took 7.167 ms
0.00.679.014 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.819.115 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.820.484 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.820.500 I llama_perf_context_print:        load time =     662.79 ms
0.00.820.501 I llama_perf_context_print: prompt eval time =     139.70 ms /   128 tokens (    1.09 ms per token,   916.25 tokens per second)
0.00.820.501 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.820.501 I llama_perf_context_print:       total time =     148.78 ms /   129 tokens
0.00.820.865 I ggml_metal_free: deallocating

real	0m0.835s
user	0m0.076s
sys	0m0.135s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.230 I build: 4581 (b636228c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.031.503 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.047.973 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.047.981 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.047.983 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.047.984 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.047.985 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.047.986 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.047.986 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.047.988 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.047.989 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.047.989 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.047.993 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.047.994 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.047.994 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.047.995 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.047.998 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.047.998 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.047.999 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.055.707 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.057.430 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.064.015 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.064.017 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.064.017 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.064.018 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.064.018 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.064.019 I llama_model_loader: - type  f32:  194 tensors
0.00.064.019 I llama_model_loader: - type  f16:   98 tensors
0.00.064.020 I print_info: file format = GGUF V3 (latest)
0.00.064.021 I print_info: file type   = all F32 (guessed)
0.00.064.022 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.076.025 I load: special tokens cache size = 25
0.00.083.972 I load: token to piece cache size = 0.2984 MB
0.00.083.975 I print_info: arch             = gptneox
0.00.083.975 I print_info: vocab_only       = 0
0.00.083.976 I print_info: n_ctx_train      = 2048
0.00.083.976 I print_info: n_embd           = 2048
0.00.083.976 I print_info: n_layer          = 24
0.00.083.979 I print_info: n_head           = 16
0.00.083.980 I print_info: n_head_kv        = 16
0.00.083.980 I print_info: n_rot            = 32
0.00.083.980 I print_info: n_swa            = 0
0.00.083.980 I print_info: n_embd_head_k    = 128
0.00.083.981 I print_info: n_embd_head_v    = 128
0.00.083.981 I print_info: n_gqa            = 1
0.00.083.982 I print_info: n_embd_k_gqa     = 2048
0.00.083.983 I print_info: n_embd_v_gqa     = 2048
0.00.083.983 I print_info: f_norm_eps       = 1.0e-05
0.00.083.984 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.083.984 I print_info: f_clamp_kqv      = 0.0e+00
0.00.083.984 I print_info: f_max_alibi_bias = 0.0e+00
0.00.083.984 I print_info: f_logit_scale    = 0.0e+00
0.00.083.985 I print_info: n_ff             = 8192
0.00.083.985 I print_info: n_expert         = 0
0.00.083.985 I print_info: n_expert_used    = 0
0.00.083.985 I print_info: causal attn      = 1
0.00.083.985 I print_info: pooling type     = 0
0.00.083.985 I print_info: rope type        = 2
0.00.083.988 I print_info: rope scaling     = linear
0.00.083.988 I print_info: freq_base_train  = 10000.0
0.00.083.989 I print_info: freq_scale_train = 1
0.00.083.989 I print_info: n_ctx_orig_yarn  = 2048
0.00.083.990 I print_info: rope_finetuned   = unknown
0.00.083.990 I print_info: ssm_d_conv       = 0
0.00.083.990 I print_info: ssm_d_inner      = 0
0.00.083.990 I print_info: ssm_d_state      = 0
0.00.083.990 I print_info: ssm_dt_rank      = 0
0.00.083.991 I print_info: ssm_dt_b_c_rms   = 0
0.00.083.991 I print_info: model type       = 1.4B
0.00.083.991 I print_info: model params     = 1.41 B
0.00.083.991 I print_info: general.name     = 1.4B
0.00.083.992 I print_info: vocab type       = BPE
0.00.083.992 I print_info: n_vocab          = 50304
0.00.083.992 I print_info: n_merges         = 50009
0.00.083.992 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.083.993 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.083.993 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.083.994 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.083.994 I print_info: LF token         = 128 'Ä'
0.00.083.995 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.083.995 I print_info: max token length = 1024
0.01.183.103 I load_tensors: offloading 24 repeating layers to GPU
0.01.183.109 I load_tensors: offloading output layer to GPU
0.01.183.109 I load_tensors: offloaded 25/25 layers to GPU
0.01.183.136 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.183.138 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.184.195 I llama_init_from_model: n_seq_max     = 1
0.01.184.197 I llama_init_from_model: n_ctx         = 128
0.01.184.197 I llama_init_from_model: n_ctx_per_seq = 128
0.01.184.197 I llama_init_from_model: n_batch       = 128
0.01.184.198 I llama_init_from_model: n_ubatch      = 128
0.01.184.198 I llama_init_from_model: flash_attn    = 0
0.01.184.199 I llama_init_from_model: freq_base     = 10000.0
0.01.184.199 I llama_init_from_model: freq_scale    = 1
0.01.184.199 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.184.200 I ggml_metal_init: allocating
0.01.184.240 I ggml_metal_init: found device: Apple M4
0.01.184.248 I ggml_metal_init: picking default device: Apple M4
0.01.185.314 I ggml_metal_init: using embedded metal library
0.01.189.295 I ggml_metal_init: GPU name:   Apple M4
0.01.189.298 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.189.298 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.189.299 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.189.299 I ggml_metal_init: simdgroup reduction   = true
0.01.189.299 I ggml_metal_init: simdgroup matrix mul. = true
0.01.189.299 I ggml_metal_init: has residency sets    = true
0.01.189.299 I ggml_metal_init: has bfloat            = true
0.01.189.299 I ggml_metal_init: use bfloat            = true
0.01.189.300 I ggml_metal_init: hasUnifiedMemory      = true
0.01.189.301 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.201.137 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.202.902 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.202.904 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.202.917 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.204.574 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.204.575 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.204.575 I llama_init_from_model: graph nodes  = 967
0.01.204.576 I llama_init_from_model: graph splits = 2
0.01.204.577 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.204.577 I 
0.01.204.614 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.204.616 I compute_imatrix: tokenizing the input ..
0.01.208.790 I compute_imatrix: tokenization took 4.174 ms
0.01.208.793 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.423.531 I compute_imatrix: 0.21 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.425.942 I llama_perf_context_print:        load time =    1392.03 ms
0.01.425.943 I llama_perf_context_print: prompt eval time =     212.92 ms /   128 tokens (    1.66 ms per token,   601.16 tokens per second)
0.01.425.943 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.425.944 I llama_perf_context_print:       total time =    1394.43 ms /   129 tokens
0.01.426.531 I ggml_metal_free: deallocating

real	0m1.638s
user	0m0.130s
sys	0m0.225s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4581 (b636228c)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13d306120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13d306790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13d306c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13d309a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13d309ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13d30a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13d30a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13d30ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13d30b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13d30b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13d30be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13d30c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13d30ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13d30d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13d30de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13d30e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13d30ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13d30f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13d30faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13d310270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13d310990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13d3110b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13d3117d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13d312070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13d312790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13d312a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13d313060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13d313cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13d314210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13d3144d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13d314970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13d314c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13d3154c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13d315a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13d315cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13d316160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13d316600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13d316aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13d316f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13d3173e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13d317880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13d317d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13d3181c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13d318660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13d318920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13d318f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13d319540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13d319e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13d31a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13d31aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13d31b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13d31b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13d31bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13d31c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13d31cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13d31cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13d31d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13d31d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13d31dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13d31e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13d31e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13d31ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13d31f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13d31f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13d31f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13d31fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13d320330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13d3207d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13d320c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13d321110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13d3215b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13d321a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13d321ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13d322440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13d322990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13d322ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13d323430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13d323980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13d323ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13d324420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13d324970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13d324ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13d325410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13d325960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13d325eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13d326400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13d326950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13d326ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13d3273f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13d327940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13d327e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13d3283e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13d328930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13d328e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13d3293d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13d329920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13d329e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13d319b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13d32a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13d32aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13d32afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13d32b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13d32ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13d32bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13d32c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13d32ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13d32cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13d32d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13d32da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13d32dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13d32e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13d32ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13d32efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13d32f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13d32f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13d32fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13d330220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13d3306c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13d330b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13d331000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13d3314a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13d331940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13d331de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13d332280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13d332720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13d332bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13d333060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13d333500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13d3339a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13d333e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13d3342e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13d334780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13d334c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13d3350c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13d335560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13d335a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13d335ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13d336340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13d3367e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13d336c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13d337120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13d3375c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13d337a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13d337f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13d3383a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13d338840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13d338ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13d339180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13d339620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13d339ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13d339f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13d33a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13d33a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13d33ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13d33b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13d33b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13d33bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13d33bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13d33c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13d33c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13d33cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13d33d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13d33d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13d33db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13d33e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13d33e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13d33e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13d33ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13d33f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13d33f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13d33fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13d340080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13d340520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13d3409c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13d340e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13d341300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13d3417a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13d341c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13d3420e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13d342580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13d342a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13d342ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13d343360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13d343800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13d343ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13d344140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13d3445e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13d344a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13d344f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13d3453c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13d345860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13d345d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13d3461a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13d3466f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13d346c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13d347190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13d3476e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13d3479a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13d347fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13d3485c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13d348bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13d3493c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13d349860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13d349b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13d34a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13d34a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13d34af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13d34b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13d34b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13d34bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13d34c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13d34ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13d34cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13d34d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13d34da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13d34df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13d34e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13d34e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13d34ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13d34f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13d34f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13d34ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13d350480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13d3509d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13d350f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13d351470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13d3519c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13d351f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13d352460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13d3529b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13d352f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13d353450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13d3539a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13d353ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13d354440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13d354990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13d354ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13d355430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13d355980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13d355ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13d356420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13d356970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13d356ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13d357410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13d357960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13d357eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13d358400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13d358950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13d358ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13d3593f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13d359940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13d359e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13d35a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13d35a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13d35ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13d35b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13d35b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13d35be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13d35c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13d35c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13d35ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13d35d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13d35d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13d35de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13d35e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13d35e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13d35ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13d35f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13d35f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13d35fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13d3600c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13d360560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13d360a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13d360ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13d361340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13d3617e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13d361c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13d362120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13d3625c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13d362a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13d362f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13d3633a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13d3638f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13d364010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13d364730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13d364e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13d365570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13d365830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13d366020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13d3662e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13d3668f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.732.777 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.732.781 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x104a04dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x104a05240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x104a056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x104a05b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x104a05f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x104a06400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x104a06870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x104a06ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x104a07150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x104a075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x104a07a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x104a08120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x104a08c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x104a093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x104a09c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x104a0a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x104a0aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x104a0b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x104a0b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x104a0bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x104a0c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x104a0cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x104a0d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x104a0dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x104a0e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x104a0e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x104a0e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x104a0ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x104a0f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x104a0f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x104a0fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x104a0ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x104a10430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x104a106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x104a10b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x104a10fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x104a11440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x104a118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x104a11d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x104a12190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x104a12600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x104a12a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x104a12ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x104a13350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x104a137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x104a13c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x104a140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x104a14510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x104a14980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x104a14df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x104a15260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x104a156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x104a15b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x104a15fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x104a16420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x104a16890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x104a16e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x104a17300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x104a17770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x104a17be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x104a18050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x104a184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x104a18930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x104a18da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x104a19210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x104a19680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x104a19af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x104a19f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x104a1a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x104a1a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x104a1acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x104a1b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x104a1b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x104a1ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x104a1be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x104a1c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x104a1c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x104a1cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x104a1d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x104a1d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x104a1d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x104a1dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x104a1e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x104a1e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x104a1ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x104a1ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x104a1f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x104a1f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x104a1fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x104a20100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x104a20570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x104a209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x104a20e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x104a212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x104a21730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x104a21ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x104a22010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x104a22480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x104a228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x104a22d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x104a231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x104a23640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x104a23ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x104a23f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x104a24390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x104a24800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x104a24c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x104a250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x104a25550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x104a259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x104a25e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x104a262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x104a26710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x104a26b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x104a26ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x104a27460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x104a278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x104a27d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x104a281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x104a28620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x104a28a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x104a28f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x104a29370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x104a297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x104a29c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x104a2a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x104a2a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x104a2a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x104a2ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x104a2b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x104a2b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x104a2bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x104a2bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x104a2c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x104a2c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x104a2cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x104a2d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x104a2d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x104a2da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x104a2dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x104a2e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x104a2e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x104a2ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x104a2f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x104a2f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x104a2f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x104a2fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x104a30260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x104a306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x104a30b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x104a30fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x104a31420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x104a31890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x104a31d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x104a32170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x104a325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x104a32a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x104a32ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x104a33330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x104a337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x104a33c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x104a34080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x104a344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x104a34960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x104a34dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x104a35240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x104a35e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x104a36130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x104a363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x104a36860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x104a36cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x104a37140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x104a375b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x104a37a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x104a37e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x104a38300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x104a38770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x104a38be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x104a39050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x104a394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x104a39930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x104a39da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x104a3a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x104a3a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x104a3aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x104a3af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x104a3b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x104a3b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x104a3bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x104a3c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x104a3c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x104a3ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x104a3ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x104a3d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x104a3d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x104a3dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x104a3e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x104a3e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x104a3e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x104a3ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x104a3f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x104a3f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x104a3fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x104a400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x104a40540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x104a409b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x104a40e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x104a41290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x104a417b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x104a41cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x104a42830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x104a42af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x104a430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x104a43670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x104a43c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x104a441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x104a447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x104a44d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x104a45330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x104a458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x104a45eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x104a46470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x104a46a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x104a46ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x104a475b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x104a47b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x104a48130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x104a486f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x104a48cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x104a49270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x104a49830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x104a49df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x104a4a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x104a4a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x104a4af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x104a4b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x104a4bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x104a4c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x104a4c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x104a4cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x104a4d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x104a4d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x104a4dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x104a4e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x104a4e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x104a4ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x104a4f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x104a4f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x104a4ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x104a50570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x104a50b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x104a510f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x104a516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x104a51c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x104a52230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x104a527f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x104a52db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x104a53370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x104a53930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x104a53ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x104a544b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x104a54a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x104a55030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x104a555f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x104a55bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x104a56170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x104a56730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x104a56cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x104a571f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x104a576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x104a57bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x104a580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x104a585f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x104a58af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x104a58ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x104a594f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x104a599f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x104a59ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x104a5a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x104a5a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x104a5adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x104a5b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x104a5b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x104a5c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x104a5c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x104a5d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x104a5d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x104a5da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x104a5e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x104a5e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x104a5eae0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x139f044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x139f04950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x139f04dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x139f05230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x139f056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x139f05b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x139f05f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x139f063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x139f06860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x139f06db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x139f07220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x139f078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x139f083c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x139f08b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x139f09380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x139f09aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x139f0a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x139f0a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x139f0b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x139f0b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x139f0bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x139f0c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x139f0cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x139f0d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x139f0db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x139f0de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x139f0e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x139f0e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x139f0e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x139f0ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x139f0f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x139f0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x139f0fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x139f0ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x139f10380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x139f107f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x139f10c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x139f110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x139f11540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x139f119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x139f11e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x139f12290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x139f12700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x139f12b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x139f12fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x139f13450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x139f138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x139f13d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x139f141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x139f14610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x139f14a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x139f14ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x139f15360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x139f157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x139f15c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x139f160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x139f16620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x139f16b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x139f16f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x139f17400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x139f17870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x139f17ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x139f18150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x139f185c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x139f18a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x139f18ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x139f19310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x139f19780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x139f19bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x139f1a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x139f1a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x139f1a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x139f1adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x139f1b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x139f1b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x139f1bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x139f1bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x139f1c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x139f1c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x139f1ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x139f1d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x139f1d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x139f1da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x139f1de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x139f1e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x139f1e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x139f1ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x139f1f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x139f1f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x139f1f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x139f1fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x139f20200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x139f20670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x139f20ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x139f20f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x139f213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x139f21830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x139f21ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x139f22110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x139f22580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x139f229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x139f22e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x139f232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x139f23b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x139f23e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x139f24290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x139f24700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x139f24b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x139f24fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x139f25450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x139f258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x139f25d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x139f261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x139f26610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x139f26a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x139f26ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x139f27360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x139f277d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x139f27c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x139f280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x139f28520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x139f28990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x139f28e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x139f29270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x139f296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x139f29b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x139f29fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x139f2a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x139f2a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x139f2ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x139f2b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x139f2b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x139f2ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x139f2bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x139f2c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x139f2c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x139f2cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x139f2d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x139f2d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x139f2d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x139f2dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x139f2e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x139f2e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x139f2eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x139f2efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x139f2f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x139f2f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x139f2fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x139f30160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x139f305d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x139f30a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x139f30eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x139f31320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x139f31790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x139f31c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x139f32070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x139f324e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x139f32950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x139f32dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x139f33230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x139f336a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x139f33b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x139f33f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x139f343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x139f34860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x139f34cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x139f35140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x139f355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x139f35a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x139f35e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x139f36300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x139f36770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x139f36be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x139f37050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x139f374c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x139f37930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x139f37da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x139f38210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x139f38680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x139f38af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x139f38f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x139f393d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x139f39840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x139f39cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x139f3a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x139f3a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x139f3aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x139f3ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x139f3b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x139f3b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x139f3bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x139f3c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x139f3c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x139f3c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x139f3cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x139f3d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x139f3d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x139f3dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x139f3df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x139f3e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x139f3e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x139f3ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x139f3f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x139f3f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x139f3f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x139f3fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x139f402c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x139f40730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x139f40ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x139f41010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x139f41b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x139f41e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x139f42110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x139f42580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x139f429f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x139f42e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x139f432d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x139f43740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x139f43bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x139f44020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x139f44490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x139f44900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x139f44d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x139f451e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x139f45650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x139f45ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x139f45f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x139f463a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x139f46810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x139f46c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x139f470f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x139f47560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x139f479d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x139f47e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x139f482b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x139f48720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x139f48b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x139f49000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x139f49470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x139f498e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x139f49d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x139f4a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x139f4a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x139f4aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x139f4af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x139f4b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x139f4b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x139f4bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x139f4c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x139f4c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x139f4c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x139f4ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x139f4d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x139f4d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x139f4db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x139f4dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x139f4e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x139f4e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x139f4ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x139f4f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x139f4f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x139f4fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x139f4fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x139f50360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x139f507d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x139f50c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x139f510b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x139f51520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x139f51990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x139f51e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x139f52270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x139f526e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x139f52b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x139f52fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x139f53430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x139f538a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x139f53d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x139f54180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x139f545f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x139f54a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x139f54ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x139f55340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x139f557b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x139f56220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x139f56940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x139f57060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x139f57780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x139f57a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x139f57eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x139f584b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x139f58ac0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.814s
user	0m0.282s
sys	0m0.333s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4581 (b636228c)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13960b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13960bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13960c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13960c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13960cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13960d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13960d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13960dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13960e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13960e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13960ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13960f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13960fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x139610510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x139610d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x139611440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x139611b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x139612280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1396129a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x139613170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x139613890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x139613fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1396146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x139614f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x139615690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x139615950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x139615f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x139616bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x139617110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1396173d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x139617870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x139617b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1396183c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x139618900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x139618bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x139619060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x139619500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1396199a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x139619e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13961a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13961a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13961ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13961b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13961b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13961b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13961be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13961c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13961cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13961d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13961d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13961df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13961e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13961ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13961f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13961f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13961fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1396202f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1396205b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x139620bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1396213b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x139621670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x139621b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x139621fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x139622450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1396228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x139622d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x139623230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1396236d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x139623b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x139624010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1396244b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x139624950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x139624df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x139625340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x139625890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x139625de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x139626330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x139626880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x139626dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x139627320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x139627870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x139627dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x139628310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x139628860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x139628db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x139629300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x139629850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x139629da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13962a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13962a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13962ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13962b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13962b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13962bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13962c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13962c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13962cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13961ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13962d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13962d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13962dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13962e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13962e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13962eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13962f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13962f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13962fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x139630410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x139630960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x139630eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x139631400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x139631950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x139631ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x139632340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1396327e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x139632c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x139633120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1396335c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x139633a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x139633f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1396343a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x139634840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x139634ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x139635180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x139635620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x139635ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x139635f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x139636400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1396368a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x139636d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1396371e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x139637680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x139637b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x139637fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x139638460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x139638900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x139638da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x139639240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1396396e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x139639b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13963a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13963a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13963a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13963ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13963b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13963b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13963bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13963c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13963c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13963c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13963ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13963d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13963d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13963dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13963e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13963e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13963ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13963eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13963f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13963f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13963fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x139640140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1396405e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x139640a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x139640f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1396413c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x139641860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x139641d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1396421a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x139642640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x139642ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x139642f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x139643420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1396438c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x139643d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x139644200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1396446a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x139644b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x139644fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x139645480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x139645920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x139645dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x139646260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x139646700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13a904230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13a9046a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13a904b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13a904f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13a9053f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13a905860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13a905cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13a906140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13a9065b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13a906a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13a906e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13a907300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13a907770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13a907be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13a908050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13a9084c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13a908930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13a908da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13a909210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13a909680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13a909af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13a909f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13a90a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13a90a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13a90add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13a90b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13a90bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13a90c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13a90c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13a90ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13a90d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13a90d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13a90ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13a90e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13a90e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13a90eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13a90f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13a90fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13a90fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13a9105a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13a910b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13a911100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13a9116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13a911c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13a912210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13a9127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13a912d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13a913320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13a9138d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13a913e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13a914430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13a9149e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13a914f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13a915540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13a915af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13a9160a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13a916650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13a916c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13a9171b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13a917760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13a917d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13a9182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13a918870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13a918e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13a9193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13a919980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13a919f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13a91a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13a91aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13a91b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13a91b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13a91bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13a91c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13a91c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13a91ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13a91d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13a91d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13a91ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13a91e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13a91e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13a91eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13a91f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13a91fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13a91ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13a9204e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13a9209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13a920ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13a9213e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13a9218e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13a921de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13a9222e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13a9227e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13a922ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13a9231e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13a9236e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13a923be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13a9240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13a9245e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13a924ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13a9254f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13a925c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13a926330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13a926a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13a926d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13a927500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13a9277c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13a927dd0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.097.748 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.097.751 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13a911970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13a90f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13a91e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13a91be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13a919c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13a917a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13a90fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13a9124d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13a90d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13a918b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13a91d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13a918580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13a913030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13a919690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13a916360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13a911f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13a90e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13a90ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13a924da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13a9113c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13a910860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13a91dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13a91fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13a90dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13a91a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13a913b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13a90c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13a928ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13a929110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13a929580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13a929a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13a929fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13a912a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13a92a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13a92aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13a92afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13a92b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13a92b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13a92bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13a92c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13a92c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13a92cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13a92d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13a92d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13a92dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13a92e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13a92e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13a92ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13a92eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13a92f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13a92f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13a92fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13a930060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13a9304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13a930940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13a931110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13a9315b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13a931870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13a931e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13a932670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13a932b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13a932fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13a933450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13a9338f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13a933d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13a934230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13a9346d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13a934b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13a935010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13a9354b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13a935950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13a935df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13a936290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13a9367e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13a936d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13a937280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13a9377d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13a937d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13a938270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13a9387c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13a938d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13a939260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13a9397b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13a939d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13a93a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13a93a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13a93acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13a93b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13a93b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13a93bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13a93c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13a93c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13a93ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13a93d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13a93d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13a93dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13a93e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13a93e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13a93ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13a93f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13a93f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13a93fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13a9401f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13a940740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13a940c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13a9411e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13a941730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13a941c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13a9421d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13a942720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13a942c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13a9431c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13a943710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13a943bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13a944050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13a9444f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13a944990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13a944e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13a9452d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13a945770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13a945c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13a9460b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13a946550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13a9469f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13a946e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13a947330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13a9477d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13a947c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13a948110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13a9485b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13a948a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13a948ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13a949390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13a949830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13a949cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13a94a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13a94a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13a94aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13a94af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13a94b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13a94b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13a94bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13a94c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13a94c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13a94cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13a94cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13a94d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13a94d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13a94dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13a94e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13a94e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13a94eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13a94f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13a94f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13a94f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13a94fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13a950290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13a950730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13a950bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13a951070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13a951510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13a9519b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13a951e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13a9522f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13a952790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13a952c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13a9530d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13a953570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13a953a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13a953eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13a954350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13a9547f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13a954c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13a955130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13a9555d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13a955a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13a955f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13a9563b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13a956850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13a956cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13a957190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13a957630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13a957ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13a957f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13a958410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13a9588b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13a958d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13a9591f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13a959690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13a959b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13a959fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13a95a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13a95a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13a95ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13a95b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13a95b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13a95be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13a95c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13a95c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13a95cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13a95d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13a95db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13a95dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13a95e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13a95e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13a95eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13a95f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13a95fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13a95ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13a960480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13a960c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13a961180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13a9616d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13a961c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13a962170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13a9626c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13a962c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13a963160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13a9636b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13a963c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13a964150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13a9646a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13a964bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13a965140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13a965690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13a965be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13a966130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13a966680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13a966bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13a967120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13a967670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13a967bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13a968110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13a968660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13a968bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13a969100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13a969650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13a969ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13a96a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13a96a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13a96ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13a96b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13a96b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13a96bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13a96c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13a96c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13a96cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13a96d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13a96d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13a96db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13a96e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13a96e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13a96eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13a96f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13a96f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13a96fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13a970090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13a9705e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13a970b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13a971080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13a9715d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13a971b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13a972070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13a9725c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13a972b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13a973060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13a9735b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13a973a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13a973ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13a974390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13a974830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13a974cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13a975170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13a975610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13a975ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13a975f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13a9763f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13a976890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13a976d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13a9771d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13a977670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13a977b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13a978060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13a978780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13a978ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13a9795c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13a979ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13a979fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13a97a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13a97aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13a97b060 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13a8044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13a804950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13a804dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13a805230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13a8056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13a805b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13a805f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13a8063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13a806860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13a806cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13a807140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13a807810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13a808330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13a808ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13a8092f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13a809a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13a80a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13a80a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13a80af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13a80b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13a80be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13a80c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13a80cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13a80d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13a80dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13a80dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13a80e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13a80e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13a80e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13a80edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13a80f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13a80f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13a80fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13a80fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13a8102f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13a810760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13a810bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13a811040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13a8114b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13a811920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13a811d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13a812200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13a812670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13a812ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13a812f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13a8133c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13a813830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13a813ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13a814110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13a814580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13a8149f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13a814e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13a8152d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13a815740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13a815bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13a816020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13a816590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13a816a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13a816f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13a817370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13a8177e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13a817c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13a8180c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13a818530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13a8189a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13a818e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13a819280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13a8196f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13a819b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13a819fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13a81a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13a81a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13a81ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13a81b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13a81b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13a81ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13a81bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13a81c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13a81c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13a81cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13a81d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13a81d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13a81d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13a81ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13a81e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13a81e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13a81eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13a81efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13a81f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13a81f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13a81fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13a820170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13a8205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13a820a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13a820ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13a821330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13a8217a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13a821c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13a822080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13a8224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13a822960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13a822dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13a823240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13a823ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13a823d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13a824200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13a824670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13a824ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13a824f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13a8253c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13a825830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13a825ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13a826110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13a826580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13a8269f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13a826e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13a8272d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13a827740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13a827bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13a828020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13a828490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13a828900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13a828d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13a8291e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13a829650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13a829ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13a829f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13a82a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13a82a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13a82ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13a82b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13a82b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13a82b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13a82be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13a82c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13a82c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13a82cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13a82d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13a82d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13a82d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13a82dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13a82e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13a82e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13a82eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13a82ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13a82f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13a82f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13a82fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13a8300d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13a830540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13a8309b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13a830e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13a831290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13a831700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13a831b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13a831fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13a832450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13a8328c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13a832d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13a8331a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13a833610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13a833a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13a833ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13a834360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13a8347d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13a834c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13a8350b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13a835520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13a835990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13a835e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13a836270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13a8366e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13a836b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13a836fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13a837430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13a8378a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13a837d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13a838180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13a8385f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13a838a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13a838ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13a839340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13a8397b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13a839c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13a83a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13a83a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13a83a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13a83ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13a83b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13a83b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13a83bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13a83bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13a83c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13a83c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13a83ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13a83d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13a83d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13a83da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13a83deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13a83e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13a83e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13a83ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13a83f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13a83f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13a83f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13a83fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13a840230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13a8406a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13a840b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13a840f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13a841b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13a841dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13a842080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13a8424f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13a842960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13a842dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13a843240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13a8436b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13a843b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13a843f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13a844400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13a844870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13a844ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13a845150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13a8455c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13a845a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13a845ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13a846310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13a846780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13a846bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13a847060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13a8474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13a847940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13a847db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13a848220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13a848690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13a848b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13a848f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13a8493e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13a849850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13a849cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13a84a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13a84a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13a84aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13a84ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13a84b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13a84b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13a84bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13a84c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13a84c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13a84c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13a84cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13a84d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13a84d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13a84dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13a84df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13a84e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13a84e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13a84eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13a84f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13a84f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13a84f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13a84fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13a8502d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13a850740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13a850bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13a851020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13a851490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13a851900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13a851d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13a8521e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13a852650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13a852ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13a852f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13a8533a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13a853810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13a853c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13a8540f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13a854560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13a8549d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13a854e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13a8552b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13a855720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13a856190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13a8568b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13a856fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13a8576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13a8579b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13a857e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13a858420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13a858a30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.956s
user	0m0.235s
sys	0m0.180s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
