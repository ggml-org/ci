### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.33 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    1.86 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.24 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.71 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.44 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.34 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.51 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.35 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    1.02 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.34 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.34 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    1.05 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.22 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.29 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    2.38 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.18 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.18 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.23 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.19 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed  179.46 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.88 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   26.43 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.33 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.21 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.24 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    = 222.93 sec*proc (27 tests)

Total Test time (real) = 222.94 sec

real	3m42.970s
user	7m30.849s
sys	0m5.900s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.24 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    0.31 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.16 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.18 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.27 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    1.04 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.17 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.17 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.17 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.17 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed   29.18 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.38 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   14.26 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.21 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.19 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.14 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    =  51.31 sec*proc (27 tests)

Total Test time (real) =  51.32 sec

real	0m51.330s
user	1m11.675s
sys	0m5.309s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.119 I build: 4210 (5acff8f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.693 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.474 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.026.483 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.486 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.026.487 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.488 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.026.489 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.026.489 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.026.491 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.026.492 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.026.493 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.026.499 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.026.500 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.026.505 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.026.506 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.026.509 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.026.510 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.026.510 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.026.511 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.026.512 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.031.720 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.033.421 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.424 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.033.425 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.033.426 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.033.426 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.033.427 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.033.427 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.033.428 I llama_model_loader: - type  f32:  124 tensors
0.00.033.429 I llama_model_loader: - type  f16:   73 tensors
0.00.038.744 I llm_load_vocab: special tokens cache size = 5
0.00.041.065 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.041.071 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.041.072 I llm_load_print_meta: arch             = bert
0.00.041.072 I llm_load_print_meta: vocab type       = WPM
0.00.041.073 I llm_load_print_meta: n_vocab          = 30522
0.00.041.073 I llm_load_print_meta: n_merges         = 0
0.00.041.073 I llm_load_print_meta: vocab_only       = 0
0.00.041.074 I llm_load_print_meta: n_ctx_train      = 512
0.00.041.074 I llm_load_print_meta: n_embd           = 384
0.00.041.074 I llm_load_print_meta: n_layer          = 12
0.00.041.078 I llm_load_print_meta: n_head           = 12
0.00.041.079 I llm_load_print_meta: n_head_kv        = 12
0.00.041.080 I llm_load_print_meta: n_rot            = 32
0.00.041.080 I llm_load_print_meta: n_swa            = 0
0.00.041.080 I llm_load_print_meta: n_embd_head_k    = 32
0.00.041.080 I llm_load_print_meta: n_embd_head_v    = 32
0.00.041.081 I llm_load_print_meta: n_gqa            = 1
0.00.041.083 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.041.083 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.041.084 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.041.085 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.041.085 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.041.085 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.041.086 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.041.087 I llm_load_print_meta: n_ff             = 1536
0.00.041.087 I llm_load_print_meta: n_expert         = 0
0.00.041.087 I llm_load_print_meta: n_expert_used    = 0
0.00.041.088 I llm_load_print_meta: causal attn      = 0
0.00.041.088 I llm_load_print_meta: pooling type     = 2
0.00.041.088 I llm_load_print_meta: rope type        = 2
0.00.041.088 I llm_load_print_meta: rope scaling     = linear
0.00.041.089 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.041.090 I llm_load_print_meta: freq_scale_train = 1
0.00.041.090 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.041.091 I llm_load_print_meta: rope_finetuned   = unknown
0.00.041.091 I llm_load_print_meta: ssm_d_conv       = 0
0.00.041.091 I llm_load_print_meta: ssm_d_inner      = 0
0.00.041.092 I llm_load_print_meta: ssm_d_state      = 0
0.00.041.092 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.041.092 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.041.106 I llm_load_print_meta: model type       = 33M
0.00.041.107 I llm_load_print_meta: model ftype      = F16
0.00.041.107 I llm_load_print_meta: model params     = 33.21 M
0.00.041.108 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.041.109 I llm_load_print_meta: general.name     = Bge Small
0.00.041.112 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.041.113 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.041.113 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.041.113 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.041.113 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.041.114 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.041.114 I llm_load_print_meta: max token length = 21
0.00.043.405 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.043.406 I llm_load_tensors: offloading output layer to GPU
0.00.043.406 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.043.434 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.043.435 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.044.097 I llama_new_context_with_model: n_seq_max     = 1
0.00.044.099 I llama_new_context_with_model: n_ctx         = 512
0.00.044.099 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.044.099 I llama_new_context_with_model: n_batch       = 2048
0.00.044.100 I llama_new_context_with_model: n_ubatch      = 2048
0.00.044.100 I llama_new_context_with_model: flash_attn    = 0
0.00.044.101 I llama_new_context_with_model: freq_base     = 10000.0
0.00.044.101 I llama_new_context_with_model: freq_scale    = 1
0.00.044.101 I ggml_metal_init: allocating
0.00.044.106 I ggml_metal_init: found device: Apple M4
0.00.044.112 I ggml_metal_init: picking default device: Apple M4
0.00.045.102 I ggml_metal_init: using embedded metal library
0.00.049.102 I ggml_metal_init: GPU name:   Apple M4
0.00.049.105 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.049.106 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.049.106 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.049.107 I ggml_metal_init: simdgroup reduction   = true
0.00.049.107 I ggml_metal_init: simdgroup matrix mul. = true
0.00.049.107 I ggml_metal_init: has bfloat            = true
0.00.049.107 I ggml_metal_init: use bfloat            = true
0.00.049.108 I ggml_metal_init: hasUnifiedMemory      = true
0.00.049.109 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.061.227 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.061.230 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.061.232 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.062.201 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.062.203 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.062.203 I llama_new_context_with_model: graph nodes  = 429
0.00.062.204 I llama_new_context_with_model: graph splits = 2
0.00.062.226 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.069.815 I 
0.00.069.847 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.070.633 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.076.020 I llama_perf_context_print:        load time =      47.11 ms
0.00.076.021 I llama_perf_context_print: prompt eval time =       5.21 ms /     9 tokens (    0.58 ms per token,  1727.12 tokens per second)
0.00.076.022 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.076.024 I llama_perf_context_print:       total time =       6.20 ms /    10 tokens
0.00.076.181 I ggml_metal_free: deallocating

real	0m0.270s
user	0m0.052s
sys	0m0.033s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.039 I build: 4210 (5acff8f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.458 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.719 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.723 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.724 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.729 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.730 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.730 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.730 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.731 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.731 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.732 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.732 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.732 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.734 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.734 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.735 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.735 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.735 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.736 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.737 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.398 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.152 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.153 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.153 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.154 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.154 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.015.154 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.154 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.015.155 I llama_model_loader: - type  f32:  124 tensors
0.00.015.155 I llama_model_loader: - type q8_0:   73 tensors
0.00.017.875 I llm_load_vocab: special tokens cache size = 5
0.00.019.288 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.019.291 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.019.291 I llm_load_print_meta: arch             = bert
0.00.019.291 I llm_load_print_meta: vocab type       = WPM
0.00.019.292 I llm_load_print_meta: n_vocab          = 30522
0.00.019.292 I llm_load_print_meta: n_merges         = 0
0.00.019.292 I llm_load_print_meta: vocab_only       = 0
0.00.019.292 I llm_load_print_meta: n_ctx_train      = 512
0.00.019.292 I llm_load_print_meta: n_embd           = 384
0.00.019.293 I llm_load_print_meta: n_layer          = 12
0.00.019.295 I llm_load_print_meta: n_head           = 12
0.00.019.297 I llm_load_print_meta: n_head_kv        = 12
0.00.019.297 I llm_load_print_meta: n_rot            = 32
0.00.019.297 I llm_load_print_meta: n_swa            = 0
0.00.019.297 I llm_load_print_meta: n_embd_head_k    = 32
0.00.019.297 I llm_load_print_meta: n_embd_head_v    = 32
0.00.019.298 I llm_load_print_meta: n_gqa            = 1
0.00.019.298 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.019.299 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.019.299 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.019.300 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.019.300 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.019.300 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.019.300 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.019.301 I llm_load_print_meta: n_ff             = 1536
0.00.019.301 I llm_load_print_meta: n_expert         = 0
0.00.019.301 I llm_load_print_meta: n_expert_used    = 0
0.00.019.301 I llm_load_print_meta: causal attn      = 0
0.00.019.301 I llm_load_print_meta: pooling type     = 2
0.00.019.301 I llm_load_print_meta: rope type        = 2
0.00.019.301 I llm_load_print_meta: rope scaling     = linear
0.00.019.302 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.019.303 I llm_load_print_meta: freq_scale_train = 1
0.00.019.304 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.019.304 I llm_load_print_meta: rope_finetuned   = unknown
0.00.019.304 I llm_load_print_meta: ssm_d_conv       = 0
0.00.019.304 I llm_load_print_meta: ssm_d_inner      = 0
0.00.019.304 I llm_load_print_meta: ssm_d_state      = 0
0.00.019.304 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.019.304 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.019.310 I llm_load_print_meta: model type       = 33M
0.00.019.311 I llm_load_print_meta: model ftype      = Q8_0
0.00.019.311 I llm_load_print_meta: model params     = 33.21 M
0.00.019.311 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.019.312 I llm_load_print_meta: general.name     = Bge Small
0.00.019.312 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.019.312 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.019.312 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.019.312 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.019.313 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.019.313 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.019.313 I llm_load_print_meta: max token length = 21
0.00.020.507 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.020.508 I llm_load_tensors: offloading output layer to GPU
0.00.020.508 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.020.514 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.515 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.901 I llama_new_context_with_model: n_seq_max     = 1
0.00.020.902 I llama_new_context_with_model: n_ctx         = 512
0.00.020.902 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.020.902 I llama_new_context_with_model: n_batch       = 2048
0.00.020.903 I llama_new_context_with_model: n_ubatch      = 2048
0.00.020.903 I llama_new_context_with_model: flash_attn    = 0
0.00.020.903 I llama_new_context_with_model: freq_base     = 10000.0
0.00.020.903 I llama_new_context_with_model: freq_scale    = 1
0.00.020.904 I ggml_metal_init: allocating
0.00.020.907 I ggml_metal_init: found device: Apple M4
0.00.020.909 I ggml_metal_init: picking default device: Apple M4
0.00.021.404 I ggml_metal_init: using embedded metal library
0.00.023.551 I ggml_metal_init: GPU name:   Apple M4
0.00.023.553 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.553 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.554 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.554 I ggml_metal_init: simdgroup reduction   = true
0.00.023.554 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.554 I ggml_metal_init: has bfloat            = true
0.00.023.554 I ggml_metal_init: use bfloat            = true
0.00.023.555 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.555 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.032.215 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.032.218 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.032.219 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.032.891 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.032.893 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.032.893 I llama_new_context_with_model: graph nodes  = 429
0.00.032.893 I llama_new_context_with_model: graph splits = 2
0.00.032.906 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.038.134 I 
0.00.038.160 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.038.691 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.043.134 I llama_perf_context_print:        load time =      28.67 ms
0.00.043.135 I llama_perf_context_print: prompt eval time =       4.30 ms /     9 tokens (    0.48 ms per token,  2091.08 tokens per second)
0.00.043.136 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.043.137 I llama_perf_context_print:       total time =       5.00 ms /    10 tokens
0.00.043.298 I ggml_metal_free: deallocating

real	0m0.057s
user	0m0.030s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.169 I build: 4210 (5acff8f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.064 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.032.478 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.032.483 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.486 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.032.487 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.488 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.032.488 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.032.489 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.032.490 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.032.491 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.032.492 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.032.493 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.032.493 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.032.497 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.032.497 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.032.498 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.032.499 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.499 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.040.527 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.043.049 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.004 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.048.006 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.006 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.048.007 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.048.007 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.048.007 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.048.008 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.048.008 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.048.008 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.048.009 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.048.009 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.048.010 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.048.010 I llama_model_loader: - type  f32:   41 tensors
0.00.048.017 I llama_model_loader: - type  f16:   29 tensors
0.00.066.852 W llm_load_vocab: empty token at index 5
0.00.071.520 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.072.877 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.072.903 I llm_load_vocab: special tokens cache size = 5
0.00.313.815 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.313.821 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.313.822 I llm_load_print_meta: arch             = jina-bert-v2
0.00.313.822 I llm_load_print_meta: vocab type       = BPE
0.00.313.823 I llm_load_print_meta: n_vocab          = 61056
0.00.313.823 I llm_load_print_meta: n_merges         = 39382
0.00.313.823 I llm_load_print_meta: vocab_only       = 0
0.00.313.823 I llm_load_print_meta: n_ctx_train      = 8192
0.00.313.823 I llm_load_print_meta: n_embd           = 384
0.00.313.823 I llm_load_print_meta: n_layer          = 4
0.00.313.829 I llm_load_print_meta: n_head           = 12
0.00.313.830 I llm_load_print_meta: n_head_kv        = 12
0.00.313.830 I llm_load_print_meta: n_rot            = 32
0.00.313.830 I llm_load_print_meta: n_swa            = 0
0.00.313.830 I llm_load_print_meta: n_embd_head_k    = 32
0.00.313.830 I llm_load_print_meta: n_embd_head_v    = 32
0.00.313.831 I llm_load_print_meta: n_gqa            = 1
0.00.313.832 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.313.832 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.313.833 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.313.833 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.313.833 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.313.834 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.313.834 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.313.835 I llm_load_print_meta: n_ff             = 1536
0.00.313.835 I llm_load_print_meta: n_expert         = 0
0.00.313.835 I llm_load_print_meta: n_expert_used    = 0
0.00.313.836 I llm_load_print_meta: causal attn      = 0
0.00.313.836 I llm_load_print_meta: pooling type     = -1
0.00.313.836 I llm_load_print_meta: rope type        = -1
0.00.313.836 I llm_load_print_meta: rope scaling     = linear
0.00.313.836 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.313.837 I llm_load_print_meta: freq_scale_train = 1
0.00.313.837 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.313.837 I llm_load_print_meta: rope_finetuned   = unknown
0.00.313.838 I llm_load_print_meta: ssm_d_conv       = 0
0.00.313.839 I llm_load_print_meta: ssm_d_inner      = 0
0.00.313.839 I llm_load_print_meta: ssm_d_state      = 0
0.00.313.839 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.313.841 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.313.866 I llm_load_print_meta: model type       = 33M
0.00.313.867 I llm_load_print_meta: model ftype      = F16
0.00.313.868 I llm_load_print_meta: model params     = 32.90 M
0.00.313.868 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.313.868 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.313.869 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.313.870 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.313.870 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.313.870 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.313.870 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.313.870 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.313.871 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.313.871 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.313.871 I llm_load_print_meta: max token length = 45
0.00.314.969 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.314.969 I llm_load_tensors: offloading output layer to GPU
0.00.314.969 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.314.994 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.314.995 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.315.915 I llama_new_context_with_model: n_seq_max     = 1
0.00.315.916 I llama_new_context_with_model: n_ctx         = 8192
0.00.315.916 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.315.916 I llama_new_context_with_model: n_batch       = 2048
0.00.315.916 I llama_new_context_with_model: n_ubatch      = 2048
0.00.315.916 I llama_new_context_with_model: flash_attn    = 0
0.00.315.917 I llama_new_context_with_model: freq_base     = 10000.0
0.00.315.917 I llama_new_context_with_model: freq_scale    = 1
0.00.315.917 I ggml_metal_init: allocating
0.00.315.921 I ggml_metal_init: found device: Apple M4
0.00.315.923 I ggml_metal_init: picking default device: Apple M4
0.00.316.846 I ggml_metal_init: using embedded metal library
0.00.319.255 I ggml_metal_init: GPU name:   Apple M4
0.00.319.257 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.319.257 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.319.257 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.319.258 I ggml_metal_init: simdgroup reduction   = true
0.00.319.258 I ggml_metal_init: simdgroup matrix mul. = true
0.00.319.258 I ggml_metal_init: has bfloat            = true
0.00.319.258 I ggml_metal_init: use bfloat            = true
0.00.319.259 I ggml_metal_init: hasUnifiedMemory      = true
0.00.319.259 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.329.696 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.329.698 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.329.699 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.330.311 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.330.312 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.330.312 I llama_new_context_with_model: graph nodes  = 154
0.00.330.312 I llama_new_context_with_model: graph splits = 2
0.00.330.330 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.342.201 I 
0.00.342.235 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.342.388 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.342.389 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.342.394 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.342.395 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.342.398 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.342.398 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.342.917 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.346.723 I llama_perf_context_print:        load time =     320.13 ms
0.00.346.724 I llama_perf_context_print: prompt eval time =       3.80 ms /    62 tokens (    0.06 ms per token, 16324.38 tokens per second)
0.00.346.725 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.346.725 I llama_perf_context_print:       total time =       4.52 ms /    63 tokens
0.00.346.898 I ggml_metal_free: deallocating

real	0m1.045s
user	0m0.322s
sys	0m0.043s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.145 I build: 4210 (5acff8f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.282 I main: llama backend init
0.00.000.303 I main: load the model and apply lora adapter, if any
0.00.038.271 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.050.789 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.050.801 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.050.808 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.050.809 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.050.809 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.050.810 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.050.811 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.050.813 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.050.813 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.050.814 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.050.815 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.050.816 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.050.816 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.050.817 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.050.821 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.050.822 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.050.822 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.059.952 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.062.099 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.069.404 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.069.406 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.069.407 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.069.407 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.069.408 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.069.409 I llama_model_loader: - type  f32:  194 tensors
0.00.069.409 I llama_model_loader: - type  f16:   98 tensors
0.00.099.022 I llm_load_vocab: special tokens cache size = 25
0.00.105.739 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.105.741 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.105.741 I llm_load_print_meta: arch             = gptneox
0.00.105.742 I llm_load_print_meta: vocab type       = BPE
0.00.105.742 I llm_load_print_meta: n_vocab          = 50304
0.00.105.742 I llm_load_print_meta: n_merges         = 50009
0.00.105.742 I llm_load_print_meta: vocab_only       = 0
0.00.105.742 I llm_load_print_meta: n_ctx_train      = 2048
0.00.105.743 I llm_load_print_meta: n_embd           = 2048
0.00.105.743 I llm_load_print_meta: n_layer          = 24
0.00.105.746 I llm_load_print_meta: n_head           = 16
0.00.105.746 I llm_load_print_meta: n_head_kv        = 16
0.00.105.747 I llm_load_print_meta: n_rot            = 32
0.00.105.747 I llm_load_print_meta: n_swa            = 0
0.00.105.749 I llm_load_print_meta: n_embd_head_k    = 128
0.00.105.749 I llm_load_print_meta: n_embd_head_v    = 128
0.00.105.749 I llm_load_print_meta: n_gqa            = 1
0.00.105.750 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.105.751 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.105.751 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.105.752 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.105.752 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.105.752 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.105.752 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.105.753 I llm_load_print_meta: n_ff             = 8192
0.00.105.753 I llm_load_print_meta: n_expert         = 0
0.00.105.753 I llm_load_print_meta: n_expert_used    = 0
0.00.105.753 I llm_load_print_meta: causal attn      = 1
0.00.105.753 I llm_load_print_meta: pooling type     = 0
0.00.105.753 I llm_load_print_meta: rope type        = 2
0.00.105.753 I llm_load_print_meta: rope scaling     = linear
0.00.105.754 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.105.754 I llm_load_print_meta: freq_scale_train = 1
0.00.105.754 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.105.755 I llm_load_print_meta: rope_finetuned   = unknown
0.00.105.755 I llm_load_print_meta: ssm_d_conv       = 0
0.00.105.755 I llm_load_print_meta: ssm_d_inner      = 0
0.00.105.755 I llm_load_print_meta: ssm_d_state      = 0
0.00.105.755 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.105.755 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.105.767 I llm_load_print_meta: model type       = 1.4B
0.00.105.769 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.105.769 I llm_load_print_meta: model params     = 1.41 B
0.00.105.769 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.105.770 I llm_load_print_meta: general.name     = 1.4B
0.00.105.770 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.105.771 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.105.771 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.105.772 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.105.772 I llm_load_print_meta: LF token         = 128 ''
0.00.105.772 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.105.772 I llm_load_print_meta: max token length = 1024
0.00.108.380 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.108.380 I llm_load_tensors: offloading output layer to GPU
0.00.108.380 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.108.397 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.108.398 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.109.311 I llama_new_context_with_model: n_seq_max     = 1
0.00.109.312 I llama_new_context_with_model: n_ctx         = 2048
0.00.109.312 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.109.312 I llama_new_context_with_model: n_batch       = 2048
0.00.109.313 I llama_new_context_with_model: n_ubatch      = 512
0.00.109.313 I llama_new_context_with_model: flash_attn    = 0
0.00.109.313 I llama_new_context_with_model: freq_base     = 10000.0
0.00.109.313 I llama_new_context_with_model: freq_scale    = 1
0.00.109.314 I ggml_metal_init: allocating
0.00.109.317 I ggml_metal_init: found device: Apple M4
0.00.109.319 I ggml_metal_init: picking default device: Apple M4
0.00.109.924 I ggml_metal_init: using embedded metal library
0.00.120.239 I ggml_metal_init: GPU name:   Apple M4
0.00.120.241 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.120.241 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.120.242 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.120.242 I ggml_metal_init: simdgroup reduction   = true
0.00.120.242 I ggml_metal_init: simdgroup matrix mul. = true
0.00.120.242 I ggml_metal_init: has bfloat            = true
0.00.120.242 I ggml_metal_init: use bfloat            = true
0.00.120.243 I ggml_metal_init: hasUnifiedMemory      = true
0.00.120.243 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.156.499 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.156.504 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.156.522 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.157.430 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.157.431 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.157.431 I llama_new_context_with_model: graph nodes  = 967
0.00.157.432 I llama_new_context_with_model: graph splits = 2
0.00.157.470 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.233.652 I main: llama threadpool init, n_threads = 4
0.00.233.684 I 
0.00.233.717 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.233.718 I 
0.00.233.791 I sampler seed: 1234
0.00.233.795 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.233.819 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.233.821 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.233.821 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling depressed, and I've had a hard time getting out of bed. I think the only thing I

0.02.093.064 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52631.58 tokens per second)
0.02.093.065 I llama_perf_context_print:        load time =     195.37 ms
0.02.093.066 I llama_perf_context_print: prompt eval time =      37.56 ms /     7 tokens (    5.37 ms per token,   186.39 tokens per second)
0.02.093.067 I llama_perf_context_print:        eval time =    1818.60 ms /    63 runs   (   28.87 ms per token,    34.64 tokens per second)
0.02.093.067 I llama_perf_context_print:       total time =    1859.41 ms /    70 tokens
0.02.093.246 I ggml_metal_free: deallocating

real	0m2.387s
user	0m0.145s
sys	0m0.101s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.615 I build: 4210 (5acff8f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.164 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.032.221 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.032.231 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.234 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.235 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.236 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.237 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.237 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.239 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.240 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.240 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.245 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.246 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.246 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.247 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.250 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.251 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.252 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.040.913 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.042.931 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.006 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.050.008 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.009 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.050.009 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.050.009 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.050.010 I llama_model_loader: - type  f32:  194 tensors
0.00.050.011 I llama_model_loader: - type  f16:   98 tensors
0.00.078.943 I llm_load_vocab: special tokens cache size = 25
0.00.085.561 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.085.564 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.085.564 I llm_load_print_meta: arch             = gptneox
0.00.085.564 I llm_load_print_meta: vocab type       = BPE
0.00.085.564 I llm_load_print_meta: n_vocab          = 50304
0.00.085.565 I llm_load_print_meta: n_merges         = 50009
0.00.085.565 I llm_load_print_meta: vocab_only       = 0
0.00.085.565 I llm_load_print_meta: n_ctx_train      = 2048
0.00.085.565 I llm_load_print_meta: n_embd           = 2048
0.00.085.565 I llm_load_print_meta: n_layer          = 24
0.00.085.567 I llm_load_print_meta: n_head           = 16
0.00.085.568 I llm_load_print_meta: n_head_kv        = 16
0.00.085.568 I llm_load_print_meta: n_rot            = 32
0.00.085.568 I llm_load_print_meta: n_swa            = 0
0.00.085.569 I llm_load_print_meta: n_embd_head_k    = 128
0.00.085.569 I llm_load_print_meta: n_embd_head_v    = 128
0.00.085.570 I llm_load_print_meta: n_gqa            = 1
0.00.085.570 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.085.571 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.085.571 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.085.574 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.085.574 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.085.574 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.085.574 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.085.575 I llm_load_print_meta: n_ff             = 8192
0.00.085.575 I llm_load_print_meta: n_expert         = 0
0.00.085.575 I llm_load_print_meta: n_expert_used    = 0
0.00.085.575 I llm_load_print_meta: causal attn      = 1
0.00.085.575 I llm_load_print_meta: pooling type     = 0
0.00.085.576 I llm_load_print_meta: rope type        = 2
0.00.085.576 I llm_load_print_meta: rope scaling     = linear
0.00.085.576 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.085.576 I llm_load_print_meta: freq_scale_train = 1
0.00.085.577 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.085.577 I llm_load_print_meta: rope_finetuned   = unknown
0.00.085.577 I llm_load_print_meta: ssm_d_conv       = 0
0.00.085.577 I llm_load_print_meta: ssm_d_inner      = 0
0.00.085.577 I llm_load_print_meta: ssm_d_state      = 0
0.00.085.577 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.085.578 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.085.589 I llm_load_print_meta: model type       = 1.4B
0.00.085.590 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.085.590 I llm_load_print_meta: model params     = 1.41 B
0.00.085.591 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.085.591 I llm_load_print_meta: general.name     = 1.4B
0.00.085.591 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.085.591 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.085.591 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.085.592 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.085.592 I llm_load_print_meta: LF token         = 128 ''
0.00.085.592 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.085.592 I llm_load_print_meta: max token length = 1024
0.00.088.095 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.088.095 I llm_load_tensors: offloading output layer to GPU
0.00.088.095 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.088.105 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.088.106 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.089.056 I llama_new_context_with_model: n_seq_max     = 1
0.00.089.057 I llama_new_context_with_model: n_ctx         = 128
0.00.089.057 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.089.058 I llama_new_context_with_model: n_batch       = 128
0.00.089.058 I llama_new_context_with_model: n_ubatch      = 128
0.00.089.058 I llama_new_context_with_model: flash_attn    = 0
0.00.089.058 I llama_new_context_with_model: freq_base     = 10000.0
0.00.089.059 I llama_new_context_with_model: freq_scale    = 1
0.00.089.059 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.089.059 I ggml_metal_init: allocating
0.00.089.076 I ggml_metal_init: found device: Apple M4
0.00.089.080 I ggml_metal_init: picking default device: Apple M4
0.00.089.635 I ggml_metal_init: using embedded metal library
0.00.091.723 I ggml_metal_init: GPU name:   Apple M4
0.00.091.725 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.091.725 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.091.726 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.091.726 I ggml_metal_init: simdgroup reduction   = true
0.00.091.726 I ggml_metal_init: simdgroup matrix mul. = true
0.00.091.726 I ggml_metal_init: has bfloat            = true
0.00.091.726 I ggml_metal_init: use bfloat            = true
0.00.091.727 I ggml_metal_init: hasUnifiedMemory      = true
0.00.091.727 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.101.151 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.101.156 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.101.170 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.102.058 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.102.059 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.102.060 I llama_new_context_with_model: graph nodes  = 967
0.00.102.060 I llama_new_context_with_model: graph splits = 2
0.00.102.072 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.857.634 I 
0.00.857.665 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.857.671 I perplexity: tokenizing the input ..
0.00.869.039 I perplexity: tokenization took 11.366 ms
0.00.869.068 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.989.863 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.00.991.741 I Final estimate: PPL = 10.1498 +/- 3.22650

0.00.991.778 I llama_perf_context_print:        load time =     837.46 ms
0.00.991.780 I llama_perf_context_print: prompt eval time =     120.42 ms /   128 tokens (    0.94 ms per token,  1062.96 tokens per second)
0.00.991.781 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.991.782 I llama_perf_context_print:       total time =     134.15 ms /   129 tokens
0.00.992.316 I ggml_metal_free: deallocating

real	0m1.186s
user	0m0.121s
sys	0m0.192s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4210 (5acff8f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.009.832 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.403 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.023.408 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.410 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.412 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.413 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.413 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.413 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.414 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.415 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.415 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.415 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.416 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.416 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.416 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.420 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.420 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.420 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.314 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.399 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.353 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.032.355 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.355 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.356 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.356 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.356 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.032.357 I llama_model_loader: - type  f32:  194 tensors
0.00.032.357 I llama_model_loader: - type q8_0:   98 tensors
0.00.054.003 I llm_load_vocab: special tokens cache size = 25
0.00.060.187 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.060.191 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.060.191 I llm_load_print_meta: arch             = gptneox
0.00.060.191 I llm_load_print_meta: vocab type       = BPE
0.00.060.192 I llm_load_print_meta: n_vocab          = 50304
0.00.060.192 I llm_load_print_meta: n_merges         = 50009
0.00.060.192 I llm_load_print_meta: vocab_only       = 0
0.00.060.192 I llm_load_print_meta: n_ctx_train      = 2048
0.00.060.192 I llm_load_print_meta: n_embd           = 2048
0.00.060.193 I llm_load_print_meta: n_layer          = 24
0.00.060.197 I llm_load_print_meta: n_head           = 16
0.00.060.200 I llm_load_print_meta: n_head_kv        = 16
0.00.060.200 I llm_load_print_meta: n_rot            = 32
0.00.060.200 I llm_load_print_meta: n_swa            = 0
0.00.060.200 I llm_load_print_meta: n_embd_head_k    = 128
0.00.060.201 I llm_load_print_meta: n_embd_head_v    = 128
0.00.060.202 I llm_load_print_meta: n_gqa            = 1
0.00.060.202 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.060.203 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.060.204 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.060.205 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.060.207 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.060.207 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.060.207 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.060.208 I llm_load_print_meta: n_ff             = 8192
0.00.060.208 I llm_load_print_meta: n_expert         = 0
0.00.060.208 I llm_load_print_meta: n_expert_used    = 0
0.00.060.208 I llm_load_print_meta: causal attn      = 1
0.00.060.208 I llm_load_print_meta: pooling type     = 0
0.00.060.208 I llm_load_print_meta: rope type        = 2
0.00.060.209 I llm_load_print_meta: rope scaling     = linear
0.00.060.209 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.060.209 I llm_load_print_meta: freq_scale_train = 1
0.00.060.209 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.060.210 I llm_load_print_meta: rope_finetuned   = unknown
0.00.060.210 I llm_load_print_meta: ssm_d_conv       = 0
0.00.060.210 I llm_load_print_meta: ssm_d_inner      = 0
0.00.060.210 I llm_load_print_meta: ssm_d_state      = 0
0.00.060.210 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.060.210 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.060.223 I llm_load_print_meta: model type       = 1.4B
0.00.060.224 I llm_load_print_meta: model ftype      = Q8_0
0.00.060.224 I llm_load_print_meta: model params     = 1.41 B
0.00.060.224 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.060.225 I llm_load_print_meta: general.name     = 1.4B
0.00.060.225 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.060.225 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.060.225 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.060.225 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.060.226 I llm_load_print_meta: LF token         = 128 ''
0.00.060.226 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.060.226 I llm_load_print_meta: max token length = 1024
0.00.062.714 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.062.715 I llm_load_tensors: offloading output layer to GPU
0.00.062.715 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.062.726 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.062.728 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.063.695 I llama_new_context_with_model: n_seq_max     = 1
0.00.063.696 I llama_new_context_with_model: n_ctx         = 2048
0.00.063.696 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.063.696 I llama_new_context_with_model: n_batch       = 2048
0.00.063.696 I llama_new_context_with_model: n_ubatch      = 512
0.00.063.696 I llama_new_context_with_model: flash_attn    = 0
0.00.063.697 I llama_new_context_with_model: freq_base     = 10000.0
0.00.063.697 I llama_new_context_with_model: freq_scale    = 1
0.00.063.698 I ggml_metal_init: allocating
0.00.063.704 I ggml_metal_init: found device: Apple M4
0.00.063.707 I ggml_metal_init: picking default device: Apple M4
0.00.064.405 I ggml_metal_init: using embedded metal library
0.00.066.553 I ggml_metal_init: GPU name:   Apple M4
0.00.066.556 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.066.556 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.066.557 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.066.557 I ggml_metal_init: simdgroup reduction   = true
0.00.066.557 I ggml_metal_init: simdgroup matrix mul. = true
0.00.066.557 I ggml_metal_init: has bfloat            = true
0.00.066.558 I ggml_metal_init: use bfloat            = true
0.00.066.558 I ggml_metal_init: hasUnifiedMemory      = true
0.00.066.559 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.100.135 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.100.142 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.100.166 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.101.236 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.101.239 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.101.239 I llama_new_context_with_model: graph nodes  = 967
0.00.101.239 I llama_new_context_with_model: graph splits = 2
0.00.101.255 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.226.776 I main: llama threadpool init, n_threads = 4
0.01.226.805 I 
0.01.226.836 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.01.226.836 I 
0.01.227.050 I sampler seed: 1234
0.01.227.054 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.227.066 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.227.066 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.227.067 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.319.092 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61578.49 tokens per second)
0.02.319.092 I llama_perf_context_print:        load time =    1216.94 ms
0.02.319.093 I llama_perf_context_print: prompt eval time =      36.62 ms /     7 tokens (    5.23 ms per token,   191.14 tokens per second)
0.02.319.094 I llama_perf_context_print:        eval time =    1052.45 ms /    63 runs   (   16.71 ms per token,    59.86 tokens per second)
0.02.319.094 I llama_perf_context_print:       total time =    1092.32 ms /    70 tokens
0.02.319.293 I ggml_metal_free: deallocating

real	0m2.339s
user	0m0.113s
sys	0m0.238s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.128 I build: 4210 (5acff8f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.698 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.056 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.020.061 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.063 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.064 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.064 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.064 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.065 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.066 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.066 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.066 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.067 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.067 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.068 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.068 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.070 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.070 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.070 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.490 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.964 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.427 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.032.429 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.429 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.429 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.430 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.430 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.032.431 I llama_model_loader: - type  f32:  194 tensors
0.00.032.431 I llama_model_loader: - type q8_0:   98 tensors
0.00.057.325 I llm_load_vocab: special tokens cache size = 25
0.00.063.484 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.063.487 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.063.487 I llm_load_print_meta: arch             = gptneox
0.00.063.487 I llm_load_print_meta: vocab type       = BPE
0.00.063.488 I llm_load_print_meta: n_vocab          = 50304
0.00.063.488 I llm_load_print_meta: n_merges         = 50009
0.00.063.488 I llm_load_print_meta: vocab_only       = 0
0.00.063.488 I llm_load_print_meta: n_ctx_train      = 2048
0.00.063.488 I llm_load_print_meta: n_embd           = 2048
0.00.063.489 I llm_load_print_meta: n_layer          = 24
0.00.063.492 I llm_load_print_meta: n_head           = 16
0.00.063.493 I llm_load_print_meta: n_head_kv        = 16
0.00.063.493 I llm_load_print_meta: n_rot            = 32
0.00.063.493 I llm_load_print_meta: n_swa            = 0
0.00.063.494 I llm_load_print_meta: n_embd_head_k    = 128
0.00.063.494 I llm_load_print_meta: n_embd_head_v    = 128
0.00.063.494 I llm_load_print_meta: n_gqa            = 1
0.00.063.495 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.063.496 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.063.496 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.063.497 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.063.497 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.063.497 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.063.497 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.063.498 I llm_load_print_meta: n_ff             = 8192
0.00.063.498 I llm_load_print_meta: n_expert         = 0
0.00.063.498 I llm_load_print_meta: n_expert_used    = 0
0.00.063.498 I llm_load_print_meta: causal attn      = 1
0.00.063.498 I llm_load_print_meta: pooling type     = 0
0.00.063.499 I llm_load_print_meta: rope type        = 2
0.00.063.499 I llm_load_print_meta: rope scaling     = linear
0.00.063.499 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.063.499 I llm_load_print_meta: freq_scale_train = 1
0.00.063.500 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.063.500 I llm_load_print_meta: rope_finetuned   = unknown
0.00.063.500 I llm_load_print_meta: ssm_d_conv       = 0
0.00.063.500 I llm_load_print_meta: ssm_d_inner      = 0
0.00.063.501 I llm_load_print_meta: ssm_d_state      = 0
0.00.063.501 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.063.501 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.063.513 I llm_load_print_meta: model type       = 1.4B
0.00.063.513 I llm_load_print_meta: model ftype      = Q8_0
0.00.063.513 I llm_load_print_meta: model params     = 1.41 B
0.00.063.514 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.063.514 I llm_load_print_meta: general.name     = 1.4B
0.00.063.514 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.063.514 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.063.515 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.063.515 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.063.515 I llm_load_print_meta: LF token         = 128 ''
0.00.063.515 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.063.515 I llm_load_print_meta: max token length = 1024
0.00.065.678 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.065.679 I llm_load_tensors: offloading output layer to GPU
0.00.065.679 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.065.689 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.065.690 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.066.644 I llama_new_context_with_model: n_seq_max     = 1
0.00.066.645 I llama_new_context_with_model: n_ctx         = 128
0.00.066.645 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.066.646 I llama_new_context_with_model: n_batch       = 128
0.00.066.646 I llama_new_context_with_model: n_ubatch      = 128
0.00.066.646 I llama_new_context_with_model: flash_attn    = 0
0.00.066.646 I llama_new_context_with_model: freq_base     = 10000.0
0.00.066.647 I llama_new_context_with_model: freq_scale    = 1
0.00.066.647 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.066.647 I ggml_metal_init: allocating
0.00.066.651 I ggml_metal_init: found device: Apple M4
0.00.066.653 I ggml_metal_init: picking default device: Apple M4
0.00.067.225 I ggml_metal_init: using embedded metal library
0.00.069.187 I ggml_metal_init: GPU name:   Apple M4
0.00.069.188 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.069.189 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.069.189 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.069.189 I ggml_metal_init: simdgroup reduction   = true
0.00.069.189 I ggml_metal_init: simdgroup matrix mul. = true
0.00.069.190 I ggml_metal_init: has bfloat            = true
0.00.069.190 I ggml_metal_init: use bfloat            = true
0.00.069.190 I ggml_metal_init: hasUnifiedMemory      = true
0.00.069.192 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.078.528 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.078.531 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.078.548 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.079.476 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.079.477 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.079.477 I llama_new_context_with_model: graph nodes  = 967
0.00.079.477 I llama_new_context_with_model: graph splits = 2
0.00.079.490 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.863.442 I 
0.00.863.469 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.863.472 I perplexity: tokenizing the input ..
0.00.871.571 I perplexity: tokenization took 8.099 ms
0.00.871.585 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.993.452 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.994.645 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.994.658 I llama_perf_context_print:        load time =     851.74 ms
0.00.994.658 I llama_perf_context_print: prompt eval time =     121.62 ms /   128 tokens (    0.95 ms per token,  1052.48 tokens per second)
0.00.994.659 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.994.660 I llama_perf_context_print:       total time =     131.22 ms /   129 tokens
0.00.994.998 I ggml_metal_free: deallocating

real	0m1.013s
user	0m0.092s
sys	0m0.154s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4210 (5acff8f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.017.267 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.666 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.033.671 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.673 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.674 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.674 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.674 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.674 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.678 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.678 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.682 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.682 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.683 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.683 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.683 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.689 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.689 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.689 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.038.421 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.039.819 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.044.604 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.044.605 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.044.606 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.044.606 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.044.607 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.044.607 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.044.608 I llama_model_loader: - type  f32:  194 tensors
0.00.044.608 I llama_model_loader: - type q4_0:   97 tensors
0.00.044.608 I llama_model_loader: - type q6_K:    1 tensors
0.00.076.393 I llm_load_vocab: special tokens cache size = 25
0.00.087.066 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.087.069 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.087.070 I llm_load_print_meta: arch             = gptneox
0.00.087.070 I llm_load_print_meta: vocab type       = BPE
0.00.087.070 I llm_load_print_meta: n_vocab          = 50304
0.00.087.071 I llm_load_print_meta: n_merges         = 50009
0.00.087.071 I llm_load_print_meta: vocab_only       = 0
0.00.087.071 I llm_load_print_meta: n_ctx_train      = 2048
0.00.087.071 I llm_load_print_meta: n_embd           = 2048
0.00.087.072 I llm_load_print_meta: n_layer          = 24
0.00.087.077 I llm_load_print_meta: n_head           = 16
0.00.087.078 I llm_load_print_meta: n_head_kv        = 16
0.00.087.078 I llm_load_print_meta: n_rot            = 32
0.00.087.079 I llm_load_print_meta: n_swa            = 0
0.00.087.079 I llm_load_print_meta: n_embd_head_k    = 128
0.00.087.079 I llm_load_print_meta: n_embd_head_v    = 128
0.00.087.080 I llm_load_print_meta: n_gqa            = 1
0.00.087.083 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.087.084 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.087.085 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.087.085 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.087.086 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.087.086 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.087.086 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.087.087 I llm_load_print_meta: n_ff             = 8192
0.00.087.088 I llm_load_print_meta: n_expert         = 0
0.00.087.088 I llm_load_print_meta: n_expert_used    = 0
0.00.087.090 I llm_load_print_meta: causal attn      = 1
0.00.087.092 I llm_load_print_meta: pooling type     = 0
0.00.087.092 I llm_load_print_meta: rope type        = 2
0.00.087.092 I llm_load_print_meta: rope scaling     = linear
0.00.087.093 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.087.093 I llm_load_print_meta: freq_scale_train = 1
0.00.087.093 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.087.094 I llm_load_print_meta: rope_finetuned   = unknown
0.00.087.094 I llm_load_print_meta: ssm_d_conv       = 0
0.00.087.094 I llm_load_print_meta: ssm_d_inner      = 0
0.00.087.094 I llm_load_print_meta: ssm_d_state      = 0
0.00.087.094 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.087.094 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.087.108 I llm_load_print_meta: model type       = 1.4B
0.00.087.108 I llm_load_print_meta: model ftype      = Q4_0
0.00.087.108 I llm_load_print_meta: model params     = 1.41 B
0.00.087.111 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.087.111 I llm_load_print_meta: general.name     = 1.4B
0.00.087.111 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.087.111 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.087.112 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.087.112 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.087.112 I llm_load_print_meta: LF token         = 128 ''
0.00.087.113 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.087.113 I llm_load_print_meta: max token length = 1024
0.00.089.996 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.089.997 I llm_load_tensors: offloading output layer to GPU
0.00.089.997 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.090.008 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.090.009 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.091.339 I llama_new_context_with_model: n_seq_max     = 1
0.00.091.340 I llama_new_context_with_model: n_ctx         = 2048
0.00.091.341 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.091.341 I llama_new_context_with_model: n_batch       = 2048
0.00.091.341 I llama_new_context_with_model: n_ubatch      = 512
0.00.091.342 I llama_new_context_with_model: flash_attn    = 0
0.00.091.342 I llama_new_context_with_model: freq_base     = 10000.0
0.00.091.342 I llama_new_context_with_model: freq_scale    = 1
0.00.091.343 I ggml_metal_init: allocating
0.00.091.347 I ggml_metal_init: found device: Apple M4
0.00.091.349 I ggml_metal_init: picking default device: Apple M4
0.00.092.215 I ggml_metal_init: using embedded metal library
0.00.095.112 I ggml_metal_init: GPU name:   Apple M4
0.00.095.114 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.095.115 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.095.115 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.095.116 I ggml_metal_init: simdgroup reduction   = true
0.00.095.116 I ggml_metal_init: simdgroup matrix mul. = true
0.00.095.116 I ggml_metal_init: has bfloat            = true
0.00.095.118 I ggml_metal_init: use bfloat            = true
0.00.095.119 I ggml_metal_init: hasUnifiedMemory      = true
0.00.095.119 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.129.387 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.129.401 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.129.425 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.130.427 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.130.429 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.130.429 I llama_new_context_with_model: graph nodes  = 967
0.00.130.429 I llama_new_context_with_model: graph splits = 2
0.00.130.453 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.795.120 I main: llama threadpool init, n_threads = 4
0.00.795.190 I 
0.00.795.241 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.795.243 I 
0.00.795.736 I sampler seed: 1234
0.00.795.743 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.795.773 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.795.775 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.795.775 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.483.899 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61792.86 tokens per second)
0.01.483.899 I llama_perf_context_print:        load time =     777.85 ms
0.01.483.901 I llama_perf_context_print: prompt eval time =      40.27 ms /     7 tokens (    5.75 ms per token,   173.85 tokens per second)
0.01.483.901 I llama_perf_context_print:        eval time =     644.91 ms /    63 runs   (   10.24 ms per token,    97.69 tokens per second)
0.01.483.902 I llama_perf_context_print:       total time =     688.78 ms /    70 tokens
0.01.484.081 I ggml_metal_free: deallocating

real	0m1.509s
user	0m0.135s
sys	0m0.175s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4210 (5acff8f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.133 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.921 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.925 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.927 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.927 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.928 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.928 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.928 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.929 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.930 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.930 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.930 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.931 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.931 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.931 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.933 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.933 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.934 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.732 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.776 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.535 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.536 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.536 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.536 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.537 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.537 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.537 I llama_model_loader: - type  f32:  194 tensors
0.00.024.538 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.538 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.101 I llm_load_vocab: special tokens cache size = 25
0.00.051.307 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.310 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.310 I llm_load_print_meta: arch             = gptneox
0.00.051.310 I llm_load_print_meta: vocab type       = BPE
0.00.051.311 I llm_load_print_meta: n_vocab          = 50304
0.00.051.311 I llm_load_print_meta: n_merges         = 50009
0.00.051.311 I llm_load_print_meta: vocab_only       = 0
0.00.051.311 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.311 I llm_load_print_meta: n_embd           = 2048
0.00.051.312 I llm_load_print_meta: n_layer          = 24
0.00.051.314 I llm_load_print_meta: n_head           = 16
0.00.051.315 I llm_load_print_meta: n_head_kv        = 16
0.00.051.315 I llm_load_print_meta: n_rot            = 32
0.00.051.315 I llm_load_print_meta: n_swa            = 0
0.00.051.315 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.316 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.316 I llm_load_print_meta: n_gqa            = 1
0.00.051.317 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.318 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.319 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.319 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.319 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.319 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.320 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.320 I llm_load_print_meta: n_ff             = 8192
0.00.051.320 I llm_load_print_meta: n_expert         = 0
0.00.051.321 I llm_load_print_meta: n_expert_used    = 0
0.00.051.321 I llm_load_print_meta: causal attn      = 1
0.00.051.321 I llm_load_print_meta: pooling type     = 0
0.00.051.321 I llm_load_print_meta: rope type        = 2
0.00.051.321 I llm_load_print_meta: rope scaling     = linear
0.00.051.322 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.322 I llm_load_print_meta: freq_scale_train = 1
0.00.051.325 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.325 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.326 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.326 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.326 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.326 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.326 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.338 I llm_load_print_meta: model type       = 1.4B
0.00.051.338 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.339 I llm_load_print_meta: model params     = 1.41 B
0.00.051.339 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.339 I llm_load_print_meta: general.name     = 1.4B
0.00.051.340 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.340 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.340 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.340 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.340 I llm_load_print_meta: LF token         = 128 ''
0.00.051.341 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.341 I llm_load_print_meta: max token length = 1024
0.00.053.282 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.282 I llm_load_tensors: offloading output layer to GPU
0.00.053.282 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.292 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.293 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.054.186 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.186 I llama_new_context_with_model: n_ctx         = 128
0.00.054.186 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.187 I llama_new_context_with_model: n_batch       = 128
0.00.054.187 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.187 I llama_new_context_with_model: flash_attn    = 0
0.00.054.187 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.188 I llama_new_context_with_model: freq_scale    = 1
0.00.054.188 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.188 I ggml_metal_init: allocating
0.00.054.191 I ggml_metal_init: found device: Apple M4
0.00.054.193 I ggml_metal_init: picking default device: Apple M4
0.00.054.745 I ggml_metal_init: using embedded metal library
0.00.056.716 I ggml_metal_init: GPU name:   Apple M4
0.00.056.718 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.718 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.718 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.718 I ggml_metal_init: simdgroup reduction   = true
0.00.056.719 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.719 I ggml_metal_init: has bfloat            = true
0.00.056.719 I ggml_metal_init: use bfloat            = true
0.00.056.719 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.720 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.224 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.229 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.242 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.165 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.166 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.166 I llama_new_context_with_model: graph nodes  = 967
0.00.067.166 I llama_new_context_with_model: graph splits = 2
0.00.067.178 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.596.744 I 
0.00.596.791 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.596.796 I perplexity: tokenizing the input ..
0.00.604.653 I perplexity: tokenization took 7.853 ms
0.00.604.664 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.727.496 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.728.670 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.728.694 I llama_perf_context_print:        load time =     586.61 ms
0.00.728.700 I llama_perf_context_print: prompt eval time =     122.60 ms /   128 tokens (    0.96 ms per token,  1044.04 tokens per second)
0.00.728.703 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.728.703 I llama_perf_context_print:       total time =     131.95 ms /   129 tokens
0.00.729.183 I ggml_metal_free: deallocating

real	0m0.746s
user	0m0.078s
sys	0m0.118s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4210 (5acff8f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.017.016 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.542 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.035.547 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.554 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.554 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.555 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.557 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.557 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.558 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.558 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.559 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.559 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.559 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.560 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.563 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.564 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.565 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.565 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.040.898 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.042.650 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.547 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.048.549 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.549 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.048.550 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.048.550 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.048.550 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.048.551 I llama_model_loader: - type  f32:  194 tensors
0.00.048.552 I llama_model_loader: - type q4_1:   97 tensors
0.00.048.552 I llama_model_loader: - type q6_K:    1 tensors
0.00.089.757 I llm_load_vocab: special tokens cache size = 25
0.00.099.285 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.099.288 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.099.289 I llm_load_print_meta: arch             = gptneox
0.00.099.289 I llm_load_print_meta: vocab type       = BPE
0.00.099.289 I llm_load_print_meta: n_vocab          = 50304
0.00.099.290 I llm_load_print_meta: n_merges         = 50009
0.00.099.290 I llm_load_print_meta: vocab_only       = 0
0.00.099.290 I llm_load_print_meta: n_ctx_train      = 2048
0.00.099.290 I llm_load_print_meta: n_embd           = 2048
0.00.099.290 I llm_load_print_meta: n_layer          = 24
0.00.099.294 I llm_load_print_meta: n_head           = 16
0.00.099.295 I llm_load_print_meta: n_head_kv        = 16
0.00.099.297 I llm_load_print_meta: n_rot            = 32
0.00.099.298 I llm_load_print_meta: n_swa            = 0
0.00.099.298 I llm_load_print_meta: n_embd_head_k    = 128
0.00.099.298 I llm_load_print_meta: n_embd_head_v    = 128
0.00.099.299 I llm_load_print_meta: n_gqa            = 1
0.00.099.300 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.099.301 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.099.301 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.099.302 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.099.302 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.099.302 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.099.302 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.099.303 I llm_load_print_meta: n_ff             = 8192
0.00.099.303 I llm_load_print_meta: n_expert         = 0
0.00.099.304 I llm_load_print_meta: n_expert_used    = 0
0.00.099.304 I llm_load_print_meta: causal attn      = 1
0.00.099.304 I llm_load_print_meta: pooling type     = 0
0.00.099.304 I llm_load_print_meta: rope type        = 2
0.00.099.304 I llm_load_print_meta: rope scaling     = linear
0.00.099.305 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.099.305 I llm_load_print_meta: freq_scale_train = 1
0.00.099.306 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.099.306 I llm_load_print_meta: rope_finetuned   = unknown
0.00.099.307 I llm_load_print_meta: ssm_d_conv       = 0
0.00.099.308 I llm_load_print_meta: ssm_d_inner      = 0
0.00.099.308 I llm_load_print_meta: ssm_d_state      = 0
0.00.099.308 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.099.308 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.099.320 I llm_load_print_meta: model type       = 1.4B
0.00.099.321 I llm_load_print_meta: model ftype      = Q4_1
0.00.099.321 I llm_load_print_meta: model params     = 1.41 B
0.00.099.322 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.099.322 I llm_load_print_meta: general.name     = 1.4B
0.00.099.322 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.099.324 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.099.324 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.099.324 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.099.325 I llm_load_print_meta: LF token         = 128 ''
0.00.099.325 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.099.325 I llm_load_print_meta: max token length = 1024
0.00.101.900 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.101.901 I llm_load_tensors: offloading output layer to GPU
0.00.101.901 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.101.911 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.101.913 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.103.203 I llama_new_context_with_model: n_seq_max     = 1
0.00.103.204 I llama_new_context_with_model: n_ctx         = 2048
0.00.103.204 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.103.204 I llama_new_context_with_model: n_batch       = 2048
0.00.103.205 I llama_new_context_with_model: n_ubatch      = 512
0.00.103.205 I llama_new_context_with_model: flash_attn    = 0
0.00.103.205 I llama_new_context_with_model: freq_base     = 10000.0
0.00.103.206 I llama_new_context_with_model: freq_scale    = 1
0.00.103.206 I ggml_metal_init: allocating
0.00.103.210 I ggml_metal_init: found device: Apple M4
0.00.103.213 I ggml_metal_init: picking default device: Apple M4
0.00.103.962 I ggml_metal_init: using embedded metal library
0.00.106.617 I ggml_metal_init: GPU name:   Apple M4
0.00.106.619 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.106.619 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.106.620 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.106.620 I ggml_metal_init: simdgroup reduction   = true
0.00.106.620 I ggml_metal_init: simdgroup matrix mul. = true
0.00.106.620 I ggml_metal_init: has bfloat            = true
0.00.106.620 I ggml_metal_init: use bfloat            = true
0.00.106.621 I ggml_metal_init: hasUnifiedMemory      = true
0.00.106.622 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.136.957 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.136.963 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.136.982 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.137.958 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.137.960 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.137.960 I llama_new_context_with_model: graph nodes  = 967
0.00.137.960 I llama_new_context_with_model: graph splits = 2
0.00.137.984 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.833.210 I main: llama threadpool init, n_threads = 4
0.00.833.286 I 
0.00.833.355 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.833.357 I 
0.00.833.853 I sampler seed: 1234
0.00.833.861 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.833.934 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.833.940 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.833.941 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.570.518 I llama_perf_sampler_print:    sampling time =       1.11 ms /    71 runs   (    0.02 ms per token, 63791.55 tokens per second)
0.01.570.518 I llama_perf_context_print:        load time =     816.18 ms
0.01.570.519 I llama_perf_context_print: prompt eval time =      41.92 ms /     7 tokens (    5.99 ms per token,   167.00 tokens per second)
0.01.570.519 I llama_perf_context_print:        eval time =     691.77 ms /    63 runs   (   10.98 ms per token,    91.07 tokens per second)
0.01.570.520 I llama_perf_context_print:       total time =     737.31 ms /    70 tokens
0.01.570.714 I ggml_metal_free: deallocating

real	0m1.611s
user	0m0.154s
sys	0m0.186s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4210 (5acff8f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.928 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.050 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.054 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.056 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.056 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.056 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.057 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.057 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.058 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.058 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.058 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.059 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.059 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.059 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.060 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.061 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.062 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.062 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.037 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.078 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.958 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.959 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.959 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.960 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.960 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.960 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.961 I llama_model_loader: - type  f32:  194 tensors
0.00.023.961 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.962 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.087 I llm_load_vocab: special tokens cache size = 25
0.00.051.189 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.192 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.192 I llm_load_print_meta: arch             = gptneox
0.00.051.192 I llm_load_print_meta: vocab type       = BPE
0.00.051.193 I llm_load_print_meta: n_vocab          = 50304
0.00.051.193 I llm_load_print_meta: n_merges         = 50009
0.00.051.193 I llm_load_print_meta: vocab_only       = 0
0.00.051.193 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.194 I llm_load_print_meta: n_embd           = 2048
0.00.051.194 I llm_load_print_meta: n_layer          = 24
0.00.051.196 I llm_load_print_meta: n_head           = 16
0.00.051.197 I llm_load_print_meta: n_head_kv        = 16
0.00.051.197 I llm_load_print_meta: n_rot            = 32
0.00.051.197 I llm_load_print_meta: n_swa            = 0
0.00.051.198 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.198 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.198 I llm_load_print_meta: n_gqa            = 1
0.00.051.199 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.200 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.200 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.201 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.201 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.201 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.201 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.202 I llm_load_print_meta: n_ff             = 8192
0.00.051.202 I llm_load_print_meta: n_expert         = 0
0.00.051.202 I llm_load_print_meta: n_expert_used    = 0
0.00.051.203 I llm_load_print_meta: causal attn      = 1
0.00.051.203 I llm_load_print_meta: pooling type     = 0
0.00.051.203 I llm_load_print_meta: rope type        = 2
0.00.051.203 I llm_load_print_meta: rope scaling     = linear
0.00.051.206 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.206 I llm_load_print_meta: freq_scale_train = 1
0.00.051.206 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.206 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.208 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.208 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.208 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.208 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.209 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.220 I llm_load_print_meta: model type       = 1.4B
0.00.051.221 I llm_load_print_meta: model ftype      = Q4_1
0.00.051.221 I llm_load_print_meta: model params     = 1.41 B
0.00.051.221 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.051.222 I llm_load_print_meta: general.name     = 1.4B
0.00.051.222 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.222 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.222 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.222 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.222 I llm_load_print_meta: LF token         = 128 ''
0.00.051.223 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.223 I llm_load_print_meta: max token length = 1024
0.00.052.821 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.821 I llm_load_tensors: offloading output layer to GPU
0.00.052.821 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.831 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.832 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.667 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.668 I llama_new_context_with_model: n_ctx         = 128
0.00.053.668 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.668 I llama_new_context_with_model: n_batch       = 128
0.00.053.668 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.668 I llama_new_context_with_model: flash_attn    = 0
0.00.053.669 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.669 I llama_new_context_with_model: freq_scale    = 1
0.00.053.670 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.670 I ggml_metal_init: allocating
0.00.053.673 I ggml_metal_init: found device: Apple M4
0.00.053.675 I ggml_metal_init: picking default device: Apple M4
0.00.054.196 I ggml_metal_init: using embedded metal library
0.00.056.119 I ggml_metal_init: GPU name:   Apple M4
0.00.056.121 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.121 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.122 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.122 I ggml_metal_init: simdgroup reduction   = true
0.00.056.122 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.122 I ggml_metal_init: has bfloat            = true
0.00.056.122 I ggml_metal_init: use bfloat            = true
0.00.056.123 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.124 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.345 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.349 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.365 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.299 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.300 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.300 I llama_new_context_with_model: graph nodes  = 967
0.00.066.301 I llama_new_context_with_model: graph splits = 2
0.00.066.313 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.640.020 I 
0.00.640.051 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.640.055 I perplexity: tokenizing the input ..
0.00.648.192 I perplexity: tokenization took 8.135 ms
0.00.648.206 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.770.591 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.771.698 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.771.709 I llama_perf_context_print:        load time =     631.09 ms
0.00.771.710 I llama_perf_context_print: prompt eval time =     122.17 ms /   128 tokens (    0.95 ms per token,  1047.75 tokens per second)
0.00.771.711 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.771.711 I llama_perf_context_print:       total time =     131.69 ms /   129 tokens
0.00.772.042 I ggml_metal_free: deallocating

real	0m0.785s
user	0m0.079s
sys	0m0.112s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4210 (5acff8f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.950 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.172 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.028.177 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.179 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.184 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.184 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.184 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.185 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.186 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.186 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.186 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.187 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.187 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.187 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.188 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.191 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.191 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.191 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.093 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.125 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.141 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.037.142 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.142 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.143 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.143 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.143 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.037.144 I llama_model_loader: - type  f32:  194 tensors
0.00.037.144 I llama_model_loader: - type q5_0:   97 tensors
0.00.037.144 I llama_model_loader: - type q6_K:    1 tensors
0.00.061.914 I llm_load_vocab: special tokens cache size = 25
0.00.068.609 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.068.612 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.068.612 I llm_load_print_meta: arch             = gptneox
0.00.068.612 I llm_load_print_meta: vocab type       = BPE
0.00.068.613 I llm_load_print_meta: n_vocab          = 50304
0.00.068.613 I llm_load_print_meta: n_merges         = 50009
0.00.068.613 I llm_load_print_meta: vocab_only       = 0
0.00.068.613 I llm_load_print_meta: n_ctx_train      = 2048
0.00.068.613 I llm_load_print_meta: n_embd           = 2048
0.00.068.613 I llm_load_print_meta: n_layer          = 24
0.00.068.616 I llm_load_print_meta: n_head           = 16
0.00.068.617 I llm_load_print_meta: n_head_kv        = 16
0.00.068.617 I llm_load_print_meta: n_rot            = 32
0.00.068.617 I llm_load_print_meta: n_swa            = 0
0.00.068.618 I llm_load_print_meta: n_embd_head_k    = 128
0.00.068.618 I llm_load_print_meta: n_embd_head_v    = 128
0.00.068.619 I llm_load_print_meta: n_gqa            = 1
0.00.068.619 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.068.620 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.068.620 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.068.620 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.068.621 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.068.622 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.068.622 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.068.623 I llm_load_print_meta: n_ff             = 8192
0.00.068.623 I llm_load_print_meta: n_expert         = 0
0.00.068.623 I llm_load_print_meta: n_expert_used    = 0
0.00.068.625 I llm_load_print_meta: causal attn      = 1
0.00.068.626 I llm_load_print_meta: pooling type     = 0
0.00.068.626 I llm_load_print_meta: rope type        = 2
0.00.068.626 I llm_load_print_meta: rope scaling     = linear
0.00.068.627 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.068.627 I llm_load_print_meta: freq_scale_train = 1
0.00.068.627 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.068.627 I llm_load_print_meta: rope_finetuned   = unknown
0.00.068.627 I llm_load_print_meta: ssm_d_conv       = 0
0.00.068.628 I llm_load_print_meta: ssm_d_inner      = 0
0.00.068.628 I llm_load_print_meta: ssm_d_state      = 0
0.00.068.628 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.068.628 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.068.640 I llm_load_print_meta: model type       = 1.4B
0.00.068.640 I llm_load_print_meta: model ftype      = Q5_0
0.00.068.640 I llm_load_print_meta: model params     = 1.41 B
0.00.068.641 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.068.641 I llm_load_print_meta: general.name     = 1.4B
0.00.068.641 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.068.641 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.068.642 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.068.642 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.068.642 I llm_load_print_meta: LF token         = 128 ''
0.00.068.643 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.068.643 I llm_load_print_meta: max token length = 1024
0.00.070.824 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.070.824 I llm_load_tensors: offloading output layer to GPU
0.00.070.824 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.070.834 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.070.835 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.071.862 I llama_new_context_with_model: n_seq_max     = 1
0.00.071.863 I llama_new_context_with_model: n_ctx         = 2048
0.00.071.863 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.071.863 I llama_new_context_with_model: n_batch       = 2048
0.00.071.864 I llama_new_context_with_model: n_ubatch      = 512
0.00.071.864 I llama_new_context_with_model: flash_attn    = 0
0.00.071.864 I llama_new_context_with_model: freq_base     = 10000.0
0.00.071.865 I llama_new_context_with_model: freq_scale    = 1
0.00.071.865 I ggml_metal_init: allocating
0.00.071.868 I ggml_metal_init: found device: Apple M4
0.00.071.870 I ggml_metal_init: picking default device: Apple M4
0.00.072.476 I ggml_metal_init: using embedded metal library
0.00.074.672 I ggml_metal_init: GPU name:   Apple M4
0.00.074.674 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.074.674 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.074.675 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.074.675 I ggml_metal_init: simdgroup reduction   = true
0.00.074.675 I ggml_metal_init: simdgroup matrix mul. = true
0.00.074.675 I ggml_metal_init: has bfloat            = true
0.00.074.675 I ggml_metal_init: use bfloat            = true
0.00.074.676 I ggml_metal_init: hasUnifiedMemory      = true
0.00.074.678 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.104.119 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.104.126 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.104.144 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.105.154 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.105.156 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.105.156 I llama_new_context_with_model: graph nodes  = 967
0.00.105.156 I llama_new_context_with_model: graph splits = 2
0.00.105.179 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.957.508 I main: llama threadpool init, n_threads = 4
0.00.957.546 I 
0.00.957.573 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.957.573 I 
0.00.957.803 I sampler seed: 1234
0.00.957.807 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.957.849 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.957.850 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.957.850 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.745.328 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60580.20 tokens per second)
0.01.745.329 I llama_perf_context_print:        load time =     948.55 ms
0.01.745.330 I llama_perf_context_print: prompt eval time =      36.61 ms /     7 tokens (    5.23 ms per token,   191.20 tokens per second)
0.01.745.330 I llama_perf_context_print:        eval time =     747.94 ms /    63 runs   (   11.87 ms per token,    84.23 tokens per second)
0.01.745.331 I llama_perf_context_print:       total time =     787.82 ms /    70 tokens
0.01.745.502 I ggml_metal_free: deallocating

real	0m1.763s
user	0m0.117s
sys	0m0.166s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4210 (5acff8f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.080 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.885 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.889 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.891 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.892 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.892 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.892 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.893 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.893 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.894 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.895 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.896 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.898 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.898 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.898 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.900 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.900 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.901 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.691 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.803 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.617 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.618 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.619 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.619 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.619 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.620 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.620 I llama_model_loader: - type  f32:  194 tensors
0.00.024.621 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.621 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.099 I llm_load_vocab: special tokens cache size = 25
0.00.051.337 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.340 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.340 I llm_load_print_meta: arch             = gptneox
0.00.051.340 I llm_load_print_meta: vocab type       = BPE
0.00.051.341 I llm_load_print_meta: n_vocab          = 50304
0.00.051.341 I llm_load_print_meta: n_merges         = 50009
0.00.051.341 I llm_load_print_meta: vocab_only       = 0
0.00.051.341 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.341 I llm_load_print_meta: n_embd           = 2048
0.00.051.341 I llm_load_print_meta: n_layer          = 24
0.00.051.344 I llm_load_print_meta: n_head           = 16
0.00.051.345 I llm_load_print_meta: n_head_kv        = 16
0.00.051.345 I llm_load_print_meta: n_rot            = 32
0.00.051.346 I llm_load_print_meta: n_swa            = 0
0.00.051.346 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.346 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.347 I llm_load_print_meta: n_gqa            = 1
0.00.051.348 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.353 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.354 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.354 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.354 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.354 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.355 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.355 I llm_load_print_meta: n_ff             = 8192
0.00.051.356 I llm_load_print_meta: n_expert         = 0
0.00.051.356 I llm_load_print_meta: n_expert_used    = 0
0.00.051.356 I llm_load_print_meta: causal attn      = 1
0.00.051.356 I llm_load_print_meta: pooling type     = 0
0.00.051.356 I llm_load_print_meta: rope type        = 2
0.00.051.356 I llm_load_print_meta: rope scaling     = linear
0.00.051.357 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.357 I llm_load_print_meta: freq_scale_train = 1
0.00.051.357 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.358 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.358 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.358 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.358 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.358 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.358 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.370 I llm_load_print_meta: model type       = 1.4B
0.00.051.370 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.370 I llm_load_print_meta: model params     = 1.41 B
0.00.051.371 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.371 I llm_load_print_meta: general.name     = 1.4B
0.00.051.371 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.371 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.372 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.372 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.372 I llm_load_print_meta: LF token         = 128 ''
0.00.051.372 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.372 I llm_load_print_meta: max token length = 1024
0.00.053.240 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.241 I llm_load_tensors: offloading output layer to GPU
0.00.053.241 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.251 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.252 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.193 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.194 I llama_new_context_with_model: n_ctx         = 128
0.00.054.194 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.194 I llama_new_context_with_model: n_batch       = 128
0.00.054.194 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.194 I llama_new_context_with_model: flash_attn    = 0
0.00.054.195 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.195 I llama_new_context_with_model: freq_scale    = 1
0.00.054.195 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.196 I ggml_metal_init: allocating
0.00.054.202 I ggml_metal_init: found device: Apple M4
0.00.054.204 I ggml_metal_init: picking default device: Apple M4
0.00.054.727 I ggml_metal_init: using embedded metal library
0.00.056.691 I ggml_metal_init: GPU name:   Apple M4
0.00.056.692 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.692 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.693 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.693 I ggml_metal_init: simdgroup reduction   = true
0.00.056.693 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.693 I ggml_metal_init: has bfloat            = true
0.00.056.693 I ggml_metal_init: use bfloat            = true
0.00.056.694 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.694 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.060 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.064 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.078 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.985 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.986 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.986 I llama_new_context_with_model: graph nodes  = 967
0.00.066.986 I llama_new_context_with_model: graph splits = 2
0.00.066.999 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.713.141 I 
0.00.713.173 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.713.177 I perplexity: tokenizing the input ..
0.00.721.551 I perplexity: tokenization took 8.372 ms
0.00.721.565 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.856.567 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.857.822 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.857.833 I llama_perf_context_print:        load time =     703.06 ms
0.00.857.837 I llama_perf_context_print: prompt eval time =     134.78 ms /   128 tokens (    1.05 ms per token,   949.73 tokens per second)
0.00.857.837 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.857.838 I llama_perf_context_print:       total time =     144.69 ms /   129 tokens
0.00.858.124 I ggml_metal_free: deallocating

real	0m0.875s
user	0m0.078s
sys	0m0.120s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4210 (5acff8f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.015.339 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.032.193 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.032.198 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.199 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.200 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.200 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.201 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.201 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.202 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.203 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.203 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.203 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.204 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.204 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.205 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.206 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.206 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.207 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.037.704 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.039.250 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.044.693 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.044.694 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.044.695 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.044.695 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.044.696 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.044.696 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.044.697 I llama_model_loader: - type  f32:  194 tensors
0.00.044.697 I llama_model_loader: - type q5_1:   97 tensors
0.00.044.697 I llama_model_loader: - type q6_K:    1 tensors
0.00.085.023 I llm_load_vocab: special tokens cache size = 25
0.00.094.241 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.094.244 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.094.245 I llm_load_print_meta: arch             = gptneox
0.00.094.245 I llm_load_print_meta: vocab type       = BPE
0.00.094.246 I llm_load_print_meta: n_vocab          = 50304
0.00.094.246 I llm_load_print_meta: n_merges         = 50009
0.00.094.246 I llm_load_print_meta: vocab_only       = 0
0.00.094.246 I llm_load_print_meta: n_ctx_train      = 2048
0.00.094.246 I llm_load_print_meta: n_embd           = 2048
0.00.094.249 I llm_load_print_meta: n_layer          = 24
0.00.094.252 I llm_load_print_meta: n_head           = 16
0.00.094.253 I llm_load_print_meta: n_head_kv        = 16
0.00.094.254 I llm_load_print_meta: n_rot            = 32
0.00.094.254 I llm_load_print_meta: n_swa            = 0
0.00.094.254 I llm_load_print_meta: n_embd_head_k    = 128
0.00.094.254 I llm_load_print_meta: n_embd_head_v    = 128
0.00.094.256 I llm_load_print_meta: n_gqa            = 1
0.00.094.257 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.094.258 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.094.259 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.094.261 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.094.261 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.094.261 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.094.261 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.094.262 I llm_load_print_meta: n_ff             = 8192
0.00.094.262 I llm_load_print_meta: n_expert         = 0
0.00.094.262 I llm_load_print_meta: n_expert_used    = 0
0.00.094.263 I llm_load_print_meta: causal attn      = 1
0.00.094.263 I llm_load_print_meta: pooling type     = 0
0.00.094.263 I llm_load_print_meta: rope type        = 2
0.00.094.263 I llm_load_print_meta: rope scaling     = linear
0.00.094.264 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.094.264 I llm_load_print_meta: freq_scale_train = 1
0.00.094.264 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.094.264 I llm_load_print_meta: rope_finetuned   = unknown
0.00.094.265 I llm_load_print_meta: ssm_d_conv       = 0
0.00.094.265 I llm_load_print_meta: ssm_d_inner      = 0
0.00.094.265 I llm_load_print_meta: ssm_d_state      = 0
0.00.094.265 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.094.265 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.094.278 I llm_load_print_meta: model type       = 1.4B
0.00.094.279 I llm_load_print_meta: model ftype      = Q5_1
0.00.094.279 I llm_load_print_meta: model params     = 1.41 B
0.00.094.280 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.094.281 I llm_load_print_meta: general.name     = 1.4B
0.00.094.281 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.094.282 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.094.282 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.094.282 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.094.283 I llm_load_print_meta: LF token         = 128 ''
0.00.094.283 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.094.284 I llm_load_print_meta: max token length = 1024
0.00.096.843 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.096.843 I llm_load_tensors: offloading output layer to GPU
0.00.096.844 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.096.854 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.096.856 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.098.095 I llama_new_context_with_model: n_seq_max     = 1
0.00.098.096 I llama_new_context_with_model: n_ctx         = 2048
0.00.098.097 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.098.097 I llama_new_context_with_model: n_batch       = 2048
0.00.098.097 I llama_new_context_with_model: n_ubatch      = 512
0.00.098.097 I llama_new_context_with_model: flash_attn    = 0
0.00.098.098 I llama_new_context_with_model: freq_base     = 10000.0
0.00.098.098 I llama_new_context_with_model: freq_scale    = 1
0.00.098.099 I ggml_metal_init: allocating
0.00.098.106 I ggml_metal_init: found device: Apple M4
0.00.098.109 I ggml_metal_init: picking default device: Apple M4
0.00.098.796 I ggml_metal_init: using embedded metal library
0.00.101.427 I ggml_metal_init: GPU name:   Apple M4
0.00.101.429 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.101.429 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.101.430 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.101.430 I ggml_metal_init: simdgroup reduction   = true
0.00.101.430 I ggml_metal_init: simdgroup matrix mul. = true
0.00.101.430 I ggml_metal_init: has bfloat            = true
0.00.101.430 I ggml_metal_init: use bfloat            = true
0.00.101.431 I ggml_metal_init: hasUnifiedMemory      = true
0.00.101.432 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.131.013 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.131.025 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.131.046 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.132.021 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.132.023 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.132.023 I llama_new_context_with_model: graph nodes  = 967
0.00.132.023 I llama_new_context_with_model: graph splits = 2
0.00.132.047 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.937.502 I main: llama threadpool init, n_threads = 4
0.00.937.589 I 
0.00.937.666 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.937.668 I 
0.00.938.179 I sampler seed: 1234
0.00.938.190 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.938.258 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.938.264 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.938.264 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, literature and art.

If one's life is not happy, one can have no other way but

0.01.776.868 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60374.15 tokens per second)
0.01.776.868 I llama_perf_context_print:        load time =     922.16 ms
0.01.776.869 I llama_perf_context_print: prompt eval time =      37.29 ms /     7 tokens (    5.33 ms per token,   187.73 tokens per second)
0.01.776.870 I llama_perf_context_print:        eval time =     798.43 ms /    63 runs   (   12.67 ms per token,    78.91 tokens per second)
0.01.776.870 I llama_perf_context_print:       total time =     839.37 ms /    70 tokens
0.01.777.050 I ggml_metal_free: deallocating

real	0m1.817s
user	0m0.152s
sys	0m0.201s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4210 (5acff8f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.553 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.382 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.386 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.387 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.388 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.388 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.389 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.389 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.390 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.390 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.390 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.391 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.391 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.391 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.392 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.393 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.393 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.394 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.227 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.340 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.237 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.238 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.238 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.239 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.239 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.239 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.240 I llama_model_loader: - type  f32:  194 tensors
0.00.024.240 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.240 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.721 I llm_load_vocab: special tokens cache size = 25
0.00.050.725 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.729 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.730 I llm_load_print_meta: arch             = gptneox
0.00.050.730 I llm_load_print_meta: vocab type       = BPE
0.00.050.730 I llm_load_print_meta: n_vocab          = 50304
0.00.050.730 I llm_load_print_meta: n_merges         = 50009
0.00.050.731 I llm_load_print_meta: vocab_only       = 0
0.00.050.731 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.732 I llm_load_print_meta: n_embd           = 2048
0.00.050.732 I llm_load_print_meta: n_layer          = 24
0.00.050.735 I llm_load_print_meta: n_head           = 16
0.00.050.736 I llm_load_print_meta: n_head_kv        = 16
0.00.050.736 I llm_load_print_meta: n_rot            = 32
0.00.050.737 I llm_load_print_meta: n_swa            = 0
0.00.050.737 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.737 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.738 I llm_load_print_meta: n_gqa            = 1
0.00.050.739 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.739 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.740 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.747 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.747 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.747 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.748 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.748 I llm_load_print_meta: n_ff             = 8192
0.00.050.749 I llm_load_print_meta: n_expert         = 0
0.00.050.749 I llm_load_print_meta: n_expert_used    = 0
0.00.050.749 I llm_load_print_meta: causal attn      = 1
0.00.050.750 I llm_load_print_meta: pooling type     = 0
0.00.050.751 I llm_load_print_meta: rope type        = 2
0.00.050.751 I llm_load_print_meta: rope scaling     = linear
0.00.050.751 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.751 I llm_load_print_meta: freq_scale_train = 1
0.00.050.752 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.752 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.752 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.753 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.753 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.753 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.753 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.760 I llm_load_print_meta: model type       = 1.4B
0.00.050.760 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.761 I llm_load_print_meta: model params     = 1.41 B
0.00.050.761 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.761 I llm_load_print_meta: general.name     = 1.4B
0.00.050.762 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.762 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.762 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.763 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.763 I llm_load_print_meta: LF token         = 128 ''
0.00.050.764 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.764 I llm_load_print_meta: max token length = 1024
0.00.052.755 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.755 I llm_load_tensors: offloading output layer to GPU
0.00.052.756 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.761 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.761 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.781 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.782 I llama_new_context_with_model: n_ctx         = 128
0.00.053.782 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.783 I llama_new_context_with_model: n_batch       = 128
0.00.053.783 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.783 I llama_new_context_with_model: flash_attn    = 0
0.00.053.783 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.784 I llama_new_context_with_model: freq_scale    = 1
0.00.053.784 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.784 I ggml_metal_init: allocating
0.00.053.787 I ggml_metal_init: found device: Apple M4
0.00.053.789 I ggml_metal_init: picking default device: Apple M4
0.00.054.381 I ggml_metal_init: using embedded metal library
0.00.056.349 I ggml_metal_init: GPU name:   Apple M4
0.00.056.351 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.351 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.352 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.352 I ggml_metal_init: simdgroup reduction   = true
0.00.056.352 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.352 I ggml_metal_init: has bfloat            = true
0.00.056.352 I ggml_metal_init: use bfloat            = true
0.00.056.353 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.354 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.775 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.777 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.790 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.714 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.715 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.716 I llama_new_context_with_model: graph nodes  = 967
0.00.066.716 I llama_new_context_with_model: graph splits = 2
0.00.066.729 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.735.117 I 
0.00.735.144 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.735.147 I perplexity: tokenizing the input ..
0.00.743.279 I perplexity: tokenization took 8.131 ms
0.00.743.295 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.878.309 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.879.554 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.879.572 I llama_perf_context_print:        load time =     725.56 ms
0.00.879.573 I llama_perf_context_print: prompt eval time =     134.79 ms /   128 tokens (    1.05 ms per token,   949.65 tokens per second)
0.00.879.574 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.879.574 I llama_perf_context_print:       total time =     144.45 ms /   129 tokens
0.00.879.976 I ggml_metal_free: deallocating

real	0m0.893s
user	0m0.077s
sys	0m0.126s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4210 (5acff8f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.010.077 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.885 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.889 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.891 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.891 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.891 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.892 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.892 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.893 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.893 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.894 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.894 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.896 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.897 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.897 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.898 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.899 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.899 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.730 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.786 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.636 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.637 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.638 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.638 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.638 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.639 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.639 I llama_model_loader: - type  f32:  194 tensors
0.00.024.640 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.640 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.640 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.100 I llm_load_vocab: special tokens cache size = 25
0.00.052.353 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.357 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.358 I llm_load_print_meta: arch             = gptneox
0.00.052.358 I llm_load_print_meta: vocab type       = BPE
0.00.052.358 I llm_load_print_meta: n_vocab          = 50304
0.00.052.359 I llm_load_print_meta: n_merges         = 50009
0.00.052.364 I llm_load_print_meta: vocab_only       = 0
0.00.052.364 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.365 I llm_load_print_meta: n_embd           = 2048
0.00.052.365 I llm_load_print_meta: n_layer          = 24
0.00.052.368 I llm_load_print_meta: n_head           = 16
0.00.052.368 I llm_load_print_meta: n_head_kv        = 16
0.00.052.369 I llm_load_print_meta: n_rot            = 32
0.00.052.369 I llm_load_print_meta: n_swa            = 0
0.00.052.369 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.369 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.370 I llm_load_print_meta: n_gqa            = 1
0.00.052.371 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.371 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.372 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.372 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.372 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.372 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.373 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.373 I llm_load_print_meta: n_ff             = 8192
0.00.052.374 I llm_load_print_meta: n_expert         = 0
0.00.052.374 I llm_load_print_meta: n_expert_used    = 0
0.00.052.375 I llm_load_print_meta: causal attn      = 1
0.00.052.376 I llm_load_print_meta: pooling type     = 0
0.00.052.376 I llm_load_print_meta: rope type        = 2
0.00.052.376 I llm_load_print_meta: rope scaling     = linear
0.00.052.376 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.377 I llm_load_print_meta: freq_scale_train = 1
0.00.052.377 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.377 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.377 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.377 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.377 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.378 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.378 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.390 I llm_load_print_meta: model type       = 1.4B
0.00.052.390 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.052.390 I llm_load_print_meta: model params     = 1.41 B
0.00.052.391 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.052.391 I llm_load_print_meta: general.name     = 1.4B
0.00.052.391 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.392 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.392 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.392 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.392 I llm_load_print_meta: LF token         = 128 ''
0.00.052.393 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.393 I llm_load_print_meta: max token length = 1024
0.00.054.344 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.345 I llm_load_tensors: offloading output layer to GPU
0.00.054.345 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.356 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.054.357 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.055.328 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.329 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.329 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.329 I llama_new_context_with_model: n_batch       = 2048
0.00.055.330 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.330 I llama_new_context_with_model: flash_attn    = 0
0.00.055.331 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.331 I llama_new_context_with_model: freq_scale    = 1
0.00.055.331 I ggml_metal_init: allocating
0.00.055.337 I ggml_metal_init: found device: Apple M4
0.00.055.340 I ggml_metal_init: picking default device: Apple M4
0.00.055.934 I ggml_metal_init: using embedded metal library
0.00.057.885 I ggml_metal_init: GPU name:   Apple M4
0.00.057.887 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.887 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.887 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.888 I ggml_metal_init: simdgroup reduction   = true
0.00.057.888 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.888 I ggml_metal_init: has bfloat            = true
0.00.057.888 I ggml_metal_init: use bfloat            = true
0.00.057.889 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.889 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.929 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.933 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.952 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.011 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.013 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.013 I llama_new_context_with_model: graph nodes  = 967
0.00.087.013 I llama_new_context_with_model: graph splits = 2
0.00.087.037 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.519.935 I main: llama threadpool init, n_threads = 4
0.00.519.968 I 
0.00.519.999 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.519.999 I 
0.00.520.221 I sampler seed: 1234
0.00.520.225 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.520.236 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.520.236 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.520.236 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.203.194 I llama_perf_sampler_print:    sampling time =       1.11 ms /    71 runs   (    0.02 ms per token, 63734.29 tokens per second)
0.01.203.194 I llama_perf_context_print:        load time =     509.85 ms
0.01.203.195 I llama_perf_context_print: prompt eval time =      35.95 ms /     7 tokens (    5.14 ms per token,   194.72 tokens per second)
0.01.203.196 I llama_perf_context_print:        eval time =     644.11 ms /    63 runs   (   10.22 ms per token,    97.81 tokens per second)
0.01.203.196 I llama_perf_context_print:       total time =     683.26 ms /    70 tokens
0.01.203.390 I ggml_metal_free: deallocating

real	0m1.223s
user	0m0.111s
sys	0m0.118s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4210 (5acff8f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.547 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.409 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.414 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.416 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.416 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.417 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.417 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.417 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.418 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.418 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.419 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.419 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.419 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.420 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.420 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.423 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.423 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.423 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.297 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.407 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.332 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.334 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.334 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.334 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.335 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.335 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.335 I llama_model_loader: - type  f32:  194 tensors
0.00.026.336 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.336 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.336 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.650 I llm_load_vocab: special tokens cache size = 25
0.00.053.624 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.627 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.627 I llm_load_print_meta: arch             = gptneox
0.00.053.627 I llm_load_print_meta: vocab type       = BPE
0.00.053.627 I llm_load_print_meta: n_vocab          = 50304
0.00.053.628 I llm_load_print_meta: n_merges         = 50009
0.00.053.628 I llm_load_print_meta: vocab_only       = 0
0.00.053.628 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.628 I llm_load_print_meta: n_embd           = 2048
0.00.053.628 I llm_load_print_meta: n_layer          = 24
0.00.053.631 I llm_load_print_meta: n_head           = 16
0.00.053.632 I llm_load_print_meta: n_head_kv        = 16
0.00.053.632 I llm_load_print_meta: n_rot            = 32
0.00.053.632 I llm_load_print_meta: n_swa            = 0
0.00.053.633 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.633 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.634 I llm_load_print_meta: n_gqa            = 1
0.00.053.634 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.635 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.636 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.636 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.636 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.636 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.636 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.637 I llm_load_print_meta: n_ff             = 8192
0.00.053.637 I llm_load_print_meta: n_expert         = 0
0.00.053.637 I llm_load_print_meta: n_expert_used    = 0
0.00.053.638 I llm_load_print_meta: causal attn      = 1
0.00.053.638 I llm_load_print_meta: pooling type     = 0
0.00.053.638 I llm_load_print_meta: rope type        = 2
0.00.053.638 I llm_load_print_meta: rope scaling     = linear
0.00.053.638 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.639 I llm_load_print_meta: freq_scale_train = 1
0.00.053.639 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.639 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.639 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.640 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.640 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.641 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.641 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.653 I llm_load_print_meta: model type       = 1.4B
0.00.053.653 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.053.654 I llm_load_print_meta: model params     = 1.41 B
0.00.053.654 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.053.654 I llm_load_print_meta: general.name     = 1.4B
0.00.053.654 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.655 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.655 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.655 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.655 I llm_load_print_meta: LF token         = 128 ''
0.00.053.657 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.657 I llm_load_print_meta: max token length = 1024
0.00.055.616 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.616 I llm_load_tensors: offloading output layer to GPU
0.00.055.616 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.626 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.055.627 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.056.530 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.530 I llama_new_context_with_model: n_ctx         = 128
0.00.056.531 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.056.531 I llama_new_context_with_model: n_batch       = 128
0.00.056.531 I llama_new_context_with_model: n_ubatch      = 128
0.00.056.531 I llama_new_context_with_model: flash_attn    = 0
0.00.056.532 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.532 I llama_new_context_with_model: freq_scale    = 1
0.00.056.532 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.056.533 I ggml_metal_init: allocating
0.00.056.539 I ggml_metal_init: found device: Apple M4
0.00.056.541 I ggml_metal_init: picking default device: Apple M4
0.00.057.093 I ggml_metal_init: using embedded metal library
0.00.059.097 I ggml_metal_init: GPU name:   Apple M4
0.00.059.098 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.099 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.099 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.099 I ggml_metal_init: simdgroup reduction   = true
0.00.059.099 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.100 I ggml_metal_init: has bfloat            = true
0.00.059.100 I ggml_metal_init: use bfloat            = true
0.00.059.100 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.101 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.463 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.468 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.481 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.384 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.385 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.385 I llama_new_context_with_model: graph nodes  = 967
0.00.069.386 I llama_new_context_with_model: graph splits = 2
0.00.069.398 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.475.680 I 
0.00.475.719 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.475.728 I perplexity: tokenizing the input ..
0.00.483.970 I perplexity: tokenization took 8.24 ms
0.00.483.979 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.616.328 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.617.574 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.617.595 I llama_perf_context_print:        load time =     464.13 ms
0.00.617.597 I llama_perf_context_print: prompt eval time =     132.12 ms /   128 tokens (    1.03 ms per token,   968.85 tokens per second)
0.00.617.598 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.617.599 I llama_perf_context_print:       total time =     141.92 ms /   129 tokens
0.00.618.066 I ggml_metal_free: deallocating

real	0m0.634s
user	0m0.078s
sys	0m0.085s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4210 (5acff8f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.985 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.643 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.647 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.649 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.650 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.652 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.652 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.652 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.653 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.653 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.654 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.654 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.654 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.655 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.655 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.657 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.657 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.657 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.608 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.677 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.572 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.574 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.574 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.574 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.575 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.575 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.575 I llama_model_loader: - type  f32:  194 tensors
0.00.024.576 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.576 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.576 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.577 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.713 I llm_load_vocab: special tokens cache size = 25
0.00.051.928 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.931 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.931 I llm_load_print_meta: arch             = gptneox
0.00.051.932 I llm_load_print_meta: vocab type       = BPE
0.00.051.932 I llm_load_print_meta: n_vocab          = 50304
0.00.051.932 I llm_load_print_meta: n_merges         = 50009
0.00.051.932 I llm_load_print_meta: vocab_only       = 0
0.00.051.932 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.933 I llm_load_print_meta: n_embd           = 2048
0.00.051.933 I llm_load_print_meta: n_layer          = 24
0.00.051.935 I llm_load_print_meta: n_head           = 16
0.00.051.936 I llm_load_print_meta: n_head_kv        = 16
0.00.051.936 I llm_load_print_meta: n_rot            = 32
0.00.051.936 I llm_load_print_meta: n_swa            = 0
0.00.051.937 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.937 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.937 I llm_load_print_meta: n_gqa            = 1
0.00.051.938 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.939 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.939 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.940 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.940 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.940 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.941 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.941 I llm_load_print_meta: n_ff             = 8192
0.00.051.942 I llm_load_print_meta: n_expert         = 0
0.00.051.945 I llm_load_print_meta: n_expert_used    = 0
0.00.051.945 I llm_load_print_meta: causal attn      = 1
0.00.051.945 I llm_load_print_meta: pooling type     = 0
0.00.051.945 I llm_load_print_meta: rope type        = 2
0.00.051.945 I llm_load_print_meta: rope scaling     = linear
0.00.051.946 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.946 I llm_load_print_meta: freq_scale_train = 1
0.00.051.946 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.946 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.947 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.947 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.947 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.948 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.949 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.960 I llm_load_print_meta: model type       = 1.4B
0.00.051.960 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.961 I llm_load_print_meta: model params     = 1.41 B
0.00.051.961 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.961 I llm_load_print_meta: general.name     = 1.4B
0.00.051.962 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.962 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.962 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.964 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.964 I llm_load_print_meta: LF token         = 128 ''
0.00.051.964 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.964 I llm_load_print_meta: max token length = 1024
0.00.053.523 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.523 I llm_load_tensors: offloading output layer to GPU
0.00.053.523 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.533 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.534 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.371 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.372 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.372 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.372 I llama_new_context_with_model: n_batch       = 2048
0.00.054.372 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.372 I llama_new_context_with_model: flash_attn    = 0
0.00.054.373 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.373 I llama_new_context_with_model: freq_scale    = 1
0.00.054.373 I ggml_metal_init: allocating
0.00.054.377 I ggml_metal_init: found device: Apple M4
0.00.054.379 I ggml_metal_init: picking default device: Apple M4
0.00.054.932 I ggml_metal_init: using embedded metal library
0.00.056.913 I ggml_metal_init: GPU name:   Apple M4
0.00.056.915 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.916 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.916 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.916 I ggml_metal_init: simdgroup reduction   = true
0.00.056.916 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.917 I ggml_metal_init: has bfloat            = true
0.00.056.917 I ggml_metal_init: use bfloat            = true
0.00.056.917 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.918 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.308 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.315 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.340 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.303 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.305 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.305 I llama_new_context_with_model: graph nodes  = 967
0.00.086.306 I llama_new_context_with_model: graph splits = 2
0.00.086.328 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.582.870 I main: llama threadpool init, n_threads = 4
0.00.582.909 I 
0.00.582.945 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.582.947 I 
0.00.583.172 I sampler seed: 1234
0.00.583.177 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.583.188 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.583.188 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.583.188 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do that is most useful to my fellow men?'"

-Albert Einstein

"The way

0.01.326.642 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57629.87 tokens per second)
0.01.326.642 I llama_perf_context_print:        load time =     573.88 ms
0.01.326.643 I llama_perf_context_print: prompt eval time =      35.64 ms /     7 tokens (    5.09 ms per token,   196.39 tokens per second)
0.01.326.644 I llama_perf_context_print:        eval time =     704.75 ms /    63 runs   (   11.19 ms per token,    89.39 tokens per second)
0.01.326.647 I llama_perf_context_print:       total time =     743.77 ms /    70 tokens
0.01.326.826 I ggml_metal_free: deallocating

real	0m1.343s
user	0m0.110s
sys	0m0.134s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4210 (5acff8f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.562 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.484 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.488 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.490 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.490 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.490 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.491 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.491 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.492 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.492 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.493 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.493 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.493 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.494 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.494 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.496 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.497 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.497 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.263 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.368 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.194 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.195 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.196 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.196 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.196 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.197 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.197 I llama_model_loader: - type  f32:  194 tensors
0.00.023.198 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.198 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.198 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.198 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.662 I llm_load_vocab: special tokens cache size = 25
0.00.049.601 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.604 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.604 I llm_load_print_meta: arch             = gptneox
0.00.049.605 I llm_load_print_meta: vocab type       = BPE
0.00.049.605 I llm_load_print_meta: n_vocab          = 50304
0.00.049.605 I llm_load_print_meta: n_merges         = 50009
0.00.049.605 I llm_load_print_meta: vocab_only       = 0
0.00.049.606 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.606 I llm_load_print_meta: n_embd           = 2048
0.00.049.606 I llm_load_print_meta: n_layer          = 24
0.00.049.608 I llm_load_print_meta: n_head           = 16
0.00.049.608 I llm_load_print_meta: n_head_kv        = 16
0.00.049.609 I llm_load_print_meta: n_rot            = 32
0.00.049.609 I llm_load_print_meta: n_swa            = 0
0.00.049.609 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.609 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.610 I llm_load_print_meta: n_gqa            = 1
0.00.049.611 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.612 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.612 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.612 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.613 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.613 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.613 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.613 I llm_load_print_meta: n_ff             = 8192
0.00.049.614 I llm_load_print_meta: n_expert         = 0
0.00.049.614 I llm_load_print_meta: n_expert_used    = 0
0.00.049.614 I llm_load_print_meta: causal attn      = 1
0.00.049.614 I llm_load_print_meta: pooling type     = 0
0.00.049.614 I llm_load_print_meta: rope type        = 2
0.00.049.614 I llm_load_print_meta: rope scaling     = linear
0.00.049.615 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.615 I llm_load_print_meta: freq_scale_train = 1
0.00.049.618 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.618 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.618 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.618 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.619 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.619 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.619 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.631 I llm_load_print_meta: model type       = 1.4B
0.00.049.631 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.632 I llm_load_print_meta: model params     = 1.41 B
0.00.049.633 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.633 I llm_load_print_meta: general.name     = 1.4B
0.00.049.633 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.633 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.633 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.634 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.634 I llm_load_print_meta: LF token         = 128 ''
0.00.049.634 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.634 I llm_load_print_meta: max token length = 1024
0.00.051.594 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.594 I llm_load_tensors: offloading output layer to GPU
0.00.051.595 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.604 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.605 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.531 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.532 I llama_new_context_with_model: n_ctx         = 128
0.00.052.532 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.532 I llama_new_context_with_model: n_batch       = 128
0.00.052.532 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.532 I llama_new_context_with_model: flash_attn    = 0
0.00.052.533 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.533 I llama_new_context_with_model: freq_scale    = 1
0.00.052.533 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.534 I ggml_metal_init: allocating
0.00.052.537 I ggml_metal_init: found device: Apple M4
0.00.052.538 I ggml_metal_init: picking default device: Apple M4
0.00.053.078 I ggml_metal_init: using embedded metal library
0.00.055.027 I ggml_metal_init: GPU name:   Apple M4
0.00.055.028 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.028 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.029 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.029 I ggml_metal_init: simdgroup reduction   = true
0.00.055.029 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.029 I ggml_metal_init: has bfloat            = true
0.00.055.030 I ggml_metal_init: use bfloat            = true
0.00.055.030 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.030 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.383 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.385 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.398 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.354 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.355 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.355 I llama_new_context_with_model: graph nodes  = 967
0.00.065.355 I llama_new_context_with_model: graph splits = 2
0.00.065.363 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.517.475 I 
0.00.517.505 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.517.508 I perplexity: tokenizing the input ..
0.00.525.718 I perplexity: tokenization took 8.208 ms
0.00.525.731 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.658.136 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.659.397 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.659.412 I llama_perf_context_print:        load time =     508.91 ms
0.00.659.413 I llama_perf_context_print: prompt eval time =     132.18 ms /   128 tokens (    1.03 ms per token,   968.41 tokens per second)
0.00.659.413 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.659.414 I llama_perf_context_print:       total time =     141.94 ms /   129 tokens
0.00.659.756 I ggml_metal_free: deallocating

real	0m0.673s
user	0m0.077s
sys	0m0.089s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4210 (5acff8f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.010.368 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.128 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.133 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.138 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.139 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.139 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.139 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.140 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.141 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.141 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.141 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.142 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.142 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.142 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.143 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.144 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.144 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.145 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.145 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.252 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.301 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.302 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.302 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.303 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.303 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.303 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.304 I llama_model_loader: - type  f32:  194 tensors
0.00.026.304 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.305 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.305 I llama_model_loader: - type q6_K:   13 tensors
0.00.047.006 I llm_load_vocab: special tokens cache size = 25
0.00.053.195 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.198 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.198 I llm_load_print_meta: arch             = gptneox
0.00.053.199 I llm_load_print_meta: vocab type       = BPE
0.00.053.199 I llm_load_print_meta: n_vocab          = 50304
0.00.053.199 I llm_load_print_meta: n_merges         = 50009
0.00.053.199 I llm_load_print_meta: vocab_only       = 0
0.00.053.199 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.200 I llm_load_print_meta: n_embd           = 2048
0.00.053.200 I llm_load_print_meta: n_layer          = 24
0.00.053.203 I llm_load_print_meta: n_head           = 16
0.00.053.203 I llm_load_print_meta: n_head_kv        = 16
0.00.053.204 I llm_load_print_meta: n_rot            = 32
0.00.053.204 I llm_load_print_meta: n_swa            = 0
0.00.053.204 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.204 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.205 I llm_load_print_meta: n_gqa            = 1
0.00.053.206 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.207 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.207 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.207 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.208 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.208 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.208 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.209 I llm_load_print_meta: n_ff             = 8192
0.00.053.209 I llm_load_print_meta: n_expert         = 0
0.00.053.209 I llm_load_print_meta: n_expert_used    = 0
0.00.053.209 I llm_load_print_meta: causal attn      = 1
0.00.053.210 I llm_load_print_meta: pooling type     = 0
0.00.053.210 I llm_load_print_meta: rope type        = 2
0.00.053.210 I llm_load_print_meta: rope scaling     = linear
0.00.053.210 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.211 I llm_load_print_meta: freq_scale_train = 1
0.00.053.211 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.211 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.211 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.211 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.211 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.212 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.212 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.224 I llm_load_print_meta: model type       = 1.4B
0.00.053.224 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.053.224 I llm_load_print_meta: model params     = 1.41 B
0.00.053.225 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.053.227 I llm_load_print_meta: general.name     = 1.4B
0.00.053.227 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.227 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.227 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.227 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.228 I llm_load_print_meta: LF token         = 128 ''
0.00.053.228 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.228 I llm_load_print_meta: max token length = 1024
0.00.055.281 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.281 I llm_load_tensors: offloading output layer to GPU
0.00.055.281 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.291 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.055.292 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.056.212 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.213 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.213 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.213 I llama_new_context_with_model: n_batch       = 2048
0.00.056.214 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.214 I llama_new_context_with_model: flash_attn    = 0
0.00.056.214 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.214 I llama_new_context_with_model: freq_scale    = 1
0.00.056.215 I ggml_metal_init: allocating
0.00.056.221 I ggml_metal_init: found device: Apple M4
0.00.056.225 I ggml_metal_init: picking default device: Apple M4
0.00.056.771 I ggml_metal_init: using embedded metal library
0.00.058.755 I ggml_metal_init: GPU name:   Apple M4
0.00.058.757 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.757 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.758 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.758 I ggml_metal_init: simdgroup reduction   = true
0.00.058.758 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.758 I ggml_metal_init: has bfloat            = true
0.00.058.758 I ggml_metal_init: use bfloat            = true
0.00.058.759 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.759 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.720 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.729 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.754 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.763 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.764 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.764 I llama_new_context_with_model: graph nodes  = 967
0.00.087.765 I llama_new_context_with_model: graph splits = 2
0.00.087.787 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.629.768 I main: llama threadpool init, n_threads = 4
0.00.629.809 I 
0.00.629.836 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.629.836 I 
0.00.630.052 I sampler seed: 1234
0.00.630.057 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.630.078 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.630.079 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.630.079 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the greatest power to create the future we want to see. I believe that we are in a wonderful time when people from all over the world are coming together to create a better world. I believe that we are going

0.01.389.788 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54322.88 tokens per second)
0.01.389.788 I llama_perf_context_print:        load time =     619.40 ms
0.01.389.789 I llama_perf_context_print: prompt eval time =      40.32 ms /     7 tokens (    5.76 ms per token,   173.62 tokens per second)
0.01.389.790 I llama_perf_context_print:        eval time =     716.27 ms /    63 runs   (   11.37 ms per token,    87.96 tokens per second)
0.01.389.793 I llama_perf_context_print:       total time =     760.02 ms /    70 tokens
0.01.389.976 I ggml_metal_free: deallocating

real	0m1.408s
user	0m0.109s
sys	0m0.146s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4210 (5acff8f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.378 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.296 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.301 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.302 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.303 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.309 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.310 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.310 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.311 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.311 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.312 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.312 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.312 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.313 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.313 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.314 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.315 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.315 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.216 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.246 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.078 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.079 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.079 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.079 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.080 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.080 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.081 I llama_model_loader: - type  f32:  194 tensors
0.00.025.081 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.081 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.081 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.497 I llm_load_vocab: special tokens cache size = 25
0.00.051.660 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.662 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.663 I llm_load_print_meta: arch             = gptneox
0.00.051.663 I llm_load_print_meta: vocab type       = BPE
0.00.051.663 I llm_load_print_meta: n_vocab          = 50304
0.00.051.663 I llm_load_print_meta: n_merges         = 50009
0.00.051.664 I llm_load_print_meta: vocab_only       = 0
0.00.051.664 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.664 I llm_load_print_meta: n_embd           = 2048
0.00.051.664 I llm_load_print_meta: n_layer          = 24
0.00.051.666 I llm_load_print_meta: n_head           = 16
0.00.051.667 I llm_load_print_meta: n_head_kv        = 16
0.00.051.667 I llm_load_print_meta: n_rot            = 32
0.00.051.668 I llm_load_print_meta: n_swa            = 0
0.00.051.670 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.670 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.671 I llm_load_print_meta: n_gqa            = 1
0.00.051.671 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.672 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.673 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.673 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.673 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.673 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.673 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.674 I llm_load_print_meta: n_ff             = 8192
0.00.051.674 I llm_load_print_meta: n_expert         = 0
0.00.051.675 I llm_load_print_meta: n_expert_used    = 0
0.00.051.675 I llm_load_print_meta: causal attn      = 1
0.00.051.675 I llm_load_print_meta: pooling type     = 0
0.00.051.675 I llm_load_print_meta: rope type        = 2
0.00.051.675 I llm_load_print_meta: rope scaling     = linear
0.00.051.676 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.680 I llm_load_print_meta: freq_scale_train = 1
0.00.051.680 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.681 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.686 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.687 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.687 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.687 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.687 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.695 I llm_load_print_meta: model type       = 1.4B
0.00.051.695 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.696 I llm_load_print_meta: model params     = 1.41 B
0.00.051.696 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.696 I llm_load_print_meta: general.name     = 1.4B
0.00.051.697 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.697 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.697 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.697 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.698 I llm_load_print_meta: LF token         = 128 ''
0.00.051.698 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.698 I llm_load_print_meta: max token length = 1024
0.00.053.411 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.411 I llm_load_tensors: offloading output layer to GPU
0.00.053.411 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.416 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.417 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.323 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.324 I llama_new_context_with_model: n_ctx         = 128
0.00.054.324 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.324 I llama_new_context_with_model: n_batch       = 128
0.00.054.324 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.324 I llama_new_context_with_model: flash_attn    = 0
0.00.054.325 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.325 I llama_new_context_with_model: freq_scale    = 1
0.00.054.325 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.326 I ggml_metal_init: allocating
0.00.054.329 I ggml_metal_init: found device: Apple M4
0.00.054.330 I ggml_metal_init: picking default device: Apple M4
0.00.054.992 I ggml_metal_init: using embedded metal library
0.00.056.908 I ggml_metal_init: GPU name:   Apple M4
0.00.056.910 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.910 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.910 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.910 I ggml_metal_init: simdgroup reduction   = true
0.00.056.911 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.911 I ggml_metal_init: has bfloat            = true
0.00.056.911 I ggml_metal_init: use bfloat            = true
0.00.056.911 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.913 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.122 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.128 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.144 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.041 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.042 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.042 I llama_new_context_with_model: graph nodes  = 967
0.00.067.043 I llama_new_context_with_model: graph splits = 2
0.00.067.055 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.583.004 I 
0.00.583.040 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.583.046 I perplexity: tokenizing the input ..
0.00.590.803 I perplexity: tokenization took 7.756 ms
0.00.590.814 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.725.262 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.726.425 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.726.437 I llama_perf_context_print:        load time =     572.62 ms
0.00.726.438 I llama_perf_context_print: prompt eval time =     134.22 ms /   128 tokens (    1.05 ms per token,   953.64 tokens per second)
0.00.726.440 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.726.440 I llama_perf_context_print:       total time =     143.43 ms /   129 tokens
0.00.726.989 I ggml_metal_free: deallocating

real	0m0.742s
user	0m0.077s
sys	0m0.116s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4210 (5acff8f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.914 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.782 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.787 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.789 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.790 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.791 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.791 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.791 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.792 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.793 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.793 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.793 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.794 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.794 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.794 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.796 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.797 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.797 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.714 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.793 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.644 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.645 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.645 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.645 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.646 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.646 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.647 I llama_model_loader: - type  f32:  194 tensors
0.00.024.647 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.647 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.189 I llm_load_vocab: special tokens cache size = 25
0.00.051.066 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.069 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.069 I llm_load_print_meta: arch             = gptneox
0.00.051.069 I llm_load_print_meta: vocab type       = BPE
0.00.051.069 I llm_load_print_meta: n_vocab          = 50304
0.00.051.070 I llm_load_print_meta: n_merges         = 50009
0.00.051.070 I llm_load_print_meta: vocab_only       = 0
0.00.051.070 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.070 I llm_load_print_meta: n_embd           = 2048
0.00.051.070 I llm_load_print_meta: n_layer          = 24
0.00.051.072 I llm_load_print_meta: n_head           = 16
0.00.051.073 I llm_load_print_meta: n_head_kv        = 16
0.00.051.073 I llm_load_print_meta: n_rot            = 32
0.00.051.073 I llm_load_print_meta: n_swa            = 0
0.00.051.074 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.076 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.076 I llm_load_print_meta: n_gqa            = 1
0.00.051.077 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.078 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.078 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.086 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.087 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.088 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.089 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.091 I llm_load_print_meta: n_ff             = 8192
0.00.051.091 I llm_load_print_meta: n_expert         = 0
0.00.051.091 I llm_load_print_meta: n_expert_used    = 0
0.00.051.093 I llm_load_print_meta: causal attn      = 1
0.00.051.094 I llm_load_print_meta: pooling type     = 0
0.00.051.094 I llm_load_print_meta: rope type        = 2
0.00.051.095 I llm_load_print_meta: rope scaling     = linear
0.00.051.095 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.095 I llm_load_print_meta: freq_scale_train = 1
0.00.051.095 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.096 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.096 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.096 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.096 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.096 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.096 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.108 I llm_load_print_meta: model type       = 1.4B
0.00.051.108 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.108 I llm_load_print_meta: model params     = 1.41 B
0.00.051.109 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.109 I llm_load_print_meta: general.name     = 1.4B
0.00.051.110 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.110 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.110 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.110 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.111 I llm_load_print_meta: LF token         = 128 ''
0.00.051.111 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.111 I llm_load_print_meta: max token length = 1024
0.00.052.693 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.693 I llm_load_tensors: offloading output layer to GPU
0.00.052.694 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.703 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.704 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.535 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.536 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.536 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.536 I llama_new_context_with_model: n_batch       = 2048
0.00.053.537 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.537 I llama_new_context_with_model: flash_attn    = 0
0.00.053.537 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.538 I llama_new_context_with_model: freq_scale    = 1
0.00.053.538 I ggml_metal_init: allocating
0.00.053.541 I ggml_metal_init: found device: Apple M4
0.00.053.543 I ggml_metal_init: picking default device: Apple M4
0.00.054.094 I ggml_metal_init: using embedded metal library
0.00.056.062 I ggml_metal_init: GPU name:   Apple M4
0.00.056.064 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.066 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.066 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.066 I ggml_metal_init: simdgroup reduction   = true
0.00.056.066 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.067 I ggml_metal_init: has bfloat            = true
0.00.056.067 I ggml_metal_init: use bfloat            = true
0.00.056.067 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.068 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.306 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.314 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.334 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.308 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.309 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.309 I llama_new_context_with_model: graph nodes  = 967
0.00.085.310 I llama_new_context_with_model: graph splits = 2
0.00.085.332 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.700.518 I main: llama threadpool init, n_threads = 4
0.00.700.558 I 
0.00.700.584 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.700.584 I 
0.00.700.818 I sampler seed: 1234
0.00.700.823 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.700.833 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.700.834 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.700.834 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.544.292 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61631.94 tokens per second)
0.01.544.293 I llama_perf_context_print:        load time =     691.60 ms
0.01.544.294 I llama_perf_context_print: prompt eval time =      38.65 ms /     7 tokens (    5.52 ms per token,   181.09 tokens per second)
0.01.544.295 I llama_perf_context_print:        eval time =     801.82 ms /    63 runs   (   12.73 ms per token,    78.57 tokens per second)
0.01.544.295 I llama_perf_context_print:       total time =     843.78 ms /    70 tokens
0.01.544.459 I ggml_metal_free: deallocating

real	0m1.562s
user	0m0.110s
sys	0m0.155s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4210 (5acff8f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.452 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.497 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.502 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.507 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.508 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.508 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.508 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.510 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.511 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.511 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.511 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.512 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.512 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.515 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.516 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.517 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.518 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.518 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.415 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.460 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.306 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.307 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.307 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.308 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.308 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.308 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.309 I llama_model_loader: - type  f32:  194 tensors
0.00.024.309 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.309 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.440 I llm_load_vocab: special tokens cache size = 25
0.00.051.637 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.640 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.640 I llm_load_print_meta: arch             = gptneox
0.00.051.640 I llm_load_print_meta: vocab type       = BPE
0.00.051.641 I llm_load_print_meta: n_vocab          = 50304
0.00.051.641 I llm_load_print_meta: n_merges         = 50009
0.00.051.641 I llm_load_print_meta: vocab_only       = 0
0.00.051.641 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.641 I llm_load_print_meta: n_embd           = 2048
0.00.051.642 I llm_load_print_meta: n_layer          = 24
0.00.051.644 I llm_load_print_meta: n_head           = 16
0.00.051.645 I llm_load_print_meta: n_head_kv        = 16
0.00.051.645 I llm_load_print_meta: n_rot            = 32
0.00.051.646 I llm_load_print_meta: n_swa            = 0
0.00.051.646 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.646 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.649 I llm_load_print_meta: n_gqa            = 1
0.00.051.650 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.650 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.651 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.652 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.652 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.652 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.652 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.653 I llm_load_print_meta: n_ff             = 8192
0.00.051.653 I llm_load_print_meta: n_expert         = 0
0.00.051.653 I llm_load_print_meta: n_expert_used    = 0
0.00.051.653 I llm_load_print_meta: causal attn      = 1
0.00.051.653 I llm_load_print_meta: pooling type     = 0
0.00.051.653 I llm_load_print_meta: rope type        = 2
0.00.051.654 I llm_load_print_meta: rope scaling     = linear
0.00.051.654 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.654 I llm_load_print_meta: freq_scale_train = 1
0.00.051.654 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.655 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.655 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.655 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.655 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.655 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.655 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.667 I llm_load_print_meta: model type       = 1.4B
0.00.051.667 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.668 I llm_load_print_meta: model params     = 1.41 B
0.00.051.668 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.668 I llm_load_print_meta: general.name     = 1.4B
0.00.051.669 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.669 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.670 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.670 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.673 I llm_load_print_meta: LF token         = 128 ''
0.00.051.673 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.674 I llm_load_print_meta: max token length = 1024
0.00.053.279 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.279 I llm_load_tensors: offloading output layer to GPU
0.00.053.280 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.289 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.290 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.142 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.143 I llama_new_context_with_model: n_ctx         = 128
0.00.054.143 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.143 I llama_new_context_with_model: n_batch       = 128
0.00.054.144 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.144 I llama_new_context_with_model: flash_attn    = 0
0.00.054.144 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.144 I llama_new_context_with_model: freq_scale    = 1
0.00.054.145 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.145 I ggml_metal_init: allocating
0.00.054.152 I ggml_metal_init: found device: Apple M4
0.00.054.155 I ggml_metal_init: picking default device: Apple M4
0.00.054.716 I ggml_metal_init: using embedded metal library
0.00.056.640 I ggml_metal_init: GPU name:   Apple M4
0.00.056.641 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.642 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.642 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.642 I ggml_metal_init: simdgroup reduction   = true
0.00.056.642 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.643 I ggml_metal_init: has bfloat            = true
0.00.056.643 I ggml_metal_init: use bfloat            = true
0.00.056.643 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.644 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.183 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.188 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.206 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.095 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.097 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.097 I llama_new_context_with_model: graph nodes  = 967
0.00.067.097 I llama_new_context_with_model: graph splits = 2
0.00.067.109 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.656.710 I 
0.00.656.737 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.656.741 I perplexity: tokenizing the input ..
0.00.665.184 I perplexity: tokenization took 8.442 ms
0.00.665.198 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.805.595 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.806.760 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.806.774 I llama_perf_context_print:        load time =     647.25 ms
0.00.806.775 I llama_perf_context_print: prompt eval time =     140.17 ms /   128 tokens (    1.10 ms per token,   913.16 tokens per second)
0.00.806.776 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.806.780 I llama_perf_context_print:       total time =     150.06 ms /   129 tokens
0.00.807.164 I ggml_metal_free: deallocating

real	0m0.820s
user	0m0.079s
sys	0m0.119s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4210 (5acff8f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.898 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.353 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.357 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.359 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.359 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.359 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.360 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.360 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.361 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.361 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.361 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.362 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.362 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.362 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.363 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.365 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.366 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.366 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.312 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.421 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.307 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.308 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.309 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.309 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.309 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.309 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.310 I llama_model_loader: - type  f32:  194 tensors
0.00.025.310 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.993 I llm_load_vocab: special tokens cache size = 25
0.00.052.093 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.095 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.095 I llm_load_print_meta: arch             = gptneox
0.00.052.096 I llm_load_print_meta: vocab type       = BPE
0.00.052.096 I llm_load_print_meta: n_vocab          = 50304
0.00.052.096 I llm_load_print_meta: n_merges         = 50009
0.00.052.096 I llm_load_print_meta: vocab_only       = 0
0.00.052.096 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.097 I llm_load_print_meta: n_embd           = 2048
0.00.052.097 I llm_load_print_meta: n_layer          = 24
0.00.052.100 I llm_load_print_meta: n_head           = 16
0.00.052.101 I llm_load_print_meta: n_head_kv        = 16
0.00.052.101 I llm_load_print_meta: n_rot            = 32
0.00.052.101 I llm_load_print_meta: n_swa            = 0
0.00.052.101 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.101 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.103 I llm_load_print_meta: n_gqa            = 1
0.00.052.104 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.105 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.105 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.106 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.106 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.106 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.106 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.107 I llm_load_print_meta: n_ff             = 8192
0.00.052.107 I llm_load_print_meta: n_expert         = 0
0.00.052.108 I llm_load_print_meta: n_expert_used    = 0
0.00.052.108 I llm_load_print_meta: causal attn      = 1
0.00.052.109 I llm_load_print_meta: pooling type     = 0
0.00.052.111 I llm_load_print_meta: rope type        = 2
0.00.052.112 I llm_load_print_meta: rope scaling     = linear
0.00.052.112 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.112 I llm_load_print_meta: freq_scale_train = 1
0.00.052.112 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.113 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.113 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.113 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.113 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.113 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.113 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.120 I llm_load_print_meta: model type       = 1.4B
0.00.052.120 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.121 I llm_load_print_meta: model params     = 1.41 B
0.00.052.121 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.121 I llm_load_print_meta: general.name     = 1.4B
0.00.052.122 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.122 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.123 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.123 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.124 I llm_load_print_meta: LF token         = 128 ''
0.00.052.124 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.124 I llm_load_print_meta: max token length = 1024
0.00.053.913 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.913 I llm_load_tensors: offloading output layer to GPU
0.00.053.913 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.918 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.919 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.843 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.843 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.843 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.844 I llama_new_context_with_model: n_batch       = 2048
0.00.054.844 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.844 I llama_new_context_with_model: flash_attn    = 0
0.00.054.845 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.845 I llama_new_context_with_model: freq_scale    = 1
0.00.054.845 I ggml_metal_init: allocating
0.00.054.853 I ggml_metal_init: found device: Apple M4
0.00.054.855 I ggml_metal_init: picking default device: Apple M4
0.00.055.415 I ggml_metal_init: using embedded metal library
0.00.057.369 I ggml_metal_init: GPU name:   Apple M4
0.00.057.370 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.371 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.371 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.371 I ggml_metal_init: simdgroup reduction   = true
0.00.057.373 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.373 I ggml_metal_init: has bfloat            = true
0.00.057.373 I ggml_metal_init: use bfloat            = true
0.00.057.374 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.374 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.379 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.384 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.403 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.380 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.381 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.382 I llama_new_context_with_model: graph nodes  = 967
0.00.087.382 I llama_new_context_with_model: graph splits = 2
0.00.087.405 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.770.737 I main: llama threadpool init, n_threads = 4
0.00.770.771 I 
0.00.770.815 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.770.817 I 
0.00.771.050 I sampler seed: 1234
0.00.771.054 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.771.064 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.771.065 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.771.065 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.643.847 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56618.82 tokens per second)
0.01.643.848 I llama_perf_context_print:        load time =     760.84 ms
0.01.643.849 I llama_perf_context_print: prompt eval time =      38.41 ms /     7 tokens (    5.49 ms per token,   182.23 tokens per second)
0.01.643.850 I llama_perf_context_print:        eval time =     831.26 ms /    63 runs   (   13.19 ms per token,    75.79 tokens per second)
0.01.643.850 I llama_perf_context_print:       total time =     873.11 ms /    70 tokens
0.01.644.023 I ggml_metal_free: deallocating

real	0m1.664s
user	0m0.110s
sys	0m0.177s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4210 (5acff8f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.360 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.038 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.042 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.044 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.048 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.049 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.049 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.049 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.050 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.051 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.051 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.053 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.053 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.054 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.054 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.056 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.056 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.058 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.895 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.976 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.816 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.817 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.818 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.818 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.818 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.819 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.819 I llama_model_loader: - type  f32:  194 tensors
0.00.025.819 I llama_model_loader: - type q6_K:   98 tensors
0.00.046.233 I llm_load_vocab: special tokens cache size = 25
0.00.052.225 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.228 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.228 I llm_load_print_meta: arch             = gptneox
0.00.052.229 I llm_load_print_meta: vocab type       = BPE
0.00.052.229 I llm_load_print_meta: n_vocab          = 50304
0.00.052.229 I llm_load_print_meta: n_merges         = 50009
0.00.052.229 I llm_load_print_meta: vocab_only       = 0
0.00.052.229 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.229 I llm_load_print_meta: n_embd           = 2048
0.00.052.230 I llm_load_print_meta: n_layer          = 24
0.00.052.232 I llm_load_print_meta: n_head           = 16
0.00.052.233 I llm_load_print_meta: n_head_kv        = 16
0.00.052.233 I llm_load_print_meta: n_rot            = 32
0.00.052.233 I llm_load_print_meta: n_swa            = 0
0.00.052.233 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.234 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.234 I llm_load_print_meta: n_gqa            = 1
0.00.052.235 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.236 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.236 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.237 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.237 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.239 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.239 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.240 I llm_load_print_meta: n_ff             = 8192
0.00.052.240 I llm_load_print_meta: n_expert         = 0
0.00.052.240 I llm_load_print_meta: n_expert_used    = 0
0.00.052.240 I llm_load_print_meta: causal attn      = 1
0.00.052.240 I llm_load_print_meta: pooling type     = 0
0.00.052.240 I llm_load_print_meta: rope type        = 2
0.00.052.241 I llm_load_print_meta: rope scaling     = linear
0.00.052.241 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.241 I llm_load_print_meta: freq_scale_train = 1
0.00.052.241 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.242 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.242 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.242 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.242 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.242 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.242 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.254 I llm_load_print_meta: model type       = 1.4B
0.00.052.255 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.255 I llm_load_print_meta: model params     = 1.41 B
0.00.052.255 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.255 I llm_load_print_meta: general.name     = 1.4B
0.00.052.256 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.256 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.256 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.256 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.256 I llm_load_print_meta: LF token         = 128 ''
0.00.052.257 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.257 I llm_load_print_meta: max token length = 1024
0.00.054.346 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.346 I llm_load_tensors: offloading output layer to GPU
0.00.054.346 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.356 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.357 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.055.278 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.279 I llama_new_context_with_model: n_ctx         = 128
0.00.055.279 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.279 I llama_new_context_with_model: n_batch       = 128
0.00.055.279 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.279 I llama_new_context_with_model: flash_attn    = 0
0.00.055.280 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.280 I llama_new_context_with_model: freq_scale    = 1
0.00.055.281 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.281 I ggml_metal_init: allocating
0.00.055.286 I ggml_metal_init: found device: Apple M4
0.00.055.291 I ggml_metal_init: picking default device: Apple M4
0.00.055.815 I ggml_metal_init: using embedded metal library
0.00.057.769 I ggml_metal_init: GPU name:   Apple M4
0.00.057.770 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.771 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.771 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.771 I ggml_metal_init: simdgroup reduction   = true
0.00.057.771 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.771 I ggml_metal_init: has bfloat            = true
0.00.057.772 I ggml_metal_init: use bfloat            = true
0.00.057.772 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.773 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.102 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.108 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.123 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.984 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.986 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.986 I llama_new_context_with_model: graph nodes  = 967
0.00.067.986 I llama_new_context_with_model: graph splits = 2
0.00.067.991 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.223.680 I 
0.00.223.727 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.223.733 I perplexity: tokenizing the input ..
0.00.231.508 I perplexity: tokenization took 7.773 ms
0.00.231.525 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.372.097 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.373.323 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.373.344 I llama_perf_context_print:        load time =     212.31 ms
0.00.373.345 I llama_perf_context_print: prompt eval time =     140.34 ms /   128 tokens (    1.10 ms per token,   912.05 tokens per second)
0.00.373.346 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.373.347 I llama_perf_context_print:       total time =     149.67 ms /   129 tokens
0.00.373.790 I ggml_metal_free: deallocating

real	0m0.391s
user	0m0.077s
sys	0m0.053s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4210 (5acff8f3)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11e70a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11e70a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11e70ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11e70b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11e70b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11e70be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11e70c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11e70c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11e70cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11e70d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11e70d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11e70de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11e70e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11e70f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11e70f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11e710090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11e7107b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11e710ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11e7115f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11e711dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11e7124e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11e712c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11e713320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11e713bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11e7142e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11e7145a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11e714bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11e715820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11e715d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11e716020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11e7164c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11e716780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11e717010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11e717550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11e717810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11e717cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11e718150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11e7185f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11e718a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11e718f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11e7193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11e719870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11e719d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11e71a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11e71a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11e71aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11e71b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11e71b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11e71bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11e71c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11e71cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11e71d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11e71d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11e71de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11e71e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11e71eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11e71ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11e71f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11e71f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11e720000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11e7202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11e720760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11e720c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11e7210a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11e721540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11e7219e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11e721e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11e722320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11e7227c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11e722c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11e723100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11e7235a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11e723a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11e723ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11e724380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11e724820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11e724cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11e725160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11e725600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11e725aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11e725f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11e7263e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11e726880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11e726d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11e7271c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11e727660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11e727b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11e727fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11e728440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11e7288e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11e728d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11e729220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11e7296c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11e729b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11e72a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11e72a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11e72a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11e71b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11e72af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11e72b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11e72b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11e72bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11e72c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11e72c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11e72cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11e72cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11e72d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11e72d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11e72ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11e72e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11e72e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11e72ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11e72f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11e72f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11e72f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11e72fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11e7302d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11e730770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11e730c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11e7310b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11e731550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11e7319f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11e731e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11e732330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11e7327d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11e732c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11e733110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11e7335b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11e733a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11e733ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11e734390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11e734830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11e734cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11e735170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11e735610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11e735ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11e735f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11e7363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11e736890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11e736d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11e7371d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11e737670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11e737b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11e737fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11e738450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11e7388f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11e738d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11e739230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11e7396d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11e739b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11e73a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11e73a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11e73a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11e73aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11e73b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11e73b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11e73be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11e73c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11e73c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11e73cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11e73d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11e73d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11e73dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11e73e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11e73ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11e73f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11e73f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11e73fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11e740270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11e7407c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11e740d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11e741260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11e7417b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11e741d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11e742250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11e7427a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11e742cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11e743240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11e743790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11e743ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11e744230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11e744780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11e744cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11e745220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11e745770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11e745cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11e746210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11e746760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11e746cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11e747200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11e747750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11e747ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11e7481f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11e748740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11e748c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11e7491e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11e749730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11e749c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11e74a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11e74a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11e74ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11e74b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11e74b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11e74bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11e74c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11e74c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11e74cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11e74d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11e74d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11e74dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11e74e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11e74e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11e74ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11e74f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11e74f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11e74fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11e750170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11e7506c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11e750c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11e751160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11e7516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11e751c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11e752150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11e7526a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11e752b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11e752fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11e753480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11e753920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11e753dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11e754260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11e754700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11e754ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11e755040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11e7554e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11e755980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11e755e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11e7562c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11e756810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11e756f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11e757650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11e757d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11e758490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11e758750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11e758d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11e759370 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.142.945 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11e604c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11e605080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11e6054f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11e605960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11e605dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11e606240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11e6066b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11e606b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11e606f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11e607400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11e607870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11e607f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11e608a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11e609200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11e609a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11e60a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11e60a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11e60af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11e60b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11e60be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11e60c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11e60cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11e60d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11e60dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11e60e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11e60e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11e60e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11e60ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11e60f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11e60f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11e60f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11e60fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11e6102e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11e6105a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11e610a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11e610e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11e6112f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11e611760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11e611bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11e612040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11e6124b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11e612920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11e612d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11e613200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11e613670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11e613ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11e613f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11e6143c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11e614830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11e614ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11e615110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11e615580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11e6159f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11e615e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11e6162d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11e616740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11e616cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11e6171b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11e617620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11e617a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11e617f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11e618370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11e6187e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11e618c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11e6190c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11e619530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11e6199a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11e619e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11e61a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11e61a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11e61ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11e61afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11e61b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11e61b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11e61bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11e61c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11e61c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11e61ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11e61cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11e61d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11e61d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11e61dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11e61e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11e61e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11e61e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11e61edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11e61f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11e61f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11e61fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11e61ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11e620420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11e620890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11e620d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11e621170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11e6215e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11e621a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11e621ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11e622330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11e6227a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11e622c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11e623080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11e6234f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11e623960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11e623dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11e624240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11e6246b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11e624b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11e624f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11e625400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11e625870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11e625ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11e626150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11e6265c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11e626a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11e626ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11e627310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11e627780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11e627bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11e628060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11e6284d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11e628940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11e628db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11e629220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11e629690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11e629b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11e629f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11e62a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11e62a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11e62acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11e62b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11e62b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11e62ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11e62be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11e62c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11e62c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11e62cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11e62d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11e62d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11e62d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11e62dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11e62e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11e62e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11e62eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11e62ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11e62f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11e62f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11e62fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11e630110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11e630580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11e6309f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11e630e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11e6312d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11e631740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11e631bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11e632020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11e632490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11e632900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11e632d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11e6331e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11e633650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11e633ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11e633f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11e6343a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11e634810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11e634c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11e6350f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11e635560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11e6360f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11e6363b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11e636670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11e636ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11e636f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11e6373c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11e637830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11e637ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11e638110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11e638580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11e6389f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11e638e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11e6392d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11e639740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11e639bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11e63a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11e63a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11e63a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11e63ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11e63b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11e63b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11e63bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11e63bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11e63c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11e63c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11e63cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11e63d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11e63d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11e63d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11e63de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11e63e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11e63e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11e63eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11e63f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11e63f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11e63f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11e63fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11e6401c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11e640630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11e640aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11e640f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11e641380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11e6417f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11e641c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11e6420d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11e642540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11e6429b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11e642e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11e643290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11e643700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11e643b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11e643fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11e644450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11e6448c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11e644d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11e6451a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11e645610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11e645a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11e645ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11e646360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11e6467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11e646c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11e6470b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11e647520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11e647990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11e647e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11e648270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11e6486e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11e648b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11e648fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11e649430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11e649f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11e64a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11e64adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11e64b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11e64b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11e64ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11e64bec0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11e70e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11e70e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11e70e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11e70ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11e70f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11e70f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11e70faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11e70ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11e710380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11e7107f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11e710c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11e711240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11e711b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11e7122b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11e712a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11e713180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11e713870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11e713f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11e714650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11e714fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11e7156c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11e715db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11e7164a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11e716b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11e717280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11e7176f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11e717b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11e717fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11e718440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11e7188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11e718d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11e719190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11e719600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11e7198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11e719d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11e71a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11e71a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11e71aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11e71aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11e71b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11e71b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11e71bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11e71c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11e71c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11e71c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11e71ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11e71d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11e71d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11e71db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11e71dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11e71e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11e71e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11e71ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11e71f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11e71f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11e71fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11e71fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11e720340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11e7207b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11e720c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11e721090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11e721500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11e721970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11e721de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11e722250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11e7226c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11e722b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11e722fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11e723410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11e723880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11e723cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11e724160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11e7245d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11e724a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11e724eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11e725320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11e725790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11e725c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11e726070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11e7264e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11e726950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11e726dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11e727230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11e7276a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11e727b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11e727f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11e7283f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11e728860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11e728cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11e729140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11e7295b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11e729a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11e729e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11e72a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11e72a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11e72abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11e72b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11e72b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11e72b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11e72bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11e72c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11e72c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11e72caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11e72cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11e72d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11e72d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11e72dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11e72e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11e72e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11e72ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11e72ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11e72f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11e72f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11e72fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11e730030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11e7304a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11e730910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11e730d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11e7311f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11e731660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11e731ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11e731f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11e7323b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11e732820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11e732c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11e733100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11e733570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11e7339e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11e733e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11e7342c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11e734730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11e734ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11e735010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11e735480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11e7358f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11e735d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11e7361d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11e736640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11e736ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11e736f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11e737390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11e737800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11e737c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11e7380e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11e738550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11e7389c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11e738e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11e7392a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11e739710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11e739b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11e739ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11e73a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11e73a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11e73ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11e73b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11e73b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11e73ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11e73bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11e73c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11e73c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11e73cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11e73d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11e73d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11e73d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11e73de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11e73e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11e73e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11e73ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11e73f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11e73f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11e73fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11e740030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11e7404a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11e740910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11e740d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11e7411f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11e741660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11e741ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11e741f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11e7423b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11e742820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11e742c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11e743100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11e743570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11e7439e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11e743e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11e7442c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11e744730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11e744ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11e745010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11e745480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11e7458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11e745d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11e7461d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11e746640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11e746ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11e746f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11e747390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11e747800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11e747c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11e7480e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11e748550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11e7489c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11e748e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11e7492a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11e749710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11e749b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11e749ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11e74a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11e74a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11e74ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11e74b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11e74b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11e74ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11e74bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11e74c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11e74c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11e74cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11e74d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11e74d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11e74d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11e74de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11e74e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11e74e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11e74eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11e74efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11e74f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11e74f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11e74fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11e750190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11e750600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11e750a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11e750ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11e751350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11e7517c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11e751c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11e7520a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11e752510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11e752c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11e7532f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11e7539e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11e7540d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11e754540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11e7549b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11e754e20 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.747s
user	0m0.289s
sys	0m0.293s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4210 (5acff8f3)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x155e0e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x155e0e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x155e0ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x155e0f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x155e0f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x155e0fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x155e103d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x155e10980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x155e10f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x155e11430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x155e11930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x155e11e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x155e12950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x155e13100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x155e13910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x155e14030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x155e14750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x155e14e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x155e15590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x155e15d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x155e16480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x155e16ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x155e172c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x155e17b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x155e18280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x155e18540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x155e18b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x155e197c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x155e19d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x155e19fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x155e1a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x155e1a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x155e1afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x155e1b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x155e1b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x155e1bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x155e1c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x155e1c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x155e1ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x155e1ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x155e1d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x155e1d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x155e1dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x155e1e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x155e1e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x155e1ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x155e1f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x155e1f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x155e1ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x155e20570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x155e20b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x155e21190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x155e217a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x155e21db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x155e225a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x155e22a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x155e22ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x155e231a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x155e237b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x155e23fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x155e24260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x155e24700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x155e24ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x155e25040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x155e254e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x155e25980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x155e25e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x155e262c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x155e26760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x155e26c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x155e270a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x155e27540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x155e279e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x155e27e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x155e28320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x155e287c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x155e28c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x155e29100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x155e295a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x155e29a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x155e29ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x155e2a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x155e2a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x155e2acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x155e2b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x155e2b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x155e2baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x155e2bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x155e2c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x155e2c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x155e2cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x155e2d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x155e2d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x155e2db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x155e2dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x155e2e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x155e2e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x155e1f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x155e2ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x155e2f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x155e2f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x155e2fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x155e301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x155e30650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x155e30af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x155e30f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x155e31430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x155e318d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x155e31d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x155e32210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x155e326b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x155e32b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x155e32ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x155e33490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x155e33930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x155e33dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x155e34270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x155e34710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x155e34bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x155e35050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x155e354f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x155e35990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x155e35e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x155e362d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x155e36770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x155e36c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x155e370b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x155e37550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x155e379f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x155e37e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x155e38330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x155e387d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x155e38c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x155e39110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x155e395b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x155e39a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x155e39ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x155e3a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x155e3a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x155e3acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x155e3b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x155e3b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x155e3bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x155e3bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x155e3c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x155e3c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x155e3cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x155e3d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x155e3d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x155e3db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x155e3dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x155e3e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x155e3e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x155e3ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x155e3f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x155e3f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x155e3fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x155e400f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x155e40700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x155e40d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x155e41320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x155e41930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x155e41f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x155e42730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x155e42bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x155e43070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x155e43510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x155e43cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x155e44210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x155e44760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x155e44cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x155e45200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x155e45750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x155e45ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x155e461f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x155e46740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x155e46c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x155e471e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x155e47730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x155e47c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x155e481d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x155e48720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x155e48c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x155e491c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x155e49710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x155e49c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x155e4a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x155e4a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x155e4ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x155e4b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x155e4b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x155e4bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x155e4c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x155e4c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x155e4cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x155e4d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x155e4d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x155e4dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x155e4e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x155e4e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x155e4ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x155e4f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x155e4f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x155e4fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x155e50150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x155e506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x155e50bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x155e51140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x155e51690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x155e51be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x155e52130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x155e52680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x155e52bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x155e53120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x155e53670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x155e53bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x155e54110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x155e54660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x155e54bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x155e55100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x155e55650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x155e55ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x155e560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x155e56640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x155e56ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x155e56f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x155e57420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x155e578c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x155e57d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x155e58200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x155e586a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x155e58b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x155e58fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x155e59480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x155e59920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x155e59dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x155e5a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x155e5a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x155e5aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x155e5b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x155e5bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x155e5c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x155e5c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x155e5cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x155e5d310 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.088.578 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x145f04bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x145f05030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x145f054a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x145f05910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x145f05d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x145f061f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x145f06660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x145f06ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x145f06f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x145f073b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x145f07820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x145f07ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x145f08a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x145f091b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x145f099c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x145f0a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x145f0a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x145f0af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x145f0b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x145f0be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x145f0c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x145f0cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x145f0d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x145f0da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x145f0e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x145f0e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x145f0e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x145f0eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x145f0f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x145f0f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x145f0f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x145f0fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x145f10290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x145f10550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x145f109c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x145f10e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x145f112a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x145f11710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x145f11b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x145f11ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x145f12460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x145f128d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x145f12d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x145f131b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x145f13620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x145f13a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x145f13f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x145f14370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x145f147e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x145f14c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x145f150c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x145f15530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x145f159a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x145f15e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x145f16280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x145f166f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x145f16c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x145f17160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x145f175d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x145f17a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x145f17eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x145f18320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x145f18790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x145f18c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x145f19070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x145f194e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x145f19950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x145f19dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x145f1a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x145f1a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x145f1ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x145f1af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x145f1b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x145f1b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x145f1bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x145f1c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x145f1c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x145f1ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x145f1ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x145f1d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x145f1d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x145f1dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x145f1e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x145f1e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x145f1e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x145f1eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x145f1f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x145f1f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x145f1faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x145f1ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x145f203d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x145f20840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x145f20cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x145f21120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x145f21590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x145f21a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x145f21e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x145f222e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x145f22750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x145f22bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x145f23030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x145f234a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x145f23910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x145f23d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x145f241f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x145f24660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x145f24ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x145f24f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x145f253b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x145f25820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x145f25c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x145f26100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x145f26570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x145f269e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x145f26e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x145f272c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x145f27730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x145f27ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x145f28010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x145f28480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x145f288f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x145f28d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x145f291d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x145f29640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x145f29ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x145f29f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x145f2a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x145f2a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x145f2ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x145f2b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x145f2b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x145f2b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x145f2be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x145f2c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x145f2c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x145f2cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x145f2cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x145f2d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x145f2d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x145f2dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x145f2e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x145f2e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x145f2ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x145f2ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x145f2f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x145f2f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x145f2fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x145f300c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x145f30530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x145f309a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x145f30e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x145f31280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x145f316f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x145f31b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x145f31fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x145f32440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x145f328b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x145f32d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x145f33190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x145f33600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x145f33a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x145f33ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x145f34350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x145f347c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x145f34c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x145f350a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x145f35510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x145f360a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x145f36360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x145f36620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x145f36a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x145f36f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x145f37370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x145f377e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x145f37c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x145f380c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x145f38530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x145f389a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x145f38e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x145f39280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x145f396f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x145f39b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x145f39fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x145f3a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x145f3a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x145f3ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x145f3b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x145f3b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x145f3ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x145f3bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x145f3c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x145f3c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x145f3cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x145f3d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x145f3d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x145f3d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x145f3ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x145f3e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x145f3e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x145f3eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x145f3efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x145f3f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x145f3f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x145f3fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x145f40170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x145f405e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x145f40a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x145f40ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x145f41330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x145f417a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x145f41c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x145f42080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x145f424f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x145f42960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x145f42dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x145f43240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x145f436b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x145f43b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x145f43f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x145f44400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x145f44870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x145f44ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x145f45150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x145f455c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x145f45a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x145f45ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x145f46310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x145f46780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x145f46bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x145f47060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x145f474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x145f47940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x145f47db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x145f48220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x145f48690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x145f48b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x145f48f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x145f493e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x145f49f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x145f4a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x145f4ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x145f4b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x145f4b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x145f4ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x145f4be70 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x145f04ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x145f04f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x145f053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x145f05830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x145f05ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x145f06110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x145f06580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x145f069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x145f06e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x145f072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x145f07740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x145f07d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x145f08610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x145f08d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x145f09570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x145f09c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x145f0a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x145f0aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x145f0b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x145f0bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x145f0c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x145f0c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x145f0cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x145f0d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x145f0dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x145f0e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x145f0e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x145f0eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x145f0ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x145f0f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x145f0f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x145f0fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x145f100e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x145f103a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x145f10810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x145f10c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x145f110f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x145f11560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x145f119d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x145f11e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x145f122b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x145f12720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x145f12b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x145f13000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x145f13470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x145f138e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x145f13d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x145f141c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x145f14630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x145f14aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x145f14f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x145f15380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x145f157f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x145f15c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x145f160d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x145f16540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x145f169b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x145f16e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x145f17290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x145f17700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x145f17b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x145f17fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x145f18450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x145f188c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x145f18d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x145f191a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x145f19610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x145f19a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x145f19ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x145f1a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x145f1a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x145f1ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x145f1b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x145f1b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x145f1b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x145f1be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x145f1c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x145f1c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x145f1cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x145f1cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x145f1d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x145f1d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x145f1dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x145f1e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x145f1e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x145f1ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x145f1eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x145f1f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x145f1f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x145f1fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x145f20090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x145f20500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x145f20970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x145f20de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x145f21250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x145f216c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x145f21b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x145f21fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x145f22410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x145f22880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x145f22cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x145f23160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x145f235d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x145f23a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x145f23eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x145f24320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x145f24790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x145f24c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x145f25070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x145f254e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x145f25950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x145f25dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x145f26230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x145f266a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x145f26b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x145f26f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x145f273f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x145f27860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x145f27cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x145f28140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x145f285b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x145f28a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x145f28e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x145f29300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x145f29770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x145f29be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x145f2a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x145f2a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x145f2a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x145f2ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x145f2b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x145f2b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x145f2baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x145f2bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x145f2c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x145f2c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x145f2ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x145f2d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x145f2d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x145f2da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x145f2de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x145f2e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x145f2e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x145f2ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x145f2f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x145f2f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x145f2f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x145f2fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x145f301f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x145f30660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x145f30ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x145f30f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x145f313b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x145f31820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x145f31c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x145f32100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x145f32570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x145f329e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x145f32e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x145f332c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x145f33730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x145f33ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x145f34010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x145f34480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x145f348f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x145f34d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x145f351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x145f35950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x145f35dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x145f36230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x145f366a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x145f36b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x145f36f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x145f373f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x145f37860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x145f37cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x145f38140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x145f385b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x145f38a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x145f38e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x145f39300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x145f39770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x145f39be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x145f3a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x145f3a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x145f3a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x145f3ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x145f3b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x145f3b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x145f3baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x145f3bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x145f3c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x145f3c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x145f3ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x145f3d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x145f3d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x145f3da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x145f3de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x145f3e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x145f3e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x145f3ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x145f3f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x145f3f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x145f3f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x145f3fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x145f401f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x145f40660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x145f40ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x145f40f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x145f413b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x145f41820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x145f41c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x145f42100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x145f42570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x145f429e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x145f42e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x145f432c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x145f43730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x145f43ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x145f44010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x145f44480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x145f448f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x145f44d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x145f451d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x145f45640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x145f45ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x145f45f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x145f46390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x145f46800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x145f46c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x145f470e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x145f47550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x145f479c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x145f47e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x145f482a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x145f48710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x145f48b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x145f48ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x145f496e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x145f49dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x145f4a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x145f4abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x145f4b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x145f4b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x145f4b900 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this


second run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this


single seq run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this

real	0m0.929s
user	0m0.242s
sys	0m0.141s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.53 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.59 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.12 sec*proc (2 tests)

Total Test time (real) =   1.13 sec
        1.15 real         0.72 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.25 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.51 sec*proc (2 tests)

Total Test time (real) =   0.52 sec
        0.52 real         0.14 user         0.04 sys
```
