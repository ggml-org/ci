### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.25 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.09 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.17 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.45 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.28 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.22 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.66 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.60 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.22 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.14 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.29 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.49 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.29 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.09 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.22 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.28 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.86 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.78 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  193.91 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.84 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   25.87 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.34 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 254.52 sec*proc (29 tests)

Total Test time (real) = 254.53 sec

real	4m14.627s
user	8m35.646s
sys	0m7.169s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.18 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.22 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.09 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.16 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.11 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.92 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.77 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.17 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.18 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.21 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.45 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.38 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   30.90 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.37 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.08 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.21 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  54.78 sec*proc (29 tests)

Total Test time (real) =  54.79 sec

real	0m54.800s
user	1m16.745s
sys	0m6.351s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.115 I build: 4732 (2eea03d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.017.407 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.751 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.022.758 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.761 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.022.761 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.762 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.022.763 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.022.763 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.022.764 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.022.769 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.022.769 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.022.770 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.022.770 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.022.773 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.022.773 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.022.774 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.022.774 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.022.775 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.022.776 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.022.776 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.027.213 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.028.381 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.384 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.028.384 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.028.385 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.028.385 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.028.386 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.028.386 I llama_model_loader: - type  f32:  124 tensors
0.00.028.387 I llama_model_loader: - type  f16:   73 tensors
0.00.028.387 I print_info: file format = GGUF V3 (latest)
0.00.028.388 I print_info: file type   = F16
0.00.028.389 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.032.716 I load: special tokens cache size = 5
0.00.034.955 I load: token to piece cache size = 0.2032 MB
0.00.034.959 I print_info: arch             = bert
0.00.034.959 I print_info: vocab_only       = 0
0.00.034.960 I print_info: n_ctx_train      = 512
0.00.034.960 I print_info: n_embd           = 384
0.00.034.960 I print_info: n_layer          = 12
0.00.034.963 I print_info: n_head           = 12
0.00.034.964 I print_info: n_head_kv        = 12
0.00.034.965 I print_info: n_rot            = 32
0.00.034.965 I print_info: n_swa            = 0
0.00.034.965 I print_info: n_embd_head_k    = 32
0.00.034.965 I print_info: n_embd_head_v    = 32
0.00.034.966 I print_info: n_gqa            = 1
0.00.034.967 I print_info: n_embd_k_gqa     = 384
0.00.034.968 I print_info: n_embd_v_gqa     = 384
0.00.034.969 I print_info: f_norm_eps       = 1.0e-12
0.00.034.969 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.034.969 I print_info: f_clamp_kqv      = 0.0e+00
0.00.034.970 I print_info: f_max_alibi_bias = 0.0e+00
0.00.034.970 I print_info: f_logit_scale    = 0.0e+00
0.00.034.972 I print_info: n_ff             = 1536
0.00.034.973 I print_info: n_expert         = 0
0.00.034.973 I print_info: n_expert_used    = 0
0.00.034.973 I print_info: causal attn      = 0
0.00.034.973 I print_info: pooling type     = 2
0.00.034.973 I print_info: rope type        = 2
0.00.034.976 I print_info: rope scaling     = linear
0.00.034.976 I print_info: freq_base_train  = 10000.0
0.00.034.977 I print_info: freq_scale_train = 1
0.00.034.977 I print_info: n_ctx_orig_yarn  = 512
0.00.034.977 I print_info: rope_finetuned   = unknown
0.00.034.978 I print_info: ssm_d_conv       = 0
0.00.034.978 I print_info: ssm_d_inner      = 0
0.00.034.978 I print_info: ssm_d_state      = 0
0.00.034.978 I print_info: ssm_dt_rank      = 0
0.00.034.978 I print_info: ssm_dt_b_c_rms   = 0
0.00.034.978 I print_info: model type       = 33M
0.00.034.979 I print_info: model params     = 33.21 M
0.00.034.979 I print_info: general.name     = Bge Small
0.00.034.980 I print_info: vocab type       = WPM
0.00.034.980 I print_info: n_vocab          = 30522
0.00.034.980 I print_info: n_merges         = 0
0.00.034.981 I print_info: BOS token        = 101 '[CLS]'
0.00.034.981 I print_info: UNK token        = 100 '[UNK]'
0.00.034.981 I print_info: SEP token        = 102 '[SEP]'
0.00.034.981 I print_info: PAD token        = 0 '[PAD]'
0.00.034.982 I print_info: MASK token       = 103 '[MASK]'
0.00.034.982 I print_info: LF token         = 0 '[PAD]'
0.00.034.982 I print_info: max token length = 21
0.00.034.983 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.038.069 I load_tensors: offloading 12 repeating layers to GPU
0.00.038.071 I load_tensors: offloading output layer to GPU
0.00.038.071 I load_tensors: offloaded 13/13 layers to GPU
0.00.038.093 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.038.095 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.038.360 I llama_init_from_model: n_seq_max     = 1
0.00.038.361 I llama_init_from_model: n_ctx         = 512
0.00.038.361 I llama_init_from_model: n_ctx_per_seq = 512
0.00.038.361 I llama_init_from_model: n_batch       = 2048
0.00.038.362 I llama_init_from_model: n_ubatch      = 2048
0.00.038.362 I llama_init_from_model: flash_attn    = 0
0.00.038.363 I llama_init_from_model: freq_base     = 10000.0
0.00.038.363 I llama_init_from_model: freq_scale    = 1
0.00.038.364 I ggml_metal_init: allocating
0.00.038.374 I ggml_metal_init: found device: Apple M4
0.00.038.380 I ggml_metal_init: picking default device: Apple M4
0.00.039.084 I ggml_metal_init: using embedded metal library
0.00.042.904 I ggml_metal_init: GPU name:   Apple M4
0.00.042.907 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.042.907 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.042.908 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.042.908 I ggml_metal_init: simdgroup reduction   = true
0.00.042.908 I ggml_metal_init: simdgroup matrix mul. = true
0.00.042.909 I ggml_metal_init: has residency sets    = true
0.00.042.909 I ggml_metal_init: has bfloat            = true
0.00.042.909 I ggml_metal_init: use bfloat            = true
0.00.042.909 I ggml_metal_init: hasUnifiedMemory      = true
0.00.042.910 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.054.777 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.055.455 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.055.457 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.055.476 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.056.665 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.056.667 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.056.667 I llama_init_from_model: graph nodes  = 429
0.00.056.667 I llama_init_from_model: graph splits = 2
0.00.056.669 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.056.669 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.062.344 I 
0.00.062.360 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.063.023 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.068.182 I llama_perf_context_print:        load time =      44.92 ms
0.00.068.183 I llama_perf_context_print: prompt eval time =       5.01 ms /     9 tokens (    0.56 ms per token,  1794.97 tokens per second)
0.00.068.184 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.068.184 I llama_perf_context_print:       total time =       5.84 ms /    10 tokens
0.00.068.324 I ggml_metal_free: deallocating

real	0m0.242s
user	0m0.047s
sys	0m0.024s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.046 I build: 4732 (2eea03d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.462 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.014.281 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.014.285 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.287 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.014.287 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.292 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.014.292 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.014.293 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.014.294 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.014.296 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.014.296 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.014.296 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.014.297 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.014.299 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.014.299 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.014.300 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.014.300 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.014.300 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.014.301 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.016.834 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.017.549 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.017.550 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.017.551 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.017.551 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.017.551 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.017.551 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.017.552 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.017.552 I llama_model_loader: - type  f32:  124 tensors
0.00.017.552 I llama_model_loader: - type q8_0:   73 tensors
0.00.017.553 I print_info: file format = GGUF V3 (latest)
0.00.017.553 I print_info: file type   = Q8_0
0.00.017.555 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.020.123 I load: special tokens cache size = 5
0.00.021.436 I load: token to piece cache size = 0.2032 MB
0.00.021.439 I print_info: arch             = bert
0.00.021.439 I print_info: vocab_only       = 0
0.00.021.440 I print_info: n_ctx_train      = 512
0.00.021.440 I print_info: n_embd           = 384
0.00.021.440 I print_info: n_layer          = 12
0.00.021.443 I print_info: n_head           = 12
0.00.021.443 I print_info: n_head_kv        = 12
0.00.021.443 I print_info: n_rot            = 32
0.00.021.445 I print_info: n_swa            = 0
0.00.021.445 I print_info: n_embd_head_k    = 32
0.00.021.445 I print_info: n_embd_head_v    = 32
0.00.021.446 I print_info: n_gqa            = 1
0.00.021.447 I print_info: n_embd_k_gqa     = 384
0.00.021.447 I print_info: n_embd_v_gqa     = 384
0.00.021.481 I print_info: f_norm_eps       = 1.0e-12
0.00.021.482 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.021.482 I print_info: f_clamp_kqv      = 0.0e+00
0.00.021.482 I print_info: f_max_alibi_bias = 0.0e+00
0.00.021.483 I print_info: f_logit_scale    = 0.0e+00
0.00.021.484 I print_info: n_ff             = 1536
0.00.021.484 I print_info: n_expert         = 0
0.00.021.484 I print_info: n_expert_used    = 0
0.00.021.484 I print_info: causal attn      = 0
0.00.021.484 I print_info: pooling type     = 2
0.00.021.484 I print_info: rope type        = 2
0.00.021.485 I print_info: rope scaling     = linear
0.00.021.485 I print_info: freq_base_train  = 10000.0
0.00.021.485 I print_info: freq_scale_train = 1
0.00.021.485 I print_info: n_ctx_orig_yarn  = 512
0.00.021.486 I print_info: rope_finetuned   = unknown
0.00.021.486 I print_info: ssm_d_conv       = 0
0.00.021.486 I print_info: ssm_d_inner      = 0
0.00.021.486 I print_info: ssm_d_state      = 0
0.00.021.486 I print_info: ssm_dt_rank      = 0
0.00.021.486 I print_info: ssm_dt_b_c_rms   = 0
0.00.021.487 I print_info: model type       = 33M
0.00.021.487 I print_info: model params     = 33.21 M
0.00.021.487 I print_info: general.name     = Bge Small
0.00.021.488 I print_info: vocab type       = WPM
0.00.021.488 I print_info: n_vocab          = 30522
0.00.021.488 I print_info: n_merges         = 0
0.00.021.488 I print_info: BOS token        = 101 '[CLS]'
0.00.021.488 I print_info: UNK token        = 100 '[UNK]'
0.00.021.488 I print_info: SEP token        = 102 '[SEP]'
0.00.021.489 I print_info: PAD token        = 0 '[PAD]'
0.00.021.489 I print_info: MASK token       = 103 '[MASK]'
0.00.021.489 I print_info: LF token         = 0 '[PAD]'
0.00.021.489 I print_info: max token length = 21
0.00.021.490 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.023.179 I load_tensors: offloading 12 repeating layers to GPU
0.00.023.180 I load_tensors: offloading output layer to GPU
0.00.023.180 I load_tensors: offloaded 13/13 layers to GPU
0.00.023.185 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.023.190 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.023.367 I llama_init_from_model: n_seq_max     = 1
0.00.023.368 I llama_init_from_model: n_ctx         = 512
0.00.023.368 I llama_init_from_model: n_ctx_per_seq = 512
0.00.023.368 I llama_init_from_model: n_batch       = 2048
0.00.023.368 I llama_init_from_model: n_ubatch      = 2048
0.00.023.369 I llama_init_from_model: flash_attn    = 0
0.00.023.369 I llama_init_from_model: freq_base     = 10000.0
0.00.023.369 I llama_init_from_model: freq_scale    = 1
0.00.023.370 I ggml_metal_init: allocating
0.00.023.375 I ggml_metal_init: found device: Apple M4
0.00.023.378 I ggml_metal_init: picking default device: Apple M4
0.00.023.885 I ggml_metal_init: using embedded metal library
0.00.026.483 I ggml_metal_init: GPU name:   Apple M4
0.00.026.485 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.026.485 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.026.486 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.026.486 I ggml_metal_init: simdgroup reduction   = true
0.00.026.486 I ggml_metal_init: simdgroup matrix mul. = true
0.00.026.486 I ggml_metal_init: has residency sets    = true
0.00.026.486 I ggml_metal_init: has bfloat            = true
0.00.026.487 I ggml_metal_init: use bfloat            = true
0.00.026.487 I ggml_metal_init: hasUnifiedMemory      = true
0.00.026.488 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.036.635 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.037.223 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.037.225 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.037.238 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.038.272 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.038.273 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.038.273 I llama_init_from_model: graph nodes  = 429
0.00.038.274 I llama_init_from_model: graph splits = 2
0.00.038.275 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.038.275 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.042.189 I 
0.00.042.204 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.042.723 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.047.181 I llama_perf_context_print:        load time =      30.72 ms
0.00.047.183 I llama_perf_context_print: prompt eval time =       4.34 ms /     9 tokens (    0.48 ms per token,  2075.17 tokens per second)
0.00.047.185 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.047.185 I llama_perf_context_print:       total time =       4.99 ms /    10 tokens
0.00.047.392 I ggml_metal_free: deallocating

real	0m0.059s
user	0m0.031s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.268 I build: 4732 (2eea03d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.647 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.476 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.481 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.484 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.036.485 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.487 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.036.488 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.036.489 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.036.490 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.036.491 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.036.491 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.036.495 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.036.495 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.036.498 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.036.505 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.036.506 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.036.507 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.507 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.044.190 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.046.312 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.794 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.050.795 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.796 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.050.796 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.050.797 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.050.797 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.050.797 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.050.798 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.050.798 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.050.799 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.050.799 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.050.799 I llama_model_loader: - type  f32:   40 tensors
0.00.050.800 I llama_model_loader: - type  f16:   30 tensors
0.00.050.801 I print_info: file format = GGUF V3 (latest)
0.00.050.801 I print_info: file type   = F16
0.00.050.802 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.054.952 W load: empty token at index 5
0.00.060.055 W load: model vocab missing newline token, using special_pad_id instead
0.00.061.569 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.061.606 I load: special tokens cache size = 5
0.00.318.012 I load: token to piece cache size = 1.5060 MB
0.00.318.022 I print_info: arch             = jina-bert-v2
0.00.318.026 I print_info: vocab_only       = 0
0.00.318.026 I print_info: n_ctx_train      = 8192
0.00.318.027 I print_info: n_embd           = 384
0.00.318.027 I print_info: n_layer          = 4
0.00.318.036 I print_info: n_head           = 12
0.00.318.037 I print_info: n_head_kv        = 12
0.00.318.038 I print_info: n_rot            = 32
0.00.318.038 I print_info: n_swa            = 0
0.00.318.038 I print_info: n_embd_head_k    = 32
0.00.318.038 I print_info: n_embd_head_v    = 32
0.00.318.039 I print_info: n_gqa            = 1
0.00.318.040 I print_info: n_embd_k_gqa     = 384
0.00.318.040 I print_info: n_embd_v_gqa     = 384
0.00.318.042 I print_info: f_norm_eps       = 1.0e-12
0.00.318.044 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.318.046 I print_info: f_clamp_kqv      = 0.0e+00
0.00.318.047 I print_info: f_max_alibi_bias = 8.0e+00
0.00.318.047 I print_info: f_logit_scale    = 0.0e+00
0.00.318.048 I print_info: n_ff             = 1536
0.00.318.048 I print_info: n_expert         = 0
0.00.318.048 I print_info: n_expert_used    = 0
0.00.318.048 I print_info: causal attn      = 0
0.00.318.049 I print_info: pooling type     = -1
0.00.318.049 I print_info: rope type        = -1
0.00.318.049 I print_info: rope scaling     = linear
0.00.318.050 I print_info: freq_base_train  = 10000.0
0.00.318.051 I print_info: freq_scale_train = 1
0.00.318.051 I print_info: n_ctx_orig_yarn  = 8192
0.00.318.051 I print_info: rope_finetuned   = unknown
0.00.318.051 I print_info: ssm_d_conv       = 0
0.00.318.052 I print_info: ssm_d_inner      = 0
0.00.318.052 I print_info: ssm_d_state      = 0
0.00.318.052 I print_info: ssm_dt_rank      = 0
0.00.318.052 I print_info: ssm_dt_b_c_rms   = 0
0.00.318.052 I print_info: model type       = 33M
0.00.318.053 I print_info: model params     = 32.90 M
0.00.318.053 I print_info: general.name     = Jina Bert Implementation
0.00.318.055 I print_info: vocab type       = BPE
0.00.318.055 I print_info: n_vocab          = 61056
0.00.318.055 I print_info: n_merges         = 39382
0.00.318.055 I print_info: BOS token        = 0 '<s>'
0.00.318.055 I print_info: EOS token        = 2 '</s>'
0.00.318.056 I print_info: UNK token        = 3 '<unk>'
0.00.318.056 I print_info: SEP token        = 2 '</s>'
0.00.318.056 I print_info: PAD token        = 1 '<pad>'
0.00.318.056 I print_info: MASK token       = 4 '<mask>'
0.00.318.057 I print_info: EOG token        = 2 '</s>'
0.00.318.057 I print_info: max token length = 45
0.00.318.058 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.320.082 I load_tensors: offloading 4 repeating layers to GPU
0.00.320.083 I load_tensors: offloading output layer to GPU
0.00.320.083 I load_tensors: offloaded 5/5 layers to GPU
0.00.320.105 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.320.106 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.320.322 I llama_init_from_model: n_seq_max     = 1
0.00.320.323 I llama_init_from_model: n_ctx         = 8192
0.00.320.323 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.320.323 I llama_init_from_model: n_batch       = 2048
0.00.320.323 I llama_init_from_model: n_ubatch      = 2048
0.00.320.323 I llama_init_from_model: flash_attn    = 0
0.00.320.324 I llama_init_from_model: freq_base     = 10000.0
0.00.320.324 I llama_init_from_model: freq_scale    = 1
0.00.320.324 I ggml_metal_init: allocating
0.00.320.328 I ggml_metal_init: found device: Apple M4
0.00.320.331 I ggml_metal_init: picking default device: Apple M4
0.00.320.988 I ggml_metal_init: using embedded metal library
0.00.323.917 I ggml_metal_init: GPU name:   Apple M4
0.00.323.919 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.323.919 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.323.920 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.323.920 I ggml_metal_init: simdgroup reduction   = true
0.00.323.920 I ggml_metal_init: simdgroup matrix mul. = true
0.00.323.920 I ggml_metal_init: has residency sets    = true
0.00.323.920 I ggml_metal_init: has bfloat            = true
0.00.323.920 I ggml_metal_init: use bfloat            = true
0.00.323.921 I ggml_metal_init: hasUnifiedMemory      = true
0.00.323.921 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.333.367 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.336.408 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.336.410 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.336.439 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.342.467 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.342.468 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.342.468 I llama_init_from_model: graph nodes  = 154
0.00.342.469 I llama_init_from_model: graph splits = 2
0.00.342.470 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.342.470 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.349.207 I 
0.00.349.224 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.349.330 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.349.330 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.349.333 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.349.333 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.349.338 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.349.338 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.349.901 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.353.598 I llama_perf_context_print:        load time =     325.53 ms
0.00.353.599 I llama_perf_context_print: prompt eval time =       3.69 ms /    62 tokens (    0.06 ms per token, 16797.62 tokens per second)
0.00.353.600 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.353.600 I llama_perf_context_print:       total time =       4.39 ms /    63 tokens
0.00.353.844 I ggml_metal_free: deallocating

real	0m1.061s
user	0m0.324s
sys	0m0.050s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.126 I build: 4732 (2eea03d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.326 I main: llama backend init
0.00.000.333 I main: load the model and apply lora adapter, if any
0.00.048.817 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.063.527 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.063.537 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.063.541 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.063.542 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.063.543 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.063.543 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.063.544 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.063.546 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.063.547 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.063.548 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.063.548 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.063.549 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.063.550 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.063.551 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.063.554 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.063.554 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.063.555 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.072.377 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.074.302 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.081.186 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.081.189 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.081.189 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.081.190 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.081.190 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.081.191 I llama_model_loader: - type  f32:  194 tensors
0.00.081.191 I llama_model_loader: - type  f16:   98 tensors
0.00.081.193 I print_info: file format = GGUF V3 (latest)
0.00.081.194 I print_info: file type   = all F32 (guessed)
0.00.081.195 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.094.328 I load: special tokens cache size = 25
0.00.102.414 I load: token to piece cache size = 0.2984 MB
0.00.102.418 I print_info: arch             = gptneox
0.00.102.418 I print_info: vocab_only       = 0
0.00.102.418 I print_info: n_ctx_train      = 2048
0.00.102.418 I print_info: n_embd           = 2048
0.00.102.419 I print_info: n_layer          = 24
0.00.102.421 I print_info: n_head           = 16
0.00.102.422 I print_info: n_head_kv        = 16
0.00.102.423 I print_info: n_rot            = 32
0.00.102.424 I print_info: n_swa            = 0
0.00.102.425 I print_info: n_embd_head_k    = 128
0.00.102.426 I print_info: n_embd_head_v    = 128
0.00.102.426 I print_info: n_gqa            = 1
0.00.102.427 I print_info: n_embd_k_gqa     = 2048
0.00.102.428 I print_info: n_embd_v_gqa     = 2048
0.00.102.428 I print_info: f_norm_eps       = 1.0e-05
0.00.102.429 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.102.429 I print_info: f_clamp_kqv      = 0.0e+00
0.00.102.429 I print_info: f_max_alibi_bias = 0.0e+00
0.00.102.431 I print_info: f_logit_scale    = 0.0e+00
0.00.102.431 I print_info: n_ff             = 8192
0.00.102.432 I print_info: n_expert         = 0
0.00.102.432 I print_info: n_expert_used    = 0
0.00.102.432 I print_info: causal attn      = 1
0.00.102.432 I print_info: pooling type     = 0
0.00.102.432 I print_info: rope type        = 2
0.00.102.432 I print_info: rope scaling     = linear
0.00.102.433 I print_info: freq_base_train  = 10000.0
0.00.102.433 I print_info: freq_scale_train = 1
0.00.102.433 I print_info: n_ctx_orig_yarn  = 2048
0.00.102.434 I print_info: rope_finetuned   = unknown
0.00.102.434 I print_info: ssm_d_conv       = 0
0.00.102.434 I print_info: ssm_d_inner      = 0
0.00.102.435 I print_info: ssm_d_state      = 0
0.00.102.435 I print_info: ssm_dt_rank      = 0
0.00.102.440 I print_info: ssm_dt_b_c_rms   = 0
0.00.102.441 I print_info: model type       = 1.4B
0.00.102.442 I print_info: model params     = 1.41 B
0.00.102.442 I print_info: general.name     = 1.4B
0.00.102.443 I print_info: vocab type       = BPE
0.00.102.443 I print_info: n_vocab          = 50304
0.00.102.443 I print_info: n_merges         = 50009
0.00.102.444 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.102.444 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.102.445 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.102.447 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.102.447 I print_info: LF token         = 187 ''
0.00.102.447 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.102.447 I print_info: max token length = 1024
0.00.102.448 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.150.099 I load_tensors: offloading 24 repeating layers to GPU
0.00.150.103 I load_tensors: offloading output layer to GPU
0.00.150.103 I load_tensors: offloaded 25/25 layers to GPU
0.00.150.125 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.150.126 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.150.558 I llama_init_from_model: n_seq_max     = 1
0.00.150.560 I llama_init_from_model: n_ctx         = 2048
0.00.150.560 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.150.560 I llama_init_from_model: n_batch       = 2048
0.00.150.560 I llama_init_from_model: n_ubatch      = 512
0.00.150.561 I llama_init_from_model: flash_attn    = 0
0.00.150.562 I llama_init_from_model: freq_base     = 10000.0
0.00.150.562 I llama_init_from_model: freq_scale    = 1
0.00.150.563 I ggml_metal_init: allocating
0.00.150.591 I ggml_metal_init: found device: Apple M4
0.00.150.597 I ggml_metal_init: picking default device: Apple M4
0.00.151.186 I ggml_metal_init: using embedded metal library
0.00.166.403 I ggml_metal_init: GPU name:   Apple M4
0.00.166.405 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.166.405 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.166.405 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.166.406 I ggml_metal_init: simdgroup reduction   = true
0.00.166.406 I ggml_metal_init: simdgroup matrix mul. = true
0.00.166.406 I ggml_metal_init: has residency sets    = true
0.00.166.406 I ggml_metal_init: has bfloat            = true
0.00.166.406 I ggml_metal_init: use bfloat            = true
0.00.166.407 I ggml_metal_init: hasUnifiedMemory      = true
0.00.166.408 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.355.494 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.388.824 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.388.831 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.388.875 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.392.805 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.392.809 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.392.809 I llama_init_from_model: graph nodes  = 967
0.00.392.810 I llama_init_from_model: graph splits = 2
0.00.392.817 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.392.947 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.392.948 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.449.471 I main: llama threadpool init, n_threads = 4
0.00.449.516 I 
0.00.449.533 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.449.535 I 
0.00.449.672 I sampler seed: 1234
0.00.449.676 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.449.699 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.449.701 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.449.701 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.252.114 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58388.16 tokens per second)
0.02.252.114 I llama_perf_context_print:        load time =     399.80 ms
0.02.252.116 I llama_perf_context_print: prompt eval time =      44.16 ms /     7 tokens (    6.31 ms per token,   158.53 tokens per second)
0.02.252.116 I llama_perf_context_print:        eval time =    1755.33 ms /    63 runs   (   27.86 ms per token,    35.89 tokens per second)
0.02.252.117 I llama_perf_context_print:       total time =    1803.47 ms /    70 tokens
0.02.252.324 I ggml_metal_free: deallocating

real	0m2.584s
user	0m0.138s
sys	0m0.144s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.661 I build: 4732 (2eea03d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.019.974 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.293 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.298 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.299 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.300 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.300 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.302 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.302 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.303 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.303 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.304 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.304 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.305 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.305 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.306 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.308 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.308 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.308 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.800 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.044.629 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.225 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.051.227 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.228 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.051.228 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.051.228 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.051.229 I llama_model_loader: - type  f32:  194 tensors
0.00.051.229 I llama_model_loader: - type  f16:   98 tensors
0.00.051.230 I print_info: file format = GGUF V3 (latest)
0.00.051.231 I print_info: file type   = all F32 (guessed)
0.00.051.232 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.062.593 I load: special tokens cache size = 25
0.00.070.617 I load: token to piece cache size = 0.2984 MB
0.00.070.620 I print_info: arch             = gptneox
0.00.070.620 I print_info: vocab_only       = 0
0.00.070.620 I print_info: n_ctx_train      = 2048
0.00.070.621 I print_info: n_embd           = 2048
0.00.070.621 I print_info: n_layer          = 24
0.00.070.623 I print_info: n_head           = 16
0.00.070.624 I print_info: n_head_kv        = 16
0.00.070.624 I print_info: n_rot            = 32
0.00.070.624 I print_info: n_swa            = 0
0.00.070.626 I print_info: n_embd_head_k    = 128
0.00.070.626 I print_info: n_embd_head_v    = 128
0.00.070.627 I print_info: n_gqa            = 1
0.00.070.628 I print_info: n_embd_k_gqa     = 2048
0.00.070.628 I print_info: n_embd_v_gqa     = 2048
0.00.070.629 I print_info: f_norm_eps       = 1.0e-05
0.00.070.629 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.070.629 I print_info: f_clamp_kqv      = 0.0e+00
0.00.070.630 I print_info: f_max_alibi_bias = 0.0e+00
0.00.070.630 I print_info: f_logit_scale    = 0.0e+00
0.00.070.630 I print_info: n_ff             = 8192
0.00.070.631 I print_info: n_expert         = 0
0.00.070.631 I print_info: n_expert_used    = 0
0.00.070.631 I print_info: causal attn      = 1
0.00.070.631 I print_info: pooling type     = 0
0.00.070.633 I print_info: rope type        = 2
0.00.070.633 I print_info: rope scaling     = linear
0.00.070.634 I print_info: freq_base_train  = 10000.0
0.00.070.634 I print_info: freq_scale_train = 1
0.00.070.634 I print_info: n_ctx_orig_yarn  = 2048
0.00.070.635 I print_info: rope_finetuned   = unknown
0.00.070.635 I print_info: ssm_d_conv       = 0
0.00.070.635 I print_info: ssm_d_inner      = 0
0.00.070.635 I print_info: ssm_d_state      = 0
0.00.070.635 I print_info: ssm_dt_rank      = 0
0.00.070.635 I print_info: ssm_dt_b_c_rms   = 0
0.00.070.635 I print_info: model type       = 1.4B
0.00.070.636 I print_info: model params     = 1.41 B
0.00.070.636 I print_info: general.name     = 1.4B
0.00.070.636 I print_info: vocab type       = BPE
0.00.070.637 I print_info: n_vocab          = 50304
0.00.070.637 I print_info: n_merges         = 50009
0.00.070.637 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.070.637 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.070.637 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.070.638 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.070.638 I print_info: LF token         = 187 ''
0.00.070.638 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.070.638 I print_info: max token length = 1024
0.00.070.640 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.580.803 I load_tensors: offloading 24 repeating layers to GPU
0.01.580.809 I load_tensors: offloading output layer to GPU
0.01.580.811 I load_tensors: offloaded 25/25 layers to GPU
0.01.580.835 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.580.836 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.581.517 I llama_init_from_model: n_seq_max     = 1
0.01.581.518 I llama_init_from_model: n_ctx         = 128
0.01.581.518 I llama_init_from_model: n_ctx_per_seq = 128
0.01.581.519 I llama_init_from_model: n_batch       = 128
0.01.581.519 I llama_init_from_model: n_ubatch      = 128
0.01.581.519 I llama_init_from_model: flash_attn    = 0
0.01.581.519 I llama_init_from_model: freq_base     = 10000.0
0.01.581.519 I llama_init_from_model: freq_scale    = 1
0.01.581.520 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.581.521 I ggml_metal_init: allocating
0.01.581.555 I ggml_metal_init: found device: Apple M4
0.01.581.560 I ggml_metal_init: picking default device: Apple M4
0.01.582.504 I ggml_metal_init: using embedded metal library
0.01.585.835 I ggml_metal_init: GPU name:   Apple M4
0.01.585.836 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.585.837 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.585.837 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.585.837 I ggml_metal_init: simdgroup reduction   = true
0.01.585.838 I ggml_metal_init: simdgroup matrix mul. = true
0.01.585.838 I ggml_metal_init: has residency sets    = true
0.01.585.838 I ggml_metal_init: has bfloat            = true
0.01.585.838 I ggml_metal_init: use bfloat            = true
0.01.585.839 I ggml_metal_init: hasUnifiedMemory      = true
0.01.585.839 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.595.434 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.597.079 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.597.081 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.597.108 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.598.582 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.598.583 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.598.584 I llama_init_from_model: graph nodes  = 967
0.01.598.584 I llama_init_from_model: graph splits = 2
0.01.598.585 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.598.585 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.631.435 I 
0.01.631.459 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.631.462 I perplexity: tokenizing the input ..
0.01.635.774 I perplexity: tokenization took 4.31 ms
0.01.635.779 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.754.076 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.755.349 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.755.376 I llama_perf_context_print:        load time =    1611.44 ms
0.01.755.377 I llama_perf_context_print: prompt eval time =     118.04 ms /   128 tokens (    0.92 ms per token,  1084.41 tokens per second)
0.01.755.378 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.755.378 I llama_perf_context_print:       total time =     123.94 ms /   129 tokens
0.01.755.725 I ggml_metal_free: deallocating

real	0m1.948s
user	0m0.095s
sys	0m0.372s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4732 (2eea03d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.863 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.977 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.982 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.988 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.988 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.989 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.989 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.989 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.990 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.991 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.991 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.991 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.992 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.992 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.993 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.994 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.995 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.995 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.837 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.854 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.692 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.028.696 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.697 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.697 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.698 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.698 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.028.698 I llama_model_loader: - type  f32:  194 tensors
0.00.028.699 I llama_model_loader: - type q8_0:   98 tensors
0.00.028.699 I print_info: file format = GGUF V3 (latest)
0.00.028.700 I print_info: file type   = Q8_0
0.00.028.701 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.036.712 I load: special tokens cache size = 25
0.00.042.777 I load: token to piece cache size = 0.2984 MB
0.00.042.782 I print_info: arch             = gptneox
0.00.042.783 I print_info: vocab_only       = 0
0.00.042.783 I print_info: n_ctx_train      = 2048
0.00.042.783 I print_info: n_embd           = 2048
0.00.042.784 I print_info: n_layer          = 24
0.00.042.790 I print_info: n_head           = 16
0.00.042.790 I print_info: n_head_kv        = 16
0.00.042.791 I print_info: n_rot            = 32
0.00.042.791 I print_info: n_swa            = 0
0.00.042.791 I print_info: n_embd_head_k    = 128
0.00.042.791 I print_info: n_embd_head_v    = 128
0.00.042.792 I print_info: n_gqa            = 1
0.00.042.792 I print_info: n_embd_k_gqa     = 2048
0.00.042.793 I print_info: n_embd_v_gqa     = 2048
0.00.042.794 I print_info: f_norm_eps       = 1.0e-05
0.00.042.794 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.794 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.794 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.796 I print_info: f_logit_scale    = 0.0e+00
0.00.042.797 I print_info: n_ff             = 8192
0.00.042.797 I print_info: n_expert         = 0
0.00.042.797 I print_info: n_expert_used    = 0
0.00.042.797 I print_info: causal attn      = 1
0.00.042.797 I print_info: pooling type     = 0
0.00.042.797 I print_info: rope type        = 2
0.00.042.798 I print_info: rope scaling     = linear
0.00.042.799 I print_info: freq_base_train  = 10000.0
0.00.042.799 I print_info: freq_scale_train = 1
0.00.042.799 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.799 I print_info: rope_finetuned   = unknown
0.00.042.800 I print_info: ssm_d_conv       = 0
0.00.042.800 I print_info: ssm_d_inner      = 0
0.00.042.800 I print_info: ssm_d_state      = 0
0.00.042.800 I print_info: ssm_dt_rank      = 0
0.00.042.800 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.800 I print_info: model type       = 1.4B
0.00.042.801 I print_info: model params     = 1.41 B
0.00.042.801 I print_info: general.name     = 1.4B
0.00.042.801 I print_info: vocab type       = BPE
0.00.042.802 I print_info: n_vocab          = 50304
0.00.042.802 I print_info: n_merges         = 50009
0.00.042.802 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.802 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.802 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.803 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.803 I print_info: LF token         = 187 ''
0.00.042.803 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.803 I print_info: max token length = 1024
0.00.042.804 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.364.572 I load_tensors: offloading 24 repeating layers to GPU
0.01.364.577 I load_tensors: offloading output layer to GPU
0.01.364.578 I load_tensors: offloaded 25/25 layers to GPU
0.01.364.602 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.364.605 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.365.372 I llama_init_from_model: n_seq_max     = 1
0.01.365.374 I llama_init_from_model: n_ctx         = 2048
0.01.365.374 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.365.375 I llama_init_from_model: n_batch       = 2048
0.01.365.376 I llama_init_from_model: n_ubatch      = 512
0.01.365.376 I llama_init_from_model: flash_attn    = 0
0.01.365.377 I llama_init_from_model: freq_base     = 10000.0
0.01.365.377 I llama_init_from_model: freq_scale    = 1
0.01.365.378 I ggml_metal_init: allocating
0.01.365.389 I ggml_metal_init: found device: Apple M4
0.01.365.395 I ggml_metal_init: picking default device: Apple M4
0.01.366.468 I ggml_metal_init: using embedded metal library
0.01.370.886 I ggml_metal_init: GPU name:   Apple M4
0.01.370.889 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.370.889 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.370.890 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.370.890 I ggml_metal_init: simdgroup reduction   = true
0.01.370.890 I ggml_metal_init: simdgroup matrix mul. = true
0.01.370.890 I ggml_metal_init: has residency sets    = true
0.01.370.890 I ggml_metal_init: has bfloat            = true
0.01.370.891 I ggml_metal_init: use bfloat            = true
0.01.370.891 I ggml_metal_init: hasUnifiedMemory      = true
0.01.370.893 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.384.014 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.414.052 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.414.059 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.414.094 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.418.693 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.418.695 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.418.695 I llama_init_from_model: graph nodes  = 967
0.01.418.695 I llama_init_from_model: graph splits = 2
0.01.418.701 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.418.824 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.418.825 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.466.362 I main: llama threadpool init, n_threads = 4
0.01.466.409 I 
0.01.466.427 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.466.428 I 
0.01.466.552 I sampler seed: 1234
0.01.466.556 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.466.590 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.466.593 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.466.593 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.556.945 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55599.06 tokens per second)
0.02.556.946 I llama_perf_context_print:        load time =    1455.81 ms
0.02.556.946 I llama_perf_context_print: prompt eval time =      49.26 ms /     7 tokens (    7.04 ms per token,   142.11 tokens per second)
0.02.556.948 I llama_perf_context_print:        eval time =    1038.21 ms /    63 runs   (   16.48 ms per token,    60.68 tokens per second)
0.02.556.948 I llama_perf_context_print:       total time =    1091.27 ms /    70 tokens
0.02.557.230 I ggml_metal_free: deallocating

real	0m2.576s
user	0m0.103s
sys	0m0.318s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.279 I build: 4732 (2eea03d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.755 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.894 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.900 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.902 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.902 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.903 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.903 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.903 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.904 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.905 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.905 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.905 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.906 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.906 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.907 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.908 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.909 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.909 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.763 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.788 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.617 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.623 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.623 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.623 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.624 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.624 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.027.625 I llama_model_loader: - type  f32:  194 tensors
0.00.027.625 I llama_model_loader: - type q8_0:   98 tensors
0.00.027.626 I print_info: file format = GGUF V3 (latest)
0.00.027.626 I print_info: file type   = Q8_0
0.00.027.627 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.036.264 I load: special tokens cache size = 25
0.00.042.384 I load: token to piece cache size = 0.2984 MB
0.00.042.389 I print_info: arch             = gptneox
0.00.042.389 I print_info: vocab_only       = 0
0.00.042.389 I print_info: n_ctx_train      = 2048
0.00.042.390 I print_info: n_embd           = 2048
0.00.042.390 I print_info: n_layer          = 24
0.00.042.394 I print_info: n_head           = 16
0.00.042.394 I print_info: n_head_kv        = 16
0.00.042.395 I print_info: n_rot            = 32
0.00.042.395 I print_info: n_swa            = 0
0.00.042.395 I print_info: n_embd_head_k    = 128
0.00.042.395 I print_info: n_embd_head_v    = 128
0.00.042.396 I print_info: n_gqa            = 1
0.00.042.397 I print_info: n_embd_k_gqa     = 2048
0.00.042.397 I print_info: n_embd_v_gqa     = 2048
0.00.042.400 I print_info: f_norm_eps       = 1.0e-05
0.00.042.401 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.401 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.401 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.401 I print_info: f_logit_scale    = 0.0e+00
0.00.042.402 I print_info: n_ff             = 8192
0.00.042.402 I print_info: n_expert         = 0
0.00.042.402 I print_info: n_expert_used    = 0
0.00.042.402 I print_info: causal attn      = 1
0.00.042.404 I print_info: pooling type     = 0
0.00.042.404 I print_info: rope type        = 2
0.00.042.404 I print_info: rope scaling     = linear
0.00.042.404 I print_info: freq_base_train  = 10000.0
0.00.042.404 I print_info: freq_scale_train = 1
0.00.042.406 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.406 I print_info: rope_finetuned   = unknown
0.00.042.406 I print_info: ssm_d_conv       = 0
0.00.042.406 I print_info: ssm_d_inner      = 0
0.00.042.407 I print_info: ssm_d_state      = 0
0.00.042.407 I print_info: ssm_dt_rank      = 0
0.00.042.407 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.407 I print_info: model type       = 1.4B
0.00.042.407 I print_info: model params     = 1.41 B
0.00.042.408 I print_info: general.name     = 1.4B
0.00.042.408 I print_info: vocab type       = BPE
0.00.042.408 I print_info: n_vocab          = 50304
0.00.042.408 I print_info: n_merges         = 50009
0.00.042.409 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.409 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.409 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.410 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.411 I print_info: LF token         = 187 ''
0.00.042.411 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.411 I print_info: max token length = 1024
0.00.042.411 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.014.842 I load_tensors: offloading 24 repeating layers to GPU
0.01.014.848 I load_tensors: offloading output layer to GPU
0.01.014.849 I load_tensors: offloaded 25/25 layers to GPU
0.01.014.875 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.014.877 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.015.780 I llama_init_from_model: n_seq_max     = 1
0.01.015.782 I llama_init_from_model: n_ctx         = 128
0.01.015.782 I llama_init_from_model: n_ctx_per_seq = 128
0.01.015.782 I llama_init_from_model: n_batch       = 128
0.01.015.783 I llama_init_from_model: n_ubatch      = 128
0.01.015.783 I llama_init_from_model: flash_attn    = 0
0.01.015.784 I llama_init_from_model: freq_base     = 10000.0
0.01.015.784 I llama_init_from_model: freq_scale    = 1
0.01.015.784 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.015.786 I ggml_metal_init: allocating
0.01.015.837 I ggml_metal_init: found device: Apple M4
0.01.015.847 I ggml_metal_init: picking default device: Apple M4
0.01.016.986 I ggml_metal_init: using embedded metal library
0.01.021.508 I ggml_metal_init: GPU name:   Apple M4
0.01.021.510 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.021.511 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.021.512 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.021.512 I ggml_metal_init: simdgroup reduction   = true
0.01.021.512 I ggml_metal_init: simdgroup matrix mul. = true
0.01.021.513 I ggml_metal_init: has residency sets    = true
0.01.021.513 I ggml_metal_init: has bfloat            = true
0.01.021.513 I ggml_metal_init: use bfloat            = true
0.01.021.514 I ggml_metal_init: hasUnifiedMemory      = true
0.01.021.515 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.034.712 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.036.610 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.036.614 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.036.645 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.038.375 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.038.377 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.038.377 I llama_init_from_model: graph nodes  = 967
0.01.038.377 I llama_init_from_model: graph splits = 2
0.01.038.379 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.038.379 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.061.648 I 
0.01.061.666 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.061.669 I perplexity: tokenizing the input ..
0.01.066.911 I perplexity: tokenization took 5.24 ms
0.01.066.919 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.204.125 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.205.403 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.205.432 I llama_perf_context_print:        load time =    1049.88 ms
0.01.205.432 I llama_perf_context_print: prompt eval time =     136.98 ms /   128 tokens (    1.07 ms per token,   934.44 tokens per second)
0.01.205.433 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.205.433 I llama_perf_context_print:       total time =     143.78 ms /   129 tokens
0.01.205.827 I ggml_metal_free: deallocating

real	0m1.221s
user	0m0.071s
sys	0m0.229s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4732 (2eea03d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.013.273 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.029.223 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.029.229 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.231 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.232 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.232 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.232 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.233 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.234 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.234 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.234 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.235 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.235 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.235 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.236 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.238 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.238 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.238 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.148 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.315 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.600 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.038.602 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.603 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.603 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.603 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.604 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.038.604 I llama_model_loader: - type  f32:  194 tensors
0.00.038.604 I llama_model_loader: - type q4_0:   97 tensors
0.00.038.604 I llama_model_loader: - type q6_K:    1 tensors
0.00.038.605 I print_info: file format = GGUF V3 (latest)
0.00.038.606 I print_info: file type   = Q4_0
0.00.038.607 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.048.075 I load: special tokens cache size = 25
0.00.055.419 I load: token to piece cache size = 0.2984 MB
0.00.055.423 I print_info: arch             = gptneox
0.00.055.423 I print_info: vocab_only       = 0
0.00.055.423 I print_info: n_ctx_train      = 2048
0.00.055.424 I print_info: n_embd           = 2048
0.00.055.424 I print_info: n_layer          = 24
0.00.055.429 I print_info: n_head           = 16
0.00.055.430 I print_info: n_head_kv        = 16
0.00.055.430 I print_info: n_rot            = 32
0.00.055.430 I print_info: n_swa            = 0
0.00.055.430 I print_info: n_embd_head_k    = 128
0.00.055.431 I print_info: n_embd_head_v    = 128
0.00.055.431 I print_info: n_gqa            = 1
0.00.055.432 I print_info: n_embd_k_gqa     = 2048
0.00.055.436 I print_info: n_embd_v_gqa     = 2048
0.00.055.436 I print_info: f_norm_eps       = 1.0e-05
0.00.055.437 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.055.437 I print_info: f_clamp_kqv      = 0.0e+00
0.00.055.438 I print_info: f_max_alibi_bias = 0.0e+00
0.00.055.440 I print_info: f_logit_scale    = 0.0e+00
0.00.055.441 I print_info: n_ff             = 8192
0.00.055.441 I print_info: n_expert         = 0
0.00.055.441 I print_info: n_expert_used    = 0
0.00.055.441 I print_info: causal attn      = 1
0.00.055.441 I print_info: pooling type     = 0
0.00.055.442 I print_info: rope type        = 2
0.00.055.442 I print_info: rope scaling     = linear
0.00.055.442 I print_info: freq_base_train  = 10000.0
0.00.055.443 I print_info: freq_scale_train = 1
0.00.055.443 I print_info: n_ctx_orig_yarn  = 2048
0.00.055.443 I print_info: rope_finetuned   = unknown
0.00.055.443 I print_info: ssm_d_conv       = 0
0.00.055.443 I print_info: ssm_d_inner      = 0
0.00.055.443 I print_info: ssm_d_state      = 0
0.00.055.445 I print_info: ssm_dt_rank      = 0
0.00.055.445 I print_info: ssm_dt_b_c_rms   = 0
0.00.055.445 I print_info: model type       = 1.4B
0.00.055.445 I print_info: model params     = 1.41 B
0.00.055.446 I print_info: general.name     = 1.4B
0.00.055.446 I print_info: vocab type       = BPE
0.00.055.447 I print_info: n_vocab          = 50304
0.00.055.447 I print_info: n_merges         = 50009
0.00.055.447 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.055.447 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.055.447 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.055.448 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.055.448 I print_info: LF token         = 187 ''
0.00.055.448 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.055.448 I print_info: max token length = 1024
0.00.055.449 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.698.267 I load_tensors: offloading 24 repeating layers to GPU
0.00.698.271 I load_tensors: offloading output layer to GPU
0.00.698.272 I load_tensors: offloaded 25/25 layers to GPU
0.00.698.294 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.698.295 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.699.416 I llama_init_from_model: n_seq_max     = 1
0.00.699.418 I llama_init_from_model: n_ctx         = 2048
0.00.699.418 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.699.418 I llama_init_from_model: n_batch       = 2048
0.00.699.419 I llama_init_from_model: n_ubatch      = 512
0.00.699.419 I llama_init_from_model: flash_attn    = 0
0.00.699.420 I llama_init_from_model: freq_base     = 10000.0
0.00.699.421 I llama_init_from_model: freq_scale    = 1
0.00.699.422 I ggml_metal_init: allocating
0.00.699.448 I ggml_metal_init: found device: Apple M4
0.00.699.458 I ggml_metal_init: picking default device: Apple M4
0.00.700.817 I ggml_metal_init: using embedded metal library
0.00.707.179 I ggml_metal_init: GPU name:   Apple M4
0.00.707.182 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.707.183 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.707.183 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.707.184 I ggml_metal_init: simdgroup reduction   = true
0.00.707.184 I ggml_metal_init: simdgroup matrix mul. = true
0.00.707.184 I ggml_metal_init: has residency sets    = true
0.00.707.185 I ggml_metal_init: has bfloat            = true
0.00.707.185 I ggml_metal_init: use bfloat            = true
0.00.707.186 I ggml_metal_init: hasUnifiedMemory      = true
0.00.707.187 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.725.390 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.790.843 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.790.850 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.790.884 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.796.422 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.796.426 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.796.426 I llama_init_from_model: graph nodes  = 967
0.00.796.426 I llama_init_from_model: graph splits = 2
0.00.796.431 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.796.561 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.796.562 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.842.629 I main: llama threadpool init, n_threads = 4
0.00.842.682 I 
0.00.842.698 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.842.700 I 
0.00.842.841 I sampler seed: 1234
0.00.842.845 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.842.855 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.842.855 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.842.855 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.530.831 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 49929.68 tokens per second)
0.01.530.832 I llama_perf_context_print:        load time =     828.66 ms
0.01.530.833 I llama_perf_context_print: prompt eval time =      49.44 ms /     7 tokens (    7.06 ms per token,   141.59 tokens per second)
0.01.530.837 I llama_perf_context_print:        eval time =     635.52 ms /    63 runs   (   10.09 ms per token,    99.13 tokens per second)
0.01.530.837 I llama_perf_context_print:       total time =     688.89 ms /    70 tokens
0.01.531.053 I ggml_metal_free: deallocating

real	0m1.548s
user	0m0.114s
sys	0m0.257s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.293 I build: 4732 (2eea03d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.417 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.716 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.722 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.729 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.730 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.730 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.732 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.732 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.733 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.733 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.734 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.734 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.734 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.735 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.735 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.737 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.737 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.738 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.575 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.627 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.468 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.469 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.470 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.470 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.470 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.471 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.471 I llama_model_loader: - type  f32:  194 tensors
0.00.025.472 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.472 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.473 I print_info: file format = GGUF V3 (latest)
0.00.025.473 I print_info: file type   = Q4_0
0.00.025.474 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.033.478 I load: special tokens cache size = 25
0.00.039.562 I load: token to piece cache size = 0.2984 MB
0.00.039.566 I print_info: arch             = gptneox
0.00.039.566 I print_info: vocab_only       = 0
0.00.039.567 I print_info: n_ctx_train      = 2048
0.00.039.567 I print_info: n_embd           = 2048
0.00.039.567 I print_info: n_layer          = 24
0.00.039.571 I print_info: n_head           = 16
0.00.039.580 I print_info: n_head_kv        = 16
0.00.039.580 I print_info: n_rot            = 32
0.00.039.580 I print_info: n_swa            = 0
0.00.039.581 I print_info: n_embd_head_k    = 128
0.00.039.582 I print_info: n_embd_head_v    = 128
0.00.039.583 I print_info: n_gqa            = 1
0.00.039.583 I print_info: n_embd_k_gqa     = 2048
0.00.039.584 I print_info: n_embd_v_gqa     = 2048
0.00.039.585 I print_info: f_norm_eps       = 1.0e-05
0.00.039.585 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.585 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.585 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.586 I print_info: f_logit_scale    = 0.0e+00
0.00.039.586 I print_info: n_ff             = 8192
0.00.039.586 I print_info: n_expert         = 0
0.00.039.586 I print_info: n_expert_used    = 0
0.00.039.587 I print_info: causal attn      = 1
0.00.039.587 I print_info: pooling type     = 0
0.00.039.587 I print_info: rope type        = 2
0.00.039.587 I print_info: rope scaling     = linear
0.00.039.587 I print_info: freq_base_train  = 10000.0
0.00.039.588 I print_info: freq_scale_train = 1
0.00.039.588 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.588 I print_info: rope_finetuned   = unknown
0.00.039.588 I print_info: ssm_d_conv       = 0
0.00.039.588 I print_info: ssm_d_inner      = 0
0.00.039.588 I print_info: ssm_d_state      = 0
0.00.039.588 I print_info: ssm_dt_rank      = 0
0.00.039.589 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.589 I print_info: model type       = 1.4B
0.00.039.589 I print_info: model params     = 1.41 B
0.00.039.589 I print_info: general.name     = 1.4B
0.00.039.590 I print_info: vocab type       = BPE
0.00.039.590 I print_info: n_vocab          = 50304
0.00.039.590 I print_info: n_merges         = 50009
0.00.039.590 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.591 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.591 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.591 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.591 I print_info: LF token         = 187 ''
0.00.039.591 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.591 I print_info: max token length = 1024
0.00.039.592 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.654.708 I load_tensors: offloading 24 repeating layers to GPU
0.00.654.715 I load_tensors: offloading output layer to GPU
0.00.654.716 I load_tensors: offloaded 25/25 layers to GPU
0.00.654.746 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.654.749 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.655.993 I llama_init_from_model: n_seq_max     = 1
0.00.655.996 I llama_init_from_model: n_ctx         = 128
0.00.655.996 I llama_init_from_model: n_ctx_per_seq = 128
0.00.655.996 I llama_init_from_model: n_batch       = 128
0.00.655.997 I llama_init_from_model: n_ubatch      = 128
0.00.655.997 I llama_init_from_model: flash_attn    = 0
0.00.656.000 I llama_init_from_model: freq_base     = 10000.0
0.00.656.000 I llama_init_from_model: freq_scale    = 1
0.00.656.001 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.656.003 I ggml_metal_init: allocating
0.00.656.067 I ggml_metal_init: found device: Apple M4
0.00.656.079 I ggml_metal_init: picking default device: Apple M4
0.00.657.546 I ggml_metal_init: using embedded metal library
0.00.663.735 I ggml_metal_init: GPU name:   Apple M4
0.00.663.739 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.663.740 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.663.741 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.663.741 I ggml_metal_init: simdgroup reduction   = true
0.00.663.741 I ggml_metal_init: simdgroup matrix mul. = true
0.00.663.742 I ggml_metal_init: has residency sets    = true
0.00.663.742 I ggml_metal_init: has bfloat            = true
0.00.663.742 I ggml_metal_init: use bfloat            = true
0.00.663.743 I ggml_metal_init: hasUnifiedMemory      = true
0.00.663.745 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.681.439 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.684.872 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.684.877 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.684.916 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.688.139 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.688.141 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.688.141 I llama_init_from_model: graph nodes  = 967
0.00.688.142 I llama_init_from_model: graph splits = 2
0.00.688.145 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.688.145 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.716.276 I 
0.00.716.324 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.716.331 I perplexity: tokenizing the input ..
0.00.721.840 I perplexity: tokenization took 5.507 ms
0.00.721.844 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.853.434 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.854.740 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.854.769 I llama_perf_context_print:        load time =     706.85 ms
0.00.854.771 I llama_perf_context_print: prompt eval time =     131.37 ms /   128 tokens (    1.03 ms per token,   974.38 tokens per second)
0.00.854.771 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.854.772 I llama_perf_context_print:       total time =     138.50 ms /   129 tokens
0.00.855.165 I ggml_metal_free: deallocating

real	0m0.871s
user	0m0.077s
sys	0m0.170s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4732 (2eea03d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.008.942 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.518 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.028.523 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.524 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.524 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.525 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.528 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.528 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.529 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.530 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.530 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.530 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.531 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.531 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.531 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.533 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.533 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.534 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.319 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.332 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.177 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.037.178 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.179 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.179 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.179 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.180 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.037.180 I llama_model_loader: - type  f32:  194 tensors
0.00.037.180 I llama_model_loader: - type q4_1:   97 tensors
0.00.037.181 I llama_model_loader: - type q6_K:    1 tensors
0.00.037.181 I print_info: file format = GGUF V3 (latest)
0.00.037.182 I print_info: file type   = Q4_1
0.00.037.182 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.046.297 I load: special tokens cache size = 25
0.00.053.911 I load: token to piece cache size = 0.2984 MB
0.00.053.914 I print_info: arch             = gptneox
0.00.053.915 I print_info: vocab_only       = 0
0.00.053.915 I print_info: n_ctx_train      = 2048
0.00.053.915 I print_info: n_embd           = 2048
0.00.053.915 I print_info: n_layer          = 24
0.00.053.918 I print_info: n_head           = 16
0.00.053.919 I print_info: n_head_kv        = 16
0.00.053.919 I print_info: n_rot            = 32
0.00.053.919 I print_info: n_swa            = 0
0.00.053.920 I print_info: n_embd_head_k    = 128
0.00.053.920 I print_info: n_embd_head_v    = 128
0.00.053.922 I print_info: n_gqa            = 1
0.00.053.924 I print_info: n_embd_k_gqa     = 2048
0.00.053.925 I print_info: n_embd_v_gqa     = 2048
0.00.053.926 I print_info: f_norm_eps       = 1.0e-05
0.00.053.926 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.926 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.927 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.927 I print_info: f_logit_scale    = 0.0e+00
0.00.053.927 I print_info: n_ff             = 8192
0.00.053.928 I print_info: n_expert         = 0
0.00.053.928 I print_info: n_expert_used    = 0
0.00.053.928 I print_info: causal attn      = 1
0.00.053.928 I print_info: pooling type     = 0
0.00.053.928 I print_info: rope type        = 2
0.00.053.929 I print_info: rope scaling     = linear
0.00.053.929 I print_info: freq_base_train  = 10000.0
0.00.053.929 I print_info: freq_scale_train = 1
0.00.053.930 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.930 I print_info: rope_finetuned   = unknown
0.00.053.934 I print_info: ssm_d_conv       = 0
0.00.053.935 I print_info: ssm_d_inner      = 0
0.00.053.935 I print_info: ssm_d_state      = 0
0.00.053.935 I print_info: ssm_dt_rank      = 0
0.00.053.935 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.935 I print_info: model type       = 1.4B
0.00.053.936 I print_info: model params     = 1.41 B
0.00.053.936 I print_info: general.name     = 1.4B
0.00.053.937 I print_info: vocab type       = BPE
0.00.053.937 I print_info: n_vocab          = 50304
0.00.053.937 I print_info: n_merges         = 50009
0.00.053.937 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.937 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.938 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.938 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.938 I print_info: LF token         = 187 ''
0.00.053.938 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.939 I print_info: max token length = 1024
0.00.053.939 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.803.810 I load_tensors: offloading 24 repeating layers to GPU
0.00.803.813 I load_tensors: offloading output layer to GPU
0.00.803.814 I load_tensors: offloaded 25/25 layers to GPU
0.00.803.836 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.803.837 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.805.051 I llama_init_from_model: n_seq_max     = 1
0.00.805.053 I llama_init_from_model: n_ctx         = 2048
0.00.805.054 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.805.054 I llama_init_from_model: n_batch       = 2048
0.00.805.055 I llama_init_from_model: n_ubatch      = 512
0.00.805.055 I llama_init_from_model: flash_attn    = 0
0.00.805.056 I llama_init_from_model: freq_base     = 10000.0
0.00.805.057 I llama_init_from_model: freq_scale    = 1
0.00.805.058 I ggml_metal_init: allocating
0.00.805.087 I ggml_metal_init: found device: Apple M4
0.00.805.098 I ggml_metal_init: picking default device: Apple M4
0.00.806.454 I ggml_metal_init: using embedded metal library
0.00.812.431 I ggml_metal_init: GPU name:   Apple M4
0.00.812.435 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.812.436 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.812.437 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.812.437 I ggml_metal_init: simdgroup reduction   = true
0.00.812.438 I ggml_metal_init: simdgroup matrix mul. = true
0.00.812.438 I ggml_metal_init: has residency sets    = true
0.00.812.438 I ggml_metal_init: has bfloat            = true
0.00.812.439 I ggml_metal_init: use bfloat            = true
0.00.812.439 I ggml_metal_init: hasUnifiedMemory      = true
0.00.812.441 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.829.459 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.882.923 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.882.931 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.882.982 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.887.834 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.887.836 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.887.836 I llama_init_from_model: graph nodes  = 967
0.00.887.837 I llama_init_from_model: graph splits = 2
0.00.887.841 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.887.969 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.887.970 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.936.273 I main: llama threadpool init, n_threads = 4
0.00.936.319 I 
0.00.936.338 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.936.338 I 
0.00.936.468 I sampler seed: 1234
0.00.936.472 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.936.482 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.936.483 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.936.483 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.669.439 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55686.27 tokens per second)
0.01.669.439 I llama_perf_context_print:        load time =     926.63 ms
0.01.669.440 I llama_perf_context_print: prompt eval time =      48.80 ms /     7 tokens (    6.97 ms per token,   143.45 tokens per second)
0.01.669.441 I llama_perf_context_print:        eval time =     681.32 ms /    63 runs   (   10.81 ms per token,    92.47 tokens per second)
0.01.669.441 I llama_perf_context_print:       total time =     733.86 ms /    70 tokens
0.01.669.693 I ggml_metal_free: deallocating

real	0m1.684s
user	0m0.110s
sys	0m0.245s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4732 (2eea03d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.557 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.534 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.540 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.541 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.546 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.546 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.547 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.547 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.548 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.548 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.549 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.549 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.549 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.550 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.550 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.553 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.553 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.553 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.376 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.345 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.094 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.095 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.096 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.096 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.096 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.097 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.097 I llama_model_loader: - type  f32:  194 tensors
0.00.026.098 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.098 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.099 I print_info: file format = GGUF V3 (latest)
0.00.026.100 I print_info: file type   = Q4_1
0.00.026.100 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.985 I load: special tokens cache size = 25
0.00.039.834 I load: token to piece cache size = 0.2984 MB
0.00.039.839 I print_info: arch             = gptneox
0.00.039.839 I print_info: vocab_only       = 0
0.00.039.839 I print_info: n_ctx_train      = 2048
0.00.039.839 I print_info: n_embd           = 2048
0.00.039.840 I print_info: n_layer          = 24
0.00.039.844 I print_info: n_head           = 16
0.00.039.844 I print_info: n_head_kv        = 16
0.00.039.845 I print_info: n_rot            = 32
0.00.039.845 I print_info: n_swa            = 0
0.00.039.845 I print_info: n_embd_head_k    = 128
0.00.039.845 I print_info: n_embd_head_v    = 128
0.00.039.846 I print_info: n_gqa            = 1
0.00.039.846 I print_info: n_embd_k_gqa     = 2048
0.00.039.847 I print_info: n_embd_v_gqa     = 2048
0.00.039.848 I print_info: f_norm_eps       = 1.0e-05
0.00.039.848 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.848 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.848 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.849 I print_info: f_logit_scale    = 0.0e+00
0.00.039.849 I print_info: n_ff             = 8192
0.00.039.849 I print_info: n_expert         = 0
0.00.039.849 I print_info: n_expert_used    = 0
0.00.039.850 I print_info: causal attn      = 1
0.00.039.850 I print_info: pooling type     = 0
0.00.039.850 I print_info: rope type        = 2
0.00.039.850 I print_info: rope scaling     = linear
0.00.039.850 I print_info: freq_base_train  = 10000.0
0.00.039.851 I print_info: freq_scale_train = 1
0.00.039.851 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.851 I print_info: rope_finetuned   = unknown
0.00.039.851 I print_info: ssm_d_conv       = 0
0.00.039.851 I print_info: ssm_d_inner      = 0
0.00.039.851 I print_info: ssm_d_state      = 0
0.00.039.851 I print_info: ssm_dt_rank      = 0
0.00.039.851 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.852 I print_info: model type       = 1.4B
0.00.039.852 I print_info: model params     = 1.41 B
0.00.039.852 I print_info: general.name     = 1.4B
0.00.039.853 I print_info: vocab type       = BPE
0.00.039.853 I print_info: n_vocab          = 50304
0.00.039.853 I print_info: n_merges         = 50009
0.00.039.853 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.853 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.854 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.856 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.857 I print_info: LF token         = 187 ''
0.00.039.857 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.857 I print_info: max token length = 1024
0.00.039.857 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.713.137 I load_tensors: offloading 24 repeating layers to GPU
0.00.713.150 I load_tensors: offloading output layer to GPU
0.00.713.151 I load_tensors: offloaded 25/25 layers to GPU
0.00.713.183 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.713.184 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.714.524 I llama_init_from_model: n_seq_max     = 1
0.00.714.526 I llama_init_from_model: n_ctx         = 128
0.00.714.527 I llama_init_from_model: n_ctx_per_seq = 128
0.00.714.527 I llama_init_from_model: n_batch       = 128
0.00.714.528 I llama_init_from_model: n_ubatch      = 128
0.00.714.528 I llama_init_from_model: flash_attn    = 0
0.00.714.530 I llama_init_from_model: freq_base     = 10000.0
0.00.714.530 I llama_init_from_model: freq_scale    = 1
0.00.714.531 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.714.532 I ggml_metal_init: allocating
0.00.714.569 I ggml_metal_init: found device: Apple M4
0.00.714.580 I ggml_metal_init: picking default device: Apple M4
0.00.716.020 I ggml_metal_init: using embedded metal library
0.00.722.162 I ggml_metal_init: GPU name:   Apple M4
0.00.722.165 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.722.166 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.722.167 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.722.167 I ggml_metal_init: simdgroup reduction   = true
0.00.722.167 I ggml_metal_init: simdgroup matrix mul. = true
0.00.722.168 I ggml_metal_init: has residency sets    = true
0.00.722.168 I ggml_metal_init: has bfloat            = true
0.00.722.168 I ggml_metal_init: use bfloat            = true
0.00.722.169 I ggml_metal_init: hasUnifiedMemory      = true
0.00.722.172 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.739.982 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.743.314 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.743.318 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.743.358 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.746.543 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.746.545 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.746.545 I llama_init_from_model: graph nodes  = 967
0.00.746.545 I llama_init_from_model: graph splits = 2
0.00.746.548 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.746.549 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.770.871 I 
0.00.770.932 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.770.941 I perplexity: tokenizing the input ..
0.00.776.569 I perplexity: tokenization took 5.626 ms
0.00.776.573 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.907.531 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.908.832 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.908.862 I llama_perf_context_print:        load time =     762.30 ms
0.00.908.863 I llama_perf_context_print: prompt eval time =     130.73 ms /   128 tokens (    1.02 ms per token,   979.09 tokens per second)
0.00.908.865 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.908.866 I llama_perf_context_print:       total time =     137.99 ms /   129 tokens
0.00.909.256 I ggml_metal_free: deallocating

real	0m0.923s
user	0m0.079s
sys	0m0.174s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4732 (2eea03d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.016.319 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.033.301 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.033.306 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.308 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.308 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.309 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.309 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.313 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.314 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.314 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.314 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.315 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.315 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.315 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.316 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.317 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.318 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.318 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.037.771 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.039.045 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.043.460 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.043.464 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.043.464 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.043.464 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.043.465 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.043.465 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.043.465 I llama_model_loader: - type  f32:  194 tensors
0.00.043.466 I llama_model_loader: - type q5_0:   97 tensors
0.00.043.466 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.467 I print_info: file format = GGUF V3 (latest)
0.00.043.467 I print_info: file type   = Q5_0
0.00.043.468 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.053.667 I load: special tokens cache size = 25
0.00.062.470 I load: token to piece cache size = 0.2984 MB
0.00.062.473 I print_info: arch             = gptneox
0.00.062.474 I print_info: vocab_only       = 0
0.00.062.474 I print_info: n_ctx_train      = 2048
0.00.062.474 I print_info: n_embd           = 2048
0.00.062.474 I print_info: n_layer          = 24
0.00.062.477 I print_info: n_head           = 16
0.00.062.478 I print_info: n_head_kv        = 16
0.00.062.478 I print_info: n_rot            = 32
0.00.062.478 I print_info: n_swa            = 0
0.00.062.479 I print_info: n_embd_head_k    = 128
0.00.062.479 I print_info: n_embd_head_v    = 128
0.00.062.480 I print_info: n_gqa            = 1
0.00.062.482 I print_info: n_embd_k_gqa     = 2048
0.00.062.483 I print_info: n_embd_v_gqa     = 2048
0.00.062.484 I print_info: f_norm_eps       = 1.0e-05
0.00.062.484 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.062.484 I print_info: f_clamp_kqv      = 0.0e+00
0.00.062.484 I print_info: f_max_alibi_bias = 0.0e+00
0.00.062.485 I print_info: f_logit_scale    = 0.0e+00
0.00.062.485 I print_info: n_ff             = 8192
0.00.062.486 I print_info: n_expert         = 0
0.00.062.486 I print_info: n_expert_used    = 0
0.00.062.486 I print_info: causal attn      = 1
0.00.062.486 I print_info: pooling type     = 0
0.00.062.486 I print_info: rope type        = 2
0.00.062.487 I print_info: rope scaling     = linear
0.00.062.487 I print_info: freq_base_train  = 10000.0
0.00.062.488 I print_info: freq_scale_train = 1
0.00.062.488 I print_info: n_ctx_orig_yarn  = 2048
0.00.062.489 I print_info: rope_finetuned   = unknown
0.00.062.489 I print_info: ssm_d_conv       = 0
0.00.062.489 I print_info: ssm_d_inner      = 0
0.00.062.489 I print_info: ssm_d_state      = 0
0.00.062.489 I print_info: ssm_dt_rank      = 0
0.00.062.489 I print_info: ssm_dt_b_c_rms   = 0
0.00.062.490 I print_info: model type       = 1.4B
0.00.062.490 I print_info: model params     = 1.41 B
0.00.062.490 I print_info: general.name     = 1.4B
0.00.062.491 I print_info: vocab type       = BPE
0.00.062.491 I print_info: n_vocab          = 50304
0.00.062.491 I print_info: n_merges         = 50009
0.00.062.492 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.062.492 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.062.492 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.062.493 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.062.493 I print_info: LF token         = 187 ''
0.00.062.493 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.062.494 I print_info: max token length = 1024
0.00.062.494 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.772.189 I load_tensors: offloading 24 repeating layers to GPU
0.00.772.192 I load_tensors: offloading output layer to GPU
0.00.772.193 I load_tensors: offloaded 25/25 layers to GPU
0.00.772.214 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.772.215 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.773.207 I llama_init_from_model: n_seq_max     = 1
0.00.773.209 I llama_init_from_model: n_ctx         = 2048
0.00.773.209 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.773.210 I llama_init_from_model: n_batch       = 2048
0.00.773.210 I llama_init_from_model: n_ubatch      = 512
0.00.773.211 I llama_init_from_model: flash_attn    = 0
0.00.773.211 I llama_init_from_model: freq_base     = 10000.0
0.00.773.212 I llama_init_from_model: freq_scale    = 1
0.00.773.213 I ggml_metal_init: allocating
0.00.773.230 I ggml_metal_init: found device: Apple M4
0.00.773.238 I ggml_metal_init: picking default device: Apple M4
0.00.774.544 I ggml_metal_init: using embedded metal library
0.00.780.267 I ggml_metal_init: GPU name:   Apple M4
0.00.780.270 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.780.271 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.780.271 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.780.272 I ggml_metal_init: simdgroup reduction   = true
0.00.780.272 I ggml_metal_init: simdgroup matrix mul. = true
0.00.780.272 I ggml_metal_init: has residency sets    = true
0.00.780.273 I ggml_metal_init: has bfloat            = true
0.00.780.273 I ggml_metal_init: use bfloat            = true
0.00.780.273 I ggml_metal_init: hasUnifiedMemory      = true
0.00.780.274 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.796.326 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.849.554 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.849.560 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.849.593 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.854.346 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.854.348 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.854.348 I llama_init_from_model: graph nodes  = 967
0.00.854.348 I llama_init_from_model: graph splits = 2
0.00.854.353 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.854.477 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.854.478 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.901.896 I main: llama threadpool init, n_threads = 4
0.00.901.942 I 
0.00.901.958 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.901.958 I 
0.00.902.089 I sampler seed: 1234
0.00.902.093 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.902.104 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.902.104 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.902.104 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.689.702 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51042.42 tokens per second)
0.01.689.703 I llama_perf_context_print:        load time =     884.87 ms
0.01.689.704 I llama_perf_context_print: prompt eval time =      42.76 ms /     7 tokens (    6.11 ms per token,   163.69 tokens per second)
0.01.689.705 I llama_perf_context_print:        eval time =     741.86 ms /    63 runs   (   11.78 ms per token,    84.92 tokens per second)
0.01.689.705 I llama_perf_context_print:       total time =     788.51 ms /    70 tokens
0.01.689.971 I ggml_metal_free: deallocating

real	0m1.716s
user	0m0.114s
sys	0m0.266s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4732 (2eea03d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.733 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.528 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.534 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.536 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.536 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.536 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.537 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.537 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.538 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.538 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.539 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.539 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.539 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.540 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.540 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.542 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.542 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.542 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.327 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.347 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.201 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.202 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.202 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.203 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.203 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.203 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.204 I llama_model_loader: - type  f32:  194 tensors
0.00.027.204 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.205 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.205 I print_info: file format = GGUF V3 (latest)
0.00.027.206 I print_info: file type   = Q5_0
0.00.027.207 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.035.310 I load: special tokens cache size = 25
0.00.041.076 I load: token to piece cache size = 0.2984 MB
0.00.041.081 I print_info: arch             = gptneox
0.00.041.081 I print_info: vocab_only       = 0
0.00.041.082 I print_info: n_ctx_train      = 2048
0.00.041.082 I print_info: n_embd           = 2048
0.00.041.082 I print_info: n_layer          = 24
0.00.041.086 I print_info: n_head           = 16
0.00.041.086 I print_info: n_head_kv        = 16
0.00.041.087 I print_info: n_rot            = 32
0.00.041.087 I print_info: n_swa            = 0
0.00.041.087 I print_info: n_embd_head_k    = 128
0.00.041.087 I print_info: n_embd_head_v    = 128
0.00.041.088 I print_info: n_gqa            = 1
0.00.041.089 I print_info: n_embd_k_gqa     = 2048
0.00.041.089 I print_info: n_embd_v_gqa     = 2048
0.00.041.090 I print_info: f_norm_eps       = 1.0e-05
0.00.041.090 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.090 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.090 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.094 I print_info: f_logit_scale    = 0.0e+00
0.00.041.094 I print_info: n_ff             = 8192
0.00.041.094 I print_info: n_expert         = 0
0.00.041.094 I print_info: n_expert_used    = 0
0.00.041.095 I print_info: causal attn      = 1
0.00.041.096 I print_info: pooling type     = 0
0.00.041.096 I print_info: rope type        = 2
0.00.041.096 I print_info: rope scaling     = linear
0.00.041.097 I print_info: freq_base_train  = 10000.0
0.00.041.097 I print_info: freq_scale_train = 1
0.00.041.097 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.097 I print_info: rope_finetuned   = unknown
0.00.041.097 I print_info: ssm_d_conv       = 0
0.00.041.098 I print_info: ssm_d_inner      = 0
0.00.041.098 I print_info: ssm_d_state      = 0
0.00.041.098 I print_info: ssm_dt_rank      = 0
0.00.041.098 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.098 I print_info: model type       = 1.4B
0.00.041.099 I print_info: model params     = 1.41 B
0.00.041.099 I print_info: general.name     = 1.4B
0.00.041.099 I print_info: vocab type       = BPE
0.00.041.099 I print_info: n_vocab          = 50304
0.00.041.100 I print_info: n_merges         = 50009
0.00.041.100 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.100 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.100 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.100 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.101 I print_info: LF token         = 187 ''
0.00.041.101 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.101 I print_info: max token length = 1024
0.00.041.101 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.768.127 I load_tensors: offloading 24 repeating layers to GPU
0.00.768.134 I load_tensors: offloading output layer to GPU
0.00.768.135 I load_tensors: offloaded 25/25 layers to GPU
0.00.768.159 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.768.162 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.769.231 I llama_init_from_model: n_seq_max     = 1
0.00.769.233 I llama_init_from_model: n_ctx         = 128
0.00.769.233 I llama_init_from_model: n_ctx_per_seq = 128
0.00.769.233 I llama_init_from_model: n_batch       = 128
0.00.769.234 I llama_init_from_model: n_ubatch      = 128
0.00.769.234 I llama_init_from_model: flash_attn    = 0
0.00.769.235 I llama_init_from_model: freq_base     = 10000.0
0.00.769.235 I llama_init_from_model: freq_scale    = 1
0.00.769.236 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.769.238 I ggml_metal_init: allocating
0.00.769.273 I ggml_metal_init: found device: Apple M4
0.00.769.283 I ggml_metal_init: picking default device: Apple M4
0.00.770.590 I ggml_metal_init: using embedded metal library
0.00.776.160 I ggml_metal_init: GPU name:   Apple M4
0.00.776.163 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.776.164 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.776.165 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.776.166 I ggml_metal_init: simdgroup reduction   = true
0.00.776.166 I ggml_metal_init: simdgroup matrix mul. = true
0.00.776.166 I ggml_metal_init: has residency sets    = true
0.00.776.166 I ggml_metal_init: has bfloat            = true
0.00.776.167 I ggml_metal_init: use bfloat            = true
0.00.776.168 I ggml_metal_init: hasUnifiedMemory      = true
0.00.776.176 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.791.995 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.795.213 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.795.216 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.795.255 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.798.222 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.798.223 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.798.224 I llama_init_from_model: graph nodes  = 967
0.00.798.224 I llama_init_from_model: graph splits = 2
0.00.798.227 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.798.227 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.827.738 I 
0.00.827.799 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.827.806 I perplexity: tokenizing the input ..
0.00.834.714 I perplexity: tokenization took 6.904 ms
0.00.834.723 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.983.530 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.984.787 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.984.819 I llama_perf_context_print:        load time =     815.99 ms
0.00.984.820 I llama_perf_context_print: prompt eval time =     147.94 ms /   128 tokens (    1.16 ms per token,   865.24 tokens per second)
0.00.984.820 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.984.821 I llama_perf_context_print:       total time =     157.08 ms /   129 tokens
0.00.985.252 I ggml_metal_free: deallocating

real	0m1.001s
user	0m0.077s
sys	0m0.198s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4732 (2eea03d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.556 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.627 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.024.632 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.633 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.633 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.634 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.634 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.634 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.635 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.635 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.635 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.636 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.636 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.637 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.637 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.639 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.639 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.642 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.532 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.541 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.407 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.408 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.409 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.409 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.409 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.410 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.033.410 I llama_model_loader: - type  f32:  194 tensors
0.00.033.410 I llama_model_loader: - type q5_1:   97 tensors
0.00.033.411 I llama_model_loader: - type q6_K:    1 tensors
0.00.033.411 I print_info: file format = GGUF V3 (latest)
0.00.033.412 I print_info: file type   = Q5_1
0.00.033.412 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.041.628 I load: special tokens cache size = 25
0.00.048.192 I load: token to piece cache size = 0.2984 MB
0.00.048.195 I print_info: arch             = gptneox
0.00.048.195 I print_info: vocab_only       = 0
0.00.048.195 I print_info: n_ctx_train      = 2048
0.00.048.195 I print_info: n_embd           = 2048
0.00.048.196 I print_info: n_layer          = 24
0.00.048.198 I print_info: n_head           = 16
0.00.048.199 I print_info: n_head_kv        = 16
0.00.048.199 I print_info: n_rot            = 32
0.00.048.199 I print_info: n_swa            = 0
0.00.048.199 I print_info: n_embd_head_k    = 128
0.00.048.199 I print_info: n_embd_head_v    = 128
0.00.048.200 I print_info: n_gqa            = 1
0.00.048.201 I print_info: n_embd_k_gqa     = 2048
0.00.048.201 I print_info: n_embd_v_gqa     = 2048
0.00.048.202 I print_info: f_norm_eps       = 1.0e-05
0.00.048.202 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.202 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.202 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.203 I print_info: f_logit_scale    = 0.0e+00
0.00.048.203 I print_info: n_ff             = 8192
0.00.048.203 I print_info: n_expert         = 0
0.00.048.203 I print_info: n_expert_used    = 0
0.00.048.204 I print_info: causal attn      = 1
0.00.048.204 I print_info: pooling type     = 0
0.00.048.204 I print_info: rope type        = 2
0.00.048.204 I print_info: rope scaling     = linear
0.00.048.204 I print_info: freq_base_train  = 10000.0
0.00.048.205 I print_info: freq_scale_train = 1
0.00.048.205 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.205 I print_info: rope_finetuned   = unknown
0.00.048.205 I print_info: ssm_d_conv       = 0
0.00.048.205 I print_info: ssm_d_inner      = 0
0.00.048.206 I print_info: ssm_d_state      = 0
0.00.048.206 I print_info: ssm_dt_rank      = 0
0.00.048.206 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.206 I print_info: model type       = 1.4B
0.00.048.206 I print_info: model params     = 1.41 B
0.00.048.207 I print_info: general.name     = 1.4B
0.00.048.207 I print_info: vocab type       = BPE
0.00.048.207 I print_info: n_vocab          = 50304
0.00.048.207 I print_info: n_merges         = 50009
0.00.048.208 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.208 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.208 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.208 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.208 I print_info: LF token         = 187 ''
0.00.048.211 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.211 I print_info: max token length = 1024
0.00.048.211 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.699.362 I load_tensors: offloading 24 repeating layers to GPU
0.00.699.365 I load_tensors: offloading output layer to GPU
0.00.699.366 I load_tensors: offloaded 25/25 layers to GPU
0.00.699.386 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.699.390 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.700.488 I llama_init_from_model: n_seq_max     = 1
0.00.700.490 I llama_init_from_model: n_ctx         = 2048
0.00.700.490 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.700.491 I llama_init_from_model: n_batch       = 2048
0.00.700.491 I llama_init_from_model: n_ubatch      = 512
0.00.700.491 I llama_init_from_model: flash_attn    = 0
0.00.700.492 I llama_init_from_model: freq_base     = 10000.0
0.00.700.492 I llama_init_from_model: freq_scale    = 1
0.00.700.494 I ggml_metal_init: allocating
0.00.700.518 I ggml_metal_init: found device: Apple M4
0.00.700.531 I ggml_metal_init: picking default device: Apple M4
0.00.701.794 I ggml_metal_init: using embedded metal library
0.00.707.070 I ggml_metal_init: GPU name:   Apple M4
0.00.707.073 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.707.074 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.707.075 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.707.076 I ggml_metal_init: simdgroup reduction   = true
0.00.707.076 I ggml_metal_init: simdgroup matrix mul. = true
0.00.707.076 I ggml_metal_init: has residency sets    = true
0.00.707.077 I ggml_metal_init: has bfloat            = true
0.00.707.077 I ggml_metal_init: use bfloat            = true
0.00.707.078 I ggml_metal_init: hasUnifiedMemory      = true
0.00.707.085 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.722.215 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.777.614 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.777.620 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.777.654 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.782.760 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.782.762 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.782.762 I llama_init_from_model: graph nodes  = 967
0.00.782.762 I llama_init_from_model: graph splits = 2
0.00.782.768 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.782.893 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.782.894 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.830.064 I main: llama threadpool init, n_threads = 4
0.00.830.112 I 
0.00.830.129 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.830.131 I 
0.00.830.262 I sampler seed: 1234
0.00.830.267 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.830.307 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.830.310 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.830.310 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.663.740 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50318.92 tokens per second)
0.01.663.741 I llama_perf_context_print:        load time =     820.75 ms
0.01.663.742 I llama_perf_context_print: prompt eval time =      42.22 ms /     7 tokens (    6.03 ms per token,   165.82 tokens per second)
0.01.663.743 I llama_perf_context_print:        eval time =     788.03 ms /    63 runs   (   12.51 ms per token,    79.95 tokens per second)
0.01.663.743 I llama_perf_context_print:       total time =     834.43 ms /    70 tokens
0.01.663.991 I ggml_metal_free: deallocating

real	0m1.679s
user	0m0.107s
sys	0m0.258s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4732 (2eea03d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.769 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.565 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.571 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.577 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.577 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.579 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.579 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.580 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.581 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.581 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.582 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.582 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.583 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.586 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.587 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.588 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.589 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.591 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.394 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.481 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.305 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.307 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.307 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.307 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.308 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.308 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.309 I llama_model_loader: - type  f32:  194 tensors
0.00.025.309 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.309 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.310 I print_info: file format = GGUF V3 (latest)
0.00.025.310 I print_info: file type   = Q5_1
0.00.025.311 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.702 I load: special tokens cache size = 25
0.00.039.464 I load: token to piece cache size = 0.2984 MB
0.00.039.467 I print_info: arch             = gptneox
0.00.039.468 I print_info: vocab_only       = 0
0.00.039.468 I print_info: n_ctx_train      = 2048
0.00.039.468 I print_info: n_embd           = 2048
0.00.039.468 I print_info: n_layer          = 24
0.00.039.472 I print_info: n_head           = 16
0.00.039.473 I print_info: n_head_kv        = 16
0.00.039.473 I print_info: n_rot            = 32
0.00.039.473 I print_info: n_swa            = 0
0.00.039.473 I print_info: n_embd_head_k    = 128
0.00.039.474 I print_info: n_embd_head_v    = 128
0.00.039.474 I print_info: n_gqa            = 1
0.00.039.475 I print_info: n_embd_k_gqa     = 2048
0.00.039.476 I print_info: n_embd_v_gqa     = 2048
0.00.039.476 I print_info: f_norm_eps       = 1.0e-05
0.00.039.477 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.477 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.483 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.483 I print_info: f_logit_scale    = 0.0e+00
0.00.039.484 I print_info: n_ff             = 8192
0.00.039.484 I print_info: n_expert         = 0
0.00.039.484 I print_info: n_expert_used    = 0
0.00.039.484 I print_info: causal attn      = 1
0.00.039.484 I print_info: pooling type     = 0
0.00.039.484 I print_info: rope type        = 2
0.00.039.485 I print_info: rope scaling     = linear
0.00.039.485 I print_info: freq_base_train  = 10000.0
0.00.039.485 I print_info: freq_scale_train = 1
0.00.039.485 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.486 I print_info: rope_finetuned   = unknown
0.00.039.486 I print_info: ssm_d_conv       = 0
0.00.039.486 I print_info: ssm_d_inner      = 0
0.00.039.486 I print_info: ssm_d_state      = 0
0.00.039.486 I print_info: ssm_dt_rank      = 0
0.00.039.486 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.487 I print_info: model type       = 1.4B
0.00.039.487 I print_info: model params     = 1.41 B
0.00.039.487 I print_info: general.name     = 1.4B
0.00.039.488 I print_info: vocab type       = BPE
0.00.039.488 I print_info: n_vocab          = 50304
0.00.039.488 I print_info: n_merges         = 50009
0.00.039.488 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.488 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.489 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.489 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.489 I print_info: LF token         = 187 ''
0.00.039.489 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.489 I print_info: max token length = 1024
0.00.039.490 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.674.414 I load_tensors: offloading 24 repeating layers to GPU
0.00.674.419 I load_tensors: offloading output layer to GPU
0.00.674.421 I load_tensors: offloaded 25/25 layers to GPU
0.00.674.444 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.674.446 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.675.538 I llama_init_from_model: n_seq_max     = 1
0.00.675.540 I llama_init_from_model: n_ctx         = 128
0.00.675.541 I llama_init_from_model: n_ctx_per_seq = 128
0.00.675.541 I llama_init_from_model: n_batch       = 128
0.00.675.542 I llama_init_from_model: n_ubatch      = 128
0.00.675.542 I llama_init_from_model: flash_attn    = 0
0.00.675.543 I llama_init_from_model: freq_base     = 10000.0
0.00.675.543 I llama_init_from_model: freq_scale    = 1
0.00.675.544 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.675.545 I ggml_metal_init: allocating
0.00.675.557 I ggml_metal_init: found device: Apple M4
0.00.675.564 I ggml_metal_init: picking default device: Apple M4
0.00.676.680 I ggml_metal_init: using embedded metal library
0.00.681.945 I ggml_metal_init: GPU name:   Apple M4
0.00.681.949 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.681.949 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.681.950 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.681.950 I ggml_metal_init: simdgroup reduction   = true
0.00.681.951 I ggml_metal_init: simdgroup matrix mul. = true
0.00.681.951 I ggml_metal_init: has residency sets    = true
0.00.681.951 I ggml_metal_init: has bfloat            = true
0.00.681.951 I ggml_metal_init: use bfloat            = true
0.00.681.952 I ggml_metal_init: hasUnifiedMemory      = true
0.00.681.953 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.697.260 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.700.518 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.700.523 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.700.564 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.703.542 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.703.543 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.703.544 I llama_init_from_model: graph nodes  = 967
0.00.703.544 I llama_init_from_model: graph splits = 2
0.00.703.547 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.703.547 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.731.280 I 
0.00.731.334 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.731.348 I perplexity: tokenizing the input ..
0.00.737.826 I perplexity: tokenization took 6.48 ms
0.00.737.832 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.871.834 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.873.084 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.873.122 I llama_perf_context_print:        load time =     721.50 ms
0.00.873.123 I llama_perf_context_print: prompt eval time =     133.71 ms /   128 tokens (    1.04 ms per token,   957.32 tokens per second)
0.00.873.123 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.873.124 I llama_perf_context_print:       total time =     141.85 ms /   129 tokens
0.00.873.564 I ggml_metal_free: deallocating

real	0m0.887s
user	0m0.075s
sys	0m0.184s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4732 (2eea03d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.009.360 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.788 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.793 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.795 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.795 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.796 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.796 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.797 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.797 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.800 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.800 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.800 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.801 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.801 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.801 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.803 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.803 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.803 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.613 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.620 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.359 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.361 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.361 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.361 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.361 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.362 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.362 I llama_model_loader: - type  f32:  194 tensors
0.00.024.362 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.362 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.363 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.363 I print_info: file format = GGUF V3 (latest)
0.00.024.364 I print_info: file type   = Q2_K - Medium
0.00.024.364 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.081 I load: special tokens cache size = 25
0.00.038.017 I load: token to piece cache size = 0.2984 MB
0.00.038.020 I print_info: arch             = gptneox
0.00.038.020 I print_info: vocab_only       = 0
0.00.038.020 I print_info: n_ctx_train      = 2048
0.00.038.020 I print_info: n_embd           = 2048
0.00.038.020 I print_info: n_layer          = 24
0.00.038.023 I print_info: n_head           = 16
0.00.038.024 I print_info: n_head_kv        = 16
0.00.038.024 I print_info: n_rot            = 32
0.00.038.024 I print_info: n_swa            = 0
0.00.038.024 I print_info: n_embd_head_k    = 128
0.00.038.024 I print_info: n_embd_head_v    = 128
0.00.038.025 I print_info: n_gqa            = 1
0.00.038.026 I print_info: n_embd_k_gqa     = 2048
0.00.038.026 I print_info: n_embd_v_gqa     = 2048
0.00.038.027 I print_info: f_norm_eps       = 1.0e-05
0.00.038.027 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.030 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.031 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.031 I print_info: f_logit_scale    = 0.0e+00
0.00.038.031 I print_info: n_ff             = 8192
0.00.038.032 I print_info: n_expert         = 0
0.00.038.032 I print_info: n_expert_used    = 0
0.00.038.032 I print_info: causal attn      = 1
0.00.038.032 I print_info: pooling type     = 0
0.00.038.032 I print_info: rope type        = 2
0.00.038.033 I print_info: rope scaling     = linear
0.00.038.033 I print_info: freq_base_train  = 10000.0
0.00.038.033 I print_info: freq_scale_train = 1
0.00.038.033 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.034 I print_info: rope_finetuned   = unknown
0.00.038.034 I print_info: ssm_d_conv       = 0
0.00.038.034 I print_info: ssm_d_inner      = 0
0.00.038.035 I print_info: ssm_d_state      = 0
0.00.038.036 I print_info: ssm_dt_rank      = 0
0.00.038.036 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.036 I print_info: model type       = 1.4B
0.00.038.036 I print_info: model params     = 1.41 B
0.00.038.036 I print_info: general.name     = 1.4B
0.00.038.037 I print_info: vocab type       = BPE
0.00.038.037 I print_info: n_vocab          = 50304
0.00.038.037 I print_info: n_merges         = 50009
0.00.038.038 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.038 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.038 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.038 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.038 I print_info: LF token         = 187 ''
0.00.038.039 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.039 I print_info: max token length = 1024
0.00.038.039 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.387.258 I load_tensors: offloading 24 repeating layers to GPU
0.00.387.265 I load_tensors: offloading output layer to GPU
0.00.387.266 I load_tensors: offloaded 25/25 layers to GPU
0.00.387.297 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.387.298 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.388.994 I llama_init_from_model: n_seq_max     = 1
0.00.388.996 I llama_init_from_model: n_ctx         = 2048
0.00.388.998 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.388.998 I llama_init_from_model: n_batch       = 2048
0.00.388.998 I llama_init_from_model: n_ubatch      = 512
0.00.388.999 I llama_init_from_model: flash_attn    = 0
0.00.389.001 I llama_init_from_model: freq_base     = 10000.0
0.00.389.002 I llama_init_from_model: freq_scale    = 1
0.00.389.004 I ggml_metal_init: allocating
0.00.389.066 I ggml_metal_init: found device: Apple M4
0.00.389.082 I ggml_metal_init: picking default device: Apple M4
0.00.391.082 I ggml_metal_init: using embedded metal library
0.00.397.512 I ggml_metal_init: GPU name:   Apple M4
0.00.397.521 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.397.522 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.397.523 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.397.523 I ggml_metal_init: simdgroup reduction   = true
0.00.397.524 I ggml_metal_init: simdgroup matrix mul. = true
0.00.397.524 I ggml_metal_init: has residency sets    = true
0.00.397.524 I ggml_metal_init: has bfloat            = true
0.00.397.525 I ggml_metal_init: use bfloat            = true
0.00.397.526 I ggml_metal_init: hasUnifiedMemory      = true
0.00.397.530 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.418.538 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.473.856 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.473.864 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.473.912 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.478.782 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.478.785 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.478.786 I llama_init_from_model: graph nodes  = 967
0.00.478.786 I llama_init_from_model: graph splits = 2
0.00.478.791 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.478.904 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.478.905 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.529.527 I main: llama threadpool init, n_threads = 4
0.00.529.574 I 
0.00.529.590 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.529.592 I 
0.00.529.734 I sampler seed: 1234
0.00.529.739 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.529.770 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.529.772 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.529.772 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.226.193 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53544.49 tokens per second)
0.01.226.194 I llama_perf_context_print:        load time =     519.17 ms
0.01.226.194 I llama_perf_context_print: prompt eval time =      44.78 ms /     7 tokens (    6.40 ms per token,   156.33 tokens per second)
0.01.226.195 I llama_perf_context_print:        eval time =     648.72 ms /    63 runs   (   10.30 ms per token,    97.11 tokens per second)
0.01.226.195 I llama_perf_context_print:       total time =     697.65 ms /    70 tokens
0.01.226.432 I ggml_metal_free: deallocating

real	0m1.241s
user	0m0.111s
sys	0m0.193s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4732 (2eea03d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.183 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.955 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.961 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.963 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.963 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.964 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.964 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.964 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.965 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.966 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.966 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.966 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.967 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.967 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.968 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.970 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.970 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.970 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.706 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.768 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.597 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.599 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.600 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.600 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.600 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.601 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.601 I llama_model_loader: - type  f32:  194 tensors
0.00.025.601 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.602 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.602 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.603 I print_info: file format = GGUF V3 (latest)
0.00.025.603 I print_info: file type   = Q2_K - Medium
0.00.025.604 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.858 I load: special tokens cache size = 25
0.00.039.991 I load: token to piece cache size = 0.2984 MB
0.00.039.996 I print_info: arch             = gptneox
0.00.039.996 I print_info: vocab_only       = 0
0.00.039.996 I print_info: n_ctx_train      = 2048
0.00.039.996 I print_info: n_embd           = 2048
0.00.039.997 I print_info: n_layer          = 24
0.00.040.001 I print_info: n_head           = 16
0.00.040.004 I print_info: n_head_kv        = 16
0.00.040.004 I print_info: n_rot            = 32
0.00.040.004 I print_info: n_swa            = 0
0.00.040.004 I print_info: n_embd_head_k    = 128
0.00.040.005 I print_info: n_embd_head_v    = 128
0.00.040.005 I print_info: n_gqa            = 1
0.00.040.006 I print_info: n_embd_k_gqa     = 2048
0.00.040.007 I print_info: n_embd_v_gqa     = 2048
0.00.040.009 I print_info: f_norm_eps       = 1.0e-05
0.00.040.009 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.009 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.009 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.009 I print_info: f_logit_scale    = 0.0e+00
0.00.040.010 I print_info: n_ff             = 8192
0.00.040.010 I print_info: n_expert         = 0
0.00.040.010 I print_info: n_expert_used    = 0
0.00.040.010 I print_info: causal attn      = 1
0.00.040.011 I print_info: pooling type     = 0
0.00.040.011 I print_info: rope type        = 2
0.00.040.011 I print_info: rope scaling     = linear
0.00.040.011 I print_info: freq_base_train  = 10000.0
0.00.040.012 I print_info: freq_scale_train = 1
0.00.040.012 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.016 I print_info: rope_finetuned   = unknown
0.00.040.017 I print_info: ssm_d_conv       = 0
0.00.040.018 I print_info: ssm_d_inner      = 0
0.00.040.018 I print_info: ssm_d_state      = 0
0.00.040.018 I print_info: ssm_dt_rank      = 0
0.00.040.018 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.018 I print_info: model type       = 1.4B
0.00.040.019 I print_info: model params     = 1.41 B
0.00.040.019 I print_info: general.name     = 1.4B
0.00.040.019 I print_info: vocab type       = BPE
0.00.040.019 I print_info: n_vocab          = 50304
0.00.040.020 I print_info: n_merges         = 50009
0.00.040.020 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.020 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.020 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.020 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.021 I print_info: LF token         = 187 ''
0.00.040.021 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.021 I print_info: max token length = 1024
0.00.040.021 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.387.915 I load_tensors: offloading 24 repeating layers to GPU
0.00.387.928 I load_tensors: offloading output layer to GPU
0.00.387.929 I load_tensors: offloaded 25/25 layers to GPU
0.00.387.958 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.387.959 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.389.458 I llama_init_from_model: n_seq_max     = 1
0.00.389.461 I llama_init_from_model: n_ctx         = 128
0.00.389.461 I llama_init_from_model: n_ctx_per_seq = 128
0.00.389.462 I llama_init_from_model: n_batch       = 128
0.00.389.462 I llama_init_from_model: n_ubatch      = 128
0.00.389.463 I llama_init_from_model: flash_attn    = 0
0.00.389.465 I llama_init_from_model: freq_base     = 10000.0
0.00.389.465 I llama_init_from_model: freq_scale    = 1
0.00.389.466 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.389.469 I ggml_metal_init: allocating
0.00.389.532 I ggml_metal_init: found device: Apple M4
0.00.389.546 I ggml_metal_init: picking default device: Apple M4
0.00.391.441 I ggml_metal_init: using embedded metal library
0.00.397.240 I ggml_metal_init: GPU name:   Apple M4
0.00.397.248 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.397.249 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.397.250 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.397.250 I ggml_metal_init: simdgroup reduction   = true
0.00.397.250 I ggml_metal_init: simdgroup matrix mul. = true
0.00.397.251 I ggml_metal_init: has residency sets    = true
0.00.397.251 I ggml_metal_init: has bfloat            = true
0.00.397.251 I ggml_metal_init: use bfloat            = true
0.00.397.252 I ggml_metal_init: hasUnifiedMemory      = true
0.00.397.255 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.417.578 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.421.166 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.421.174 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.421.236 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.424.612 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.424.614 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.424.615 I llama_init_from_model: graph nodes  = 967
0.00.424.615 I llama_init_from_model: graph splits = 2
0.00.424.618 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.424.619 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.455.144 I 
0.00.455.217 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.455.226 I perplexity: tokenizing the input ..
0.00.461.410 I perplexity: tokenization took 6.183 ms
0.00.461.414 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.601.997 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.603.269 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.603.299 I llama_perf_context_print:        load time =     444.95 ms
0.00.603.300 I llama_perf_context_print: prompt eval time =     140.36 ms /   128 tokens (    1.10 ms per token,   911.94 tokens per second)
0.00.603.300 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.603.301 I llama_perf_context_print:       total time =     148.16 ms /   129 tokens
0.00.603.691 I ggml_metal_free: deallocating

real	0m0.619s
user	0m0.080s
sys	0m0.128s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4732 (2eea03d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.011.440 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.912 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.018.918 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.923 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.924 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.924 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.925 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.925 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.926 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.926 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.927 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.927 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.927 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.928 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.928 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.929 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.930 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.930 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.697 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.740 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.466 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.468 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.468 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.468 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.469 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.469 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.027.470 I llama_model_loader: - type  f32:  194 tensors
0.00.027.470 I llama_model_loader: - type q3_K:   25 tensors
0.00.027.470 I llama_model_loader: - type q4_K:   71 tensors
0.00.027.470 I llama_model_loader: - type q5_K:    1 tensors
0.00.027.471 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.471 I print_info: file format = GGUF V3 (latest)
0.00.027.472 I print_info: file type   = Q3_K - Medium
0.00.027.472 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.035.145 I load: special tokens cache size = 25
0.00.041.194 I load: token to piece cache size = 0.2984 MB
0.00.041.199 I print_info: arch             = gptneox
0.00.041.199 I print_info: vocab_only       = 0
0.00.041.200 I print_info: n_ctx_train      = 2048
0.00.041.200 I print_info: n_embd           = 2048
0.00.041.200 I print_info: n_layer          = 24
0.00.041.202 I print_info: n_head           = 16
0.00.041.203 I print_info: n_head_kv        = 16
0.00.041.203 I print_info: n_rot            = 32
0.00.041.203 I print_info: n_swa            = 0
0.00.041.203 I print_info: n_embd_head_k    = 128
0.00.041.204 I print_info: n_embd_head_v    = 128
0.00.041.204 I print_info: n_gqa            = 1
0.00.041.205 I print_info: n_embd_k_gqa     = 2048
0.00.041.206 I print_info: n_embd_v_gqa     = 2048
0.00.041.206 I print_info: f_norm_eps       = 1.0e-05
0.00.041.207 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.207 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.207 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.207 I print_info: f_logit_scale    = 0.0e+00
0.00.041.208 I print_info: n_ff             = 8192
0.00.041.208 I print_info: n_expert         = 0
0.00.041.208 I print_info: n_expert_used    = 0
0.00.041.208 I print_info: causal attn      = 1
0.00.041.208 I print_info: pooling type     = 0
0.00.041.211 I print_info: rope type        = 2
0.00.041.211 I print_info: rope scaling     = linear
0.00.041.211 I print_info: freq_base_train  = 10000.0
0.00.041.212 I print_info: freq_scale_train = 1
0.00.041.212 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.212 I print_info: rope_finetuned   = unknown
0.00.041.212 I print_info: ssm_d_conv       = 0
0.00.041.212 I print_info: ssm_d_inner      = 0
0.00.041.212 I print_info: ssm_d_state      = 0
0.00.041.213 I print_info: ssm_dt_rank      = 0
0.00.041.213 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.213 I print_info: model type       = 1.4B
0.00.041.216 I print_info: model params     = 1.41 B
0.00.041.216 I print_info: general.name     = 1.4B
0.00.041.216 I print_info: vocab type       = BPE
0.00.041.217 I print_info: n_vocab          = 50304
0.00.041.217 I print_info: n_merges         = 50009
0.00.041.217 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.217 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.217 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.217 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.218 I print_info: LF token         = 187 ''
0.00.041.218 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.218 I print_info: max token length = 1024
0.00.041.219 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.487.090 I load_tensors: offloading 24 repeating layers to GPU
0.00.487.099 I load_tensors: offloading output layer to GPU
0.00.487.100 I load_tensors: offloaded 25/25 layers to GPU
0.00.487.130 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.487.131 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.488.415 I llama_init_from_model: n_seq_max     = 1
0.00.488.418 I llama_init_from_model: n_ctx         = 2048
0.00.488.419 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.488.420 I llama_init_from_model: n_batch       = 2048
0.00.488.420 I llama_init_from_model: n_ubatch      = 512
0.00.488.421 I llama_init_from_model: flash_attn    = 0
0.00.488.423 I llama_init_from_model: freq_base     = 10000.0
0.00.488.426 I llama_init_from_model: freq_scale    = 1
0.00.488.428 I ggml_metal_init: allocating
0.00.488.482 I ggml_metal_init: found device: Apple M4
0.00.488.493 I ggml_metal_init: picking default device: Apple M4
0.00.490.304 I ggml_metal_init: using embedded metal library
0.00.497.208 I ggml_metal_init: GPU name:   Apple M4
0.00.497.212 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.497.212 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.497.213 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.497.214 I ggml_metal_init: simdgroup reduction   = true
0.00.497.214 I ggml_metal_init: simdgroup matrix mul. = true
0.00.497.214 I ggml_metal_init: has residency sets    = true
0.00.497.214 I ggml_metal_init: has bfloat            = true
0.00.497.215 I ggml_metal_init: use bfloat            = true
0.00.497.215 I ggml_metal_init: hasUnifiedMemory      = true
0.00.497.217 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.514.863 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.569.601 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.569.610 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.569.650 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.575.045 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.575.048 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.575.048 I llama_init_from_model: graph nodes  = 967
0.00.575.048 I llama_init_from_model: graph splits = 2
0.00.575.053 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.575.177 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.575.178 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.622.456 I main: llama threadpool init, n_threads = 4
0.00.622.499 I 
0.00.622.516 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.622.516 I 
0.00.622.643 I sampler seed: 1234
0.00.622.648 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.622.679 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.622.680 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.622.680 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.371.527 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52670.62 tokens per second)
0.01.371.527 I llama_perf_context_print:        load time =     610.01 ms
0.01.371.528 I llama_perf_context_print: prompt eval time =      50.26 ms /     7 tokens (    7.18 ms per token,   139.27 tokens per second)
0.01.371.529 I llama_perf_context_print:        eval time =     695.58 ms /    63 runs   (   11.04 ms per token,    90.57 tokens per second)
0.01.371.529 I llama_perf_context_print:       total time =     750.07 ms /    70 tokens
0.01.371.808 I ggml_metal_free: deallocating

real	0m1.387s
user	0m0.108s
sys	0m0.210s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.108 I build: 4732 (2eea03d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.206 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.677 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.683 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.685 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.686 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.686 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.686 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.687 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.695 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.695 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.695 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.696 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.696 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.697 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.697 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.699 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.699 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.699 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.479 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.471 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.296 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.297 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.298 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.298 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.298 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.299 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.299 I llama_model_loader: - type  f32:  194 tensors
0.00.026.300 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.300 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.300 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.300 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.301 I print_info: file format = GGUF V3 (latest)
0.00.026.302 I print_info: file type   = Q3_K - Medium
0.00.026.303 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.034.359 I load: special tokens cache size = 25
0.00.040.357 I load: token to piece cache size = 0.2984 MB
0.00.040.360 I print_info: arch             = gptneox
0.00.040.360 I print_info: vocab_only       = 0
0.00.040.360 I print_info: n_ctx_train      = 2048
0.00.040.361 I print_info: n_embd           = 2048
0.00.040.361 I print_info: n_layer          = 24
0.00.040.365 I print_info: n_head           = 16
0.00.040.365 I print_info: n_head_kv        = 16
0.00.040.365 I print_info: n_rot            = 32
0.00.040.366 I print_info: n_swa            = 0
0.00.040.366 I print_info: n_embd_head_k    = 128
0.00.040.366 I print_info: n_embd_head_v    = 128
0.00.040.367 I print_info: n_gqa            = 1
0.00.040.370 I print_info: n_embd_k_gqa     = 2048
0.00.040.370 I print_info: n_embd_v_gqa     = 2048
0.00.040.371 I print_info: f_norm_eps       = 1.0e-05
0.00.040.372 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.372 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.372 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.372 I print_info: f_logit_scale    = 0.0e+00
0.00.040.373 I print_info: n_ff             = 8192
0.00.040.373 I print_info: n_expert         = 0
0.00.040.373 I print_info: n_expert_used    = 0
0.00.040.373 I print_info: causal attn      = 1
0.00.040.373 I print_info: pooling type     = 0
0.00.040.376 I print_info: rope type        = 2
0.00.040.376 I print_info: rope scaling     = linear
0.00.040.376 I print_info: freq_base_train  = 10000.0
0.00.040.377 I print_info: freq_scale_train = 1
0.00.040.377 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.378 I print_info: rope_finetuned   = unknown
0.00.040.378 I print_info: ssm_d_conv       = 0
0.00.040.379 I print_info: ssm_d_inner      = 0
0.00.040.379 I print_info: ssm_d_state      = 0
0.00.040.379 I print_info: ssm_dt_rank      = 0
0.00.040.379 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.379 I print_info: model type       = 1.4B
0.00.040.380 I print_info: model params     = 1.41 B
0.00.040.380 I print_info: general.name     = 1.4B
0.00.040.380 I print_info: vocab type       = BPE
0.00.040.381 I print_info: n_vocab          = 50304
0.00.040.381 I print_info: n_merges         = 50009
0.00.040.381 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.381 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.381 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.386 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.388 I print_info: LF token         = 187 ''
0.00.040.389 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.390 I print_info: max token length = 1024
0.00.040.390 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.469.696 I load_tensors: offloading 24 repeating layers to GPU
0.00.469.699 I load_tensors: offloading output layer to GPU
0.00.469.700 I load_tensors: offloaded 25/25 layers to GPU
0.00.469.721 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.469.723 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.471.031 I llama_init_from_model: n_seq_max     = 1
0.00.471.034 I llama_init_from_model: n_ctx         = 128
0.00.471.034 I llama_init_from_model: n_ctx_per_seq = 128
0.00.471.034 I llama_init_from_model: n_batch       = 128
0.00.471.035 I llama_init_from_model: n_ubatch      = 128
0.00.471.035 I llama_init_from_model: flash_attn    = 0
0.00.471.036 I llama_init_from_model: freq_base     = 10000.0
0.00.471.036 I llama_init_from_model: freq_scale    = 1
0.00.471.037 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.471.038 I ggml_metal_init: allocating
0.00.471.073 I ggml_metal_init: found device: Apple M4
0.00.471.084 I ggml_metal_init: picking default device: Apple M4
0.00.472.502 I ggml_metal_init: using embedded metal library
0.00.478.591 I ggml_metal_init: GPU name:   Apple M4
0.00.478.595 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.478.595 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.478.596 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.478.597 I ggml_metal_init: simdgroup reduction   = true
0.00.478.597 I ggml_metal_init: simdgroup matrix mul. = true
0.00.478.598 I ggml_metal_init: has residency sets    = true
0.00.478.598 I ggml_metal_init: has bfloat            = true
0.00.478.598 I ggml_metal_init: use bfloat            = true
0.00.478.599 I ggml_metal_init: hasUnifiedMemory      = true
0.00.478.601 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.496.201 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.499.464 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.499.468 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.499.508 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.502.497 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.502.498 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.502.499 I llama_init_from_model: graph nodes  = 967
0.00.502.499 I llama_init_from_model: graph splits = 2
0.00.502.502 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.502.503 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.532.676 I 
0.00.532.736 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.532.744 I perplexity: tokenizing the input ..
0.00.538.681 I perplexity: tokenization took 5.936 ms
0.00.538.687 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.678.660 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.679.908 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.679.940 I llama_perf_context_print:        load time =     522.46 ms
0.00.679.941 I llama_perf_context_print: prompt eval time =     139.59 ms /   128 tokens (    1.09 ms per token,   916.98 tokens per second)
0.00.679.942 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.679.942 I llama_perf_context_print:       total time =     147.27 ms /   129 tokens
0.00.680.369 I ggml_metal_free: deallocating

real	0m0.694s
user	0m0.078s
sys	0m0.128s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4732 (2eea03d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.009.011 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.101 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.105 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.111 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.112 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.112 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.113 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.114 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.115 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.115 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.116 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.116 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.116 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.117 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.120 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.122 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.122 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.122 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.918 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.980 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.766 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.767 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.768 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.768 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.768 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.769 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.769 I llama_model_loader: - type  f32:  194 tensors
0.00.025.769 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.770 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.770 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.770 I print_info: file format = GGUF V3 (latest)
0.00.025.771 I print_info: file type   = Q4_K - Medium
0.00.025.772 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.878 I load: special tokens cache size = 25
0.00.039.987 I load: token to piece cache size = 0.2984 MB
0.00.039.990 I print_info: arch             = gptneox
0.00.039.991 I print_info: vocab_only       = 0
0.00.039.991 I print_info: n_ctx_train      = 2048
0.00.039.991 I print_info: n_embd           = 2048
0.00.039.991 I print_info: n_layer          = 24
0.00.039.993 I print_info: n_head           = 16
0.00.039.994 I print_info: n_head_kv        = 16
0.00.039.994 I print_info: n_rot            = 32
0.00.039.995 I print_info: n_swa            = 0
0.00.039.995 I print_info: n_embd_head_k    = 128
0.00.039.995 I print_info: n_embd_head_v    = 128
0.00.039.996 I print_info: n_gqa            = 1
0.00.039.997 I print_info: n_embd_k_gqa     = 2048
0.00.039.997 I print_info: n_embd_v_gqa     = 2048
0.00.039.998 I print_info: f_norm_eps       = 1.0e-05
0.00.039.998 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.999 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.999 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.999 I print_info: f_logit_scale    = 0.0e+00
0.00.040.000 I print_info: n_ff             = 8192
0.00.040.002 I print_info: n_expert         = 0
0.00.040.002 I print_info: n_expert_used    = 0
0.00.040.002 I print_info: causal attn      = 1
0.00.040.002 I print_info: pooling type     = 0
0.00.040.003 I print_info: rope type        = 2
0.00.040.003 I print_info: rope scaling     = linear
0.00.040.003 I print_info: freq_base_train  = 10000.0
0.00.040.003 I print_info: freq_scale_train = 1
0.00.040.004 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.004 I print_info: rope_finetuned   = unknown
0.00.040.004 I print_info: ssm_d_conv       = 0
0.00.040.004 I print_info: ssm_d_inner      = 0
0.00.040.004 I print_info: ssm_d_state      = 0
0.00.040.005 I print_info: ssm_dt_rank      = 0
0.00.040.005 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.005 I print_info: model type       = 1.4B
0.00.040.005 I print_info: model params     = 1.41 B
0.00.040.006 I print_info: general.name     = 1.4B
0.00.040.006 I print_info: vocab type       = BPE
0.00.040.006 I print_info: n_vocab          = 50304
0.00.040.006 I print_info: n_merges         = 50009
0.00.040.007 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.007 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.007 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.007 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.008 I print_info: LF token         = 187 ''
0.00.040.008 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.008 I print_info: max token length = 1024
0.00.040.008 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.580.465 I load_tensors: offloading 24 repeating layers to GPU
0.00.580.470 I load_tensors: offloading output layer to GPU
0.00.580.470 I load_tensors: offloaded 25/25 layers to GPU
0.00.580.496 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.580.497 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.581.091 I llama_init_from_model: n_seq_max     = 1
0.00.581.092 I llama_init_from_model: n_ctx         = 2048
0.00.581.093 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.581.093 I llama_init_from_model: n_batch       = 2048
0.00.581.093 I llama_init_from_model: n_ubatch      = 512
0.00.581.093 I llama_init_from_model: flash_attn    = 0
0.00.581.094 I llama_init_from_model: freq_base     = 10000.0
0.00.581.094 I llama_init_from_model: freq_scale    = 1
0.00.581.095 I ggml_metal_init: allocating
0.00.581.113 I ggml_metal_init: found device: Apple M4
0.00.581.123 I ggml_metal_init: picking default device: Apple M4
0.00.582.179 I ggml_metal_init: using embedded metal library
0.00.585.580 I ggml_metal_init: GPU name:   Apple M4
0.00.585.582 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.585.583 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.585.583 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.585.584 I ggml_metal_init: simdgroup reduction   = true
0.00.585.584 I ggml_metal_init: simdgroup matrix mul. = true
0.00.585.584 I ggml_metal_init: has residency sets    = true
0.00.585.584 I ggml_metal_init: has bfloat            = true
0.00.585.584 I ggml_metal_init: use bfloat            = true
0.00.585.585 I ggml_metal_init: hasUnifiedMemory      = true
0.00.585.586 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.596.350 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.629.584 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.629.591 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.629.627 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.634.792 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.634.794 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.634.795 I llama_init_from_model: graph nodes  = 967
0.00.634.795 I llama_init_from_model: graph splits = 2
0.00.634.801 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.634.926 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.634.926 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.683.605 I main: llama threadpool init, n_threads = 4
0.00.683.655 I 
0.00.683.669 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.683.669 I 
0.00.683.784 I sampler seed: 1234
0.00.683.789 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.683.799 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.683.799 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.683.799 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.448.938 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50823.19 tokens per second)
0.01.448.939 I llama_perf_context_print:        load time =     673.87 ms
0.01.448.940 I llama_perf_context_print: prompt eval time =      58.50 ms /     7 tokens (    8.36 ms per token,   119.66 tokens per second)
0.01.448.940 I llama_perf_context_print:        eval time =     703.61 ms /    63 runs   (   11.17 ms per token,    89.54 tokens per second)
0.01.448.941 I llama_perf_context_print:       total time =     766.06 ms /    70 tokens
0.01.449.177 I ggml_metal_free: deallocating

real	0m1.464s
user	0m0.099s
sys	0m0.209s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4732 (2eea03d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.697 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.911 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.918 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.919 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.920 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.920 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.921 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.921 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.922 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.922 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.923 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.923 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.923 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.924 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.924 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.926 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.926 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.927 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.698 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.695 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.488 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.490 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.490 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.491 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.491 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.491 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.492 I llama_model_loader: - type  f32:  194 tensors
0.00.025.492 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.492 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.493 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.494 I print_info: file format = GGUF V3 (latest)
0.00.025.494 I print_info: file type   = Q4_K - Medium
0.00.025.495 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.770 I load: special tokens cache size = 25
0.00.039.953 I load: token to piece cache size = 0.2984 MB
0.00.039.957 I print_info: arch             = gptneox
0.00.039.958 I print_info: vocab_only       = 0
0.00.039.958 I print_info: n_ctx_train      = 2048
0.00.039.958 I print_info: n_embd           = 2048
0.00.039.958 I print_info: n_layer          = 24
0.00.039.962 I print_info: n_head           = 16
0.00.039.963 I print_info: n_head_kv        = 16
0.00.039.963 I print_info: n_rot            = 32
0.00.039.963 I print_info: n_swa            = 0
0.00.039.963 I print_info: n_embd_head_k    = 128
0.00.039.963 I print_info: n_embd_head_v    = 128
0.00.039.964 I print_info: n_gqa            = 1
0.00.039.965 I print_info: n_embd_k_gqa     = 2048
0.00.039.965 I print_info: n_embd_v_gqa     = 2048
0.00.039.966 I print_info: f_norm_eps       = 1.0e-05
0.00.039.966 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.966 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.966 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.967 I print_info: f_logit_scale    = 0.0e+00
0.00.039.967 I print_info: n_ff             = 8192
0.00.039.967 I print_info: n_expert         = 0
0.00.039.968 I print_info: n_expert_used    = 0
0.00.039.968 I print_info: causal attn      = 1
0.00.039.968 I print_info: pooling type     = 0
0.00.039.968 I print_info: rope type        = 2
0.00.039.968 I print_info: rope scaling     = linear
0.00.039.969 I print_info: freq_base_train  = 10000.0
0.00.039.969 I print_info: freq_scale_train = 1
0.00.039.969 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.969 I print_info: rope_finetuned   = unknown
0.00.039.969 I print_info: ssm_d_conv       = 0
0.00.039.970 I print_info: ssm_d_inner      = 0
0.00.039.970 I print_info: ssm_d_state      = 0
0.00.039.970 I print_info: ssm_dt_rank      = 0
0.00.039.970 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.970 I print_info: model type       = 1.4B
0.00.039.970 I print_info: model params     = 1.41 B
0.00.039.971 I print_info: general.name     = 1.4B
0.00.039.971 I print_info: vocab type       = BPE
0.00.039.971 I print_info: n_vocab          = 50304
0.00.039.972 I print_info: n_merges         = 50009
0.00.039.972 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.972 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.972 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.972 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.972 I print_info: LF token         = 187 ''
0.00.039.973 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.973 I print_info: max token length = 1024
0.00.039.973 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.559.938 I load_tensors: offloading 24 repeating layers to GPU
0.00.559.941 I load_tensors: offloading output layer to GPU
0.00.559.942 I load_tensors: offloaded 25/25 layers to GPU
0.00.559.963 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.559.964 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.561.128 I llama_init_from_model: n_seq_max     = 1
0.00.561.131 I llama_init_from_model: n_ctx         = 128
0.00.561.131 I llama_init_from_model: n_ctx_per_seq = 128
0.00.561.131 I llama_init_from_model: n_batch       = 128
0.00.561.132 I llama_init_from_model: n_ubatch      = 128
0.00.561.132 I llama_init_from_model: flash_attn    = 0
0.00.561.133 I llama_init_from_model: freq_base     = 10000.0
0.00.561.133 I llama_init_from_model: freq_scale    = 1
0.00.561.134 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.561.135 I ggml_metal_init: allocating
0.00.561.153 I ggml_metal_init: found device: Apple M4
0.00.561.162 I ggml_metal_init: picking default device: Apple M4
0.00.562.618 I ggml_metal_init: using embedded metal library
0.00.568.300 I ggml_metal_init: GPU name:   Apple M4
0.00.568.303 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.568.304 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.568.305 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.568.305 I ggml_metal_init: simdgroup reduction   = true
0.00.568.306 I ggml_metal_init: simdgroup matrix mul. = true
0.00.568.306 I ggml_metal_init: has residency sets    = true
0.00.568.306 I ggml_metal_init: has bfloat            = true
0.00.568.306 I ggml_metal_init: use bfloat            = true
0.00.568.307 I ggml_metal_init: hasUnifiedMemory      = true
0.00.568.308 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.584.511 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.587.744 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.587.747 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.587.786 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.590.689 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.590.690 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.590.691 I llama_init_from_model: graph nodes  = 967
0.00.590.691 I llama_init_from_model: graph splits = 2
0.00.590.693 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.590.693 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.620.828 I 
0.00.620.882 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.620.890 I perplexity: tokenizing the input ..
0.00.627.349 I perplexity: tokenization took 6.456 ms
0.00.627.361 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.770.380 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.771.632 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.771.657 I llama_perf_context_print:        load time =     611.12 ms
0.00.771.658 I llama_perf_context_print: prompt eval time =     142.62 ms /   128 tokens (    1.11 ms per token,   897.46 tokens per second)
0.00.771.658 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.771.659 I llama_perf_context_print:       total time =     150.83 ms /   129 tokens
0.00.772.046 I ggml_metal_free: deallocating

real	0m0.785s
user	0m0.077s
sys	0m0.169s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4732 (2eea03d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.010.092 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.988 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.992 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.993 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.997 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.997 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.997 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.998 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.999 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.999 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.000 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.000 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.000 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.001 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.001 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.003 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.003 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.003 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.663 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.726 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.395 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.397 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.397 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.397 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.398 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.398 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.398 I llama_model_loader: - type  f32:  194 tensors
0.00.026.399 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.399 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.400 I print_info: file format = GGUF V3 (latest)
0.00.026.400 I print_info: file type   = Q5_K - Medium
0.00.026.405 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.988 I load: special tokens cache size = 25
0.00.039.997 I load: token to piece cache size = 0.2984 MB
0.00.040.000 I print_info: arch             = gptneox
0.00.040.001 I print_info: vocab_only       = 0
0.00.040.001 I print_info: n_ctx_train      = 2048
0.00.040.001 I print_info: n_embd           = 2048
0.00.040.001 I print_info: n_layer          = 24
0.00.040.005 I print_info: n_head           = 16
0.00.040.005 I print_info: n_head_kv        = 16
0.00.040.006 I print_info: n_rot            = 32
0.00.040.006 I print_info: n_swa            = 0
0.00.040.006 I print_info: n_embd_head_k    = 128
0.00.040.006 I print_info: n_embd_head_v    = 128
0.00.040.007 I print_info: n_gqa            = 1
0.00.040.008 I print_info: n_embd_k_gqa     = 2048
0.00.040.008 I print_info: n_embd_v_gqa     = 2048
0.00.040.009 I print_info: f_norm_eps       = 1.0e-05
0.00.040.009 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.009 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.010 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.010 I print_info: f_logit_scale    = 0.0e+00
0.00.040.011 I print_info: n_ff             = 8192
0.00.040.011 I print_info: n_expert         = 0
0.00.040.011 I print_info: n_expert_used    = 0
0.00.040.011 I print_info: causal attn      = 1
0.00.040.011 I print_info: pooling type     = 0
0.00.040.011 I print_info: rope type        = 2
0.00.040.012 I print_info: rope scaling     = linear
0.00.040.012 I print_info: freq_base_train  = 10000.0
0.00.040.012 I print_info: freq_scale_train = 1
0.00.040.012 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.013 I print_info: rope_finetuned   = unknown
0.00.040.013 I print_info: ssm_d_conv       = 0
0.00.040.013 I print_info: ssm_d_inner      = 0
0.00.040.013 I print_info: ssm_d_state      = 0
0.00.040.013 I print_info: ssm_dt_rank      = 0
0.00.040.014 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.014 I print_info: model type       = 1.4B
0.00.040.014 I print_info: model params     = 1.41 B
0.00.040.014 I print_info: general.name     = 1.4B
0.00.040.015 I print_info: vocab type       = BPE
0.00.040.015 I print_info: n_vocab          = 50304
0.00.040.015 I print_info: n_merges         = 50009
0.00.040.015 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.016 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.016 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.016 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.016 I print_info: LF token         = 187 ''
0.00.040.017 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.017 I print_info: max token length = 1024
0.00.040.017 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.678.464 I load_tensors: offloading 24 repeating layers to GPU
0.00.678.467 I load_tensors: offloading output layer to GPU
0.00.678.468 I load_tensors: offloaded 25/25 layers to GPU
0.00.678.486 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.678.489 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.679.455 I llama_init_from_model: n_seq_max     = 1
0.00.679.457 I llama_init_from_model: n_ctx         = 2048
0.00.679.457 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.679.458 I llama_init_from_model: n_batch       = 2048
0.00.679.458 I llama_init_from_model: n_ubatch      = 512
0.00.679.458 I llama_init_from_model: flash_attn    = 0
0.00.679.459 I llama_init_from_model: freq_base     = 10000.0
0.00.679.459 I llama_init_from_model: freq_scale    = 1
0.00.679.460 I ggml_metal_init: allocating
0.00.679.477 I ggml_metal_init: found device: Apple M4
0.00.679.487 I ggml_metal_init: picking default device: Apple M4
0.00.680.757 I ggml_metal_init: using embedded metal library
0.00.686.543 I ggml_metal_init: GPU name:   Apple M4
0.00.686.547 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.686.548 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.686.548 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.686.549 I ggml_metal_init: simdgroup reduction   = true
0.00.686.549 I ggml_metal_init: simdgroup matrix mul. = true
0.00.686.550 I ggml_metal_init: has residency sets    = true
0.00.686.550 I ggml_metal_init: has bfloat            = true
0.00.686.550 I ggml_metal_init: use bfloat            = true
0.00.686.551 I ggml_metal_init: hasUnifiedMemory      = true
0.00.686.552 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.703.431 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.760.941 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.760.949 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.760.991 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.766.364 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.766.366 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.766.367 I llama_init_from_model: graph nodes  = 967
0.00.766.367 I llama_init_from_model: graph splits = 2
0.00.766.371 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.766.510 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.766.511 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.821.618 I main: llama threadpool init, n_threads = 4
0.00.821.680 I 
0.00.821.709 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.821.710 I 
0.00.821.835 I sampler seed: 1234
0.00.821.840 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.821.873 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.821.875 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.821.875 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.664.638 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53423.63 tokens per second)
0.01.664.638 I llama_perf_context_print:        load time =     810.54 ms
0.01.664.639 I llama_perf_context_print: prompt eval time =      52.57 ms /     7 tokens (    7.51 ms per token,   133.15 tokens per second)
0.01.664.640 I llama_perf_context_print:        eval time =     787.21 ms /    63 runs   (   12.50 ms per token,    80.03 tokens per second)
0.01.664.640 I llama_perf_context_print:       total time =     844.00 ms /    70 tokens
0.01.664.902 I ggml_metal_free: deallocating

real	0m1.682s
user	0m0.107s
sys	0m0.266s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4732 (2eea03d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.869 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.601 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.608 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.610 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.610 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.611 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.611 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.611 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.612 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.612 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.613 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.613 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.614 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.614 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.615 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.616 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.617 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.617 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.456 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.471 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.367 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.368 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.369 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.369 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.369 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.370 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.370 I llama_model_loader: - type  f32:  194 tensors
0.00.026.371 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.371 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.372 I print_info: file format = GGUF V3 (latest)
0.00.026.372 I print_info: file type   = Q5_K - Medium
0.00.026.375 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.577 I load: special tokens cache size = 25
0.00.040.561 I load: token to piece cache size = 0.2984 MB
0.00.040.565 I print_info: arch             = gptneox
0.00.040.565 I print_info: vocab_only       = 0
0.00.040.565 I print_info: n_ctx_train      = 2048
0.00.040.565 I print_info: n_embd           = 2048
0.00.040.566 I print_info: n_layer          = 24
0.00.040.570 I print_info: n_head           = 16
0.00.040.570 I print_info: n_head_kv        = 16
0.00.040.571 I print_info: n_rot            = 32
0.00.040.571 I print_info: n_swa            = 0
0.00.040.571 I print_info: n_embd_head_k    = 128
0.00.040.571 I print_info: n_embd_head_v    = 128
0.00.040.572 I print_info: n_gqa            = 1
0.00.040.573 I print_info: n_embd_k_gqa     = 2048
0.00.040.573 I print_info: n_embd_v_gqa     = 2048
0.00.040.574 I print_info: f_norm_eps       = 1.0e-05
0.00.040.574 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.574 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.577 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.578 I print_info: f_logit_scale    = 0.0e+00
0.00.040.578 I print_info: n_ff             = 8192
0.00.040.578 I print_info: n_expert         = 0
0.00.040.579 I print_info: n_expert_used    = 0
0.00.040.579 I print_info: causal attn      = 1
0.00.040.579 I print_info: pooling type     = 0
0.00.040.579 I print_info: rope type        = 2
0.00.040.580 I print_info: rope scaling     = linear
0.00.040.580 I print_info: freq_base_train  = 10000.0
0.00.040.580 I print_info: freq_scale_train = 1
0.00.040.580 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.581 I print_info: rope_finetuned   = unknown
0.00.040.582 I print_info: ssm_d_conv       = 0
0.00.040.582 I print_info: ssm_d_inner      = 0
0.00.040.582 I print_info: ssm_d_state      = 0
0.00.040.582 I print_info: ssm_dt_rank      = 0
0.00.040.582 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.582 I print_info: model type       = 1.4B
0.00.040.583 I print_info: model params     = 1.41 B
0.00.040.583 I print_info: general.name     = 1.4B
0.00.040.584 I print_info: vocab type       = BPE
0.00.040.584 I print_info: n_vocab          = 50304
0.00.040.586 I print_info: n_merges         = 50009
0.00.040.586 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.586 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.586 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.586 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.587 I print_info: LF token         = 187 ''
0.00.040.587 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.587 I print_info: max token length = 1024
0.00.040.587 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.672.525 I load_tensors: offloading 24 repeating layers to GPU
0.00.672.530 I load_tensors: offloading output layer to GPU
0.00.672.532 I load_tensors: offloaded 25/25 layers to GPU
0.00.672.555 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.672.558 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.673.784 I llama_init_from_model: n_seq_max     = 1
0.00.673.787 I llama_init_from_model: n_ctx         = 128
0.00.673.787 I llama_init_from_model: n_ctx_per_seq = 128
0.00.673.787 I llama_init_from_model: n_batch       = 128
0.00.673.788 I llama_init_from_model: n_ubatch      = 128
0.00.673.788 I llama_init_from_model: flash_attn    = 0
0.00.673.789 I llama_init_from_model: freq_base     = 10000.0
0.00.673.789 I llama_init_from_model: freq_scale    = 1
0.00.673.790 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.673.792 I ggml_metal_init: allocating
0.00.673.812 I ggml_metal_init: found device: Apple M4
0.00.673.823 I ggml_metal_init: picking default device: Apple M4
0.00.675.114 I ggml_metal_init: using embedded metal library
0.00.681.090 I ggml_metal_init: GPU name:   Apple M4
0.00.681.094 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.681.094 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.681.096 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.681.096 I ggml_metal_init: simdgroup reduction   = true
0.00.681.096 I ggml_metal_init: simdgroup matrix mul. = true
0.00.681.097 I ggml_metal_init: has residency sets    = true
0.00.681.097 I ggml_metal_init: has bfloat            = true
0.00.681.097 I ggml_metal_init: use bfloat            = true
0.00.681.098 I ggml_metal_init: hasUnifiedMemory      = true
0.00.681.099 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.697.672 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.700.893 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.700.899 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.700.957 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.703.938 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.703.939 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.703.940 I llama_init_from_model: graph nodes  = 967
0.00.703.940 I llama_init_from_model: graph splits = 2
0.00.703.943 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.703.943 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.739.188 I 
0.00.739.242 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.739.249 I perplexity: tokenizing the input ..
0.00.746.106 I perplexity: tokenization took 6.853 ms
0.00.746.115 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.896.019 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.897.294 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.897.325 I llama_perf_context_print:        load time =     728.31 ms
0.00.897.326 I llama_perf_context_print: prompt eval time =     148.97 ms /   128 tokens (    1.16 ms per token,   859.20 tokens per second)
0.00.897.327 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.897.327 I llama_perf_context_print:       total time =     158.14 ms /   129 tokens
0.00.897.697 I ggml_metal_free: deallocating

real	0m0.913s
user	0m0.078s
sys	0m0.195s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4732 (2eea03d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.008.830 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.222 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.226 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.228 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.228 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.229 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.229 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.229 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.230 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.231 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.231 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.231 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.232 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.232 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.232 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.234 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.234 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.236 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.020 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.074 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.842 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.843 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.843 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.844 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.844 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.844 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.844 I llama_model_loader: - type  f32:  194 tensors
0.00.024.845 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.845 I print_info: file format = GGUF V3 (latest)
0.00.024.846 I print_info: file type   = Q6_K
0.00.024.846 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.498 I load: special tokens cache size = 25
0.00.038.489 I load: token to piece cache size = 0.2984 MB
0.00.038.492 I print_info: arch             = gptneox
0.00.038.492 I print_info: vocab_only       = 0
0.00.038.493 I print_info: n_ctx_train      = 2048
0.00.038.493 I print_info: n_embd           = 2048
0.00.038.493 I print_info: n_layer          = 24
0.00.038.496 I print_info: n_head           = 16
0.00.038.496 I print_info: n_head_kv        = 16
0.00.038.497 I print_info: n_rot            = 32
0.00.038.497 I print_info: n_swa            = 0
0.00.038.498 I print_info: n_embd_head_k    = 128
0.00.038.498 I print_info: n_embd_head_v    = 128
0.00.038.499 I print_info: n_gqa            = 1
0.00.038.500 I print_info: n_embd_k_gqa     = 2048
0.00.038.501 I print_info: n_embd_v_gqa     = 2048
0.00.038.501 I print_info: f_norm_eps       = 1.0e-05
0.00.038.502 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.502 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.502 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.502 I print_info: f_logit_scale    = 0.0e+00
0.00.038.503 I print_info: n_ff             = 8192
0.00.038.503 I print_info: n_expert         = 0
0.00.038.503 I print_info: n_expert_used    = 0
0.00.038.503 I print_info: causal attn      = 1
0.00.038.503 I print_info: pooling type     = 0
0.00.038.508 I print_info: rope type        = 2
0.00.038.510 I print_info: rope scaling     = linear
0.00.038.510 I print_info: freq_base_train  = 10000.0
0.00.038.511 I print_info: freq_scale_train = 1
0.00.038.511 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.511 I print_info: rope_finetuned   = unknown
0.00.038.511 I print_info: ssm_d_conv       = 0
0.00.038.511 I print_info: ssm_d_inner      = 0
0.00.038.512 I print_info: ssm_d_state      = 0
0.00.038.512 I print_info: ssm_dt_rank      = 0
0.00.038.512 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.512 I print_info: model type       = 1.4B
0.00.038.512 I print_info: model params     = 1.41 B
0.00.038.513 I print_info: general.name     = 1.4B
0.00.038.513 I print_info: vocab type       = BPE
0.00.038.513 I print_info: n_vocab          = 50304
0.00.038.513 I print_info: n_merges         = 50009
0.00.038.514 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.514 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.514 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.514 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.514 I print_info: LF token         = 187 ''
0.00.038.515 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.515 I print_info: max token length = 1024
0.00.038.515 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.738.768 I load_tensors: offloading 24 repeating layers to GPU
0.00.738.774 I load_tensors: offloading output layer to GPU
0.00.738.776 I load_tensors: offloaded 25/25 layers to GPU
0.00.738.799 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.738.800 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.739.822 I llama_init_from_model: n_seq_max     = 1
0.00.739.824 I llama_init_from_model: n_ctx         = 2048
0.00.739.824 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.739.825 I llama_init_from_model: n_batch       = 2048
0.00.739.825 I llama_init_from_model: n_ubatch      = 512
0.00.739.825 I llama_init_from_model: flash_attn    = 0
0.00.739.826 I llama_init_from_model: freq_base     = 10000.0
0.00.739.827 I llama_init_from_model: freq_scale    = 1
0.00.739.828 I ggml_metal_init: allocating
0.00.739.840 I ggml_metal_init: found device: Apple M4
0.00.739.848 I ggml_metal_init: picking default device: Apple M4
0.00.741.087 I ggml_metal_init: using embedded metal library
0.00.746.166 I ggml_metal_init: GPU name:   Apple M4
0.00.746.169 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.746.170 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.746.170 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.746.171 I ggml_metal_init: simdgroup reduction   = true
0.00.746.171 I ggml_metal_init: simdgroup matrix mul. = true
0.00.746.171 I ggml_metal_init: has residency sets    = true
0.00.746.172 I ggml_metal_init: has bfloat            = true
0.00.746.172 I ggml_metal_init: use bfloat            = true
0.00.746.172 I ggml_metal_init: hasUnifiedMemory      = true
0.00.746.173 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.760.939 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.816.078 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.816.084 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.816.126 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.821.512 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.821.515 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.821.515 I llama_init_from_model: graph nodes  = 967
0.00.821.516 I llama_init_from_model: graph splits = 2
0.00.821.520 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.821.645 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.821.646 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.877.736 I main: llama threadpool init, n_threads = 4
0.00.877.784 I 
0.00.877.801 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.877.801 I 
0.00.877.932 I sampler seed: 1234
0.00.877.936 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.877.946 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.877.946 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.877.950 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.746.736 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52129.22 tokens per second)
0.01.746.737 I llama_perf_context_print:        load time =     868.21 ms
0.01.746.738 I llama_perf_context_print: prompt eval time =      57.45 ms /     7 tokens (    8.21 ms per token,   121.84 tokens per second)
0.01.746.740 I llama_perf_context_print:        eval time =     808.32 ms /    63 runs   (   12.83 ms per token,    77.94 tokens per second)
0.01.746.740 I llama_perf_context_print:       total time =     869.69 ms /    70 tokens
0.01.746.961 I ggml_metal_free: deallocating

real	0m1.763s
user	0m0.104s
sys	0m0.281s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4732 (2eea03d8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.587 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.448 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.453 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.455 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.456 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.456 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.457 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.457 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.458 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.458 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.459 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.459 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.459 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.461 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.462 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.464 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.464 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.464 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.246 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.258 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.090 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.092 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.092 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.093 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.093 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.093 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.094 I llama_model_loader: - type  f32:  194 tensors
0.00.024.094 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.095 I print_info: file format = GGUF V3 (latest)
0.00.024.096 I print_info: file type   = Q6_K
0.00.024.101 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.031.845 I load: special tokens cache size = 25
0.00.037.903 I load: token to piece cache size = 0.2984 MB
0.00.037.907 I print_info: arch             = gptneox
0.00.037.907 I print_info: vocab_only       = 0
0.00.037.907 I print_info: n_ctx_train      = 2048
0.00.037.907 I print_info: n_embd           = 2048
0.00.037.908 I print_info: n_layer          = 24
0.00.037.911 I print_info: n_head           = 16
0.00.037.912 I print_info: n_head_kv        = 16
0.00.037.912 I print_info: n_rot            = 32
0.00.037.912 I print_info: n_swa            = 0
0.00.037.913 I print_info: n_embd_head_k    = 128
0.00.037.913 I print_info: n_embd_head_v    = 128
0.00.037.914 I print_info: n_gqa            = 1
0.00.037.914 I print_info: n_embd_k_gqa     = 2048
0.00.037.915 I print_info: n_embd_v_gqa     = 2048
0.00.037.916 I print_info: f_norm_eps       = 1.0e-05
0.00.037.916 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.916 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.916 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.917 I print_info: f_logit_scale    = 0.0e+00
0.00.037.917 I print_info: n_ff             = 8192
0.00.037.917 I print_info: n_expert         = 0
0.00.037.918 I print_info: n_expert_used    = 0
0.00.037.918 I print_info: causal attn      = 1
0.00.037.918 I print_info: pooling type     = 0
0.00.037.918 I print_info: rope type        = 2
0.00.037.918 I print_info: rope scaling     = linear
0.00.037.919 I print_info: freq_base_train  = 10000.0
0.00.037.919 I print_info: freq_scale_train = 1
0.00.037.919 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.919 I print_info: rope_finetuned   = unknown
0.00.037.919 I print_info: ssm_d_conv       = 0
0.00.037.920 I print_info: ssm_d_inner      = 0
0.00.037.920 I print_info: ssm_d_state      = 0
0.00.037.920 I print_info: ssm_dt_rank      = 0
0.00.037.920 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.920 I print_info: model type       = 1.4B
0.00.037.920 I print_info: model params     = 1.41 B
0.00.037.921 I print_info: general.name     = 1.4B
0.00.037.921 I print_info: vocab type       = BPE
0.00.037.921 I print_info: n_vocab          = 50304
0.00.037.923 I print_info: n_merges         = 50009
0.00.037.924 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.925 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.925 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.925 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.925 I print_info: LF token         = 187 ''
0.00.037.925 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.926 I print_info: max token length = 1024
0.00.037.926 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.686.000 I load_tensors: offloading 24 repeating layers to GPU
0.00.686.005 I load_tensors: offloading output layer to GPU
0.00.686.006 I load_tensors: offloaded 25/25 layers to GPU
0.00.686.029 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.686.031 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.687.085 I llama_init_from_model: n_seq_max     = 1
0.00.687.087 I llama_init_from_model: n_ctx         = 128
0.00.687.088 I llama_init_from_model: n_ctx_per_seq = 128
0.00.687.088 I llama_init_from_model: n_batch       = 128
0.00.687.089 I llama_init_from_model: n_ubatch      = 128
0.00.687.089 I llama_init_from_model: flash_attn    = 0
0.00.687.090 I llama_init_from_model: freq_base     = 10000.0
0.00.687.090 I llama_init_from_model: freq_scale    = 1
0.00.687.091 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.687.092 I ggml_metal_init: allocating
0.00.687.116 I ggml_metal_init: found device: Apple M4
0.00.687.124 I ggml_metal_init: picking default device: Apple M4
0.00.688.338 I ggml_metal_init: using embedded metal library
0.00.693.873 I ggml_metal_init: GPU name:   Apple M4
0.00.693.876 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.693.877 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.693.877 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.693.878 I ggml_metal_init: simdgroup reduction   = true
0.00.693.878 I ggml_metal_init: simdgroup matrix mul. = true
0.00.693.878 I ggml_metal_init: has residency sets    = true
0.00.693.879 I ggml_metal_init: has bfloat            = true
0.00.693.879 I ggml_metal_init: use bfloat            = true
0.00.693.880 I ggml_metal_init: hasUnifiedMemory      = true
0.00.693.881 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.709.947 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.713.290 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.713.293 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.713.336 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.716.441 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.716.443 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.716.443 I llama_init_from_model: graph nodes  = 967
0.00.716.443 I llama_init_from_model: graph splits = 2
0.00.716.446 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.716.446 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.749.046 I 
0.00.749.130 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.749.147 I perplexity: tokenizing the input ..
0.00.755.833 I perplexity: tokenization took 6.683 ms
0.00.755.839 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.888.611 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.889.927 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.889.955 I llama_perf_context_print:        load time =     740.45 ms
0.00.889.956 I llama_perf_context_print: prompt eval time =     131.89 ms /   128 tokens (    1.03 ms per token,   970.53 tokens per second)
0.00.889.956 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.889.957 I llama_perf_context_print:       total time =     140.91 ms /   129 tokens
0.00.890.313 I ggml_metal_free: deallocating

real	0m0.904s
user	0m0.076s
sys	0m0.195s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4732 (2eea03d8)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1180047a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x118004f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1180054d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x118005a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x118006030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1180065e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x118006b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x118007140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1180076f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x118007bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1180080f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1180085f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x118009110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1180098c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11800a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11800a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11800af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11800b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11800bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11800c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11800cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11800d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11800da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11800e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11800ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11800ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11800f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11800ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1180104c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x118010780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x118010c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x118010ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x118011770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x118011cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x118011f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x118012410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1180128b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x118012d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1180131f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x118013690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x118013b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x118013fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x118014470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x118014910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x118014bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1180151e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1180157f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x118016110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x118016720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x118016d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x118017340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x118017950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x118017f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x118018570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x118018d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x118019200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1180196a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x118019960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x118019f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11801a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11801aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11801aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11801b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11801b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11801bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11801c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11801c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11801ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11801cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11801d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11801d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11801dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11801e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11801e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11801ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11801f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11801f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11801fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x118020180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1180206d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x118020c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x118021170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1180216c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x118021c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x118022160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1180226b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x118022c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x118023150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1180236a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x118023bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x118024140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x118024690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x118024be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x118025130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x118025680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x118025bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x118026120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x118015e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x118026590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x118026d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x118027290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1180277e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x118027d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x118028280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1180287d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x118028d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x118029270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1180297c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x118029d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11802a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11802a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11802ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11802b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11802b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11802bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11802c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11802c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11802c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11802ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11802d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11802d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11802dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11802e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11802e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11802e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11802ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11802f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11802f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11802fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1180300f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x118030590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x118030a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x118030ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x118031370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x118031810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x118031cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x118032150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1180325f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x118032a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x118032f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1180333d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x118033870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x118033d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1180341b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x118034650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x118034af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x118034f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x118035430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1180358d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x118035d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x118036210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1180366b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x118036b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x118036ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x118037490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x118037930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x118037dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x118038270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x118038710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x118038bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x118039050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1180394f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x118039990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x118039e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11803a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11803a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11803ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11803b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11803b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11803b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11803be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11803c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11803c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11803cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11803d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11803d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11803da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11803def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11803e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11803e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11803ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11803f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11803f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11803fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11803ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1180403f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x118040890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x118040d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1180411d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x118041670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x118041b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x118041fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x118042450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1180429a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x118042ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x118043440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x118043990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x118043c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x118044260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x118044870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x118044e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x118045670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x118045b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x118045dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1180463e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1180469f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1180471e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x118047680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x118047b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x118047fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x118048770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x118048cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x118049210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x118049760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x118049cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11804a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11804a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11804aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11804b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11804b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11804bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11804c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11804c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11804cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11804d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11804d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11804dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11804e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11804e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11804ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11804f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11804f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11804fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1180501a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1180506f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x118050c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x118051190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1180516e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x118051c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x118052180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1180526d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x118052c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x118053170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1180536c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x118053c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x118054160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1180546b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x118054c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x118055150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1180556a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x118055bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x118056140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x118056690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x118056be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x118057130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x118057680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x118057bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x118058120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x118058670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x118058bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x118059110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x118059660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x118059bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11805a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11805a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11805aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11805b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11805b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11805ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11805bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11805c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11805c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11805ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11805d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11805d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11805da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11805df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11805e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11805e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11805ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11805f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11805f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11805fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1180602c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1180609e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x118061100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x118061820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x118061ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1180622d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x118062590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x118062ba0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.764.947 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.764.951 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x118044520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x118046090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x118062850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x118043f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x118044b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x118017c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x118017600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x118019c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1180466a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11800efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x118015ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1180163d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1180154a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x118018220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x118016ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11800dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x118026850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x118061da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1180111a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x118011460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x118046cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x118045140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11800f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11800f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11800fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x118063000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1180632c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x118063580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x118063840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x118063b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x118063dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x118064080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x118064340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x118064600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1180648c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x118064b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x118064e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x118065100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1180653c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x118065680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x118065940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x118065c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x118065ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x118066180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x118066440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x118066700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1180669c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x118066c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x118066f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x118067200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1180674c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x118067780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x118067a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x118067d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x118067fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x118068280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x118068540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x118068800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x118068ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x118068d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x118069040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x118069300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1180695c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x118069880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x118069b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x118069e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11806a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11806a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11806a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11806a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11806abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11806ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11806b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11806b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11806b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11806b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11806bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11806bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11806c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11806c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11806c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11806ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11806ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11806cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11806d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11806d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11806d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11806da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11806dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11806e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11806e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11806e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11806e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11806eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11806edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11806f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11806f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11806f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11806f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11806fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11806fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x118070100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1180703c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x118070680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x118070940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x118070c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x118070ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x118071180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x118071440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x118071700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1180719c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x118071c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x118071f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x118072200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1180724c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x118072780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x118072a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x118072d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x118072fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x118073280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x118073540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x118073800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x118073ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x118073d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x118074040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x118074300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1180745c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x118074880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x118074b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x118074e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1180750c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x118075380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x118075640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x118075900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x118075bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x118075e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x118076140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x118076400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1180766c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x118076980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x118076c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x118076f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1180771c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x118077480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x118077740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x118077a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x118077cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x118077f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x118078240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x118078500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1180787c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x118078a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x118078d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x118079000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1180792c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x118079580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x118079840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x118079b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x118079dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11807a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11807a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11807a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11807a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11807ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11807ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11807b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11807b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11807b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11807b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11807bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11807bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11807c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11807c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11807c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11807c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11807cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11807cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11807d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11807d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11807d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11807da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11807dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11807dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11807e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11807e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11807e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11807eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11807ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11807f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11807f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11807f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11807f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11807fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11807fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1180800c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x118080380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x118080640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x118080900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x118080bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x118080e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x118081140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x118081400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1180816c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x118081980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x118081ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x118082400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1180826c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x118082b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x118083000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1180834a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x118083c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x118083f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1180841d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x118084640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x118084ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x118084f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x118085390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x118085800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x118085c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1180860e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x118086550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1180869c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x118086e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1180872a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x118087710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x118087b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x118087ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x118088460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1180888d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x118088d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1180891b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x118089620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x118089a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x118089f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11808a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11808a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11808ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11808b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11808b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11808b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11808be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11808c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11808c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11808cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11808cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11808d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11808d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11808dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11808e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11808e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11808ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11808eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11808f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11808f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11808fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1180900a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x118090510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x118090980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x118090df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x118091260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1180916d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x118091b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x118091fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x118092420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x118092890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x118092d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x118093170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1180935e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x118093a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x118093ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x118094330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1180947a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x118094c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x118095080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1180954f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x118095960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x118095dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x118096240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1180966b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x118096b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x118096f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x118097400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x118097870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1180982e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x118098a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x118099120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x118099840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x118099b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11809a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11809a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11809abc0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x118f07b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x118f07fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x118f08450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x118f088c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x118f08d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x118f091a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x118f09610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x118f09a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x118f09ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x118f0a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x118f0a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x118f0aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x118f0ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x118f0c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x118f0c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x118f0d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x118f0d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x118f0df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x118f0e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x118f0ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x118f0f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x118f0fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x118f102e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x118f10a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x118f11120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x118f113e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x118f116a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x118f11b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x118f11f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x118f123f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x118f128f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x118f12e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x118f13270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x118f13530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x118f139a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x118f13e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x118f14370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x118f14870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x118f14d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x118f15270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x118f15770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x118f15c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x118f16170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x118f16670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x118f16b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x118f16fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x118f17450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x118f178c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x118f17d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x118f181a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x118f18610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x118f18a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x118f18ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x118f19360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x118f197d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x118f19fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x118f1a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x118f1a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x118f1ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x118f1b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x118f1b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x118f1be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x118f1c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x118f1c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x118f1cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x118f1d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x118f1d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x118f1da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x118f1dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x118f1e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x118f1e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x118f1ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x118f1f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x118f1f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x118f1fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x118f20110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x118f20660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x118f20bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x118f21100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x118f21650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x118f21ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x118f220f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x118f22640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x118f22b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x118f230e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x118f23630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x118f23b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x118f240d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x118f24620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x118f24b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x118f250c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x118f25610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x118f25b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x118f260b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x118f26600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x118f26b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x118f270a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x118f275f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x118f27b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x118f28090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x118f285e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x118f28b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x118f29080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x118f295d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x118f29b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x118f2a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x118f2a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x118f2ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x118f2b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x118f2b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x118f2bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x118f2c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x118f2c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x118f2ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x118f2cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x118f2d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x118f2d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x118f2dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x118f2e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x118f2e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x118f2eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x118f2ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x118f2f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x118f2f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x118f2fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x118f301c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x118f30660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x118f30b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x118f30fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x118f31440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x118f318e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x118f31d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x118f32220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x118f326c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x118f32b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x118f33000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x118f334a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x118f33940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x118f33de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x118f34280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x118f34720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x118f34bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x118f35060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x118f35500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x118f359a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x118f35e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x118f362e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x118f36780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x118f36c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x118f370c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x118f37560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x118f37a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x118f37ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x118f38340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x118f387e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x118f38c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x118f39120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x118f395c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x118f39a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x118f39f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x118f3a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x118f3a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x118f3ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x118f3b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x118f3b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x118f3bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x118f3bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x118f3c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x118f3c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x118f3cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x118f3d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x118f3d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x118f3db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x118f3dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x118f3e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x118f3e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x118f3eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x118f3f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x118f3f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x118f3fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x118f40020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x118f404c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x118f40960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x118f40e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x118f412a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x118f41740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x118f41be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x118f42080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x118f42520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x118f429c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x118f42e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x118f43300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x118f437a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x118f43cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x118f44240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x118f44790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x118f44ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x118f44fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x118f455b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x118f45bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x118f461d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x118f469c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x118f46e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x118f47120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x118f47730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x118f47d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x118f48530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x118f489d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x118f48e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x118f49310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x118f49ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x118f4a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x118f4a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x118f4aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x118f4b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x118f4b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x118f4baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x118f4bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x118f4c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x118f4ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x118f4cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x118f4d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x118f4da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x118f4dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x118f4e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x118f4ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x118f4efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x118f4f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x118f4fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x118f4ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x118f50500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x118f50a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x118f50fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x118f514f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x118f51a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x118f51f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x118f524e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x118f52a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x118f52f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x118f534d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x118f53a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x118f53f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x118f544c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x118f54a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x118f54f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x118f554b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x118f55a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x118f55f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x118f564a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x118f569f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x118f56f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x118f57490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x118f579e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x118f57f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x118f58480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x118f589d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x118f58f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x118f59470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x118f599c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x118f59f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x118f5a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x118f5a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x118f5af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x118f5b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x118f5b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x118f5bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x118f5c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x118f5c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x118f5cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x118f5d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x118f5d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x118f5db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x118f5e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x118f5e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x118f5e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x118f5ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x118f5f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x118f5f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x118f5fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x118f60060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x118f60500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x118f609a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x118f60ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x118f61610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x118f61d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x118f62450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x118f62b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x118f62e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x118f63620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x118f638e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x118f63ef0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.814s
user	0m0.280s
sys	0m0.336s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4732 (2eea03d8)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14100df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14100e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14100ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14100f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14100f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14100fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1410102e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x141010890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x141010e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x141011340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x141011840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x141011d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x141012860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x141013010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x141013820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x141013f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x141014660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x141014d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1410154a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x141015c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x141016390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x141016ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1410171d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x141017a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x141018190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x141018450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x141018a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1410196d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x141019c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x141019ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14101a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14101a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14101aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14101b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14101b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14101bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14101c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14101c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14101c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14101cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14101d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14101d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14101dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14101e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14101e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14101e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14101ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14101f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14101fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x141020480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x141020a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1410210a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1410216b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x141021cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1410224b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x141022950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x141022df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1410230b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1410236c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x141023eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x141024170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x141024610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x141024ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x141024f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1410253f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x141025890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x141025d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1410261d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x141026670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x141026b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x141026fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x141027450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1410278f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x141027e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x141028390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1410288e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x141028e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x141029380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1410298d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x141029e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14102a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14102a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14102ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14102b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14102b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14102be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14102c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14102c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14102cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14102d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14102d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14102dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14102e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14102e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14102edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14102f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14102f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14101f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14102fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x141030490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1410309e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x141030f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x141031480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1410319d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x141031f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x141032470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1410329c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x141032f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x141033460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1410339b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x141033f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x141034450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1410349a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x141034e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1410352e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x141035780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x141035c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1410360c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x141036560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x141036a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x141036ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x141037340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1410377e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x141037c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x141038120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1410385c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x141038a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x141038f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1410393a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x141039840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x141039ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14103a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14103a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14103aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14103af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14103b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14103b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14103bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14103c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14103c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14103cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14103cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14103d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14103d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14103dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14103e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14103e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14103eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14103f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14103f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14103f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14103fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1410402a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x141040740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x141040be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x141041080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x141041520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1410419c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x141041e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x141042300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1410427a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x141042c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1410430e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x141043580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x141043a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x141043ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x141044360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x141044800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x141044ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x141045140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1410455e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x141045a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x141045f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1410463c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x141046860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x141046d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1410471a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x141047640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x141047ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x141047f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x141048420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1410488c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x141048d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x141049200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1410496a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x141049b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x141049fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14104a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14104a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14104adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14104b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14104b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14104bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14104c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14104c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14104cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14104d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14104d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14104d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14104dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14104e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14104edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14104f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14104f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14104fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x141050140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x141050930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x141050dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x141051270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x141051710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x141051ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x141052410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x141052960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x141052eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x141053400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x141053950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x141053ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1410543f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x141054940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x141054e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1410553e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x141055930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x141055e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1410563d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x141056920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x141056e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1410573c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x141057910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x141057e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1410583b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x141058900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x141058e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1410593a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1410598f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x141059e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14105a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14105a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14105ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14105b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14105b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14105be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14105c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14105c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14105ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14105d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14105d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14105de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14105e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14105e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14105edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14105f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14105f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14105fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x141060330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x141060880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x141060dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x141061320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x141061870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x141061dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x141062310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x141062860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x141062db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x141063300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x141063850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x141063da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1410642f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x141064840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x141064ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x141065180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x141065620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x141065ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x141065f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x141066400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1410668a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x141066d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1410671e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x141067680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x141067b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x141067fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x141068460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x141068900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x141068da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1410692f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x141069a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14106a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14106a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14106af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14106b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14106ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14106bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14106c2f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.097.531 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.097.535 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1406055b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x140605a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x140605e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x140606300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x140606770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x140606be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x140607050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1406074c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x140607930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x140607da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x140608210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1406088d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1406093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x140609ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14060a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14060aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14060b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14060b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14060c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14060c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14060cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14060d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14060dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14060e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14060eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14060ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14060f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14060f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14060fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14060fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1406102e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x140610810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x140610c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x140610f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1406113b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x140611820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x140611c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x140612100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x140612570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1406129e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x140612e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1406132c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x140613730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x140613ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x140614010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x140614480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1406148f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x140614d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1406151d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x140615640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x140615ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x140615f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x140616390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x140616800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x140616c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1406170e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x140617650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x140617b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x140617fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x140618430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1406188a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x140618d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x140619180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1406195f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x140619a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x140619ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14061a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14061a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14061ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14061b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14061b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14061b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14061bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14061c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14061c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14061cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14061cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14061d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14061d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14061dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14061e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14061e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14061ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14061eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14061f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14061f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14061fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x140620070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1406204e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x140620950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x140620dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x140621230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1406216a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x140621b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x140621f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1406223f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x140622860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x140622cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x140623140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1406235b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x140623a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x140623e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x140624300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x140624770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x140624be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x140625050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1406254c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x140625930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x140625da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x140626210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x140626680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x140626af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x140626f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1406273d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x140627840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x140627cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x140628120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x140628590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x140628a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x140628e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1406292e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x140629750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x140629bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14062a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14062a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14062a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14062ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14062b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14062b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14062bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14062bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14062c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14062c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14062cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14062d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14062d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14062d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14062de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14062e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14062e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14062eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14062f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14062f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14062f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14062fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1406301d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x140630640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x140630ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x140630f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x140631390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x140631800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x140631c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1406320e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x140632550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1406329c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x140632e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1406332a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x140633710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x140633b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x140633ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x140634460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1406348d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x140634d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1406351b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x140635620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x140635a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1406366c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x140636980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x140636c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1406370b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x140637520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x140637990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x140637e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x140638270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1406386e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x140638b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x140638fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x140639430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1406398a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x140639d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14063a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14063a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14063aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14063aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14063b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14063b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14063bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14063c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14063c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14063c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14063cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14063d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14063d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14063db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14063dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14063e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14063e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14063ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14063f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14063f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14063fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14063feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x140640410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x140640920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x140640d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x140641200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x140641670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x140641ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x140642000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x140642510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x140643080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x140643340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x140643900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x140643ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x140644480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x140644a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x140645000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1406455c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x140645b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x140646140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x140646700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x140646cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x140647280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x140647840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x140647e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1406483c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x140648980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x140648f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x140649500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x140649ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14064a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14064a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14064ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14064b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14064b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14064bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14064c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14064c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14064ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14064d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14064da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14064dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14064e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14064eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14064f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14064f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14064fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x140650240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x140650800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x140650dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x140651380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x140651940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x140651f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1406524c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x140652a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x140653040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x140653600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x140653bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x140654180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x140654740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x140654d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1406552c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x140655880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x140655e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x140656400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1406569c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x140656f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x140657540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x140657a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x140657f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x140658440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x140658940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x140658e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x140659340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x140659840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x140659d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14065a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14065a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14065ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14065b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14065b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14065bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14065c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14065ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14065d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14065d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14065dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14065e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14065ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14065ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14065f330 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1412044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x141204950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x141204dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x141205230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1412056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x141205b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x141205f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1412063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x141206860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x141206cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x141207140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x141207870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x141208390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x141208b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x141209350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x141209a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14120a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14120a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14120afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14120b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14120be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14120c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14120cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14120d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14120daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14120dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14120e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14120e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14120e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14120ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14120f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14120f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14120fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14120fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1412102b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x141210720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x141210b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x141211000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x141211470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1412118e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x141211d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1412121c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x141212630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x141212aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x141212f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x141213380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1412137f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x141213c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1412140d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x141214540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1412149b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x141214e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x141215290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x141215700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x141215b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x141215fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x141216550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x141216a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x141216ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x141217330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1412177a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x141217c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x141218080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1412184f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x141218960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x141218dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x141219240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1412196b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x141219b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x141219f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14121a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14121a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14121ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14121b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14121b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14121ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14121bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14121c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14121c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14121cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14121d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14121d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14121d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14121ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14121e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14121e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14121eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14121ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14121f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14121f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14121fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x141220130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1412205a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x141220a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x141220e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1412212f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x141221760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x141221bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x141222040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1412224b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x141222920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x141222d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x141223200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x141223670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x141223fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1412242a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x141224710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x141224b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x141224ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x141225460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1412258d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x141225d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1412261b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x141226620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x141226a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x141226f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x141227370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1412277e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x141227c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1412280c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x141228530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1412289a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x141228e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x141229280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1412296f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x141229b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x141229fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14122a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14122a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14122ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14122b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14122b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14122ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14122bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14122c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14122c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14122cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14122d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14122d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14122d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14122ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14122e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14122e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14122eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14122efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14122f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14122f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14122fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x141230170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1412305e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x141230a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x141230ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x141231330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1412317a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x141231c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x141232080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1412324f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x141232960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x141232dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x141233240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1412336b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x141233b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x141233f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x141234400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x141234870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x141234ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x141235150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1412355c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x141235a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x141235ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x141236310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x141236780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x141236bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x141237060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1412374d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x141237940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x141237db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x141238220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x141238690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x141238b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x141238f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1412393e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x141239850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x141239cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14123a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14123a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14123aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14123ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14123b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14123b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14123bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14123c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14123c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14123c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14123cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14123d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14123d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14123dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14123df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14123e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14123e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14123eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14123f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14123f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14123f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14123fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1412402d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x141240860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x141240cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x141241140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x141241c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x141241f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x141242210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x141242680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x141242af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x141242f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1412433d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x141243840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x141243cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x141244120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x141244590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x141244a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x141244e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1412452e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x141245750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x141245bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x141246030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1412464a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x141246910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x141246d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1412471f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x141247660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x141247ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x141247f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1412483b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x141248820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x141248c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x141249100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x141249570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1412499e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x141249e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14124a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14124a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14124aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14124b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14124b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14124b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14124bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14124c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14124c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14124cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14124cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14124d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14124d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14124dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14124e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14124e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14124e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14124ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14124f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14124f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14124fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14124fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x141250460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1412508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x141250d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1412511b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x141251620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x141251a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x141251f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x141252370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1412527e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x141252c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1412530c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x141253530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1412539a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x141253e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x141254280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1412546f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x141254b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x141254fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x141255440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1412558b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x141256320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x141256a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x141257160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x141257880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x141257b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x141257fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1412585b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x141258bc0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.955s
user	0m0.230s
sys	0m0.185s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.43 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.63 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   2.06 sec*proc (2 tests)

Total Test time (real) =   2.07 sec
        2.09 real         0.51 user         0.25 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.24 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.30 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.54 sec*proc (2 tests)

Total Test time (real) =   0.55 sec
        0.56 real         0.13 user         0.08 sys
```
