Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.634s
user	0m0.884s
sys	0m1.211s
++ nproc
+ make -j10
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Built target sha256
[  5%] Built target sha1
[  5%] Built target build_info
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target xxhash
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 14%] Linking C shared library libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 15%] Linking CXX shared library libggml.dylib
[ 15%] Built target ggml
[ 15%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 19%] Linking CXX executable ../../bin/llama-gguf-hash
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Linking CXX executable ../../bin/llama-gguf
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 26%] Built target llama-gguf-hash
[ 26%] Linking CXX shared library libllama.dylib
[ 26%] Built target llama-gguf
[ 26%] Built target llama
[ 26%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 26%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 27%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 29%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 29%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 29%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 30%] Linking CXX executable ../../bin/llama-quantize-stats
[ 31%] Linking C executable ../bin/test-c
[ 32%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Linking CXX executable ../../bin/llama-simple
[ 34%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 35%] Built target llava
[ 36%] Linking CXX static library libllava_static.a
[ 37%] Linking CXX static library libcommon.a
[ 37%] Linking CXX shared library libllava_shared.dylib
[ 37%] Built target llama-simple
[ 37%] Built target test-c
[ 37%] Built target llama-simple-chat
[ 37%] Built target llama-quantize-stats
[ 37%] Built target llava_static
[ 37%] Built target common
[ 37%] Built target llava_shared
[ 39%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 43%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-0
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 46%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 48%] Linking CXX executable ../bin/test-log
[ 49%] Linking CXX executable ../bin/test-grammar-integration
[ 49%] Linking CXX executable ../bin/test-grammar-parser
[ 49%] Linking CXX executable ../bin/test-llama-grammar
[ 49%] Linking CXX executable ../bin/test-sampling
[ 50%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 50%] Linking CXX executable ../bin/test-arg-parser
[ 50%] Built target test-tokenizer-0
[ 50%] Built target test-tokenizer-1-bpe
[ 50%] Built target test-tokenizer-1-spm
[ 50%] Built target test-grammar-integration
[ 50%] Built target test-log
[ 50%] Built target test-llama-grammar
[ 50%] Built target test-grammar-parser
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Built target test-sampling
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Built target test-arg-parser
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 58%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-chat-template
[ 58%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-backend-ops
[ 59%] Linking CXX executable ../bin/test-model-load-cancel
[ 61%] Linking CXX executable ../bin/test-autorelease
[ 61%] Linking CXX executable ../bin/test-barrier
[ 59%] Linking CXX executable ../bin/test-gguf
[ 61%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 63%] Linking CXX executable ../../bin/llama-batched-bench
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Linking CXX executable ../bin/test-quantize-perf
[ 64%] Built target test-chat-template
[ 64%] Built target test-barrier
[ 64%] Built target test-model-load-cancel
[ 64%] Built target test-backend-ops
[ 64%] Built target test-gguf
[ 64%] Built target test-autorelease
[ 64%] Built target test-quantize-fns
[ 64%] Built target llama-batched-bench
[ 64%] Built target test-rope
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 64%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Built target test-quantize-perf
[ 65%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 66%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 67%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 67%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-eval-callback
[ 70%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Linking CXX executable ../../bin/llama-batched
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-embedding
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-gguf-split
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Built target llama-embedding
[ 73%] Built target llama-imatrix
[ 73%] Built target llama-gritlm
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-infill
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-batched
[ 73%] Built target llama-bench
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 78%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 79%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Built target llama-lookahead
[ 81%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Linking CXX executable ../../bin/llama-lookup
[ 81%] Linking CXX executable ../../bin/llama-lookup-stats
[ 81%] Linking CXX executable ../../bin/llama-lookup-merge
[ 81%] Linking CXX executable ../../bin/llama-lookup-create
[ 81%] Linking CXX executable ../../bin/llama-passkey
[ 82%] Linking CXX executable ../../bin/llama-cli
[ 82%] Linking CXX executable ../../bin/llama-parallel
[ 82%] Linking CXX executable ../../bin/llama-perplexity
[ 83%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Built target llama-lookup-merge
[ 83%] Built target llama-lookup-stats
[ 83%] Built target llama-parallel
[ 83%] Built target llama-lookup-create
[ 83%] Built target llama-lookup
[ 83%] Built target llama-passkey
[ 83%] Built target llama-cli
[ 83%] Built target llama-perplexity
[ 84%] Generating loading.html.hpp
[ 84%] Generating index.html.gz.hpp
[ 84%] Built target llama-quantize
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 85%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 86%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 86%] Built target llama-retrieval
[ 86%] Linking CXX executable ../../bin/llama-run
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 87%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 90%] Linking CXX executable ../../bin/llama-speculative
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-save-load-state
[ 93%] Built target llama-gen-docs
[ 93%] Built target llama-speculative
[ 93%] Built target llama-run
[ 93%] Built target llama-speculative-simple
[ 93%] Built target llama-convert-llama2c-to-ggml
[ 93%] Built target llama-tokenize
[ 93%] Built target llama-tts
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 97%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-llava-cli
[ 98%] Linking CXX executable ../../bin/llama-export-lora
[ 98%] Built target llama-cvector-generator
[ 98%] Linking CXX executable ../../bin/llama-q8dot
[ 98%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-vdot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.143s
user	0m5.927s
sys	0m9.777s

main: quantize time =  6803.37 ms
main:    total time =  6803.37 ms

main: quantize time =  1275.45 ms
main:    total time =  1275.45 ms

main: quantize time =  1330.51 ms
main:    total time =  1330.51 ms

main: quantize time =  2089.19 ms
main:    total time =  2089.19 ms

main: quantize time =  2263.47 ms
main:    total time =  2263.47 ms

main: quantize time =  4987.13 ms
main:    total time =  4987.13 ms

main: quantize time =  6150.79 ms
main:    total time =  6150.79 ms

main: quantize time =  6937.73 ms
main:    total time =  6937.73 ms

main: quantize time =  5789.37 ms
main:    total time =  5789.37 ms

main: quantize time =  4496.40 ms
main:    total time =  4496.40 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.202 I build: 4489 (f11cfdfd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.350 I main: llama backend init
0.00.000.358 I main: load the model and apply lora adapter, if any
0.00.057.914 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.070.430 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.070.444 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.070.447 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.070.448 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.070.449 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.070.449 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.070.449 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.070.451 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.070.452 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.070.453 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.070.453 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.070.454 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.070.459 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.070.460 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.070.463 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.070.463 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.070.464 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.077.620 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.079.784 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.086.675 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.086.681 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.086.682 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.086.683 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.086.683 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.086.684 I llama_model_loader: - type  f32:  194 tensors
0.00.086.685 I llama_model_loader: - type  f16:   98 tensors
0.00.086.686 I print_info: file format = GGUF V3 (latest)
0.00.086.687 I print_info: file type   = all F32 (guessed)
0.00.086.689 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.106.619 I load: special tokens cache size = 25
0.00.112.610 I load: token to piece cache size = 0.2984 MB
0.00.112.615 I print_info: arch             = gptneox
0.00.112.615 I print_info: vocab_only       = 0
0.00.112.616 I print_info: n_ctx_train      = 2048
0.00.112.616 I print_info: n_embd           = 2048
0.00.112.616 I print_info: n_layer          = 24
0.00.112.623 I print_info: n_head           = 16
0.00.112.624 I print_info: n_head_kv        = 16
0.00.112.624 I print_info: n_rot            = 32
0.00.112.624 I print_info: n_swa            = 0
0.00.112.624 I print_info: n_embd_head_k    = 128
0.00.112.624 I print_info: n_embd_head_v    = 128
0.00.112.625 I print_info: n_gqa            = 1
0.00.112.627 I print_info: n_embd_k_gqa     = 2048
0.00.112.628 I print_info: n_embd_v_gqa     = 2048
0.00.112.629 I print_info: f_norm_eps       = 1.0e-05
0.00.112.629 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.112.629 I print_info: f_clamp_kqv      = 0.0e+00
0.00.112.629 I print_info: f_max_alibi_bias = 0.0e+00
0.00.112.630 I print_info: f_logit_scale    = 0.0e+00
0.00.112.630 I print_info: n_ff             = 8192
0.00.112.631 I print_info: n_expert         = 0
0.00.112.631 I print_info: n_expert_used    = 0
0.00.112.631 I print_info: causal attn      = 1
0.00.112.631 I print_info: pooling type     = 0
0.00.112.631 I print_info: rope type        = 2
0.00.112.631 I print_info: rope scaling     = linear
0.00.112.632 I print_info: freq_base_train  = 10000.0
0.00.112.632 I print_info: freq_scale_train = 1
0.00.112.632 I print_info: n_ctx_orig_yarn  = 2048
0.00.112.632 I print_info: rope_finetuned   = unknown
0.00.112.632 I print_info: ssm_d_conv       = 0
0.00.112.632 I print_info: ssm_d_inner      = 0
0.00.112.633 I print_info: ssm_d_state      = 0
0.00.112.633 I print_info: ssm_dt_rank      = 0
0.00.112.634 I print_info: ssm_dt_b_c_rms   = 0
0.00.112.635 I print_info: model type       = 1.4B
0.00.112.635 I print_info: model params     = 1.41 B
0.00.112.635 I print_info: general.name     = 1.4B
0.00.112.636 I print_info: vocab type       = BPE
0.00.112.636 I print_info: n_vocab          = 50304
0.00.112.636 I print_info: n_merges         = 50009
0.00.112.636 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.112.636 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.112.636 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.112.637 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.112.637 I print_info: LF token         = 128 'Ä'
0.00.112.637 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.112.637 I print_info: max token length = 1024
0.00.115.084 I load_tensors: offloading 24 repeating layers to GPU
0.00.115.084 I load_tensors: offloading output layer to GPU
0.00.115.084 I load_tensors: offloaded 25/25 layers to GPU
0.00.115.104 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.115.106 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.115.415 I llama_init_from_model: n_seq_max     = 1
0.00.115.416 I llama_init_from_model: n_ctx         = 2048
0.00.115.416 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.115.416 I llama_init_from_model: n_batch       = 2048
0.00.115.417 I llama_init_from_model: n_ubatch      = 512
0.00.115.417 I llama_init_from_model: flash_attn    = 0
0.00.115.417 I llama_init_from_model: freq_base     = 10000.0
0.00.115.417 I llama_init_from_model: freq_scale    = 1
0.00.115.418 I ggml_metal_init: allocating
0.00.115.422 I ggml_metal_init: found device: Apple M4
0.00.115.424 I ggml_metal_init: picking default device: Apple M4
0.00.116.080 I ggml_metal_init: using embedded metal library
0.00.162.932 I ggml_metal_init: GPU name:   Apple M4
0.00.162.937 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.162.938 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.162.938 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.162.938 I ggml_metal_init: simdgroup reduction   = true
0.00.162.938 I ggml_metal_init: simdgroup matrix mul. = true
0.00.162.939 I ggml_metal_init: has bfloat            = true
0.00.162.939 I ggml_metal_init: use bfloat            = true
0.00.162.939 I ggml_metal_init: hasUnifiedMemory      = true
0.00.162.945 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.310.679 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.343.015 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.343.022 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.343.046 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.344.099 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.344.100 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.344.100 I llama_init_from_model: graph nodes  = 967
0.00.344.101 I llama_init_from_model: graph splits = 2
0.00.344.104 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.344.237 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.344.238 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.431.971 I main: llama threadpool init, n_threads = 4
0.00.432.014 I 
0.00.432.036 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.432.036 I 
0.00.432.247 I sampler seed: 1234
0.00.432.251 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.432.276 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.432.278 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.432.278 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.289.602 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52129.22 tokens per second)
0.02.289.603 I llama_perf_context_print:        load time =     374.05 ms
0.02.289.604 I llama_perf_context_print: prompt eval time =      43.46 ms /     7 tokens (    6.21 ms per token,   161.08 tokens per second)
0.02.289.604 I llama_perf_context_print:        eval time =    1811.09 ms /    63 runs   (   28.75 ms per token,    34.79 tokens per second)
0.02.289.605 I llama_perf_context_print:       total time =    1857.64 ms /    70 tokens
0.02.289.895 I ggml_metal_free: deallocating

real	0m2.620s
user	0m0.140s
sys	0m0.119s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4489 (f11cfdfd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.010.118 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.723 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.024.728 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.733 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.734 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.734 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.734 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.734 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.736 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.736 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.736 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.737 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.737 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.737 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.738 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.740 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.740 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.741 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.472 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.516 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.334 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.336 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.337 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.337 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.338 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.338 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.339 I llama_model_loader: - type  f32:  194 tensors
0.00.033.339 I llama_model_loader: - type q8_0:   98 tensors
0.00.033.340 I print_info: file format = GGUF V3 (latest)
0.00.033.346 I print_info: file type   = Q8_0
0.00.033.349 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.053.486 I load: special tokens cache size = 25
0.00.059.669 I load: token to piece cache size = 0.2984 MB
0.00.059.674 I print_info: arch             = gptneox
0.00.059.674 I print_info: vocab_only       = 0
0.00.059.674 I print_info: n_ctx_train      = 2048
0.00.059.675 I print_info: n_embd           = 2048
0.00.059.677 I print_info: n_layer          = 24
0.00.059.683 I print_info: n_head           = 16
0.00.059.684 I print_info: n_head_kv        = 16
0.00.059.684 I print_info: n_rot            = 32
0.00.059.686 I print_info: n_swa            = 0
0.00.059.686 I print_info: n_embd_head_k    = 128
0.00.059.687 I print_info: n_embd_head_v    = 128
0.00.059.687 I print_info: n_gqa            = 1
0.00.059.688 I print_info: n_embd_k_gqa     = 2048
0.00.059.689 I print_info: n_embd_v_gqa     = 2048
0.00.059.689 I print_info: f_norm_eps       = 1.0e-05
0.00.059.690 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.059.690 I print_info: f_clamp_kqv      = 0.0e+00
0.00.059.690 I print_info: f_max_alibi_bias = 0.0e+00
0.00.059.691 I print_info: f_logit_scale    = 0.0e+00
0.00.059.691 I print_info: n_ff             = 8192
0.00.059.691 I print_info: n_expert         = 0
0.00.059.692 I print_info: n_expert_used    = 0
0.00.059.692 I print_info: causal attn      = 1
0.00.059.693 I print_info: pooling type     = 0
0.00.059.693 I print_info: rope type        = 2
0.00.059.694 I print_info: rope scaling     = linear
0.00.059.694 I print_info: freq_base_train  = 10000.0
0.00.059.695 I print_info: freq_scale_train = 1
0.00.059.695 I print_info: n_ctx_orig_yarn  = 2048
0.00.059.695 I print_info: rope_finetuned   = unknown
0.00.059.695 I print_info: ssm_d_conv       = 0
0.00.059.695 I print_info: ssm_d_inner      = 0
0.00.059.695 I print_info: ssm_d_state      = 0
0.00.059.695 I print_info: ssm_dt_rank      = 0
0.00.059.695 I print_info: ssm_dt_b_c_rms   = 0
0.00.059.696 I print_info: model type       = 1.4B
0.00.059.696 I print_info: model params     = 1.41 B
0.00.059.696 I print_info: general.name     = 1.4B
0.00.059.697 I print_info: vocab type       = BPE
0.00.059.697 I print_info: n_vocab          = 50304
0.00.059.697 I print_info: n_merges         = 50009
0.00.059.697 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.059.699 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.059.699 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.059.699 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.059.699 I print_info: LF token         = 128 'Ä'
0.00.059.700 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.059.700 I print_info: max token length = 1024
0.00.062.121 I load_tensors: offloading 24 repeating layers to GPU
0.00.062.121 I load_tensors: offloading output layer to GPU
0.00.062.121 I load_tensors: offloaded 25/25 layers to GPU
0.00.062.133 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.062.135 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.062.480 I llama_init_from_model: n_seq_max     = 1
0.00.062.480 I llama_init_from_model: n_ctx         = 2048
0.00.062.481 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.062.481 I llama_init_from_model: n_batch       = 2048
0.00.062.481 I llama_init_from_model: n_ubatch      = 512
0.00.062.481 I llama_init_from_model: flash_attn    = 0
0.00.062.481 I llama_init_from_model: freq_base     = 10000.0
0.00.062.482 I llama_init_from_model: freq_scale    = 1
0.00.062.482 I ggml_metal_init: allocating
0.00.062.486 I ggml_metal_init: found device: Apple M4
0.00.062.487 I ggml_metal_init: picking default device: Apple M4
0.00.063.231 I ggml_metal_init: using embedded metal library
0.00.065.837 I ggml_metal_init: GPU name:   Apple M4
0.00.065.839 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.065.839 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.065.839 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.065.840 I ggml_metal_init: simdgroup reduction   = true
0.00.065.840 I ggml_metal_init: simdgroup matrix mul. = true
0.00.065.840 I ggml_metal_init: has bfloat            = true
0.00.065.840 I ggml_metal_init: use bfloat            = true
0.00.065.841 I ggml_metal_init: hasUnifiedMemory      = true
0.00.065.842 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.076.316 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.100.855 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.100.867 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.100.895 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.102.199 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.102.201 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.102.202 I llama_init_from_model: graph nodes  = 967
0.00.102.202 I llama_init_from_model: graph splits = 2
0.00.102.206 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.102.335 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.102.336 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.238.284 I main: llama threadpool init, n_threads = 4
0.01.238.320 I 
0.01.238.342 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.238.343 I 
0.01.238.492 I sampler seed: 1234
0.01.238.497 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.238.535 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.238.539 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.238.539 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.331.332 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60220.53 tokens per second)
0.02.331.333 I llama_perf_context_print:        load time =    1228.16 ms
0.02.331.334 I llama_perf_context_print: prompt eval time =      43.77 ms /     7 tokens (    6.25 ms per token,   159.95 tokens per second)
0.02.331.335 I llama_perf_context_print:        eval time =    1046.01 ms /    63 runs   (   16.60 ms per token,    60.23 tokens per second)
0.02.331.335 I llama_perf_context_print:       total time =    1093.05 ms /    70 tokens
0.02.331.570 I ggml_metal_free: deallocating

real	0m2.351s
user	0m0.113s
sys	0m0.210s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4489 (f11cfdfd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.015.717 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.221 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.024.227 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.228 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.229 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.229 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.230 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.230 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.232 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.232 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.233 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.233 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.233 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.233 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.234 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.236 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.236 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.237 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.095 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.170 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.431 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.432 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.433 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.433 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.433 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.434 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.033.434 I llama_model_loader: - type  f32:  194 tensors
0.00.033.434 I llama_model_loader: - type q4_0:   97 tensors
0.00.033.435 I llama_model_loader: - type q6_K:    1 tensors
0.00.033.435 I print_info: file format = GGUF V3 (latest)
0.00.033.436 I print_info: file type   = Q4_0
0.00.033.437 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.057.710 I load: special tokens cache size = 25
0.00.064.313 I load: token to piece cache size = 0.2984 MB
0.00.064.317 I print_info: arch             = gptneox
0.00.064.317 I print_info: vocab_only       = 0
0.00.064.317 I print_info: n_ctx_train      = 2048
0.00.064.317 I print_info: n_embd           = 2048
0.00.064.317 I print_info: n_layer          = 24
0.00.064.321 I print_info: n_head           = 16
0.00.064.322 I print_info: n_head_kv        = 16
0.00.064.322 I print_info: n_rot            = 32
0.00.064.322 I print_info: n_swa            = 0
0.00.064.322 I print_info: n_embd_head_k    = 128
0.00.064.323 I print_info: n_embd_head_v    = 128
0.00.064.323 I print_info: n_gqa            = 1
0.00.064.324 I print_info: n_embd_k_gqa     = 2048
0.00.064.324 I print_info: n_embd_v_gqa     = 2048
0.00.064.325 I print_info: f_norm_eps       = 1.0e-05
0.00.064.325 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.064.325 I print_info: f_clamp_kqv      = 0.0e+00
0.00.064.326 I print_info: f_max_alibi_bias = 0.0e+00
0.00.064.326 I print_info: f_logit_scale    = 0.0e+00
0.00.064.326 I print_info: n_ff             = 8192
0.00.064.328 I print_info: n_expert         = 0
0.00.064.328 I print_info: n_expert_used    = 0
0.00.064.328 I print_info: causal attn      = 1
0.00.064.328 I print_info: pooling type     = 0
0.00.064.328 I print_info: rope type        = 2
0.00.064.329 I print_info: rope scaling     = linear
0.00.064.329 I print_info: freq_base_train  = 10000.0
0.00.064.330 I print_info: freq_scale_train = 1
0.00.064.330 I print_info: n_ctx_orig_yarn  = 2048
0.00.064.331 I print_info: rope_finetuned   = unknown
0.00.064.331 I print_info: ssm_d_conv       = 0
0.00.064.331 I print_info: ssm_d_inner      = 0
0.00.064.331 I print_info: ssm_d_state      = 0
0.00.064.331 I print_info: ssm_dt_rank      = 0
0.00.064.331 I print_info: ssm_dt_b_c_rms   = 0
0.00.064.332 I print_info: model type       = 1.4B
0.00.064.332 I print_info: model params     = 1.41 B
0.00.064.332 I print_info: general.name     = 1.4B
0.00.064.333 I print_info: vocab type       = BPE
0.00.064.333 I print_info: n_vocab          = 50304
0.00.064.333 I print_info: n_merges         = 50009
0.00.064.333 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.064.333 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.064.335 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.064.335 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.064.336 I print_info: LF token         = 128 'Ä'
0.00.064.336 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.064.336 I print_info: max token length = 1024
0.00.066.879 I load_tensors: offloading 24 repeating layers to GPU
0.00.066.879 I load_tensors: offloading output layer to GPU
0.00.066.880 I load_tensors: offloaded 25/25 layers to GPU
0.00.066.892 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.066.893 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.067.274 I llama_init_from_model: n_seq_max     = 1
0.00.067.275 I llama_init_from_model: n_ctx         = 2048
0.00.067.275 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.067.275 I llama_init_from_model: n_batch       = 2048
0.00.067.276 I llama_init_from_model: n_ubatch      = 512
0.00.067.276 I llama_init_from_model: flash_attn    = 0
0.00.067.276 I llama_init_from_model: freq_base     = 10000.0
0.00.067.277 I llama_init_from_model: freq_scale    = 1
0.00.067.277 I ggml_metal_init: allocating
0.00.067.280 I ggml_metal_init: found device: Apple M4
0.00.067.282 I ggml_metal_init: picking default device: Apple M4
0.00.068.081 I ggml_metal_init: using embedded metal library
0.00.071.171 I ggml_metal_init: GPU name:   Apple M4
0.00.071.173 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.071.174 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.071.174 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.071.174 I ggml_metal_init: simdgroup reduction   = true
0.00.071.175 I ggml_metal_init: simdgroup matrix mul. = true
0.00.071.175 I ggml_metal_init: has bfloat            = true
0.00.071.175 I ggml_metal_init: use bfloat            = true
0.00.071.175 I ggml_metal_init: hasUnifiedMemory      = true
0.00.071.176 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.081.966 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.108.565 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.108.572 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.108.594 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.109.801 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.109.803 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.109.803 I llama_init_from_model: graph nodes  = 967
0.00.109.803 I llama_init_from_model: graph splits = 2
0.00.109.807 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.109.937 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.109.937 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.718.658 I main: llama threadpool init, n_threads = 4
0.00.718.707 I 
0.00.718.733 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.718.734 I 
0.00.718.965 I sampler seed: 1234
0.00.718.970 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.719.026 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.719.030 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.719.030 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.394.066 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59019.12 tokens per second)
0.01.394.066 I llama_perf_context_print:        load time =     702.93 ms
0.01.394.067 I llama_perf_context_print: prompt eval time =      39.80 ms /     7 tokens (    5.69 ms per token,   175.90 tokens per second)
0.01.394.068 I llama_perf_context_print:        eval time =     632.31 ms /    63 runs   (   10.04 ms per token,    99.63 tokens per second)
0.01.394.068 I llama_perf_context_print:       total time =     675.41 ms /    70 tokens
0.01.394.287 I ggml_metal_free: deallocating

real	0m1.418s
user	0m0.118s
sys	0m0.148s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4489 (f11cfdfd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.792 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.513 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.027.517 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.518 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.518 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.519 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.519 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.521 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.523 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.523 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.524 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.524 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.524 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.525 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.525 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.528 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.528 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.529 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.243 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.231 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.003 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.004 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.005 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.005 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.005 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.005 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.036.006 I llama_model_loader: - type  f32:  194 tensors
0.00.036.006 I llama_model_loader: - type q4_1:   97 tensors
0.00.036.006 I llama_model_loader: - type q6_K:    1 tensors
0.00.036.007 I print_info: file format = GGUF V3 (latest)
0.00.036.007 I print_info: file type   = Q4_1
0.00.036.008 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.056.645 I load: special tokens cache size = 25
0.00.062.670 I load: token to piece cache size = 0.2984 MB
0.00.062.672 I print_info: arch             = gptneox
0.00.062.673 I print_info: vocab_only       = 0
0.00.062.673 I print_info: n_ctx_train      = 2048
0.00.062.673 I print_info: n_embd           = 2048
0.00.062.673 I print_info: n_layer          = 24
0.00.062.676 I print_info: n_head           = 16
0.00.062.677 I print_info: n_head_kv        = 16
0.00.062.677 I print_info: n_rot            = 32
0.00.062.677 I print_info: n_swa            = 0
0.00.062.678 I print_info: n_embd_head_k    = 128
0.00.062.679 I print_info: n_embd_head_v    = 128
0.00.062.679 I print_info: n_gqa            = 1
0.00.062.680 I print_info: n_embd_k_gqa     = 2048
0.00.062.681 I print_info: n_embd_v_gqa     = 2048
0.00.062.681 I print_info: f_norm_eps       = 1.0e-05
0.00.062.682 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.062.682 I print_info: f_clamp_kqv      = 0.0e+00
0.00.062.682 I print_info: f_max_alibi_bias = 0.0e+00
0.00.062.682 I print_info: f_logit_scale    = 0.0e+00
0.00.062.683 I print_info: n_ff             = 8192
0.00.062.683 I print_info: n_expert         = 0
0.00.062.683 I print_info: n_expert_used    = 0
0.00.062.684 I print_info: causal attn      = 1
0.00.062.684 I print_info: pooling type     = 0
0.00.062.685 I print_info: rope type        = 2
0.00.062.687 I print_info: rope scaling     = linear
0.00.062.688 I print_info: freq_base_train  = 10000.0
0.00.062.688 I print_info: freq_scale_train = 1
0.00.062.688 I print_info: n_ctx_orig_yarn  = 2048
0.00.062.688 I print_info: rope_finetuned   = unknown
0.00.062.688 I print_info: ssm_d_conv       = 0
0.00.062.689 I print_info: ssm_d_inner      = 0
0.00.062.689 I print_info: ssm_d_state      = 0
0.00.062.689 I print_info: ssm_dt_rank      = 0
0.00.062.689 I print_info: ssm_dt_b_c_rms   = 0
0.00.062.689 I print_info: model type       = 1.4B
0.00.062.689 I print_info: model params     = 1.41 B
0.00.062.690 I print_info: general.name     = 1.4B
0.00.062.690 I print_info: vocab type       = BPE
0.00.062.690 I print_info: n_vocab          = 50304
0.00.062.691 I print_info: n_merges         = 50009
0.00.062.691 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.062.691 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.062.691 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.062.695 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.062.696 I print_info: LF token         = 128 'Ä'
0.00.062.696 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.062.696 I print_info: max token length = 1024
0.00.064.488 I load_tensors: offloading 24 repeating layers to GPU
0.00.064.489 I load_tensors: offloading output layer to GPU
0.00.064.489 I load_tensors: offloaded 25/25 layers to GPU
0.00.064.494 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.064.495 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.064.772 I llama_init_from_model: n_seq_max     = 1
0.00.064.773 I llama_init_from_model: n_ctx         = 2048
0.00.064.773 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.064.773 I llama_init_from_model: n_batch       = 2048
0.00.064.773 I llama_init_from_model: n_ubatch      = 512
0.00.064.773 I llama_init_from_model: flash_attn    = 0
0.00.064.774 I llama_init_from_model: freq_base     = 10000.0
0.00.064.774 I llama_init_from_model: freq_scale    = 1
0.00.064.775 I ggml_metal_init: allocating
0.00.064.778 I ggml_metal_init: found device: Apple M4
0.00.064.780 I ggml_metal_init: picking default device: Apple M4
0.00.065.383 I ggml_metal_init: using embedded metal library
0.00.067.732 I ggml_metal_init: GPU name:   Apple M4
0.00.067.733 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.734 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.734 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.734 I ggml_metal_init: simdgroup reduction   = true
0.00.067.734 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.735 I ggml_metal_init: has bfloat            = true
0.00.067.735 I ggml_metal_init: use bfloat            = true
0.00.067.735 I ggml_metal_init: hasUnifiedMemory      = true
0.00.067.736 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.077.547 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.098.732 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.098.749 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.098.776 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.099.936 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.099.938 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.099.938 I llama_init_from_model: graph nodes  = 967
0.00.099.939 I llama_init_from_model: graph splits = 2
0.00.099.942 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.100.097 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.100.098 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.848.253 I main: llama threadpool init, n_threads = 4
0.00.848.294 I 
0.00.848.319 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.848.319 I 
0.00.848.544 I sampler seed: 1234
0.00.848.548 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.848.569 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.848.569 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.848.569 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.577.510 I llama_perf_sampler_print:    sampling time =       1.13 ms /    71 runs   (    0.02 ms per token, 62665.49 tokens per second)
0.01.577.511 I llama_perf_context_print:        load time =     839.46 ms
0.01.577.512 I llama_perf_context_print: prompt eval time =      45.98 ms /     7 tokens (    6.57 ms per token,   152.26 tokens per second)
0.01.577.513 I llama_perf_context_print:        eval time =     680.09 ms /    63 runs   (   10.80 ms per token,    92.64 tokens per second)
0.01.577.513 I llama_perf_context_print:       total time =     729.26 ms /    70 tokens
0.01.577.776 I ggml_metal_free: deallocating

real	0m1.593s
user	0m0.111s
sys	0m0.156s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4489 (f11cfdfd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.009.884 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.698 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.702 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.704 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.705 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.709 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.710 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.710 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.713 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.714 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.714 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.714 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.715 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.715 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.716 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.717 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.718 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.718 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.493 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.474 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.229 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.230 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.231 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.231 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.231 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.232 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.232 I llama_model_loader: - type  f32:  194 tensors
0.00.026.232 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.232 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.233 I print_info: file format = GGUF V3 (latest)
0.00.026.233 I print_info: file type   = Q5_0
0.00.026.234 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.045.725 I load: special tokens cache size = 25
0.00.051.502 I load: token to piece cache size = 0.2984 MB
0.00.051.505 I print_info: arch             = gptneox
0.00.051.506 I print_info: vocab_only       = 0
0.00.051.506 I print_info: n_ctx_train      = 2048
0.00.051.506 I print_info: n_embd           = 2048
0.00.051.506 I print_info: n_layer          = 24
0.00.051.509 I print_info: n_head           = 16
0.00.051.510 I print_info: n_head_kv        = 16
0.00.051.510 I print_info: n_rot            = 32
0.00.051.510 I print_info: n_swa            = 0
0.00.051.510 I print_info: n_embd_head_k    = 128
0.00.051.511 I print_info: n_embd_head_v    = 128
0.00.051.511 I print_info: n_gqa            = 1
0.00.051.512 I print_info: n_embd_k_gqa     = 2048
0.00.051.513 I print_info: n_embd_v_gqa     = 2048
0.00.051.513 I print_info: f_norm_eps       = 1.0e-05
0.00.051.514 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.514 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.514 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.514 I print_info: f_logit_scale    = 0.0e+00
0.00.051.515 I print_info: n_ff             = 8192
0.00.051.517 I print_info: n_expert         = 0
0.00.051.517 I print_info: n_expert_used    = 0
0.00.051.517 I print_info: causal attn      = 1
0.00.051.517 I print_info: pooling type     = 0
0.00.051.519 I print_info: rope type        = 2
0.00.051.521 I print_info: rope scaling     = linear
0.00.051.521 I print_info: freq_base_train  = 10000.0
0.00.051.521 I print_info: freq_scale_train = 1
0.00.051.521 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.522 I print_info: rope_finetuned   = unknown
0.00.051.522 I print_info: ssm_d_conv       = 0
0.00.051.522 I print_info: ssm_d_inner      = 0
0.00.051.522 I print_info: ssm_d_state      = 0
0.00.051.522 I print_info: ssm_dt_rank      = 0
0.00.051.522 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.523 I print_info: model type       = 1.4B
0.00.051.523 I print_info: model params     = 1.41 B
0.00.051.523 I print_info: general.name     = 1.4B
0.00.051.524 I print_info: vocab type       = BPE
0.00.051.524 I print_info: n_vocab          = 50304
0.00.051.524 I print_info: n_merges         = 50009
0.00.051.524 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.524 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.528 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.528 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.529 I print_info: LF token         = 128 'Ä'
0.00.051.529 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.530 I print_info: max token length = 1024
0.00.053.599 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.599 I load_tensors: offloading output layer to GPU
0.00.053.600 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.610 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.611 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.916 I llama_init_from_model: n_seq_max     = 1
0.00.053.917 I llama_init_from_model: n_ctx         = 2048
0.00.053.917 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.917 I llama_init_from_model: n_batch       = 2048
0.00.053.917 I llama_init_from_model: n_ubatch      = 512
0.00.053.917 I llama_init_from_model: flash_attn    = 0
0.00.053.918 I llama_init_from_model: freq_base     = 10000.0
0.00.053.918 I llama_init_from_model: freq_scale    = 1
0.00.053.918 I ggml_metal_init: allocating
0.00.053.922 I ggml_metal_init: found device: Apple M4
0.00.053.924 I ggml_metal_init: picking default device: Apple M4
0.00.054.531 I ggml_metal_init: using embedded metal library
0.00.056.891 I ggml_metal_init: GPU name:   Apple M4
0.00.056.892 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.892 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.893 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.893 I ggml_metal_init: simdgroup reduction   = true
0.00.056.893 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.893 I ggml_metal_init: has bfloat            = true
0.00.056.893 I ggml_metal_init: use bfloat            = true
0.00.056.894 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.894 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.778 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.429 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.440 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.462 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.567 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.568 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.569 I llama_init_from_model: graph nodes  = 967
0.00.087.569 I llama_init_from_model: graph splits = 2
0.00.087.572 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.718 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.718 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.755.898 I main: llama threadpool init, n_threads = 4
0.00.755.941 I 
0.00.755.963 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.755.963 I 
0.00.756.191 I sampler seed: 1234
0.00.756.195 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.756.217 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.756.217 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.756.217 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.543.770 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55686.27 tokens per second)
0.01.543.771 I llama_perf_context_print:        load time =     746.01 ms
0.01.543.771 I llama_perf_context_print: prompt eval time =      43.06 ms /     7 tokens (    6.15 ms per token,   162.55 tokens per second)
0.01.543.772 I llama_perf_context_print:        eval time =     741.38 ms /    63 runs   (   11.77 ms per token,    84.98 tokens per second)
0.01.543.772 I llama_perf_context_print:       total time =     787.87 ms /    70 tokens
0.01.543.980 I ggml_metal_free: deallocating

real	0m1.562s
user	0m0.110s
sys	0m0.150s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4489 (f11cfdfd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.008.829 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.101 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.106 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.107 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.108 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.108 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.108 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.109 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.112 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.114 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.115 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.115 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.115 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.116 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.116 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.120 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.121 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.125 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.807 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.780 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.403 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.404 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.404 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.404 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.405 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.405 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.406 I llama_model_loader: - type  f32:  194 tensors
0.00.025.406 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.406 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.407 I print_info: file format = GGUF V3 (latest)
0.00.025.407 I print_info: file type   = Q5_1
0.00.025.408 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.044.067 I load: special tokens cache size = 25
0.00.050.015 I load: token to piece cache size = 0.2984 MB
0.00.050.018 I print_info: arch             = gptneox
0.00.050.018 I print_info: vocab_only       = 0
0.00.050.018 I print_info: n_ctx_train      = 2048
0.00.050.018 I print_info: n_embd           = 2048
0.00.050.019 I print_info: n_layer          = 24
0.00.050.022 I print_info: n_head           = 16
0.00.050.023 I print_info: n_head_kv        = 16
0.00.050.023 I print_info: n_rot            = 32
0.00.050.023 I print_info: n_swa            = 0
0.00.050.023 I print_info: n_embd_head_k    = 128
0.00.050.023 I print_info: n_embd_head_v    = 128
0.00.050.024 I print_info: n_gqa            = 1
0.00.050.025 I print_info: n_embd_k_gqa     = 2048
0.00.050.025 I print_info: n_embd_v_gqa     = 2048
0.00.050.026 I print_info: f_norm_eps       = 1.0e-05
0.00.050.026 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.026 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.027 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.027 I print_info: f_logit_scale    = 0.0e+00
0.00.050.029 I print_info: n_ff             = 8192
0.00.050.029 I print_info: n_expert         = 0
0.00.050.030 I print_info: n_expert_used    = 0
0.00.050.030 I print_info: causal attn      = 1
0.00.050.030 I print_info: pooling type     = 0
0.00.050.032 I print_info: rope type        = 2
0.00.050.032 I print_info: rope scaling     = linear
0.00.050.032 I print_info: freq_base_train  = 10000.0
0.00.050.032 I print_info: freq_scale_train = 1
0.00.050.033 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.033 I print_info: rope_finetuned   = unknown
0.00.050.033 I print_info: ssm_d_conv       = 0
0.00.050.033 I print_info: ssm_d_inner      = 0
0.00.050.033 I print_info: ssm_d_state      = 0
0.00.050.033 I print_info: ssm_dt_rank      = 0
0.00.050.033 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.034 I print_info: model type       = 1.4B
0.00.050.034 I print_info: model params     = 1.41 B
0.00.050.034 I print_info: general.name     = 1.4B
0.00.050.039 I print_info: vocab type       = BPE
0.00.050.039 I print_info: n_vocab          = 50304
0.00.050.040 I print_info: n_merges         = 50009
0.00.050.041 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.041 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.041 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.041 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.041 I print_info: LF token         = 128 'Ä'
0.00.050.042 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.042 I print_info: max token length = 1024
0.00.052.016 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.016 I load_tensors: offloading output layer to GPU
0.00.052.016 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.027 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.028 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.052.307 I llama_init_from_model: n_seq_max     = 1
0.00.052.308 I llama_init_from_model: n_ctx         = 2048
0.00.052.308 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.308 I llama_init_from_model: n_batch       = 2048
0.00.052.309 I llama_init_from_model: n_ubatch      = 512
0.00.052.309 I llama_init_from_model: flash_attn    = 0
0.00.052.309 I llama_init_from_model: freq_base     = 10000.0
0.00.052.309 I llama_init_from_model: freq_scale    = 1
0.00.052.310 I ggml_metal_init: allocating
0.00.052.313 I ggml_metal_init: found device: Apple M4
0.00.052.314 I ggml_metal_init: picking default device: Apple M4
0.00.052.920 I ggml_metal_init: using embedded metal library
0.00.055.251 I ggml_metal_init: GPU name:   Apple M4
0.00.055.252 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.253 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.253 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.253 I ggml_metal_init: simdgroup reduction   = true
0.00.055.253 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.253 I ggml_metal_init: has bfloat            = true
0.00.055.254 I ggml_metal_init: use bfloat            = true
0.00.055.254 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.255 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.788 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.375 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.381 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.400 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.438 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.439 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.440 I llama_init_from_model: graph nodes  = 967
0.00.085.440 I llama_init_from_model: graph splits = 2
0.00.085.443 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.558 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.559 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.691.416 I main: llama threadpool init, n_threads = 4
0.00.691.461 I 
0.00.691.504 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.691.505 I 
0.00.691.738 I sampler seed: 1234
0.00.691.742 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.691.780 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.691.796 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.691.796 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.536.730 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53143.71 tokens per second)
0.01.536.731 I llama_perf_context_print:        load time =     682.58 ms
0.01.536.732 I llama_perf_context_print: prompt eval time =      46.13 ms /     7 tokens (    6.59 ms per token,   151.73 tokens per second)
0.01.536.733 I llama_perf_context_print:        eval time =     795.75 ms /    63 runs   (   12.63 ms per token,    79.17 tokens per second)
0.01.536.733 I llama_perf_context_print:       total time =     845.32 ms /    70 tokens
0.01.536.980 I ggml_metal_free: deallocating

real	0m1.553s
user	0m0.108s
sys	0m0.150s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4489 (f11cfdfd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.009.896 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.385 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.392 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.394 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.394 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.394 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.395 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.395 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.396 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.396 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.397 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.397 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.397 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.399 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.400 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.402 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.402 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.402 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.188 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.192 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.993 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.994 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.994 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.995 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.995 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.995 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.996 I llama_model_loader: - type  f32:  194 tensors
0.00.024.996 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.997 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.997 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.998 I print_info: file format = GGUF V3 (latest)
0.00.024.998 I print_info: file type   = Q2_K - Medium
0.00.025.000 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.044.816 I load: special tokens cache size = 25
0.00.050.911 I load: token to piece cache size = 0.2984 MB
0.00.050.915 I print_info: arch             = gptneox
0.00.050.915 I print_info: vocab_only       = 0
0.00.050.916 I print_info: n_ctx_train      = 2048
0.00.050.916 I print_info: n_embd           = 2048
0.00.050.916 I print_info: n_layer          = 24
0.00.050.921 I print_info: n_head           = 16
0.00.050.924 I print_info: n_head_kv        = 16
0.00.050.924 I print_info: n_rot            = 32
0.00.050.924 I print_info: n_swa            = 0
0.00.050.925 I print_info: n_embd_head_k    = 128
0.00.050.925 I print_info: n_embd_head_v    = 128
0.00.050.926 I print_info: n_gqa            = 1
0.00.050.926 I print_info: n_embd_k_gqa     = 2048
0.00.050.927 I print_info: n_embd_v_gqa     = 2048
0.00.050.927 I print_info: f_norm_eps       = 1.0e-05
0.00.050.928 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.928 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.929 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.929 I print_info: f_logit_scale    = 0.0e+00
0.00.050.930 I print_info: n_ff             = 8192
0.00.050.955 I print_info: n_expert         = 0
0.00.050.957 I print_info: n_expert_used    = 0
0.00.050.957 I print_info: causal attn      = 1
0.00.050.957 I print_info: pooling type     = 0
0.00.050.957 I print_info: rope type        = 2
0.00.050.960 I print_info: rope scaling     = linear
0.00.050.961 I print_info: freq_base_train  = 10000.0
0.00.050.962 I print_info: freq_scale_train = 1
0.00.050.962 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.962 I print_info: rope_finetuned   = unknown
0.00.050.962 I print_info: ssm_d_conv       = 0
0.00.050.962 I print_info: ssm_d_inner      = 0
0.00.050.963 I print_info: ssm_d_state      = 0
0.00.050.963 I print_info: ssm_dt_rank      = 0
0.00.050.963 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.963 I print_info: model type       = 1.4B
0.00.050.964 I print_info: model params     = 1.41 B
0.00.050.964 I print_info: general.name     = 1.4B
0.00.050.965 I print_info: vocab type       = BPE
0.00.050.965 I print_info: n_vocab          = 50304
0.00.050.965 I print_info: n_merges         = 50009
0.00.050.965 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.966 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.966 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.966 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.966 I print_info: LF token         = 128 'Ä'
0.00.050.969 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.969 I print_info: max token length = 1024
0.00.052.783 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.783 I load_tensors: offloading output layer to GPU
0.00.052.784 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.794 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.795 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.053.201 I llama_init_from_model: n_seq_max     = 1
0.00.053.202 I llama_init_from_model: n_ctx         = 2048
0.00.053.202 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.202 I llama_init_from_model: n_batch       = 2048
0.00.053.202 I llama_init_from_model: n_ubatch      = 512
0.00.053.203 I llama_init_from_model: flash_attn    = 0
0.00.053.203 I llama_init_from_model: freq_base     = 10000.0
0.00.053.203 I llama_init_from_model: freq_scale    = 1
0.00.053.204 I ggml_metal_init: allocating
0.00.053.208 I ggml_metal_init: found device: Apple M4
0.00.053.210 I ggml_metal_init: picking default device: Apple M4
0.00.053.871 I ggml_metal_init: using embedded metal library
0.00.056.403 I ggml_metal_init: GPU name:   Apple M4
0.00.056.405 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.405 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.406 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.407 I ggml_metal_init: simdgroup reduction   = true
0.00.056.408 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.408 I ggml_metal_init: has bfloat            = true
0.00.056.408 I ggml_metal_init: use bfloat            = true
0.00.056.408 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.409 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.737 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.104 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.115 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.138 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.092 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.094 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.094 I llama_init_from_model: graph nodes  = 967
0.00.087.094 I llama_init_from_model: graph splits = 2
0.00.087.098 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.214 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.214 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.439.915 I main: llama threadpool init, n_threads = 4
0.00.439.960 I 
0.00.439.982 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.439.983 I 
0.00.440.212 I sampler seed: 1234
0.00.440.216 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.440.227 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.440.228 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.440.228 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.119.715 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59513.83 tokens per second)
0.01.119.716 I llama_perf_context_print:        load time =     430.01 ms
0.01.119.717 I llama_perf_context_print: prompt eval time =      35.76 ms /     7 tokens (    5.11 ms per token,   195.75 tokens per second)
0.01.119.717 I llama_perf_context_print:        eval time =     640.68 ms /    63 runs   (   10.17 ms per token,    98.33 tokens per second)
0.01.119.718 I llama_perf_context_print:       total time =     679.80 ms /    70 tokens
0.01.119.949 I ggml_metal_free: deallocating

real	0m1.139s
user	0m0.110s
sys	0m0.112s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4489 (f11cfdfd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.819 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.226 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.231 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.233 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.234 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.234 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.234 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.235 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.239 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.239 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.239 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.240 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.240 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.240 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.245 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.248 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.249 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.249 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.930 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.919 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.607 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.608 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.609 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.609 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.609 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.609 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.610 I llama_model_loader: - type  f32:  194 tensors
0.00.025.610 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.610 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.611 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.611 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.611 I print_info: file format = GGUF V3 (latest)
0.00.025.612 I print_info: file type   = Q3_K - Medium
0.00.025.613 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.044.149 I load: special tokens cache size = 25
0.00.050.070 I load: token to piece cache size = 0.2984 MB
0.00.050.073 I print_info: arch             = gptneox
0.00.050.073 I print_info: vocab_only       = 0
0.00.050.073 I print_info: n_ctx_train      = 2048
0.00.050.073 I print_info: n_embd           = 2048
0.00.050.074 I print_info: n_layer          = 24
0.00.050.076 I print_info: n_head           = 16
0.00.050.077 I print_info: n_head_kv        = 16
0.00.050.077 I print_info: n_rot            = 32
0.00.050.077 I print_info: n_swa            = 0
0.00.050.078 I print_info: n_embd_head_k    = 128
0.00.050.078 I print_info: n_embd_head_v    = 128
0.00.050.079 I print_info: n_gqa            = 1
0.00.050.079 I print_info: n_embd_k_gqa     = 2048
0.00.050.080 I print_info: n_embd_v_gqa     = 2048
0.00.050.080 I print_info: f_norm_eps       = 1.0e-05
0.00.050.081 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.081 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.081 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.081 I print_info: f_logit_scale    = 0.0e+00
0.00.050.084 I print_info: n_ff             = 8192
0.00.050.084 I print_info: n_expert         = 0
0.00.050.084 I print_info: n_expert_used    = 0
0.00.050.086 I print_info: causal attn      = 1
0.00.050.087 I print_info: pooling type     = 0
0.00.050.087 I print_info: rope type        = 2
0.00.050.087 I print_info: rope scaling     = linear
0.00.050.088 I print_info: freq_base_train  = 10000.0
0.00.050.088 I print_info: freq_scale_train = 1
0.00.050.088 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.089 I print_info: rope_finetuned   = unknown
0.00.050.089 I print_info: ssm_d_conv       = 0
0.00.050.089 I print_info: ssm_d_inner      = 0
0.00.050.089 I print_info: ssm_d_state      = 0
0.00.050.089 I print_info: ssm_dt_rank      = 0
0.00.050.089 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.090 I print_info: model type       = 1.4B
0.00.050.090 I print_info: model params     = 1.41 B
0.00.050.090 I print_info: general.name     = 1.4B
0.00.050.091 I print_info: vocab type       = BPE
0.00.050.091 I print_info: n_vocab          = 50304
0.00.050.091 I print_info: n_merges         = 50009
0.00.050.091 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.091 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.092 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.092 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.092 I print_info: LF token         = 128 'Ä'
0.00.050.092 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.093 I print_info: max token length = 1024
0.00.052.014 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.014 I load_tensors: offloading output layer to GPU
0.00.052.015 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.025 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.026 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.052.308 I llama_init_from_model: n_seq_max     = 1
0.00.052.309 I llama_init_from_model: n_ctx         = 2048
0.00.052.309 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.309 I llama_init_from_model: n_batch       = 2048
0.00.052.309 I llama_init_from_model: n_ubatch      = 512
0.00.052.309 I llama_init_from_model: flash_attn    = 0
0.00.052.310 I llama_init_from_model: freq_base     = 10000.0
0.00.052.310 I llama_init_from_model: freq_scale    = 1
0.00.052.310 I ggml_metal_init: allocating
0.00.052.313 I ggml_metal_init: found device: Apple M4
0.00.052.315 I ggml_metal_init: picking default device: Apple M4
0.00.052.915 I ggml_metal_init: using embedded metal library
0.00.055.275 I ggml_metal_init: GPU name:   Apple M4
0.00.055.276 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.276 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.277 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.277 I ggml_metal_init: simdgroup reduction   = true
0.00.055.277 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.277 I ggml_metal_init: has bfloat            = true
0.00.055.277 I ggml_metal_init: use bfloat            = true
0.00.055.278 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.278 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.866 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.964 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.973 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.006 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.084.990 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.084.991 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.084.991 I llama_init_from_model: graph nodes  = 967
0.00.084.991 I llama_init_from_model: graph splits = 2
0.00.084.994 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.125 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.126 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.525.363 I main: llama threadpool init, n_threads = 4
0.00.525.406 I 
0.00.525.428 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.525.429 I 
0.00.525.661 I sampler seed: 1234
0.00.525.666 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.525.708 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.525.709 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.525.709 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.276.071 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57536.47 tokens per second)
0.01.276.072 I llama_perf_context_print:        load time =     515.54 ms
0.01.276.073 I llama_perf_context_print: prompt eval time =      44.40 ms /     7 tokens (    6.34 ms per token,   157.66 tokens per second)
0.01.276.073 I llama_perf_context_print:        eval time =     702.86 ms /    63 runs   (   11.16 ms per token,    89.63 tokens per second)
0.01.276.074 I llama_perf_context_print:       total time =     750.71 ms /    70 tokens
0.01.276.275 I ggml_metal_free: deallocating

real	0m1.294s
user	0m0.108s
sys	0m0.121s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4489 (f11cfdfd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.010.417 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.700 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.705 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.712 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.712 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.714 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.715 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.715 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.716 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.716 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.716 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.717 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.718 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.718 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.718 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.720 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.720 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.721 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.444 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.438 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.127 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.128 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.128 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.128 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.129 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.129 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.130 I llama_model_loader: - type  f32:  194 tensors
0.00.026.130 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.130 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.130 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.131 I print_info: file format = GGUF V3 (latest)
0.00.026.131 I print_info: file type   = Q4_K - Medium
0.00.026.132 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.045.601 I load: special tokens cache size = 25
0.00.051.623 I load: token to piece cache size = 0.2984 MB
0.00.051.625 I print_info: arch             = gptneox
0.00.051.626 I print_info: vocab_only       = 0
0.00.051.626 I print_info: n_ctx_train      = 2048
0.00.051.626 I print_info: n_embd           = 2048
0.00.051.626 I print_info: n_layer          = 24
0.00.051.629 I print_info: n_head           = 16
0.00.051.630 I print_info: n_head_kv        = 16
0.00.051.630 I print_info: n_rot            = 32
0.00.051.630 I print_info: n_swa            = 0
0.00.051.631 I print_info: n_embd_head_k    = 128
0.00.051.631 I print_info: n_embd_head_v    = 128
0.00.051.632 I print_info: n_gqa            = 1
0.00.051.632 I print_info: n_embd_k_gqa     = 2048
0.00.051.633 I print_info: n_embd_v_gqa     = 2048
0.00.051.634 I print_info: f_norm_eps       = 1.0e-05
0.00.051.634 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.634 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.636 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.636 I print_info: f_logit_scale    = 0.0e+00
0.00.051.637 I print_info: n_ff             = 8192
0.00.051.637 I print_info: n_expert         = 0
0.00.051.637 I print_info: n_expert_used    = 0
0.00.051.637 I print_info: causal attn      = 1
0.00.051.638 I print_info: pooling type     = 0
0.00.051.638 I print_info: rope type        = 2
0.00.051.638 I print_info: rope scaling     = linear
0.00.051.638 I print_info: freq_base_train  = 10000.0
0.00.051.639 I print_info: freq_scale_train = 1
0.00.051.639 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.639 I print_info: rope_finetuned   = unknown
0.00.051.639 I print_info: ssm_d_conv       = 0
0.00.051.640 I print_info: ssm_d_inner      = 0
0.00.051.640 I print_info: ssm_d_state      = 0
0.00.051.640 I print_info: ssm_dt_rank      = 0
0.00.051.640 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.640 I print_info: model type       = 1.4B
0.00.051.641 I print_info: model params     = 1.41 B
0.00.051.641 I print_info: general.name     = 1.4B
0.00.051.641 I print_info: vocab type       = BPE
0.00.051.641 I print_info: n_vocab          = 50304
0.00.051.642 I print_info: n_merges         = 50009
0.00.051.642 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.642 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.642 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.642 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.643 I print_info: LF token         = 128 'Ä'
0.00.051.643 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.643 I print_info: max token length = 1024
0.00.053.603 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.604 I load_tensors: offloading output layer to GPU
0.00.053.604 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.614 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.615 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.053.901 I llama_init_from_model: n_seq_max     = 1
0.00.053.902 I llama_init_from_model: n_ctx         = 2048
0.00.053.902 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.902 I llama_init_from_model: n_batch       = 2048
0.00.053.902 I llama_init_from_model: n_ubatch      = 512
0.00.053.902 I llama_init_from_model: flash_attn    = 0
0.00.053.903 I llama_init_from_model: freq_base     = 10000.0
0.00.053.903 I llama_init_from_model: freq_scale    = 1
0.00.053.903 I ggml_metal_init: allocating
0.00.053.907 I ggml_metal_init: found device: Apple M4
0.00.053.909 I ggml_metal_init: picking default device: Apple M4
0.00.054.495 I ggml_metal_init: using embedded metal library
0.00.056.849 I ggml_metal_init: GPU name:   Apple M4
0.00.056.850 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.851 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.851 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.851 I ggml_metal_init: simdgroup reduction   = true
0.00.056.852 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.852 I ggml_metal_init: has bfloat            = true
0.00.056.852 I ggml_metal_init: use bfloat            = true
0.00.056.852 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.853 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.709 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.943 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.952 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.974 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.972 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.973 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.973 I llama_init_from_model: graph nodes  = 967
0.00.086.973 I llama_init_from_model: graph splits = 2
0.00.086.976 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.105 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.106 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.604.779 I main: llama threadpool init, n_threads = 4
0.00.604.819 I 
0.00.604.854 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.604.855 I 
0.00.605.081 I sampler seed: 1234
0.00.605.086 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.605.096 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.605.098 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.605.098 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.365.535 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60944.21 tokens per second)
0.01.365.535 I llama_perf_context_print:        load time =     594.36 ms
0.01.365.536 I llama_perf_context_print: prompt eval time =      47.11 ms /     7 tokens (    6.73 ms per token,   148.58 tokens per second)
0.01.365.537 I llama_perf_context_print:        eval time =     710.41 ms /    63 runs   (   11.28 ms per token,    88.68 tokens per second)
0.01.365.537 I llama_perf_context_print:       total time =     760.76 ms /    70 tokens
0.01.365.742 I ggml_metal_free: deallocating

real	0m1.384s
user	0m0.109s
sys	0m0.136s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4489 (f11cfdfd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.010.287 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.001 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.006 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.008 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.013 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.013 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.015 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.015 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.021 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.022 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.022 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.023 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.023 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.023 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.024 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.027 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.028 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.028 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.839 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.885 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.657 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.658 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.658 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.659 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.659 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.659 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.660 I llama_model_loader: - type  f32:  194 tensors
0.00.026.660 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.660 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.661 I print_info: file format = GGUF V3 (latest)
0.00.026.661 I print_info: file type   = Q5_K - Medium
0.00.026.662 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.046.082 I load: special tokens cache size = 25
0.00.052.199 I load: token to piece cache size = 0.2984 MB
0.00.052.202 I print_info: arch             = gptneox
0.00.052.202 I print_info: vocab_only       = 0
0.00.052.202 I print_info: n_ctx_train      = 2048
0.00.052.202 I print_info: n_embd           = 2048
0.00.052.202 I print_info: n_layer          = 24
0.00.052.205 I print_info: n_head           = 16
0.00.052.206 I print_info: n_head_kv        = 16
0.00.052.207 I print_info: n_rot            = 32
0.00.052.207 I print_info: n_swa            = 0
0.00.052.207 I print_info: n_embd_head_k    = 128
0.00.052.208 I print_info: n_embd_head_v    = 128
0.00.052.208 I print_info: n_gqa            = 1
0.00.052.209 I print_info: n_embd_k_gqa     = 2048
0.00.052.210 I print_info: n_embd_v_gqa     = 2048
0.00.052.210 I print_info: f_norm_eps       = 1.0e-05
0.00.052.210 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.211 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.211 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.211 I print_info: f_logit_scale    = 0.0e+00
0.00.052.212 I print_info: n_ff             = 8192
0.00.052.212 I print_info: n_expert         = 0
0.00.052.212 I print_info: n_expert_used    = 0
0.00.052.212 I print_info: causal attn      = 1
0.00.052.212 I print_info: pooling type     = 0
0.00.052.214 I print_info: rope type        = 2
0.00.052.216 I print_info: rope scaling     = linear
0.00.052.216 I print_info: freq_base_train  = 10000.0
0.00.052.216 I print_info: freq_scale_train = 1
0.00.052.216 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.217 I print_info: rope_finetuned   = unknown
0.00.052.217 I print_info: ssm_d_conv       = 0
0.00.052.217 I print_info: ssm_d_inner      = 0
0.00.052.217 I print_info: ssm_d_state      = 0
0.00.052.217 I print_info: ssm_dt_rank      = 0
0.00.052.217 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.218 I print_info: model type       = 1.4B
0.00.052.218 I print_info: model params     = 1.41 B
0.00.052.218 I print_info: general.name     = 1.4B
0.00.052.218 I print_info: vocab type       = BPE
0.00.052.219 I print_info: n_vocab          = 50304
0.00.052.219 I print_info: n_merges         = 50009
0.00.052.219 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.219 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.219 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.220 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.220 I print_info: LF token         = 128 'Ä'
0.00.052.224 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.224 I print_info: max token length = 1024
0.00.054.231 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.231 I load_tensors: offloading output layer to GPU
0.00.054.231 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.242 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.054.243 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.054.530 I llama_init_from_model: n_seq_max     = 1
0.00.054.530 I llama_init_from_model: n_ctx         = 2048
0.00.054.530 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.530 I llama_init_from_model: n_batch       = 2048
0.00.054.531 I llama_init_from_model: n_ubatch      = 512
0.00.054.531 I llama_init_from_model: flash_attn    = 0
0.00.054.531 I llama_init_from_model: freq_base     = 10000.0
0.00.054.531 I llama_init_from_model: freq_scale    = 1
0.00.054.532 I ggml_metal_init: allocating
0.00.054.535 I ggml_metal_init: found device: Apple M4
0.00.054.536 I ggml_metal_init: picking default device: Apple M4
0.00.055.135 I ggml_metal_init: using embedded metal library
0.00.057.487 I ggml_metal_init: GPU name:   Apple M4
0.00.057.488 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.488 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.489 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.489 I ggml_metal_init: simdgroup reduction   = true
0.00.057.489 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.489 I ggml_metal_init: has bfloat            = true
0.00.057.489 I ggml_metal_init: use bfloat            = true
0.00.057.490 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.490 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.303 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.159 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.165 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.183 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.088.335 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.088.336 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.088.337 I llama_init_from_model: graph nodes  = 967
0.00.088.337 I llama_init_from_model: graph splits = 2
0.00.088.340 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.480 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.481 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.904.610 I main: llama threadpool init, n_threads = 4
0.00.904.705 I 
0.00.904.767 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.904.769 I 
0.00.905.374 I sampler seed: 1234
0.00.905.383 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.905.464 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.905.465 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.905.465 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.746.708 I llama_perf_sampler_print:    sampling time =       1.48 ms /    71 runs   (    0.02 ms per token, 48135.59 tokens per second)
0.01.746.709 I llama_perf_context_print:        load time =     894.31 ms
0.01.746.710 I llama_perf_context_print: prompt eval time =      52.15 ms /     7 tokens (    7.45 ms per token,   134.23 tokens per second)
0.01.746.710 I llama_perf_context_print:        eval time =     786.24 ms /    63 runs   (   12.48 ms per token,    80.13 tokens per second)
0.01.746.712 I llama_perf_context_print:       total time =     842.11 ms /    70 tokens
0.01.746.967 I ggml_metal_free: deallocating

real	0m1.767s
user	0m0.122s
sys	0m0.213s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4489 (f11cfdfd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.008.858 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.944 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.949 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.951 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.951 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.951 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.952 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.952 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.953 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.953 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.954 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.954 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.957 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.957 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.958 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.959 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.959 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.959 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.717 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.764 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.515 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.516 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.517 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.517 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.517 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.518 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.518 I llama_model_loader: - type  f32:  194 tensors
0.00.025.518 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.519 I print_info: file format = GGUF V3 (latest)
0.00.025.519 I print_info: file type   = Q6_K
0.00.025.520 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.044.940 I load: special tokens cache size = 25
0.00.050.915 I load: token to piece cache size = 0.2984 MB
0.00.050.918 I print_info: arch             = gptneox
0.00.050.918 I print_info: vocab_only       = 0
0.00.050.918 I print_info: n_ctx_train      = 2048
0.00.050.918 I print_info: n_embd           = 2048
0.00.050.919 I print_info: n_layer          = 24
0.00.050.921 I print_info: n_head           = 16
0.00.050.922 I print_info: n_head_kv        = 16
0.00.050.922 I print_info: n_rot            = 32
0.00.050.922 I print_info: n_swa            = 0
0.00.050.923 I print_info: n_embd_head_k    = 128
0.00.050.923 I print_info: n_embd_head_v    = 128
0.00.050.923 I print_info: n_gqa            = 1
0.00.050.924 I print_info: n_embd_k_gqa     = 2048
0.00.050.926 I print_info: n_embd_v_gqa     = 2048
0.00.050.926 I print_info: f_norm_eps       = 1.0e-05
0.00.050.927 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.927 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.928 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.929 I print_info: f_logit_scale    = 0.0e+00
0.00.050.929 I print_info: n_ff             = 8192
0.00.050.929 I print_info: n_expert         = 0
0.00.050.929 I print_info: n_expert_used    = 0
0.00.050.930 I print_info: causal attn      = 1
0.00.050.931 I print_info: pooling type     = 0
0.00.050.931 I print_info: rope type        = 2
0.00.050.931 I print_info: rope scaling     = linear
0.00.050.932 I print_info: freq_base_train  = 10000.0
0.00.050.932 I print_info: freq_scale_train = 1
0.00.050.932 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.933 I print_info: rope_finetuned   = unknown
0.00.050.933 I print_info: ssm_d_conv       = 0
0.00.050.933 I print_info: ssm_d_inner      = 0
0.00.050.933 I print_info: ssm_d_state      = 0
0.00.050.933 I print_info: ssm_dt_rank      = 0
0.00.050.933 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.934 I print_info: model type       = 1.4B
0.00.050.934 I print_info: model params     = 1.41 B
0.00.050.934 I print_info: general.name     = 1.4B
0.00.050.935 I print_info: vocab type       = BPE
0.00.050.935 I print_info: n_vocab          = 50304
0.00.050.935 I print_info: n_merges         = 50009
0.00.050.935 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.935 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.939 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.940 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.940 I print_info: LF token         = 128 'Ä'
0.00.050.940 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.940 I print_info: max token length = 1024
0.00.052.703 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.704 I load_tensors: offloading output layer to GPU
0.00.052.704 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.709 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.710 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.052.988 I llama_init_from_model: n_seq_max     = 1
0.00.052.989 I llama_init_from_model: n_ctx         = 2048
0.00.052.989 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.989 I llama_init_from_model: n_batch       = 2048
0.00.052.989 I llama_init_from_model: n_ubatch      = 512
0.00.052.989 I llama_init_from_model: flash_attn    = 0
0.00.052.990 I llama_init_from_model: freq_base     = 10000.0
0.00.052.990 I llama_init_from_model: freq_scale    = 1
0.00.052.991 I ggml_metal_init: allocating
0.00.052.994 I ggml_metal_init: found device: Apple M4
0.00.052.996 I ggml_metal_init: picking default device: Apple M4
0.00.053.591 I ggml_metal_init: using embedded metal library
0.00.055.946 I ggml_metal_init: GPU name:   Apple M4
0.00.055.948 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.948 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.949 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.949 I ggml_metal_init: simdgroup reduction   = true
0.00.055.949 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.949 I ggml_metal_init: has bfloat            = true
0.00.055.949 I ggml_metal_init: use bfloat            = true
0.00.055.950 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.950 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.328 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.134 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.142 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.165 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.090 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.092 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.092 I llama_init_from_model: graph nodes  = 967
0.00.086.092 I llama_init_from_model: graph splits = 2
0.00.086.099 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.227 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.227 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.738.262 I main: llama threadpool init, n_threads = 4
0.00.738.310 I 
0.00.738.348 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.738.350 I 
0.00.738.580 I sampler seed: 1234
0.00.738.587 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.738.620 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.738.621 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.738.621 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.620.678 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59764.31 tokens per second)
0.01.620.679 I llama_perf_context_print:        load time =     729.40 ms
0.01.620.679 I llama_perf_context_print: prompt eval time =      54.47 ms /     7 tokens (    7.78 ms per token,   128.51 tokens per second)
0.01.620.680 I llama_perf_context_print:        eval time =     824.66 ms /    63 runs   (   13.09 ms per token,    76.40 tokens per second)
0.01.620.680 I llama_perf_context_print:       total time =     882.42 ms /    70 tokens
0.01.620.917 I ggml_metal_free: deallocating

real	0m1.636s
user	0m0.108s
sys	0m0.155s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.577 I build: 4489 (f11cfdfd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.410 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.032.850 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.032.862 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.867 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.868 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.868 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.869 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.869 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.871 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.872 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.872 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.873 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.874 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.874 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.875 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.878 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.879 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.879 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.038.847 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.039.858 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.043.493 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.043.494 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.043.495 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.043.495 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.043.495 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.043.496 I llama_model_loader: - type  f32:  194 tensors
0.00.043.496 I llama_model_loader: - type  f16:   98 tensors
0.00.043.497 I print_info: file format = GGUF V3 (latest)
0.00.043.498 I print_info: file type   = all F32 (guessed)
0.00.043.499 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.063.172 I load: special tokens cache size = 25
0.00.069.301 I load: token to piece cache size = 0.2984 MB
0.00.069.305 I print_info: arch             = gptneox
0.00.069.306 I print_info: vocab_only       = 0
0.00.069.306 I print_info: n_ctx_train      = 2048
0.00.069.306 I print_info: n_embd           = 2048
0.00.069.307 I print_info: n_layer          = 24
0.00.069.311 I print_info: n_head           = 16
0.00.069.312 I print_info: n_head_kv        = 16
0.00.069.312 I print_info: n_rot            = 32
0.00.069.312 I print_info: n_swa            = 0
0.00.069.312 I print_info: n_embd_head_k    = 128
0.00.069.313 I print_info: n_embd_head_v    = 128
0.00.069.313 I print_info: n_gqa            = 1
0.00.069.314 I print_info: n_embd_k_gqa     = 2048
0.00.069.315 I print_info: n_embd_v_gqa     = 2048
0.00.069.316 I print_info: f_norm_eps       = 1.0e-05
0.00.069.316 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.069.316 I print_info: f_clamp_kqv      = 0.0e+00
0.00.069.316 I print_info: f_max_alibi_bias = 0.0e+00
0.00.069.317 I print_info: f_logit_scale    = 0.0e+00
0.00.069.317 I print_info: n_ff             = 8192
0.00.069.318 I print_info: n_expert         = 0
0.00.069.318 I print_info: n_expert_used    = 0
0.00.069.318 I print_info: causal attn      = 1
0.00.069.318 I print_info: pooling type     = 0
0.00.069.318 I print_info: rope type        = 2
0.00.069.319 I print_info: rope scaling     = linear
0.00.069.322 I print_info: freq_base_train  = 10000.0
0.00.069.322 I print_info: freq_scale_train = 1
0.00.069.323 I print_info: n_ctx_orig_yarn  = 2048
0.00.069.323 I print_info: rope_finetuned   = unknown
0.00.069.323 I print_info: ssm_d_conv       = 0
0.00.069.323 I print_info: ssm_d_inner      = 0
0.00.069.323 I print_info: ssm_d_state      = 0
0.00.069.323 I print_info: ssm_dt_rank      = 0
0.00.069.324 I print_info: ssm_dt_b_c_rms   = 0
0.00.069.324 I print_info: model type       = 1.4B
0.00.069.324 I print_info: model params     = 1.41 B
0.00.069.324 I print_info: general.name     = 1.4B
0.00.069.325 I print_info: vocab type       = BPE
0.00.069.325 I print_info: n_vocab          = 50304
0.00.069.325 I print_info: n_merges         = 50009
0.00.069.325 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.069.326 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.069.326 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.069.326 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.069.326 I print_info: LF token         = 128 'Ä'
0.00.069.327 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.069.329 I print_info: max token length = 1024
0.00.071.651 I load_tensors: offloading 24 repeating layers to GPU
0.00.071.651 I load_tensors: offloading output layer to GPU
0.00.071.651 I load_tensors: offloaded 25/25 layers to GPU
0.00.071.662 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.071.663 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.071.942 I llama_init_from_model: n_seq_max     = 1
0.00.071.942 I llama_init_from_model: n_ctx         = 128
0.00.071.943 I llama_init_from_model: n_ctx_per_seq = 128
0.00.071.943 I llama_init_from_model: n_batch       = 128
0.00.071.943 I llama_init_from_model: n_ubatch      = 128
0.00.071.943 I llama_init_from_model: flash_attn    = 0
0.00.071.943 I llama_init_from_model: freq_base     = 10000.0
0.00.071.944 I llama_init_from_model: freq_scale    = 1
0.00.071.944 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.071.945 I ggml_metal_init: allocating
0.00.071.948 I ggml_metal_init: found device: Apple M4
0.00.071.950 I ggml_metal_init: picking default device: Apple M4
0.00.072.551 I ggml_metal_init: using embedded metal library
0.00.074.989 I ggml_metal_init: GPU name:   Apple M4
0.00.074.990 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.074.991 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.074.991 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.074.991 I ggml_metal_init: simdgroup reduction   = true
0.00.074.991 I ggml_metal_init: simdgroup matrix mul. = true
0.00.074.991 I ggml_metal_init: has bfloat            = true
0.00.074.992 I ggml_metal_init: use bfloat            = true
0.00.074.992 I ggml_metal_init: hasUnifiedMemory      = true
0.00.074.993 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.249 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.542 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.086.547 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.086.561 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.493 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.087.494 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.087.494 I llama_init_from_model: graph nodes  = 967
0.00.087.494 I llama_init_from_model: graph splits = 2
0.00.087.495 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.087.495 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.510.820 I 
0.01.510.889 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.510.899 I perplexity: tokenizing the input ..
0.01.523.184 I perplexity: tokenization took 12.281 ms
0.01.523.191 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.642.493 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.648.176 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.648.231 I llama_perf_context_print:        load time =    1490.40 ms
0.01.648.236 I llama_perf_context_print: prompt eval time =     119.06 ms /   128 tokens (    0.93 ms per token,  1075.04 tokens per second)
0.01.648.237 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.648.238 I llama_perf_context_print:       total time =     137.42 ms /   129 tokens
0.01.648.914 I ggml_metal_free: deallocating

real	0m1.837s
user	0m0.118s
sys	0m0.196s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.742 I build: 4489 (f11cfdfd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.236 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.043.139 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.043.151 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.043.154 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.043.155 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.043.155 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.043.156 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.043.156 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.043.158 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.043.158 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.043.159 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.043.159 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.043.160 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.043.161 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.043.162 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.043.164 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.043.165 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.043.165 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.050.123 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.052.383 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.168 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.170 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.171 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.171 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.171 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.172 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.056.172 I llama_model_loader: - type  f32:  194 tensors
0.00.056.173 I llama_model_loader: - type q8_0:   98 tensors
0.00.056.173 I print_info: file format = GGUF V3 (latest)
0.00.056.174 I print_info: file type   = Q8_0
0.00.056.175 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.075.692 I load: special tokens cache size = 25
0.00.081.866 I load: token to piece cache size = 0.2984 MB
0.00.081.870 I print_info: arch             = gptneox
0.00.081.870 I print_info: vocab_only       = 0
0.00.081.870 I print_info: n_ctx_train      = 2048
0.00.081.870 I print_info: n_embd           = 2048
0.00.081.871 I print_info: n_layer          = 24
0.00.081.874 I print_info: n_head           = 16
0.00.081.875 I print_info: n_head_kv        = 16
0.00.081.875 I print_info: n_rot            = 32
0.00.081.875 I print_info: n_swa            = 0
0.00.081.876 I print_info: n_embd_head_k    = 128
0.00.081.877 I print_info: n_embd_head_v    = 128
0.00.081.877 I print_info: n_gqa            = 1
0.00.081.878 I print_info: n_embd_k_gqa     = 2048
0.00.081.879 I print_info: n_embd_v_gqa     = 2048
0.00.081.879 I print_info: f_norm_eps       = 1.0e-05
0.00.081.881 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.081.881 I print_info: f_clamp_kqv      = 0.0e+00
0.00.081.882 I print_info: f_max_alibi_bias = 0.0e+00
0.00.081.882 I print_info: f_logit_scale    = 0.0e+00
0.00.081.882 I print_info: n_ff             = 8192
0.00.081.883 I print_info: n_expert         = 0
0.00.081.883 I print_info: n_expert_used    = 0
0.00.081.883 I print_info: causal attn      = 1
0.00.081.883 I print_info: pooling type     = 0
0.00.081.883 I print_info: rope type        = 2
0.00.081.883 I print_info: rope scaling     = linear
0.00.081.883 I print_info: freq_base_train  = 10000.0
0.00.081.884 I print_info: freq_scale_train = 1
0.00.081.884 I print_info: n_ctx_orig_yarn  = 2048
0.00.081.884 I print_info: rope_finetuned   = unknown
0.00.081.884 I print_info: ssm_d_conv       = 0
0.00.081.884 I print_info: ssm_d_inner      = 0
0.00.081.885 I print_info: ssm_d_state      = 0
0.00.081.885 I print_info: ssm_dt_rank      = 0
0.00.081.885 I print_info: ssm_dt_b_c_rms   = 0
0.00.081.885 I print_info: model type       = 1.4B
0.00.081.885 I print_info: model params     = 1.41 B
0.00.081.885 I print_info: general.name     = 1.4B
0.00.081.886 I print_info: vocab type       = BPE
0.00.081.886 I print_info: n_vocab          = 50304
0.00.081.886 I print_info: n_merges         = 50009
0.00.081.887 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.081.887 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.081.887 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.081.887 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.081.887 I print_info: LF token         = 128 'Ä'
0.00.081.889 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.081.889 I print_info: max token length = 1024
0.00.083.888 I load_tensors: offloading 24 repeating layers to GPU
0.00.083.889 I load_tensors: offloading output layer to GPU
0.00.083.889 I load_tensors: offloaded 25/25 layers to GPU
0.00.083.900 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.083.901 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.084.197 I llama_init_from_model: n_seq_max     = 1
0.00.084.198 I llama_init_from_model: n_ctx         = 128
0.00.084.198 I llama_init_from_model: n_ctx_per_seq = 128
0.00.084.198 I llama_init_from_model: n_batch       = 128
0.00.084.198 I llama_init_from_model: n_ubatch      = 128
0.00.084.198 I llama_init_from_model: flash_attn    = 0
0.00.084.199 I llama_init_from_model: freq_base     = 10000.0
0.00.084.199 I llama_init_from_model: freq_scale    = 1
0.00.084.199 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.084.200 I ggml_metal_init: allocating
0.00.084.203 I ggml_metal_init: found device: Apple M4
0.00.084.205 I ggml_metal_init: picking default device: Apple M4
0.00.084.842 I ggml_metal_init: using embedded metal library
0.00.087.447 I ggml_metal_init: GPU name:   Apple M4
0.00.087.449 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.087.449 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.087.450 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.087.450 I ggml_metal_init: simdgroup reduction   = true
0.00.087.450 I ggml_metal_init: simdgroup matrix mul. = true
0.00.087.450 I ggml_metal_init: has bfloat            = true
0.00.087.450 I ggml_metal_init: use bfloat            = true
0.00.087.451 I ggml_metal_init: hasUnifiedMemory      = true
0.00.087.452 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.100.247 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.101.580 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.101.586 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.101.603 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.102.481 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.102.482 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.102.483 I llama_init_from_model: graph nodes  = 967
0.00.102.483 I llama_init_from_model: graph splits = 2
0.00.102.484 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.102.484 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.304.478 I 
0.01.304.547 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.304.559 I perplexity: tokenizing the input ..
0.01.322.696 I perplexity: tokenization took 18.136 ms
0.01.322.709 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.464.968 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.466.378 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.466.415 I llama_perf_context_print:        load time =    1279.23 ms
0.01.466.416 I llama_perf_context_print: prompt eval time =     141.36 ms /   128 tokens (    1.10 ms per token,   905.46 tokens per second)
0.01.466.417 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.466.417 I llama_perf_context_print:       total time =     161.94 ms /   129 tokens
0.01.467.045 I ggml_metal_free: deallocating

real	0m1.508s
user	0m0.108s
sys	0m0.177s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.001.190 I build: 4489 (f11cfdfd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.714 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.044.472 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.044.493 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.044.497 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.044.498 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.044.498 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.044.499 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.044.499 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.044.501 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.044.502 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.044.502 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.044.503 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.044.504 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.044.505 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.044.506 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.044.510 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.044.515 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.044.515 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.051.511 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.053.738 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.061.184 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.061.190 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.061.191 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.061.191 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.061.192 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.061.193 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.061.194 I llama_model_loader: - type  f32:  194 tensors
0.00.061.194 I llama_model_loader: - type q4_0:   97 tensors
0.00.061.195 I llama_model_loader: - type q6_K:    1 tensors
0.00.061.201 I print_info: file format = GGUF V3 (latest)
0.00.061.202 I print_info: file type   = Q4_0
0.00.061.204 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.097.776 I load: special tokens cache size = 25
0.00.105.777 I load: token to piece cache size = 0.2984 MB
0.00.105.781 I print_info: arch             = gptneox
0.00.105.781 I print_info: vocab_only       = 0
0.00.105.781 I print_info: n_ctx_train      = 2048
0.00.105.781 I print_info: n_embd           = 2048
0.00.105.782 I print_info: n_layer          = 24
0.00.105.785 I print_info: n_head           = 16
0.00.105.786 I print_info: n_head_kv        = 16
0.00.105.786 I print_info: n_rot            = 32
0.00.105.786 I print_info: n_swa            = 0
0.00.105.786 I print_info: n_embd_head_k    = 128
0.00.105.786 I print_info: n_embd_head_v    = 128
0.00.105.787 I print_info: n_gqa            = 1
0.00.105.788 I print_info: n_embd_k_gqa     = 2048
0.00.105.789 I print_info: n_embd_v_gqa     = 2048
0.00.105.790 I print_info: f_norm_eps       = 1.0e-05
0.00.105.790 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.105.790 I print_info: f_clamp_kqv      = 0.0e+00
0.00.105.790 I print_info: f_max_alibi_bias = 0.0e+00
0.00.105.791 I print_info: f_logit_scale    = 0.0e+00
0.00.105.791 I print_info: n_ff             = 8192
0.00.105.792 I print_info: n_expert         = 0
0.00.105.792 I print_info: n_expert_used    = 0
0.00.105.792 I print_info: causal attn      = 1
0.00.105.792 I print_info: pooling type     = 0
0.00.105.793 I print_info: rope type        = 2
0.00.105.794 I print_info: rope scaling     = linear
0.00.105.794 I print_info: freq_base_train  = 10000.0
0.00.105.794 I print_info: freq_scale_train = 1
0.00.105.795 I print_info: n_ctx_orig_yarn  = 2048
0.00.105.795 I print_info: rope_finetuned   = unknown
0.00.105.797 I print_info: ssm_d_conv       = 0
0.00.105.797 I print_info: ssm_d_inner      = 0
0.00.105.797 I print_info: ssm_d_state      = 0
0.00.105.797 I print_info: ssm_dt_rank      = 0
0.00.105.797 I print_info: ssm_dt_b_c_rms   = 0
0.00.105.798 I print_info: model type       = 1.4B
0.00.105.798 I print_info: model params     = 1.41 B
0.00.105.798 I print_info: general.name     = 1.4B
0.00.105.799 I print_info: vocab type       = BPE
0.00.105.799 I print_info: n_vocab          = 50304
0.00.105.799 I print_info: n_merges         = 50009
0.00.105.800 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.105.800 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.105.800 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.105.800 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.105.801 I print_info: LF token         = 128 'Ä'
0.00.105.801 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.105.801 I print_info: max token length = 1024
0.00.108.159 I load_tensors: offloading 24 repeating layers to GPU
0.00.108.160 I load_tensors: offloading output layer to GPU
0.00.108.160 I load_tensors: offloaded 25/25 layers to GPU
0.00.108.171 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.108.173 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.108.498 I llama_init_from_model: n_seq_max     = 1
0.00.108.499 I llama_init_from_model: n_ctx         = 128
0.00.108.499 I llama_init_from_model: n_ctx_per_seq = 128
0.00.108.499 I llama_init_from_model: n_batch       = 128
0.00.108.499 I llama_init_from_model: n_ubatch      = 128
0.00.108.500 I llama_init_from_model: flash_attn    = 0
0.00.108.500 I llama_init_from_model: freq_base     = 10000.0
0.00.108.500 I llama_init_from_model: freq_scale    = 1
0.00.108.501 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.108.501 I ggml_metal_init: allocating
0.00.108.505 I ggml_metal_init: found device: Apple M4
0.00.108.507 I ggml_metal_init: picking default device: Apple M4
0.00.109.197 I ggml_metal_init: using embedded metal library
0.00.112.175 I ggml_metal_init: GPU name:   Apple M4
0.00.112.177 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.112.177 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.112.178 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.112.178 I ggml_metal_init: simdgroup reduction   = true
0.00.112.178 I ggml_metal_init: simdgroup matrix mul. = true
0.00.112.178 I ggml_metal_init: has bfloat            = true
0.00.112.179 I ggml_metal_init: use bfloat            = true
0.00.112.179 I ggml_metal_init: hasUnifiedMemory      = true
0.00.112.180 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.121.884 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.123.319 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.123.321 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.123.335 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.124.295 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.124.296 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.124.297 I llama_init_from_model: graph nodes  = 967
0.00.124.297 I llama_init_from_model: graph splits = 2
0.00.124.298 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.124.298 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.686.675 I 
0.00.686.787 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.686.797 I perplexity: tokenizing the input ..
0.00.704.641 I perplexity: tokenization took 17.839 ms
0.00.704.667 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.844.302 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.845.812 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.845.851 I llama_perf_context_print:        load time =     663.95 ms
0.00.845.852 I llama_perf_context_print: prompt eval time =     138.62 ms /   128 tokens (    1.08 ms per token,   923.39 tokens per second)
0.00.845.852 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.845.853 I llama_perf_context_print:       total time =     159.18 ms /   129 tokens
0.00.846.368 I ggml_metal_free: deallocating

real	0m0.883s
user	0m0.129s
sys	0m0.099s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4489 (f11cfdfd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.994 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.702 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.019.706 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.707 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.709 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.709 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.710 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.710 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.711 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.711 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.711 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.712 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.712 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.715 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.715 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.716 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.717 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.717 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.409 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.393 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.092 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.093 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.093 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.093 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.094 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.094 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.028.094 I llama_model_loader: - type  f32:  194 tensors
0.00.028.095 I llama_model_loader: - type q4_1:   97 tensors
0.00.028.095 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.095 I print_info: file format = GGUF V3 (latest)
0.00.028.096 I print_info: file type   = Q4_1
0.00.028.096 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.047.462 I load: special tokens cache size = 25
0.00.053.499 I load: token to piece cache size = 0.2984 MB
0.00.053.502 I print_info: arch             = gptneox
0.00.053.502 I print_info: vocab_only       = 0
0.00.053.502 I print_info: n_ctx_train      = 2048
0.00.053.503 I print_info: n_embd           = 2048
0.00.053.503 I print_info: n_layer          = 24
0.00.053.506 I print_info: n_head           = 16
0.00.053.507 I print_info: n_head_kv        = 16
0.00.053.507 I print_info: n_rot            = 32
0.00.053.507 I print_info: n_swa            = 0
0.00.053.507 I print_info: n_embd_head_k    = 128
0.00.053.507 I print_info: n_embd_head_v    = 128
0.00.053.508 I print_info: n_gqa            = 1
0.00.053.509 I print_info: n_embd_k_gqa     = 2048
0.00.053.511 I print_info: n_embd_v_gqa     = 2048
0.00.053.511 I print_info: f_norm_eps       = 1.0e-05
0.00.053.512 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.512 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.512 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.512 I print_info: f_logit_scale    = 0.0e+00
0.00.053.513 I print_info: n_ff             = 8192
0.00.053.513 I print_info: n_expert         = 0
0.00.053.513 I print_info: n_expert_used    = 0
0.00.053.513 I print_info: causal attn      = 1
0.00.053.514 I print_info: pooling type     = 0
0.00.053.514 I print_info: rope type        = 2
0.00.053.514 I print_info: rope scaling     = linear
0.00.053.514 I print_info: freq_base_train  = 10000.0
0.00.053.515 I print_info: freq_scale_train = 1
0.00.053.515 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.515 I print_info: rope_finetuned   = unknown
0.00.053.515 I print_info: ssm_d_conv       = 0
0.00.053.516 I print_info: ssm_d_inner      = 0
0.00.053.516 I print_info: ssm_d_state      = 0
0.00.053.516 I print_info: ssm_dt_rank      = 0
0.00.053.516 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.516 I print_info: model type       = 1.4B
0.00.053.517 I print_info: model params     = 1.41 B
0.00.053.517 I print_info: general.name     = 1.4B
0.00.053.517 I print_info: vocab type       = BPE
0.00.053.518 I print_info: n_vocab          = 50304
0.00.053.518 I print_info: n_merges         = 50009
0.00.053.518 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.519 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.519 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.519 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.519 I print_info: LF token         = 128 'Ä'
0.00.053.521 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.521 I print_info: max token length = 1024
0.00.055.472 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.472 I load_tensors: offloading output layer to GPU
0.00.055.473 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.483 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.055.484 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.055.763 I llama_init_from_model: n_seq_max     = 1
0.00.055.764 I llama_init_from_model: n_ctx         = 128
0.00.055.764 I llama_init_from_model: n_ctx_per_seq = 128
0.00.055.764 I llama_init_from_model: n_batch       = 128
0.00.055.764 I llama_init_from_model: n_ubatch      = 128
0.00.055.764 I llama_init_from_model: flash_attn    = 0
0.00.055.765 I llama_init_from_model: freq_base     = 10000.0
0.00.055.765 I llama_init_from_model: freq_scale    = 1
0.00.055.765 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.766 I ggml_metal_init: allocating
0.00.055.769 I ggml_metal_init: found device: Apple M4
0.00.055.770 I ggml_metal_init: picking default device: Apple M4
0.00.056.344 I ggml_metal_init: using embedded metal library
0.00.058.736 I ggml_metal_init: GPU name:   Apple M4
0.00.058.737 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.737 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.738 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.738 I ggml_metal_init: simdgroup reduction   = true
0.00.058.738 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.738 I ggml_metal_init: has bfloat            = true
0.00.058.738 I ggml_metal_init: use bfloat            = true
0.00.058.739 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.739 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.456 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.069.683 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.687 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.702 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.070.564 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.070.565 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.070.565 I llama_init_from_model: graph nodes  = 967
0.00.070.565 I llama_init_from_model: graph splits = 2
0.00.070.566 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.567 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.748.979 I 
0.00.749.007 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.749.010 I perplexity: tokenizing the input ..
0.00.756.708 I perplexity: tokenization took 7.696 ms
0.00.756.711 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.879.383 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.880.545 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.880.572 I llama_perf_context_print:        load time =     739.98 ms
0.00.880.573 I llama_perf_context_print: prompt eval time =     122.45 ms /   128 tokens (    0.96 ms per token,  1045.36 tokens per second)
0.00.880.574 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.880.574 I llama_perf_context_print:       total time =     131.59 ms /   129 tokens
0.00.881.039 I ggml_metal_free: deallocating

real	0m0.894s
user	0m0.077s
sys	0m0.094s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4489 (f11cfdfd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.838 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.816 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.019.820 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.821 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.826 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.826 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.826 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.827 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.828 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.828 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.828 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.829 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.829 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.829 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.830 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.833 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.834 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.834 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.415 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.392 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.039 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.041 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.041 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.041 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.042 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.042 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.028.042 I llama_model_loader: - type  f32:  194 tensors
0.00.028.043 I llama_model_loader: - type q5_0:   97 tensors
0.00.028.043 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.044 I print_info: file format = GGUF V3 (latest)
0.00.028.044 I print_info: file type   = Q5_0
0.00.028.048 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.046.412 I load: special tokens cache size = 25
0.00.052.323 I load: token to piece cache size = 0.2984 MB
0.00.052.326 I print_info: arch             = gptneox
0.00.052.326 I print_info: vocab_only       = 0
0.00.052.327 I print_info: n_ctx_train      = 2048
0.00.052.327 I print_info: n_embd           = 2048
0.00.052.327 I print_info: n_layer          = 24
0.00.052.330 I print_info: n_head           = 16
0.00.052.331 I print_info: n_head_kv        = 16
0.00.052.331 I print_info: n_rot            = 32
0.00.052.331 I print_info: n_swa            = 0
0.00.052.331 I print_info: n_embd_head_k    = 128
0.00.052.331 I print_info: n_embd_head_v    = 128
0.00.052.332 I print_info: n_gqa            = 1
0.00.052.333 I print_info: n_embd_k_gqa     = 2048
0.00.052.333 I print_info: n_embd_v_gqa     = 2048
0.00.052.334 I print_info: f_norm_eps       = 1.0e-05
0.00.052.334 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.334 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.335 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.335 I print_info: f_logit_scale    = 0.0e+00
0.00.052.335 I print_info: n_ff             = 8192
0.00.052.335 I print_info: n_expert         = 0
0.00.052.336 I print_info: n_expert_used    = 0
0.00.052.336 I print_info: causal attn      = 1
0.00.052.336 I print_info: pooling type     = 0
0.00.052.336 I print_info: rope type        = 2
0.00.052.336 I print_info: rope scaling     = linear
0.00.052.337 I print_info: freq_base_train  = 10000.0
0.00.052.339 I print_info: freq_scale_train = 1
0.00.052.339 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.339 I print_info: rope_finetuned   = unknown
0.00.052.340 I print_info: ssm_d_conv       = 0
0.00.052.340 I print_info: ssm_d_inner      = 0
0.00.052.340 I print_info: ssm_d_state      = 0
0.00.052.340 I print_info: ssm_dt_rank      = 0
0.00.052.340 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.340 I print_info: model type       = 1.4B
0.00.052.341 I print_info: model params     = 1.41 B
0.00.052.341 I print_info: general.name     = 1.4B
0.00.052.341 I print_info: vocab type       = BPE
0.00.052.341 I print_info: n_vocab          = 50304
0.00.052.342 I print_info: n_merges         = 50009
0.00.052.342 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.342 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.342 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.342 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.344 I print_info: LF token         = 128 'Ä'
0.00.052.344 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.344 I print_info: max token length = 1024
0.00.054.204 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.205 I load_tensors: offloading output layer to GPU
0.00.054.205 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.215 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.217 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.054.490 I llama_init_from_model: n_seq_max     = 1
0.00.054.491 I llama_init_from_model: n_ctx         = 128
0.00.054.491 I llama_init_from_model: n_ctx_per_seq = 128
0.00.054.491 I llama_init_from_model: n_batch       = 128
0.00.054.491 I llama_init_from_model: n_ubatch      = 128
0.00.054.491 I llama_init_from_model: flash_attn    = 0
0.00.054.492 I llama_init_from_model: freq_base     = 10000.0
0.00.054.492 I llama_init_from_model: freq_scale    = 1
0.00.054.492 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.493 I ggml_metal_init: allocating
0.00.054.495 I ggml_metal_init: found device: Apple M4
0.00.054.497 I ggml_metal_init: picking default device: Apple M4
0.00.055.057 I ggml_metal_init: using embedded metal library
0.00.057.373 I ggml_metal_init: GPU name:   Apple M4
0.00.057.375 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.375 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.375 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.376 I ggml_metal_init: simdgroup reduction   = true
0.00.057.376 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.376 I ggml_metal_init: has bfloat            = true
0.00.057.376 I ggml_metal_init: use bfloat            = true
0.00.057.376 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.377 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.771 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.037 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.039 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.052 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.069.008 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.069.009 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.069.010 I llama_init_from_model: graph nodes  = 967
0.00.069.010 I llama_init_from_model: graph splits = 2
0.00.069.011 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.011 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.728.230 I 
0.00.728.312 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.728.327 I perplexity: tokenizing the input ..
0.00.736.442 I perplexity: tokenization took 8.114 ms
0.00.736.445 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.870.838 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.871.998 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.872.024 I llama_perf_context_print:        load time =     718.38 ms
0.00.872.025 I llama_perf_context_print: prompt eval time =     134.16 ms /   128 tokens (    1.05 ms per token,   954.11 tokens per second)
0.00.872.026 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.872.027 I llama_perf_context_print:       total time =     143.80 ms /   129 tokens
0.00.872.523 I ggml_metal_free: deallocating

real	0m0.887s
user	0m0.076s
sys	0m0.113s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4489 (f11cfdfd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.848 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.022.486 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.022.490 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.491 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.492 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.492 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.492 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.492 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.494 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.494 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.496 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.496 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.497 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.497 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.500 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.503 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.503 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.503 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.259 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.272 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.134 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.135 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.136 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.136 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.136 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.136 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.031.137 I llama_model_loader: - type  f32:  194 tensors
0.00.031.137 I llama_model_loader: - type q5_1:   97 tensors
0.00.031.137 I llama_model_loader: - type q6_K:    1 tensors
0.00.031.138 I print_info: file format = GGUF V3 (latest)
0.00.031.138 I print_info: file type   = Q5_1
0.00.031.139 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.050.636 I load: special tokens cache size = 25
0.00.056.677 I load: token to piece cache size = 0.2984 MB
0.00.056.680 I print_info: arch             = gptneox
0.00.056.680 I print_info: vocab_only       = 0
0.00.056.680 I print_info: n_ctx_train      = 2048
0.00.056.681 I print_info: n_embd           = 2048
0.00.056.681 I print_info: n_layer          = 24
0.00.056.684 I print_info: n_head           = 16
0.00.056.685 I print_info: n_head_kv        = 16
0.00.056.685 I print_info: n_rot            = 32
0.00.056.685 I print_info: n_swa            = 0
0.00.056.685 I print_info: n_embd_head_k    = 128
0.00.056.686 I print_info: n_embd_head_v    = 128
0.00.056.686 I print_info: n_gqa            = 1
0.00.056.687 I print_info: n_embd_k_gqa     = 2048
0.00.056.688 I print_info: n_embd_v_gqa     = 2048
0.00.056.688 I print_info: f_norm_eps       = 1.0e-05
0.00.056.691 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.056.691 I print_info: f_clamp_kqv      = 0.0e+00
0.00.056.691 I print_info: f_max_alibi_bias = 0.0e+00
0.00.056.691 I print_info: f_logit_scale    = 0.0e+00
0.00.056.692 I print_info: n_ff             = 8192
0.00.056.692 I print_info: n_expert         = 0
0.00.056.692 I print_info: n_expert_used    = 0
0.00.056.692 I print_info: causal attn      = 1
0.00.056.693 I print_info: pooling type     = 0
0.00.056.693 I print_info: rope type        = 2
0.00.056.693 I print_info: rope scaling     = linear
0.00.056.695 I print_info: freq_base_train  = 10000.0
0.00.056.696 I print_info: freq_scale_train = 1
0.00.056.696 I print_info: n_ctx_orig_yarn  = 2048
0.00.056.696 I print_info: rope_finetuned   = unknown
0.00.056.697 I print_info: ssm_d_conv       = 0
0.00.056.697 I print_info: ssm_d_inner      = 0
0.00.056.697 I print_info: ssm_d_state      = 0
0.00.056.697 I print_info: ssm_dt_rank      = 0
0.00.056.697 I print_info: ssm_dt_b_c_rms   = 0
0.00.056.697 I print_info: model type       = 1.4B
0.00.056.698 I print_info: model params     = 1.41 B
0.00.056.698 I print_info: general.name     = 1.4B
0.00.056.698 I print_info: vocab type       = BPE
0.00.056.699 I print_info: n_vocab          = 50304
0.00.056.699 I print_info: n_merges         = 50009
0.00.056.700 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.056.701 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.056.701 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.056.701 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.056.701 I print_info: LF token         = 128 'Ä'
0.00.056.705 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.056.705 I print_info: max token length = 1024
0.00.058.703 I load_tensors: offloading 24 repeating layers to GPU
0.00.058.703 I load_tensors: offloading output layer to GPU
0.00.058.703 I load_tensors: offloaded 25/25 layers to GPU
0.00.058.714 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.058.715 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.058.997 I llama_init_from_model: n_seq_max     = 1
0.00.058.997 I llama_init_from_model: n_ctx         = 128
0.00.058.997 I llama_init_from_model: n_ctx_per_seq = 128
0.00.058.998 I llama_init_from_model: n_batch       = 128
0.00.058.998 I llama_init_from_model: n_ubatch      = 128
0.00.058.998 I llama_init_from_model: flash_attn    = 0
0.00.058.998 I llama_init_from_model: freq_base     = 10000.0
0.00.058.998 I llama_init_from_model: freq_scale    = 1
0.00.058.999 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.058.999 I ggml_metal_init: allocating
0.00.059.002 I ggml_metal_init: found device: Apple M4
0.00.059.004 I ggml_metal_init: picking default device: Apple M4
0.00.059.593 I ggml_metal_init: using embedded metal library
0.00.061.947 I ggml_metal_init: GPU name:   Apple M4
0.00.061.949 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.949 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.950 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.950 I ggml_metal_init: simdgroup reduction   = true
0.00.061.950 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.950 I ggml_metal_init: has bfloat            = true
0.00.061.950 I ggml_metal_init: use bfloat            = true
0.00.061.951 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.951 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.071.652 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.072.895 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.072.897 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.072.911 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.073.879 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.073.881 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.073.881 I llama_init_from_model: graph nodes  = 967
0.00.073.881 I llama_init_from_model: graph splits = 2
0.00.073.882 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.073.882 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.671.295 I 
0.00.671.323 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.671.326 I perplexity: tokenizing the input ..
0.00.679.172 I perplexity: tokenization took 7.844 ms
0.00.679.179 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.814.188 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.815.330 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.815.357 I llama_perf_context_print:        load time =     662.44 ms
0.00.815.358 I llama_perf_context_print: prompt eval time =     134.78 ms /   128 tokens (    1.05 ms per token,   949.70 tokens per second)
0.00.815.359 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.815.360 I llama_perf_context_print:       total time =     144.06 ms /   129 tokens
0.00.815.767 I ggml_metal_free: deallocating

real	0m0.829s
user	0m0.078s
sys	0m0.109s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4489 (f11cfdfd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.792 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.097 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.020.102 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.104 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.105 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.105 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.105 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.106 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.108 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.109 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.109 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.110 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.110 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.110 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.114 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.116 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.116 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.116 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.769 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.007 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.524 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.030.526 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.526 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.526 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.527 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.527 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.030.527 I llama_model_loader: - type  f32:  194 tensors
0.00.030.528 I llama_model_loader: - type q2_K:   49 tensors
0.00.030.528 I llama_model_loader: - type q3_K:   48 tensors
0.00.030.528 I llama_model_loader: - type q6_K:    1 tensors
0.00.030.529 I print_info: file format = GGUF V3 (latest)
0.00.030.529 I print_info: file type   = Q2_K - Medium
0.00.030.530 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.058.727 I load: special tokens cache size = 25
0.00.067.274 I load: token to piece cache size = 0.2984 MB
0.00.067.278 I print_info: arch             = gptneox
0.00.067.278 I print_info: vocab_only       = 0
0.00.067.278 I print_info: n_ctx_train      = 2048
0.00.067.279 I print_info: n_embd           = 2048
0.00.067.279 I print_info: n_layer          = 24
0.00.067.282 I print_info: n_head           = 16
0.00.067.283 I print_info: n_head_kv        = 16
0.00.067.283 I print_info: n_rot            = 32
0.00.067.284 I print_info: n_swa            = 0
0.00.067.284 I print_info: n_embd_head_k    = 128
0.00.067.286 I print_info: n_embd_head_v    = 128
0.00.067.287 I print_info: n_gqa            = 1
0.00.067.288 I print_info: n_embd_k_gqa     = 2048
0.00.067.289 I print_info: n_embd_v_gqa     = 2048
0.00.067.290 I print_info: f_norm_eps       = 1.0e-05
0.00.067.296 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.067.297 I print_info: f_clamp_kqv      = 0.0e+00
0.00.067.297 I print_info: f_max_alibi_bias = 0.0e+00
0.00.067.299 I print_info: f_logit_scale    = 0.0e+00
0.00.067.300 I print_info: n_ff             = 8192
0.00.067.300 I print_info: n_expert         = 0
0.00.067.300 I print_info: n_expert_used    = 0
0.00.067.301 I print_info: causal attn      = 1
0.00.067.301 I print_info: pooling type     = 0
0.00.067.302 I print_info: rope type        = 2
0.00.067.304 I print_info: rope scaling     = linear
0.00.067.305 I print_info: freq_base_train  = 10000.0
0.00.067.305 I print_info: freq_scale_train = 1
0.00.067.305 I print_info: n_ctx_orig_yarn  = 2048
0.00.067.306 I print_info: rope_finetuned   = unknown
0.00.067.306 I print_info: ssm_d_conv       = 0
0.00.067.306 I print_info: ssm_d_inner      = 0
0.00.067.306 I print_info: ssm_d_state      = 0
0.00.067.307 I print_info: ssm_dt_rank      = 0
0.00.067.307 I print_info: ssm_dt_b_c_rms   = 0
0.00.067.307 I print_info: model type       = 1.4B
0.00.067.308 I print_info: model params     = 1.41 B
0.00.067.308 I print_info: general.name     = 1.4B
0.00.067.308 I print_info: vocab type       = BPE
0.00.067.309 I print_info: n_vocab          = 50304
0.00.067.309 I print_info: n_merges         = 50009
0.00.067.309 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.067.310 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.067.310 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.067.311 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.067.313 I print_info: LF token         = 128 'Ä'
0.00.067.313 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.067.313 I print_info: max token length = 1024
0.00.069.716 I load_tensors: offloading 24 repeating layers to GPU
0.00.069.716 I load_tensors: offloading output layer to GPU
0.00.069.717 I load_tensors: offloaded 25/25 layers to GPU
0.00.069.728 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.069.729 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.070.174 I llama_init_from_model: n_seq_max     = 1
0.00.070.176 I llama_init_from_model: n_ctx         = 128
0.00.070.176 I llama_init_from_model: n_ctx_per_seq = 128
0.00.070.176 I llama_init_from_model: n_batch       = 128
0.00.070.177 I llama_init_from_model: n_ubatch      = 128
0.00.070.177 I llama_init_from_model: flash_attn    = 0
0.00.070.178 I llama_init_from_model: freq_base     = 10000.0
0.00.070.178 I llama_init_from_model: freq_scale    = 1
0.00.070.179 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.070.179 I ggml_metal_init: allocating
0.00.070.184 I ggml_metal_init: found device: Apple M4
0.00.070.187 I ggml_metal_init: picking default device: Apple M4
0.00.071.055 I ggml_metal_init: using embedded metal library
0.00.075.090 I ggml_metal_init: GPU name:   Apple M4
0.00.075.093 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.075.093 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.075.094 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.075.094 I ggml_metal_init: simdgroup reduction   = true
0.00.075.094 I ggml_metal_init: simdgroup matrix mul. = true
0.00.075.095 I ggml_metal_init: has bfloat            = true
0.00.075.095 I ggml_metal_init: use bfloat            = true
0.00.075.095 I ggml_metal_init: hasUnifiedMemory      = true
0.00.075.096 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.091 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.089.821 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.089.824 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.089.839 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.091.005 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.091.007 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.091.008 I llama_init_from_model: graph nodes  = 967
0.00.091.008 I llama_init_from_model: graph splits = 2
0.00.091.009 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.091.009 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.412.470 I 
0.00.412.510 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.412.516 I perplexity: tokenizing the input ..
0.00.420.666 I perplexity: tokenization took 8.148 ms
0.00.420.670 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.553.132 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.554.310 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.554.339 I llama_perf_context_print:        load time =     400.67 ms
0.00.554.340 I llama_perf_context_print: prompt eval time =     132.22 ms /   128 tokens (    1.03 ms per token,   968.08 tokens per second)
0.00.554.340 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.554.341 I llama_perf_context_print:       total time =     141.87 ms /   129 tokens
0.00.554.881 I ggml_metal_free: deallocating

real	0m0.587s
user	0m0.098s
sys	0m0.073s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4489 (f11cfdfd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.739 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.667 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.674 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.676 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.676 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.677 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.677 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.677 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.678 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.679 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.679 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.680 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.680 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.680 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.681 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.683 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.683 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.686 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.368 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.418 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.070 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.071 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.071 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.071 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.072 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.072 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.073 I llama_model_loader: - type  f32:  194 tensors
0.00.024.073 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.073 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.073 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.073 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.074 I print_info: file format = GGUF V3 (latest)
0.00.024.075 I print_info: file type   = Q3_K - Medium
0.00.024.076 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.042.522 I load: special tokens cache size = 25
0.00.048.537 I load: token to piece cache size = 0.2984 MB
0.00.048.540 I print_info: arch             = gptneox
0.00.048.540 I print_info: vocab_only       = 0
0.00.048.540 I print_info: n_ctx_train      = 2048
0.00.048.540 I print_info: n_embd           = 2048
0.00.048.541 I print_info: n_layer          = 24
0.00.048.543 I print_info: n_head           = 16
0.00.048.544 I print_info: n_head_kv        = 16
0.00.048.544 I print_info: n_rot            = 32
0.00.048.546 I print_info: n_swa            = 0
0.00.048.547 I print_info: n_embd_head_k    = 128
0.00.048.547 I print_info: n_embd_head_v    = 128
0.00.048.548 I print_info: n_gqa            = 1
0.00.048.548 I print_info: n_embd_k_gqa     = 2048
0.00.048.555 I print_info: n_embd_v_gqa     = 2048
0.00.048.556 I print_info: f_norm_eps       = 1.0e-05
0.00.048.556 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.557 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.557 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.557 I print_info: f_logit_scale    = 0.0e+00
0.00.048.559 I print_info: n_ff             = 8192
0.00.048.559 I print_info: n_expert         = 0
0.00.048.559 I print_info: n_expert_used    = 0
0.00.048.559 I print_info: causal attn      = 1
0.00.048.559 I print_info: pooling type     = 0
0.00.048.559 I print_info: rope type        = 2
0.00.048.560 I print_info: rope scaling     = linear
0.00.048.560 I print_info: freq_base_train  = 10000.0
0.00.048.562 I print_info: freq_scale_train = 1
0.00.048.562 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.562 I print_info: rope_finetuned   = unknown
0.00.048.562 I print_info: ssm_d_conv       = 0
0.00.048.562 I print_info: ssm_d_inner      = 0
0.00.048.562 I print_info: ssm_d_state      = 0
0.00.048.562 I print_info: ssm_dt_rank      = 0
0.00.048.563 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.563 I print_info: model type       = 1.4B
0.00.048.563 I print_info: model params     = 1.41 B
0.00.048.563 I print_info: general.name     = 1.4B
0.00.048.566 I print_info: vocab type       = BPE
0.00.048.566 I print_info: n_vocab          = 50304
0.00.048.566 I print_info: n_merges         = 50009
0.00.048.570 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.572 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.572 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.572 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.572 I print_info: LF token         = 128 'Ä'
0.00.048.573 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.574 I print_info: max token length = 1024
0.00.050.497 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.497 I load_tensors: offloading output layer to GPU
0.00.050.498 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.508 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.050.510 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.050.797 I llama_init_from_model: n_seq_max     = 1
0.00.050.798 I llama_init_from_model: n_ctx         = 128
0.00.050.798 I llama_init_from_model: n_ctx_per_seq = 128
0.00.050.798 I llama_init_from_model: n_batch       = 128
0.00.050.798 I llama_init_from_model: n_ubatch      = 128
0.00.050.798 I llama_init_from_model: flash_attn    = 0
0.00.050.799 I llama_init_from_model: freq_base     = 10000.0
0.00.050.799 I llama_init_from_model: freq_scale    = 1
0.00.050.799 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.800 I ggml_metal_init: allocating
0.00.050.802 I ggml_metal_init: found device: Apple M4
0.00.050.804 I ggml_metal_init: picking default device: Apple M4
0.00.051.377 I ggml_metal_init: using embedded metal library
0.00.053.749 I ggml_metal_init: GPU name:   Apple M4
0.00.053.751 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.751 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.751 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.752 I ggml_metal_init: simdgroup reduction   = true
0.00.053.752 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.752 I ggml_metal_init: has bfloat            = true
0.00.053.752 I ggml_metal_init: use bfloat            = true
0.00.053.752 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.753 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.140 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.063.368 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.370 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.384 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.064.211 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.064.213 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.064.213 I llama_init_from_model: graph nodes  = 967
0.00.064.213 I llama_init_from_model: graph splits = 2
0.00.064.214 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.064.215 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.466.537 I 
0.00.466.567 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.466.582 I perplexity: tokenizing the input ..
0.00.474.384 I perplexity: tokenization took 7.8 ms
0.00.474.387 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.606.492 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.607.664 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.607.691 I llama_perf_context_print:        load time =     457.79 ms
0.00.607.692 I llama_perf_context_print: prompt eval time =     131.88 ms /   128 tokens (    1.03 ms per token,   970.59 tokens per second)
0.00.607.693 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.607.693 I llama_perf_context_print:       total time =     141.15 ms /   129 tokens
0.00.608.243 I ggml_metal_free: deallocating

real	0m0.622s
user	0m0.075s
sys	0m0.078s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4489 (f11cfdfd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.894 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.941 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.947 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.952 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.953 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.955 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.955 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.955 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.956 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.956 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.957 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.957 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.958 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.958 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.958 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.960 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.966 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.967 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.643 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.710 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.472 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.474 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.474 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.474 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.475 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.475 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.475 I llama_model_loader: - type  f32:  194 tensors
0.00.024.476 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.476 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.476 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.477 I print_info: file format = GGUF V3 (latest)
0.00.024.477 I print_info: file type   = Q4_K - Medium
0.00.024.479 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.043.565 I load: special tokens cache size = 25
0.00.049.514 I load: token to piece cache size = 0.2984 MB
0.00.049.517 I print_info: arch             = gptneox
0.00.049.517 I print_info: vocab_only       = 0
0.00.049.518 I print_info: n_ctx_train      = 2048
0.00.049.518 I print_info: n_embd           = 2048
0.00.049.518 I print_info: n_layer          = 24
0.00.049.521 I print_info: n_head           = 16
0.00.049.522 I print_info: n_head_kv        = 16
0.00.049.522 I print_info: n_rot            = 32
0.00.049.522 I print_info: n_swa            = 0
0.00.049.522 I print_info: n_embd_head_k    = 128
0.00.049.523 I print_info: n_embd_head_v    = 128
0.00.049.523 I print_info: n_gqa            = 1
0.00.049.524 I print_info: n_embd_k_gqa     = 2048
0.00.049.525 I print_info: n_embd_v_gqa     = 2048
0.00.049.527 I print_info: f_norm_eps       = 1.0e-05
0.00.049.527 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.528 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.528 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.528 I print_info: f_logit_scale    = 0.0e+00
0.00.049.529 I print_info: n_ff             = 8192
0.00.049.529 I print_info: n_expert         = 0
0.00.049.529 I print_info: n_expert_used    = 0
0.00.049.529 I print_info: causal attn      = 1
0.00.049.529 I print_info: pooling type     = 0
0.00.049.531 I print_info: rope type        = 2
0.00.049.531 I print_info: rope scaling     = linear
0.00.049.532 I print_info: freq_base_train  = 10000.0
0.00.049.532 I print_info: freq_scale_train = 1
0.00.049.532 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.532 I print_info: rope_finetuned   = unknown
0.00.049.532 I print_info: ssm_d_conv       = 0
0.00.049.533 I print_info: ssm_d_inner      = 0
0.00.049.533 I print_info: ssm_d_state      = 0
0.00.049.533 I print_info: ssm_dt_rank      = 0
0.00.049.533 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.533 I print_info: model type       = 1.4B
0.00.049.534 I print_info: model params     = 1.41 B
0.00.049.534 I print_info: general.name     = 1.4B
0.00.049.534 I print_info: vocab type       = BPE
0.00.049.535 I print_info: n_vocab          = 50304
0.00.049.535 I print_info: n_merges         = 50009
0.00.049.535 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.535 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.535 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.536 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.536 I print_info: LF token         = 128 'Ä'
0.00.049.536 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.536 I print_info: max token length = 1024
0.00.051.491 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.491 I load_tensors: offloading output layer to GPU
0.00.051.491 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.502 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.503 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.051.781 I llama_init_from_model: n_seq_max     = 1
0.00.051.781 I llama_init_from_model: n_ctx         = 128
0.00.051.781 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.782 I llama_init_from_model: n_batch       = 128
0.00.051.782 I llama_init_from_model: n_ubatch      = 128
0.00.051.782 I llama_init_from_model: flash_attn    = 0
0.00.051.782 I llama_init_from_model: freq_base     = 10000.0
0.00.051.782 I llama_init_from_model: freq_scale    = 1
0.00.051.783 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.783 I ggml_metal_init: allocating
0.00.051.786 I ggml_metal_init: found device: Apple M4
0.00.051.788 I ggml_metal_init: picking default device: Apple M4
0.00.052.346 I ggml_metal_init: using embedded metal library
0.00.054.696 I ggml_metal_init: GPU name:   Apple M4
0.00.054.698 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.698 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.699 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.699 I ggml_metal_init: simdgroup reduction   = true
0.00.054.699 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.699 I ggml_metal_init: has bfloat            = true
0.00.054.699 I ggml_metal_init: use bfloat            = true
0.00.054.700 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.700 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.330 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.613 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.616 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.630 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.562 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.563 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.563 I llama_init_from_model: graph nodes  = 967
0.00.066.563 I llama_init_from_model: graph splits = 2
0.00.066.564 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.565 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.563.556 I 
0.00.563.626 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.563.636 I perplexity: tokenizing the input ..
0.00.571.596 I perplexity: tokenization took 7.957 ms
0.00.571.599 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.706.214 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.707.402 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.707.426 I llama_perf_context_print:        load time =     554.65 ms
0.00.707.427 I llama_perf_context_print: prompt eval time =     134.39 ms /   128 tokens (    1.05 ms per token,   952.47 tokens per second)
0.00.707.428 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.707.428 I llama_perf_context_print:       total time =     143.88 ms /   129 tokens
0.00.707.787 I ggml_metal_free: deallocating

real	0m0.721s
user	0m0.077s
sys	0m0.104s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4489 (f11cfdfd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.913 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.513 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.518 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.519 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.520 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.520 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.521 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.521 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.522 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.522 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.523 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.523 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.524 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.525 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.525 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.528 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.528 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.528 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.124 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.142 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.760 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.761 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.762 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.762 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.762 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.763 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.763 I llama_model_loader: - type  f32:  194 tensors
0.00.024.764 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.764 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.764 I print_info: file format = GGUF V3 (latest)
0.00.024.765 I print_info: file type   = Q5_K - Medium
0.00.024.767 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.043.154 I load: special tokens cache size = 25
0.00.049.003 I load: token to piece cache size = 0.2984 MB
0.00.049.006 I print_info: arch             = gptneox
0.00.049.006 I print_info: vocab_only       = 0
0.00.049.006 I print_info: n_ctx_train      = 2048
0.00.049.006 I print_info: n_embd           = 2048
0.00.049.007 I print_info: n_layer          = 24
0.00.049.010 I print_info: n_head           = 16
0.00.049.010 I print_info: n_head_kv        = 16
0.00.049.010 I print_info: n_rot            = 32
0.00.049.011 I print_info: n_swa            = 0
0.00.049.011 I print_info: n_embd_head_k    = 128
0.00.049.012 I print_info: n_embd_head_v    = 128
0.00.049.012 I print_info: n_gqa            = 1
0.00.049.013 I print_info: n_embd_k_gqa     = 2048
0.00.049.014 I print_info: n_embd_v_gqa     = 2048
0.00.049.014 I print_info: f_norm_eps       = 1.0e-05
0.00.049.015 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.015 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.015 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.015 I print_info: f_logit_scale    = 0.0e+00
0.00.049.016 I print_info: n_ff             = 8192
0.00.049.016 I print_info: n_expert         = 0
0.00.049.016 I print_info: n_expert_used    = 0
0.00.049.017 I print_info: causal attn      = 1
0.00.049.017 I print_info: pooling type     = 0
0.00.049.017 I print_info: rope type        = 2
0.00.049.017 I print_info: rope scaling     = linear
0.00.049.020 I print_info: freq_base_train  = 10000.0
0.00.049.020 I print_info: freq_scale_train = 1
0.00.049.020 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.021 I print_info: rope_finetuned   = unknown
0.00.049.021 I print_info: ssm_d_conv       = 0
0.00.049.021 I print_info: ssm_d_inner      = 0
0.00.049.021 I print_info: ssm_d_state      = 0
0.00.049.021 I print_info: ssm_dt_rank      = 0
0.00.049.021 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.022 I print_info: model type       = 1.4B
0.00.049.022 I print_info: model params     = 1.41 B
0.00.049.022 I print_info: general.name     = 1.4B
0.00.049.023 I print_info: vocab type       = BPE
0.00.049.027 I print_info: n_vocab          = 50304
0.00.049.027 I print_info: n_merges         = 50009
0.00.049.029 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.029 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.029 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.029 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.029 I print_info: LF token         = 128 'Ä'
0.00.049.030 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.030 I print_info: max token length = 1024
0.00.050.952 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.952 I load_tensors: offloading output layer to GPU
0.00.050.952 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.963 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.050.964 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.051.242 I llama_init_from_model: n_seq_max     = 1
0.00.051.243 I llama_init_from_model: n_ctx         = 128
0.00.051.243 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.243 I llama_init_from_model: n_batch       = 128
0.00.051.243 I llama_init_from_model: n_ubatch      = 128
0.00.051.244 I llama_init_from_model: flash_attn    = 0
0.00.051.244 I llama_init_from_model: freq_base     = 10000.0
0.00.051.244 I llama_init_from_model: freq_scale    = 1
0.00.051.245 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.245 I ggml_metal_init: allocating
0.00.051.247 I ggml_metal_init: found device: Apple M4
0.00.051.249 I ggml_metal_init: picking default device: Apple M4
0.00.051.821 I ggml_metal_init: using embedded metal library
0.00.054.166 I ggml_metal_init: GPU name:   Apple M4
0.00.054.167 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.168 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.168 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.168 I ggml_metal_init: simdgroup reduction   = true
0.00.054.169 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.169 I ggml_metal_init: has bfloat            = true
0.00.054.169 I ggml_metal_init: use bfloat            = true
0.00.054.169 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.170 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.885 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.171 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.173 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.187 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.056 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.057 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.058 I llama_init_from_model: graph nodes  = 967
0.00.065.058 I llama_init_from_model: graph splits = 2
0.00.065.059 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.059 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.617.659 I 
0.00.617.686 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.617.689 I perplexity: tokenizing the input ..
0.00.625.733 I perplexity: tokenization took 8.041 ms
0.00.625.736 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.766.465 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.767.633 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.767.672 I llama_perf_context_print:        load time =     607.74 ms
0.00.767.673 I llama_perf_context_print: prompt eval time =     140.50 ms /   128 tokens (    1.10 ms per token,   911.03 tokens per second)
0.00.767.675 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.767.676 I llama_perf_context_print:       total time =     150.01 ms /   129 tokens
0.00.768.203 I ggml_metal_free: deallocating

real	0m0.784s
user	0m0.076s
sys	0m0.108s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4489 (f11cfdfd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.438 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.153 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.157 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.162 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.163 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.165 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.165 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.165 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.166 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.167 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.167 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.167 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.168 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.172 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.172 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.174 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.174 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.175 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.844 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.843 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.454 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.455 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.456 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.456 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.456 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.456 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.457 I llama_model_loader: - type  f32:  194 tensors
0.00.024.457 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.458 I print_info: file format = GGUF V3 (latest)
0.00.024.458 I print_info: file type   = Q6_K
0.00.024.459 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.042.950 I load: special tokens cache size = 25
0.00.048.839 I load: token to piece cache size = 0.2984 MB
0.00.048.842 I print_info: arch             = gptneox
0.00.048.842 I print_info: vocab_only       = 0
0.00.048.843 I print_info: n_ctx_train      = 2048
0.00.048.843 I print_info: n_embd           = 2048
0.00.048.843 I print_info: n_layer          = 24
0.00.048.846 I print_info: n_head           = 16
0.00.048.846 I print_info: n_head_kv        = 16
0.00.048.847 I print_info: n_rot            = 32
0.00.048.847 I print_info: n_swa            = 0
0.00.048.847 I print_info: n_embd_head_k    = 128
0.00.048.847 I print_info: n_embd_head_v    = 128
0.00.048.848 I print_info: n_gqa            = 1
0.00.048.849 I print_info: n_embd_k_gqa     = 2048
0.00.048.849 I print_info: n_embd_v_gqa     = 2048
0.00.048.850 I print_info: f_norm_eps       = 1.0e-05
0.00.048.850 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.851 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.851 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.851 I print_info: f_logit_scale    = 0.0e+00
0.00.048.852 I print_info: n_ff             = 8192
0.00.048.852 I print_info: n_expert         = 0
0.00.048.852 I print_info: n_expert_used    = 0
0.00.048.852 I print_info: causal attn      = 1
0.00.048.853 I print_info: pooling type     = 0
0.00.048.853 I print_info: rope type        = 2
0.00.048.853 I print_info: rope scaling     = linear
0.00.048.853 I print_info: freq_base_train  = 10000.0
0.00.048.854 I print_info: freq_scale_train = 1
0.00.048.854 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.854 I print_info: rope_finetuned   = unknown
0.00.048.855 I print_info: ssm_d_conv       = 0
0.00.048.855 I print_info: ssm_d_inner      = 0
0.00.048.855 I print_info: ssm_d_state      = 0
0.00.048.855 I print_info: ssm_dt_rank      = 0
0.00.048.855 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.855 I print_info: model type       = 1.4B
0.00.048.856 I print_info: model params     = 1.41 B
0.00.048.856 I print_info: general.name     = 1.4B
0.00.048.857 I print_info: vocab type       = BPE
0.00.048.857 I print_info: n_vocab          = 50304
0.00.048.859 I print_info: n_merges         = 50009
0.00.048.859 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.860 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.860 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.860 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.860 I print_info: LF token         = 128 'Ä'
0.00.048.860 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.861 I print_info: max token length = 1024
0.00.050.511 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.511 I load_tensors: offloading output layer to GPU
0.00.050.511 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.521 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.050.522 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.050.799 I llama_init_from_model: n_seq_max     = 1
0.00.050.800 I llama_init_from_model: n_ctx         = 128
0.00.050.800 I llama_init_from_model: n_ctx_per_seq = 128
0.00.050.800 I llama_init_from_model: n_batch       = 128
0.00.050.800 I llama_init_from_model: n_ubatch      = 128
0.00.050.800 I llama_init_from_model: flash_attn    = 0
0.00.050.801 I llama_init_from_model: freq_base     = 10000.0
0.00.050.801 I llama_init_from_model: freq_scale    = 1
0.00.050.801 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.802 I ggml_metal_init: allocating
0.00.050.805 I ggml_metal_init: found device: Apple M4
0.00.050.807 I ggml_metal_init: picking default device: Apple M4
0.00.051.360 I ggml_metal_init: using embedded metal library
0.00.053.682 I ggml_metal_init: GPU name:   Apple M4
0.00.053.684 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.684 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.684 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.684 I ggml_metal_init: simdgroup reduction   = true
0.00.053.685 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.685 I ggml_metal_init: has bfloat            = true
0.00.053.685 I ggml_metal_init: use bfloat            = true
0.00.053.685 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.686 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.914 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.230 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.234 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.250 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.090 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.091 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.091 I llama_init_from_model: graph nodes  = 967
0.00.065.091 I llama_init_from_model: graph splits = 2
0.00.065.093 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.093 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.653.094 I 
0.00.653.121 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.653.124 I perplexity: tokenizing the input ..
0.00.660.941 I perplexity: tokenization took 7.815 ms
0.00.660.944 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.800.086 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.801.477 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.801.503 I llama_perf_context_print:        load time =     643.65 ms
0.00.801.504 I llama_perf_context_print: prompt eval time =     138.89 ms /   128 tokens (    1.09 ms per token,   921.57 tokens per second)
0.00.801.505 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.801.505 I llama_perf_context_print:       total time =     148.41 ms /   129 tokens
0.00.801.862 I ggml_metal_free: deallocating

real	0m0.815s
user	0m0.077s
sys	0m0.125s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.182 I build: 4489 (f11cfdfd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.016.928 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.103 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.026.108 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.110 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.116 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.117 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.117 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.117 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.118 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.119 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.119 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.120 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.120 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.120 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.121 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.123 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.123 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.123 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.703 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.691 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.390 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.392 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.392 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.392 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.393 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.393 I llama_model_loader: - type  f32:  194 tensors
0.00.034.394 I llama_model_loader: - type  f16:   98 tensors
0.00.034.394 I print_info: file format = GGUF V3 (latest)
0.00.034.395 I print_info: file type   = all F32 (guessed)
0.00.034.397 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.054.499 I load: special tokens cache size = 25
0.00.060.525 I load: token to piece cache size = 0.2984 MB
0.00.060.530 I print_info: arch             = gptneox
0.00.060.531 I print_info: vocab_only       = 0
0.00.060.531 I print_info: n_ctx_train      = 2048
0.00.060.531 I print_info: n_embd           = 2048
0.00.060.531 I print_info: n_layer          = 24
0.00.060.536 I print_info: n_head           = 16
0.00.060.537 I print_info: n_head_kv        = 16
0.00.060.537 I print_info: n_rot            = 32
0.00.060.537 I print_info: n_swa            = 0
0.00.060.537 I print_info: n_embd_head_k    = 128
0.00.060.537 I print_info: n_embd_head_v    = 128
0.00.060.541 I print_info: n_gqa            = 1
0.00.060.541 I print_info: n_embd_k_gqa     = 2048
0.00.060.546 I print_info: n_embd_v_gqa     = 2048
0.00.060.550 I print_info: f_norm_eps       = 1.0e-05
0.00.060.550 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.060.550 I print_info: f_clamp_kqv      = 0.0e+00
0.00.060.550 I print_info: f_max_alibi_bias = 0.0e+00
0.00.060.551 I print_info: f_logit_scale    = 0.0e+00
0.00.060.552 I print_info: n_ff             = 8192
0.00.060.552 I print_info: n_expert         = 0
0.00.060.552 I print_info: n_expert_used    = 0
0.00.060.552 I print_info: causal attn      = 1
0.00.060.552 I print_info: pooling type     = 0
0.00.060.552 I print_info: rope type        = 2
0.00.060.553 I print_info: rope scaling     = linear
0.00.060.553 I print_info: freq_base_train  = 10000.0
0.00.060.553 I print_info: freq_scale_train = 1
0.00.060.553 I print_info: n_ctx_orig_yarn  = 2048
0.00.060.553 I print_info: rope_finetuned   = unknown
0.00.060.554 I print_info: ssm_d_conv       = 0
0.00.060.554 I print_info: ssm_d_inner      = 0
0.00.060.554 I print_info: ssm_d_state      = 0
0.00.060.554 I print_info: ssm_dt_rank      = 0
0.00.060.554 I print_info: ssm_dt_b_c_rms   = 0
0.00.060.554 I print_info: model type       = 1.4B
0.00.060.555 I print_info: model params     = 1.41 B
0.00.060.555 I print_info: general.name     = 1.4B
0.00.060.555 I print_info: vocab type       = BPE
0.00.060.555 I print_info: n_vocab          = 50304
0.00.060.556 I print_info: n_merges         = 50009
0.00.060.556 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.060.556 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.060.556 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.060.556 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.060.556 I print_info: LF token         = 128 'Ä'
0.00.060.557 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.060.557 I print_info: max token length = 1024
0.00.062.875 I load_tensors: offloading 24 repeating layers to GPU
0.00.062.875 I load_tensors: offloading output layer to GPU
0.00.062.876 I load_tensors: offloaded 25/25 layers to GPU
0.00.062.887 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.062.888 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.063.177 I llama_init_from_model: n_seq_max     = 1
0.00.063.178 I llama_init_from_model: n_ctx         = 128
0.00.063.178 I llama_init_from_model: n_ctx_per_seq = 128
0.00.063.178 I llama_init_from_model: n_batch       = 128
0.00.063.179 I llama_init_from_model: n_ubatch      = 128
0.00.063.179 I llama_init_from_model: flash_attn    = 0
0.00.063.179 I llama_init_from_model: freq_base     = 10000.0
0.00.063.179 I llama_init_from_model: freq_scale    = 1
0.00.063.180 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.063.180 I ggml_metal_init: allocating
0.00.063.184 I ggml_metal_init: found device: Apple M4
0.00.063.186 I ggml_metal_init: picking default device: Apple M4
0.00.063.803 I ggml_metal_init: using embedded metal library
0.00.066.148 I ggml_metal_init: GPU name:   Apple M4
0.00.066.150 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.066.150 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.066.151 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.066.151 I ggml_metal_init: simdgroup reduction   = true
0.00.066.151 I ggml_metal_init: simdgroup matrix mul. = true
0.00.066.151 I ggml_metal_init: has bfloat            = true
0.00.066.151 I ggml_metal_init: use bfloat            = true
0.00.066.152 I ggml_metal_init: hasUnifiedMemory      = true
0.00.066.153 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.076.419 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.077.786 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.077.788 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.077.804 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.078.694 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.078.695 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.078.696 I llama_init_from_model: graph nodes  = 967
0.00.078.696 I llama_init_from_model: graph splits = 2
0.00.078.697 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.078.698 I 
0.00.078.729 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.078.730 I compute_imatrix: tokenizing the input ..
0.00.085.791 I compute_imatrix: tokenization took 7.059 ms
0.00.085.794 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.620.728 I compute_imatrix: 1.53 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.623.323 I llama_perf_context_print:        load time =    1603.80 ms
0.01.623.324 I llama_perf_context_print: prompt eval time =    1534.28 ms /   128 tokens (   11.99 ms per token,    83.43 tokens per second)
0.01.623.325 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.623.325 I llama_perf_context_print:       total time =    1606.39 ms /   129 tokens
0.01.623.873 I ggml_metal_free: deallocating

real	0m1.804s
user	0m0.140s
sys	0m0.236s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4489 (f11cfdfd)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11f70a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11f70aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11f70aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11f70b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11f70bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11f70c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11f70c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11f70cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11f70d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11f70d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11f70dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11f70e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11f70ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11f70f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11f70fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11f710310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11f710a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11f711150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11f711870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11f712040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11f712760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11f712e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11f7135a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11f713e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11f714560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11f714820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11f714e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11f715aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11f715fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11f7162a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11f716740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11f716a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11f717290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11f7177d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11f717a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11f717f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11f7183d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11f718870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11f718d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11f7191b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11f719650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11f719af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11f719f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11f71a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11f71a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11f71ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11f71b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11f71bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11f71c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11f71c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11f71ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11f71d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11f71da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11f71e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11f71e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11f71ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11f71f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11f71f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11f71fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11f720280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11f720540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11f7209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11f720e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11f721320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11f7217c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11f721c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11f722100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11f7225a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11f722a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11f722ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11f723380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11f723820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11f723cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11f724210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11f724760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11f724cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11f725200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11f725750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11f725ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11f7261f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11f726740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11f726c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11f7271e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11f727730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11f727c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11f7281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11f728720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11f728c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11f7291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11f729710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11f729c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11f72a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11f72a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11f72ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11f72b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11f72b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11f72bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11f71b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11f72c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11f72c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11f72cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11f72d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11f72d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11f72dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11f72e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11f72e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11f72ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11f72f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11f72f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11f72fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11f7302d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11f730820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11f730d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11f731210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11f7316b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11f731b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11f731ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11f732490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11f732930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11f732dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11f733270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11f733710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11f733bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11f734050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11f7344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11f734990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11f734e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11f7352d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11f735770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11f735c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11f7360b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11f736550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11f7369f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11f736e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11f737330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11f7377d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11f737c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11f738110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11f7385b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11f738a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11f738ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11f739390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11f739830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11f739cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11f73a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11f73a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11f73aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11f73af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11f73b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11f73b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11f73bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11f73c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11f73c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11f73cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11f73cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11f73d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11f73d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11f73dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11f73e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11f73e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11f73eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11f73f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11f73f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11f73f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11f73fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11f740290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11f740730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11f740bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11f741070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11f741510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11f7419b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11f741e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11f7422f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11f742790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11f742c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11f7430d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11f743570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11f743a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11f743eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11f744350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11f7447f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11f744c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11f745130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11f7455d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11f745a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11f745f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11f7463b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11f746850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11f746cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11f747190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11f747630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11f747ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11f747f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11f7484c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11f748a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11f748f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11f7494b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11f749770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11f749d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11f74a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11f74a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11f74b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11f74b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11f74b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11f74bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11f74c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11f74cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11f74d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11f74d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11f74dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11f74e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11f74e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11f74ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11f74f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11f74f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11f74fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11f750270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11f7507c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11f750d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11f751260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11f7517b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11f751d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11f752250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11f7527a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11f752cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11f753240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11f753790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11f753ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11f754230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11f754780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11f754cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11f755220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11f755770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11f755cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11f756210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11f756760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11f756cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11f757200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11f757750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11f757ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11f7581f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11f758740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11f758c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11f7591e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11f759730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11f759c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11f75a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11f75a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11f75ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11f75b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11f75b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11f75bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11f75c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11f75c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11f75cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11f75d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11f75d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11f75dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11f75e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11f75e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11f75ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11f75f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11f75f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11f75fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11f760170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11f7606c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11f760c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11f7610b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11f761550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11f7619f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11f761e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11f762330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11f7627d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11f762c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11f763110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11f7635b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11f763a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11f763ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11f764390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11f764830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11f764cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11f765170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11f7656c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11f765de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11f766500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11f766c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11f767340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11f767600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11f767df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11f7680b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11f7686c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.140.885 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.140.889 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10ff04d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10ff051c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10ff05630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10ff05aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10ff05f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10ff06380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10ff067f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10ff06c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10ff070d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10ff07540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10ff079b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10ff080a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10ff08bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10ff09370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10ff09b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10ff0a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10ff0a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10ff0b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10ff0b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10ff0bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10ff0c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10ff0cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10ff0d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10ff0dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10ff0e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10ff0e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10ff0e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10ff0ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10ff0f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10ff0f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10ff0fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10ff0ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10ff103b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10ff10670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10ff10ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10ff10f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10ff113c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10ff11830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10ff11ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10ff12110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10ff12580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10ff129f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10ff12e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10ff132d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10ff13740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10ff13bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10ff14020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10ff14490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10ff14900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10ff14d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10ff151e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10ff15650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10ff15ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10ff15f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10ff163a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10ff16810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10ff16d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10ff17280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10ff176f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10ff17b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10ff17fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10ff18440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10ff188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10ff18d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10ff19190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10ff19600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10ff19a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10ff19ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10ff1a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10ff1a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10ff1ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10ff1b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10ff1b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10ff1b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10ff1bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10ff1c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10ff1c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10ff1cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10ff1cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10ff1d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10ff1d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10ff1dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10ff1e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10ff1e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10ff1ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10ff1eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10ff1f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10ff1f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10ff1fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10ff20080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10ff204f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10ff20960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10ff20dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10ff21240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10ff216b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10ff21b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10ff21f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10ff22400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10ff22870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10ff22ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10ff23150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10ff235c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10ff23a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10ff23ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10ff24310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10ff24780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10ff24bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10ff25060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10ff254d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10ff25940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10ff25db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10ff26220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10ff26690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10ff26b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10ff26f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10ff273e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10ff27850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10ff27cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10ff28130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10ff285a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10ff28a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10ff28e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10ff292f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10ff29760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10ff29bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10ff2a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10ff2a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10ff2a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10ff2ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10ff2b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10ff2b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10ff2bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10ff2bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10ff2c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10ff2c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10ff2cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10ff2d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10ff2d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10ff2d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10ff2de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10ff2e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10ff2e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10ff2ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10ff2f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10ff2f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10ff2f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10ff2fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10ff301e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10ff30650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10ff30ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10ff30f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10ff313a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10ff31810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10ff31c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10ff320f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10ff32560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10ff329d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10ff32e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10ff332b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10ff33720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10ff33b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10ff34000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10ff34470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10ff348e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10ff34d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10ff351c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10ff35df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10ff360b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10ff36370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10ff367e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10ff36c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10ff370c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10ff37530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10ff379a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10ff37e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10ff38280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10ff386f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10ff38b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10ff38fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10ff39440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10ff398b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10ff39d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10ff3a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10ff3a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10ff3aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10ff3aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10ff3b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10ff3b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10ff3bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10ff3c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10ff3c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10ff3c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10ff3cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10ff3d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10ff3d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10ff3db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10ff3dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10ff3e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10ff3e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10ff3ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10ff3f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10ff3f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10ff3fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10ff40050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10ff404c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10ff40930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10ff40da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10ff41210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10ff41730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10ff41c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10ff427b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10ff42a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10ff43030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10ff435f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10ff43bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10ff44170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10ff44730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10ff44cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10ff452b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10ff45870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10ff45e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10ff463f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10ff469b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10ff46f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10ff47530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10ff47af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10ff480b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10ff48670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10ff48c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10ff491f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10ff497b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10ff49d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10ff4a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10ff4a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10ff4aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10ff4b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10ff4ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10ff4bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10ff4c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10ff4cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10ff4d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10ff4d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10ff4dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10ff4e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10ff4e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10ff4edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10ff4f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10ff4f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10ff4ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10ff504f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10ff50ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10ff51070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10ff51630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10ff51bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10ff521b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10ff52770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10ff52d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10ff532f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10ff538b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10ff53e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10ff54430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10ff549f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10ff54fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10ff55570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10ff55b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10ff560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10ff566b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10ff56c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10ff57170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10ff57670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10ff57b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10ff58070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10ff58570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10ff58a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10ff58f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10ff59470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10ff59970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10ff59e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10ff5a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10ff5a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10ff5ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10ff5b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10ff5b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10ff5c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10ff5c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10ff5cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10ff5d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10ff5d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10ff5e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10ff5e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10ff5ea60 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10ff5ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10ff4c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10ff4b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10ff48370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10ff45b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10ff55270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10ff52a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10ff507b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10ff4e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10ff466b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10ff43e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10ff48ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10ff4a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10ff4f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10ff4c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10ff54130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10ff47db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10ff51330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10ff4abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10ff4ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10ff477f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10ff55830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10ff449f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10ff432f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10ff45570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10ff55df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10ff4b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10ff535b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10ff494b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10ff4bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10ff4fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10ff47230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10ff501f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10ff518f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10ff460f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10ff546f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10ff51eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10ff4d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10ff56970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10ff44fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10ff563b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10ff44430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10ff54cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10ff4eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10ff50d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10ff53b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10ff52470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10ff4a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10ff41f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10ff04880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10ff5dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10ff0bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10ff5f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10ff5f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10ff5f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10ff5f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10ff5fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10ff5ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10ff601c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10ff60480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10ff60740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10ff60a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10ff60cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10ff60f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10ff61240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10ff61500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10ff617c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10ff61a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10ff61d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10ff62000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10ff622c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10ff62580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10ff62840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10ff62b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10ff62dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10ff63080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10ff63340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10ff63600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10ff638c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10ff63b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10ff63e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10ff64100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10ff643c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10ff64680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10ff64940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10ff64c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10ff64ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10ff65180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10ff65440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10ff65700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10ff659c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10ff65c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10ff65f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10ff66200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10ff664c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10ff66780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10ff66a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10ff66d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10ff66fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10ff67280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10ff67540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10ff67800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10ff67ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10ff67d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10ff68040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10ff68300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10ff685c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10ff68880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10ff68b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10ff68e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10ff690c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10ff69380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10ff69640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10ff69900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10ff69bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10ff69e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10ff6a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10ff6a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10ff6a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10ff6a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10ff6ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10ff6af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10ff6b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10ff6b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10ff6b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10ff6ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10ff6bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10ff6bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10ff6c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10ff6c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10ff6c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10ff6ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10ff6cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10ff6d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10ff6d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10ff6d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10ff6d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10ff6db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10ff6ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10ff6e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10ff6e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10ff6e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10ff6e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10ff6eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10ff6ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10ff6f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10ff6f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10ff6f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10ff6f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10ff6fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10ff6fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10ff70180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10ff70440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10ff70700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10ff709c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10ff70c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10ff70f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10ff71200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10ff714c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10ff71780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10ff71a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10ff71d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10ff71fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10ff72280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10ff72540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10ff72800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10ff72ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10ff72d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10ff73040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10ff73300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10ff735c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10ff73880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10ff73b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10ff73e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10ff740c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10ff74380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10ff74640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10ff74900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10ff74bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10ff74e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10ff75140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10ff75400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10ff756c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10ff75980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10ff75c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10ff75f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10ff761c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10ff76480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10ff76740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10ff76a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10ff76cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10ff76f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10ff77240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10ff77500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10ff777c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10ff77a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11b704080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11b7044f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11b704960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11b704dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11b705240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11b7056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11b705b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11b705f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11b706400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11b706870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11b706ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11b707150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11b7075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11b707a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11b7085b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11b708870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11b708b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11b708fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11b709410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11b709880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11b709cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11b70a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11b70a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11b70aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11b70aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11b70b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11b70b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11b70bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11b70c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11b70c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11b70c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11b70cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11b70d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11b70d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11b70db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11b70df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11b70e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11b70e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11b70ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11b70f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11b70f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11b70fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11b70fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11b710300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11b710770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11b710be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11b711050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11b7114c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11b711930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11b711da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11b712210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11b712680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11b712af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11b712f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11b7133d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11b713840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11b713cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11b714120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11b714590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11b714a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11b714e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11b7152e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11b715750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11b715bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11b716030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11b7164a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11b716910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11b716d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11b7171f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11b717660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11b717ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11b717f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11b7183b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11b718820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11b718c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11b719100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11b719570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11b7199e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11b719e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11b71a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11b71a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11b71aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11b71b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11b71b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11b71b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11b71bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11b71c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11b71cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11b71d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11b71da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11b71e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11b71e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11b71e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11b71eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11b71f4e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.799s
user	0m0.309s
sys	0m0.278s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4489 (f11cfdfd)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13af10120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13af10830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13af10de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13af11390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13af11940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13af11ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13af124a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13af12a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13af13000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13af13500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13af13a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13af13f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13af14a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13af151d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13af159e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13af16100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13af16820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13af16f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13af17660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13af17e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13af18550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13af18c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13af19390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13af19c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13af1a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13af1a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13af1ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13af1b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13af1bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13af1c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13af1c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13af1c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13af1d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13af1d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13af1d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13af1dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13af1e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13af1e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13af1eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13af1efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13af1f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13af1f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13af1fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13af20220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13af204e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13af20af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13af21100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13af21a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13af22030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13af22640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13af22c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13af23260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13af23870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13af23e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13af24670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13af24b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13af24fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13af25270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13af25880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13af26070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13af26330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13af267d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13af26c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13af27110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13af275b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13af27a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13af27ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13af28390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13af28830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13af28cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13af29170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13af29610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13af29ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13af2a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13af2a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13af2aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13af2aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13af2b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13af2ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13af2bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13af2c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13af2ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13af2cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13af2d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13af2da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13af2dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13af2e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13af2ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13af2efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13af2f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13af2fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13af2ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13af304f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13af30a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13af30f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13af314e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13af31a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13af21710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13af31ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13af32650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13af32ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13af330f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13af33640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13af33b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13af340e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13af34630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13af34b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13af350d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13af35620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13af35b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13af360c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13af36610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13af36b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13af37000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13af374a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13af37940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13af37de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13af38280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13af38720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13af38bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13af39060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13af39500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13af399a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13af39e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13af3a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13af3a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13af3ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13af3b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13af3b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13af3ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13af3bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13af3c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13af3c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13af3cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13af3d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13af3d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13af3da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13af3df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13af3e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13af3e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13af3ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13af3f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13af3f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13af3fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13af3ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13af40400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13af408a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13af40d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13af411e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13af41680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13af41b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13af41fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13af42460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13af42900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13af42da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13af43240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13af436e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13af43b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13af44020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13af444c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13af44960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13af44e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13af452a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13af45740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13af45be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13af46080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13af46520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13af469c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13af46e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13af47300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13af477a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13af47c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13af480e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13af48580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13af48a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13af48ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13af49360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13af49800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13af49ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13af4a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13af4a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13af4aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13af4af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13af4b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13af4b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13af4bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13af4c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13af4c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13af4cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13af4cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13af4d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13af4d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13af4dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13af4e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13af4e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13af4ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13af4f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13af4f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13af4fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13af50180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13af50790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13af50f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13af51420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13af516e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13af51cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13af52300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13af52af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13af52f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13af53430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13af538d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13af54080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13af545d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13af54b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13af55070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13af555c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13af55b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13af56060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13af565b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13af56b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13af57050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13af575a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13af57af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13af58040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13af58590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13af58ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13af59030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13af59580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13af59ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13af5a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13af5a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13af5aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13af5b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13af5b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13af5bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13af5c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13af5c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13af5caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13af5cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13af5d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13af5da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13af5dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13af5e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13af5ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13af5efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13af5f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13af5fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13af5ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13af60510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13af60a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13af60fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13af61500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13af61a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13af61fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13af624f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13af62a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13af62f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13af634e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13af63a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13af63f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13af644d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13af64a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13af64f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13af654c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13af65a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13af65f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13af664b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13af66a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13af66ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13af67340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13af677e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13af67c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13af68120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13af685c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13af68a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13af68f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13af693a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13af69840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13af69ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13af6a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13af6a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13af6aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13af6af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13af6b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13af6bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13af6c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13af6ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13af6d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13af6d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13af6dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13af6dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13af6e4b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.099.699 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.099.703 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x138904b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x138904fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x138905430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1389058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x138905d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x138906180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1389065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x138906a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x138906ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x138907340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1389077b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x138907ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1389089c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x138909170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x138909980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13890a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13890a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13890aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13890b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13890bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13890c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13890cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13890d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13890d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13890e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13890e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13890e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13890eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13890ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13890f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13890f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13890fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1389101b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x138910470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1389108e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x138910d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1389111c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x138911630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x138911aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x138911f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x138912380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1389127f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x138912c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1389130d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x138913540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1389139b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x138913e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x138914290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x138914700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x138914b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x138914fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x138915450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1389158c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x138915d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1389161a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x138916610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x138916b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x138917080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1389174f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x138917960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x138917dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x138918240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1389186b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x138918b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x138918f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x138919400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x138919870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x138919ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13891a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13891a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13891aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13891aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13891b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13891b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13891bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13891c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13891c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13891c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13891cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13891d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13891d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13891db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13891df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13891e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13891e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13891ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13891f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13891f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13891fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13891fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1389202f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x138920760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x138920bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x138921040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1389214b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x138921920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x138921d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x138922200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x138922670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x138922ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x138922f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1389233c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x138923830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x138923ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x138924110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x138924580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1389249f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x138924e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1389252d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x138925740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x138925bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x138926020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x138926490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x138926900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x138926d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1389271e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x138927650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x138927ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x138927f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1389283a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x138928810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x138928c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1389290f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x138929560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1389299d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x138929e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13892a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13892a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13892ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13892b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13892b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13892b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13892bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13892c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13892c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13892caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13892cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13892d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13892d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13892dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13892e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13892e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13892e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13892ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13892f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13892f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13892fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13892ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x138930450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1389308c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x138930d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1389311a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x138931610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x138931a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x138931ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x138932360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1389327d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x138932c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1389330b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x138933520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x138933990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x138933e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x138934270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1389346e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x138934b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x138934fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x138935bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x138935eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x138936170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1389365e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x138936a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x138936ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x138937330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1389377a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x138937c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x138938080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1389384f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x138938960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x138938dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x138939240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1389396b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x138939b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x138939f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13893a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13893a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13893ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13893b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13893b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13893ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13893bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13893c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13893c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13893cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13893d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13893d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13893d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13893ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13893e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13893e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13893eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13893ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13893f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13893f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13893fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1389402c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x138940730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x138940ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x138941010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x138941530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x138941a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1389425b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x138942870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x138942e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1389433f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1389439b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x138943f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x138944530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x138944af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1389450b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x138945670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x138945c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1389461f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1389467b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x138946d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x138947330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1389478f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x138947eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x138948470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x138948a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x138948ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1389495b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x138949b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13894a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13894a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13894acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13894b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13894b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13894bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13894c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13894c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13894cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13894d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13894dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13894e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13894e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13894ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13894f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13894f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13894fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1389502f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1389508b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x138950e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x138951430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1389519f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x138951fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x138952570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x138952b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1389530f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1389536b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x138953c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x138954230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1389547f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x138954db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x138955370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x138955930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x138955ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1389564b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x138956a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x138956f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x138957470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x138957970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x138957e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x138958370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x138958870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x138958d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x138959270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x138959770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x138959c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13895a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13895a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13895ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13895b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13895b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13895bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13895c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13895cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13895d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13895d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13895df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13895e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13895e860 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13895b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13894c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13894b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x138948170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x138945930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x138955070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x138952830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1389505b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13894e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1389464b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x138943c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x138948cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x138949e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13894f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13894c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x138953f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x138947bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x138951130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13894a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13894cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1389475f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x138955630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1389447f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1389430f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x138945370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x138955bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13894af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1389533b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1389492b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13894baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13894fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x138947030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13894fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1389516f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x138945ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1389544f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x138951cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13894d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x138956770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x138944db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1389561b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x138944230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x138954ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13894e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x138950b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x138953970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x138952270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13894a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x138941d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x138904680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13895da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13890b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13895ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13895f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13895f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13895f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13895fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13895fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13895ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x138960280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x138960540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x138960800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x138960ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x138960d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x138961040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x138961300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1389615c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x138961880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x138961b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x138961e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1389620c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x138962380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x138962640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x138962900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x138962bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x138962e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x138963140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x138963400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1389636c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x138963980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x138963c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x138963f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1389641c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x138964480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x138964740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x138964a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x138964cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x138964f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x138965240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x138965500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1389657c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x138965a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x138965d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x138966000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1389662c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x138966580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x138966840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x138966b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x138966dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x138967080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x138967340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x138967600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1389678c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x138967b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x138967e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x138968100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1389683c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x138968680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x138968940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x138968c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x138968ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x138969180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x138969440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x138969700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1389699c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x138969c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x138969f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13896a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13896a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13896a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13896aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13896ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13896afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13896b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13896b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13896b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13896bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13896bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13896c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13896c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13896c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13896c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13896cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13896ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13896d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13896d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13896d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13896d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13896dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13896de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13896e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13896e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13896e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13896e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13896ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13896ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13896f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13896f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13896f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13896fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13896fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13896ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x138970240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x138970500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1389707c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x138970a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x138970d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x138971000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1389712c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x138971580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x138971840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x138971b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x138971dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x138972080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x138972340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x138972600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1389728c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x138972b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x138972e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x138973100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1389733c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x138973680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x138973940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x138973c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x138973ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x138974180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x138974440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x138974700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1389749c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x138974c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x138974f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x138975200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1389754c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x138975780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x138975a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x138975d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x138975fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x138976280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x138976540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x138976800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x138976ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x138976d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x138977040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x138977300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1389775c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x138977880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x138977b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x138977e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1389780c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x138978380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x138978640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x138978900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x138978bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x138978e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x138979140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x138979400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1389796c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x138979980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x138979c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x138979f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13897a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13897a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13897aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13897ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13897afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13897b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13897b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13897b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13897bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13897c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13897c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13897cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13897d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13897d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13897dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13897e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13897e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13897eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13897eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13897f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13897fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13897ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x138980530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x138980a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x138980fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x138981520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x138981a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x138981fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x138982510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x138982a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x138982fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x138983500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x138983a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x138983fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1389844f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x138984a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x138984f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1389854e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x138985a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x138985f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1389864d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x138986a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x138986f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1389874c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x138987a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x138987f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1389884b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x138988a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x138988f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1389894a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1389899f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x138989f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13898a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13898a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13898af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13898b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13898b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13898bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13898bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13898c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13898c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13898caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13898cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13898d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13898d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13898dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13898e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13898e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13898ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13898ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13898f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13898f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13898fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x138990030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x138990d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x138991440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x138991b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x138991e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x138992290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x138992890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x138992ea0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.962s
user	0m0.244s
sys	0m0.132s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
